2025-09-29 09:29:17,443 | INFO | Device: cuda
2025-09-29 09:29:21,381 | INFO | Loaded checkpoint: runs/20250926_094858/best_model (strict=True)
2025-09-29 09:30:11,400 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=6.86e-03 BER=2.70e-03 FER=2.82e-02
2025-09-29 09:30:11,842 | INFO | Epoch 1 Train Time 50.4583625793457s

2025-09-29 09:30:11,842 | INFO | [P1] saving best_model with loss 0.006864 at epoch 1
2025-09-29 09:31:06,192 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=6.71e-03 BER=2.65e-03 FER=2.80e-02
2025-09-29 09:31:06,673 | INFO | Epoch 2 Train Time 54.77829909324646s

2025-09-29 09:31:06,673 | INFO | [P1] saving best_model with loss 0.006713 at epoch 2
2025-09-29 09:32:03,259 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=6.89e-03 BER=2.69e-03 FER=2.79e-02
2025-09-29 09:32:03,734 | INFO | Epoch 3 Train Time 57.00501561164856s

2025-09-29 09:33:02,358 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=6.96e-03 BER=2.72e-03 FER=2.86e-02
2025-09-29 09:33:02,894 | INFO | Epoch 4 Train Time 59.159504652023315s

2025-09-29 09:34:02,340 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=6.94e-03 BER=2.71e-03 FER=2.78e-02
2025-09-29 09:34:02,818 | INFO | Epoch 5 Train Time 59.92252802848816s

2025-09-29 09:35:02,345 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=7.04e-03 BER=2.77e-03 FER=2.88e-02
2025-09-29 09:35:02,812 | INFO | Epoch 6 Train Time 59.99304962158203s

2025-09-29 09:36:02,391 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=6.92e-03 BER=2.70e-03 FER=2.81e-02
2025-09-29 09:36:02,855 | INFO | Epoch 7 Train Time 60.04264736175537s

2025-09-29 09:37:02,171 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=7.12e-03 BER=2.81e-03 FER=2.96e-02
2025-09-29 09:37:02,633 | INFO | Epoch 8 Train Time 59.77584266662598s

2025-09-29 09:38:02,607 | INFO | Training epoch 9, Batch 1000/1000: LR=9.99e-05, Loss=7.00e-03 BER=2.72e-03 FER=2.84e-02
2025-09-29 09:38:03,076 | INFO | Epoch 9 Train Time 60.44141173362732s

2025-09-29 09:39:02,592 | INFO | Training epoch 10, Batch 1000/1000: LR=9.99e-05, Loss=7.05e-03 BER=2.79e-03 FER=2.89e-02
2025-09-29 09:39:03,048 | INFO | Epoch 10 Train Time 59.97177743911743s

2025-09-29 09:40:02,669 | INFO | Training epoch 11, Batch 1000/1000: LR=9.99e-05, Loss=6.77e-03 BER=2.70e-03 FER=2.80e-02
2025-09-29 09:40:03,138 | INFO | Epoch 11 Train Time 60.08723306655884s

2025-09-29 09:41:02,849 | INFO | Training epoch 12, Batch 1000/1000: LR=9.99e-05, Loss=7.18e-03 BER=2.82e-03 FER=2.96e-02
2025-09-29 09:41:03,327 | INFO | Epoch 12 Train Time 60.18885684013367s

2025-09-29 09:42:04,349 | INFO | Training epoch 13, Batch 1000/1000: LR=9.99e-05, Loss=6.91e-03 BER=2.74e-03 FER=2.85e-02
2025-09-29 09:42:04,818 | INFO | Epoch 13 Train Time 61.4894540309906s

2025-09-29 09:43:06,224 | INFO | Training epoch 14, Batch 1000/1000: LR=9.98e-05, Loss=6.93e-03 BER=2.72e-03 FER=2.85e-02
2025-09-29 09:43:06,686 | INFO | Epoch 14 Train Time 61.86644244194031s

2025-09-29 09:44:07,605 | INFO | Training epoch 15, Batch 1000/1000: LR=9.98e-05, Loss=6.95e-03 BER=2.75e-03 FER=2.86e-02
2025-09-29 09:44:08,054 | INFO | Epoch 15 Train Time 61.36699438095093s

2025-09-29 09:45:08,933 | INFO | Training epoch 16, Batch 1000/1000: LR=9.98e-05, Loss=7.04e-03 BER=2.76e-03 FER=2.86e-02
2025-09-29 09:45:09,370 | INFO | Epoch 16 Train Time 61.31533098220825s

2025-09-29 09:46:10,467 | INFO | Training epoch 17, Batch 1000/1000: LR=9.98e-05, Loss=7.00e-03 BER=2.76e-03 FER=2.86e-02
2025-09-29 09:46:10,939 | INFO | Epoch 17 Train Time 61.56806421279907s

2025-09-29 09:47:11,630 | INFO | Training epoch 18, Batch 1000/1000: LR=9.97e-05, Loss=6.78e-03 BER=2.65e-03 FER=2.78e-02
2025-09-29 09:47:12,084 | INFO | Epoch 18 Train Time 61.14359474182129s

2025-09-29 09:48:13,191 | INFO | Training epoch 19, Batch 1000/1000: LR=9.97e-05, Loss=6.86e-03 BER=2.70e-03 FER=2.83e-02
2025-09-29 09:48:13,629 | INFO | Epoch 19 Train Time 61.5442898273468s

2025-09-29 09:49:13,463 | INFO | Training epoch 20, Batch 1000/1000: LR=9.96e-05, Loss=6.82e-03 BER=2.65e-03 FER=2.81e-02
2025-09-29 09:49:13,956 | INFO | Epoch 20 Train Time 60.32623028755188s

2025-09-29 09:50:14,943 | INFO | Training epoch 21, Batch 1000/1000: LR=9.96e-05, Loss=6.92e-03 BER=2.72e-03 FER=2.82e-02
2025-09-29 09:50:15,394 | INFO | Epoch 21 Train Time 61.43627095222473s

2025-09-29 09:51:16,128 | INFO | Training epoch 22, Batch 1000/1000: LR=9.96e-05, Loss=7.01e-03 BER=2.77e-03 FER=2.88e-02
2025-09-29 09:51:16,590 | INFO | Epoch 22 Train Time 61.194273471832275s

2025-09-29 09:52:16,407 | INFO | Training epoch 23, Batch 1000/1000: LR=9.95e-05, Loss=6.83e-03 BER=2.68e-03 FER=2.81e-02
2025-09-29 09:52:16,896 | INFO | Epoch 23 Train Time 60.305076360702515s

2025-09-29 09:53:17,259 | INFO | Training epoch 24, Batch 1000/1000: LR=9.95e-05, Loss=6.84e-03 BER=2.67e-03 FER=2.78e-02
2025-09-29 09:53:17,720 | INFO | Epoch 24 Train Time 60.82326889038086s

2025-09-29 09:54:18,856 | INFO | Training epoch 25, Batch 1000/1000: LR=9.94e-05, Loss=7.11e-03 BER=2.82e-03 FER=2.89e-02
2025-09-29 09:54:19,297 | INFO | Epoch 25 Train Time 61.57572364807129s

2025-09-29 09:55:20,972 | INFO | Training epoch 26, Batch 1000/1000: LR=9.94e-05, Loss=6.97e-03 BER=2.72e-03 FER=2.80e-02
2025-09-29 09:55:21,436 | INFO | Epoch 26 Train Time 62.13850998878479s

2025-09-29 09:56:21,942 | INFO | Training epoch 27, Batch 1000/1000: LR=9.93e-05, Loss=6.95e-03 BER=2.76e-03 FER=2.87e-02
2025-09-29 09:56:22,682 | INFO | Epoch 27 Train Time 61.244704723358154s

2025-09-29 09:57:21,796 | INFO | Training epoch 28, Batch 1000/1000: LR=9.93e-05, Loss=7.05e-03 BER=2.74e-03 FER=2.82e-02
2025-09-29 09:57:22,262 | INFO | Epoch 28 Train Time 59.5765655040741s

2025-09-29 09:58:23,931 | INFO | Training epoch 29, Batch 1000/1000: LR=9.92e-05, Loss=7.04e-03 BER=2.74e-03 FER=2.88e-02
2025-09-29 09:58:24,675 | INFO | Epoch 29 Train Time 62.41219687461853s

2025-09-29 09:59:25,435 | INFO | Training epoch 30, Batch 1000/1000: LR=9.92e-05, Loss=6.99e-03 BER=2.74e-03 FER=2.89e-02
2025-09-29 09:59:25,877 | INFO | Epoch 30 Train Time 61.200740814208984s

2025-09-29 10:00:26,010 | INFO | Training epoch 31, Batch 1000/1000: LR=9.91e-05, Loss=7.09e-03 BER=2.80e-03 FER=2.91e-02
2025-09-29 10:00:26,505 | INFO | Epoch 31 Train Time 60.626675844192505s

2025-09-29 10:01:26,915 | INFO | Training epoch 32, Batch 1000/1000: LR=9.91e-05, Loss=7.04e-03 BER=2.72e-03 FER=2.87e-02
2025-09-29 10:01:27,377 | INFO | Epoch 32 Train Time 60.87019968032837s

2025-09-29 10:02:28,261 | INFO | Training epoch 33, Batch 1000/1000: LR=9.90e-05, Loss=7.01e-03 BER=2.72e-03 FER=2.86e-02
2025-09-29 10:02:28,828 | INFO | Epoch 33 Train Time 61.45015358924866s

2025-09-29 10:03:29,446 | INFO | Training epoch 34, Batch 1000/1000: LR=9.89e-05, Loss=6.92e-03 BER=2.74e-03 FER=2.83e-02
2025-09-29 10:03:29,918 | INFO | Epoch 34 Train Time 61.085933685302734s

2025-09-29 10:04:30,506 | INFO | Training epoch 35, Batch 1000/1000: LR=9.89e-05, Loss=6.89e-03 BER=2.71e-03 FER=2.83e-02
2025-09-29 10:04:30,948 | INFO | Epoch 35 Train Time 61.02843475341797s

2025-09-29 10:05:30,947 | INFO | Training epoch 36, Batch 1000/1000: LR=9.88e-05, Loss=6.92e-03 BER=2.69e-03 FER=2.81e-02
2025-09-29 10:05:31,407 | INFO | Epoch 36 Train Time 60.458011865615845s

2025-09-29 10:06:31,416 | INFO | Training epoch 37, Batch 1000/1000: LR=9.87e-05, Loss=6.93e-03 BER=2.73e-03 FER=2.86e-02
2025-09-29 10:06:31,867 | INFO | Epoch 37 Train Time 60.45898628234863s

2025-09-29 10:07:33,104 | INFO | Training epoch 38, Batch 1000/1000: LR=9.87e-05, Loss=6.93e-03 BER=2.70e-03 FER=2.84e-02
2025-09-29 10:07:33,570 | INFO | Epoch 38 Train Time 61.70210313796997s

2025-09-29 10:08:34,783 | INFO | Training epoch 39, Batch 1000/1000: LR=9.86e-05, Loss=6.87e-03 BER=2.72e-03 FER=2.84e-02
2025-09-29 10:08:35,244 | INFO | Epoch 39 Train Time 61.67328929901123s

2025-09-29 10:09:47,382 | INFO | Training epoch 40, Batch 1000/1000: LR=9.85e-05, Loss=6.78e-03 BER=2.66e-03 FER=2.78e-02
2025-09-29 10:09:48,220 | INFO | Epoch 40 Train Time 72.9747793674469s

2025-09-29 10:10:56,105 | INFO | Training epoch 41, Batch 1000/1000: LR=9.84e-05, Loss=6.84e-03 BER=2.69e-03 FER=2.84e-02
2025-09-29 10:10:56,603 | INFO | Epoch 41 Train Time 68.3756411075592s

2025-09-29 10:12:03,408 | INFO | Training epoch 42, Batch 1000/1000: LR=9.84e-05, Loss=6.94e-03 BER=2.74e-03 FER=2.86e-02
2025-09-29 10:12:03,932 | INFO | Epoch 42 Train Time 67.32668137550354s

2025-09-29 10:13:09,098 | INFO | Training epoch 43, Batch 1000/1000: LR=9.83e-05, Loss=7.08e-03 BER=2.80e-03 FER=2.90e-02
2025-09-29 10:13:09,594 | INFO | Epoch 43 Train Time 65.66061282157898s

2025-09-29 10:14:13,672 | INFO | Training epoch 44, Batch 1000/1000: LR=9.82e-05, Loss=6.99e-03 BER=2.76e-03 FER=2.88e-02
2025-09-29 10:14:14,179 | INFO | Epoch 44 Train Time 64.58258628845215s

2025-09-29 10:15:18,395 | INFO | Training epoch 45, Batch 1000/1000: LR=9.81e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.87e-02
2025-09-29 10:15:18,865 | INFO | Epoch 45 Train Time 64.68568253517151s

2025-09-29 10:16:21,218 | INFO | Training epoch 46, Batch 1000/1000: LR=9.80e-05, Loss=6.93e-03 BER=2.69e-03 FER=2.83e-02
2025-09-29 10:16:21,705 | INFO | Epoch 46 Train Time 62.83815002441406s

2025-09-29 10:17:25,888 | INFO | Training epoch 47, Batch 1000/1000: LR=9.79e-05, Loss=7.12e-03 BER=2.82e-03 FER=2.92e-02
2025-09-29 10:17:26,374 | INFO | Epoch 47 Train Time 64.66873979568481s

2025-09-29 10:18:31,595 | INFO | Training epoch 48, Batch 1000/1000: LR=9.79e-05, Loss=6.97e-03 BER=2.75e-03 FER=2.86e-02
2025-09-29 10:18:32,071 | INFO | Epoch 48 Train Time 65.69508194923401s

2025-09-29 10:19:37,048 | INFO | Training epoch 49, Batch 1000/1000: LR=9.78e-05, Loss=7.03e-03 BER=2.76e-03 FER=2.89e-02
2025-09-29 10:19:37,538 | INFO | Epoch 49 Train Time 65.46461176872253s

2025-09-29 10:20:43,833 | INFO | Training epoch 50, Batch 1000/1000: LR=9.77e-05, Loss=6.89e-03 BER=2.72e-03 FER=2.84e-02
2025-09-29 10:20:44,320 | INFO | Epoch 50 Train Time 66.7798843383789s

2025-09-29 10:21:49,631 | INFO | Training epoch 51, Batch 1000/1000: LR=9.76e-05, Loss=6.75e-03 BER=2.65e-03 FER=2.82e-02
2025-09-29 10:21:50,382 | INFO | Epoch 51 Train Time 66.06047105789185s

2025-09-29 10:22:54,608 | INFO | Training epoch 52, Batch 1000/1000: LR=9.75e-05, Loss=6.91e-03 BER=2.70e-03 FER=2.82e-02
2025-09-29 10:22:55,090 | INFO | Epoch 52 Train Time 64.70537543296814s

2025-09-29 10:23:58,263 | INFO | Training epoch 53, Batch 1000/1000: LR=9.74e-05, Loss=6.98e-03 BER=2.73e-03 FER=2.84e-02
2025-09-29 10:23:58,771 | INFO | Epoch 53 Train Time 63.68022298812866s

2025-09-29 10:25:02,118 | INFO | Training epoch 54, Batch 1000/1000: LR=9.73e-05, Loss=6.93e-03 BER=2.75e-03 FER=2.84e-02
2025-09-29 10:25:02,608 | INFO | Epoch 54 Train Time 63.83501362800598s

2025-09-29 10:26:07,031 | INFO | Training epoch 55, Batch 1000/1000: LR=9.72e-05, Loss=7.18e-03 BER=2.82e-03 FER=2.93e-02
2025-09-29 10:26:07,525 | INFO | Epoch 55 Train Time 64.91653561592102s

2025-09-29 10:27:12,023 | INFO | Training epoch 56, Batch 1000/1000: LR=9.71e-05, Loss=6.97e-03 BER=2.75e-03 FER=2.85e-02
2025-09-29 10:27:12,514 | INFO | Epoch 56 Train Time 64.98713684082031s

2025-09-29 10:28:16,649 | INFO | Training epoch 57, Batch 1000/1000: LR=9.70e-05, Loss=7.11e-03 BER=2.79e-03 FER=2.92e-02
2025-09-29 10:28:17,106 | INFO | Epoch 57 Train Time 64.5884473323822s

2025-09-29 10:29:18,698 | INFO | Training epoch 58, Batch 1000/1000: LR=9.69e-05, Loss=6.93e-03 BER=2.75e-03 FER=2.88e-02
2025-09-29 10:29:19,145 | INFO | Epoch 58 Train Time 62.037758350372314s

2025-09-29 10:30:20,842 | INFO | Training epoch 59, Batch 1000/1000: LR=9.67e-05, Loss=7.03e-03 BER=2.77e-03 FER=2.91e-02
2025-09-29 10:30:21,305 | INFO | Epoch 59 Train Time 62.159387826919556s

2025-09-29 10:31:21,508 | INFO | Training epoch 60, Batch 1000/1000: LR=9.66e-05, Loss=7.05e-03 BER=2.75e-03 FER=2.84e-02
2025-09-29 10:31:21,960 | INFO | Epoch 60 Train Time 60.654134035110474s

2025-09-29 10:32:22,877 | INFO | Training epoch 61, Batch 1000/1000: LR=9.65e-05, Loss=6.96e-03 BER=2.73e-03 FER=2.87e-02
2025-09-29 10:32:23,333 | INFO | Epoch 61 Train Time 61.37228178977966s

2025-09-29 10:33:24,905 | INFO | Training epoch 62, Batch 1000/1000: LR=9.64e-05, Loss=7.09e-03 BER=2.80e-03 FER=2.88e-02
2025-09-29 10:33:25,344 | INFO | Epoch 62 Train Time 62.0096275806427s

2025-09-29 10:34:26,305 | INFO | Training epoch 63, Batch 1000/1000: LR=9.63e-05, Loss=6.83e-03 BER=2.68e-03 FER=2.83e-02
2025-09-29 10:34:26,772 | INFO | Epoch 63 Train Time 61.426764726638794s

2025-09-29 10:35:28,006 | INFO | Training epoch 64, Batch 1000/1000: LR=9.62e-05, Loss=7.13e-03 BER=2.79e-03 FER=2.90e-02
2025-09-29 10:35:28,448 | INFO | Epoch 64 Train Time 61.675461769104004s

2025-09-29 10:36:29,270 | INFO | Training epoch 65, Batch 1000/1000: LR=9.61e-05, Loss=6.99e-03 BER=2.76e-03 FER=2.88e-02
2025-09-29 10:36:29,733 | INFO | Epoch 65 Train Time 61.28400897979736s

2025-09-29 10:37:29,724 | INFO | Training epoch 66, Batch 1000/1000: LR=9.59e-05, Loss=6.77e-03 BER=2.66e-03 FER=2.80e-02
2025-09-29 10:37:30,185 | INFO | Epoch 66 Train Time 60.450379371643066s

2025-09-29 10:38:29,837 | INFO | Training epoch 67, Batch 1000/1000: LR=9.58e-05, Loss=6.88e-03 BER=2.71e-03 FER=2.83e-02
2025-09-29 10:38:30,303 | INFO | Epoch 67 Train Time 60.11657476425171s

2025-09-29 10:39:29,656 | INFO | Training epoch 68, Batch 1000/1000: LR=9.57e-05, Loss=6.94e-03 BER=2.72e-03 FER=2.83e-02
2025-09-29 10:39:30,098 | INFO | Epoch 68 Train Time 59.793463706970215s

2025-09-29 10:40:30,605 | INFO | Training epoch 69, Batch 1000/1000: LR=9.56e-05, Loss=6.85e-03 BER=2.71e-03 FER=2.81e-02
2025-09-29 10:40:31,064 | INFO | Epoch 69 Train Time 60.96521353721619s

2025-09-29 10:41:32,049 | INFO | Training epoch 70, Batch 1000/1000: LR=9.54e-05, Loss=6.79e-03 BER=2.67e-03 FER=2.74e-02
2025-09-29 10:41:32,509 | INFO | Epoch 70 Train Time 61.44378137588501s

2025-09-29 10:42:32,481 | INFO | Training epoch 71, Batch 1000/1000: LR=9.53e-05, Loss=6.80e-03 BER=2.69e-03 FER=2.80e-02
2025-09-29 10:42:32,955 | INFO | Epoch 71 Train Time 60.44470167160034s

2025-09-29 10:43:32,386 | INFO | Training epoch 72, Batch 1000/1000: LR=9.52e-05, Loss=6.95e-03 BER=2.72e-03 FER=2.84e-02
2025-09-29 10:43:32,835 | INFO | Epoch 72 Train Time 59.87899303436279s

2025-09-29 10:44:34,025 | INFO | Training epoch 73, Batch 1000/1000: LR=9.50e-05, Loss=7.01e-03 BER=2.72e-03 FER=2.89e-02
2025-09-29 10:44:34,473 | INFO | Epoch 73 Train Time 61.63667106628418s

2025-09-29 10:45:34,791 | INFO | Training epoch 74, Batch 1000/1000: LR=9.49e-05, Loss=6.74e-03 BER=2.65e-03 FER=2.78e-02
2025-09-29 10:45:35,277 | INFO | Epoch 74 Train Time 60.80283999443054s

2025-09-29 10:46:36,080 | INFO | Training epoch 75, Batch 1000/1000: LR=9.47e-05, Loss=6.79e-03 BER=2.68e-03 FER=2.79e-02
2025-09-29 10:46:36,529 | INFO | Epoch 75 Train Time 61.25036668777466s

2025-09-29 10:47:37,912 | INFO | Training epoch 76, Batch 1000/1000: LR=9.46e-05, Loss=6.78e-03 BER=2.67e-03 FER=2.78e-02
2025-09-29 10:47:38,355 | INFO | Epoch 76 Train Time 61.824055910110474s

2025-09-29 10:48:40,142 | INFO | Training epoch 77, Batch 1000/1000: LR=9.45e-05, Loss=6.75e-03 BER=2.65e-03 FER=2.77e-02
2025-09-29 10:48:40,599 | INFO | Epoch 77 Train Time 62.24298405647278s

2025-09-29 10:49:41,108 | INFO | Training epoch 78, Batch 1000/1000: LR=9.43e-05, Loss=6.85e-03 BER=2.69e-03 FER=2.81e-02
2025-09-29 10:49:41,565 | INFO | Epoch 78 Train Time 60.96410775184631s

2025-09-29 10:50:42,174 | INFO | Training epoch 79, Batch 1000/1000: LR=9.42e-05, Loss=6.73e-03 BER=2.64e-03 FER=2.78e-02
2025-09-29 10:50:42,626 | INFO | Epoch 79 Train Time 61.06043481826782s

2025-09-29 10:51:42,978 | INFO | Training epoch 80, Batch 1000/1000: LR=9.40e-05, Loss=6.99e-03 BER=2.73e-03 FER=2.85e-02
2025-09-29 10:51:43,428 | INFO | Epoch 80 Train Time 60.800564765930176s

2025-09-29 10:52:43,421 | INFO | Training epoch 81, Batch 1000/1000: LR=9.39e-05, Loss=6.84e-03 BER=2.68e-03 FER=2.80e-02
2025-09-29 10:52:44,166 | INFO | Epoch 81 Train Time 60.73678660392761s

2025-09-29 10:53:44,319 | INFO | Training epoch 82, Batch 1000/1000: LR=9.37e-05, Loss=6.97e-03 BER=2.76e-03 FER=2.85e-02
2025-09-29 10:53:44,797 | INFO | Epoch 82 Train Time 60.6290442943573s

2025-09-29 10:54:45,523 | INFO | Training epoch 83, Batch 1000/1000: LR=9.36e-05, Loss=6.81e-03 BER=2.68e-03 FER=2.81e-02
2025-09-29 10:54:45,979 | INFO | Epoch 83 Train Time 61.18025302886963s

2025-09-29 10:55:46,783 | INFO | Training epoch 84, Batch 1000/1000: LR=9.34e-05, Loss=6.88e-03 BER=2.73e-03 FER=2.81e-02
2025-09-29 10:55:47,230 | INFO | Epoch 84 Train Time 61.250494718551636s

2025-09-29 10:56:47,911 | INFO | Training epoch 85, Batch 1000/1000: LR=9.33e-05, Loss=6.92e-03 BER=2.71e-03 FER=2.84e-02
2025-09-29 10:56:48,383 | INFO | Epoch 85 Train Time 61.15222692489624s

2025-09-29 10:57:49,476 | INFO | Training epoch 86, Batch 1000/1000: LR=9.31e-05, Loss=6.67e-03 BER=2.60e-03 FER=2.72e-02
2025-09-29 10:57:49,912 | INFO | Epoch 86 Train Time 61.52689981460571s

2025-09-29 10:57:49,913 | INFO | [P1] saving best_model with loss 0.006671 at epoch 86
2025-09-29 10:58:51,278 | INFO | Training epoch 87, Batch 1000/1000: LR=9.29e-05, Loss=7.16e-03 BER=2.80e-03 FER=2.92e-02
2025-09-29 10:58:51,751 | INFO | Epoch 87 Train Time 61.78006625175476s

2025-09-29 10:59:53,148 | INFO | Training epoch 88, Batch 1000/1000: LR=9.28e-05, Loss=6.92e-03 BER=2.72e-03 FER=2.81e-02
2025-09-29 10:59:53,650 | INFO | Epoch 88 Train Time 61.89720392227173s

2025-09-29 11:00:53,316 | INFO | Training epoch 89, Batch 1000/1000: LR=9.26e-05, Loss=6.93e-03 BER=2.73e-03 FER=2.85e-02
2025-09-29 11:00:54,027 | INFO | Epoch 89 Train Time 60.37523794174194s

2025-09-29 11:01:54,098 | INFO | Training epoch 90, Batch 1000/1000: LR=9.25e-05, Loss=6.83e-03 BER=2.70e-03 FER=2.80e-02
2025-09-29 11:01:54,557 | INFO | Epoch 90 Train Time 60.52932167053223s

2025-09-29 11:02:55,986 | INFO | Training epoch 91, Batch 1000/1000: LR=9.23e-05, Loss=7.06e-03 BER=2.77e-03 FER=2.89e-02
2025-09-29 11:02:56,435 | INFO | Epoch 91 Train Time 61.87610483169556s

2025-09-29 11:03:57,383 | INFO | Training epoch 92, Batch 1000/1000: LR=9.21e-05, Loss=6.79e-03 BER=2.65e-03 FER=2.74e-02
2025-09-29 11:03:57,979 | INFO | Epoch 92 Train Time 61.54274392127991s

2025-09-29 11:04:57,580 | INFO | Training epoch 93, Batch 1000/1000: LR=9.20e-05, Loss=6.86e-03 BER=2.70e-03 FER=2.80e-02
2025-09-29 11:04:58,071 | INFO | Epoch 93 Train Time 60.09139060974121s

2025-09-29 11:05:58,260 | INFO | Training epoch 94, Batch 1000/1000: LR=9.18e-05, Loss=6.83e-03 BER=2.69e-03 FER=2.79e-02
2025-09-29 11:05:58,712 | INFO | Epoch 94 Train Time 60.640472173690796s

2025-09-29 11:06:58,534 | INFO | Training epoch 95, Batch 1000/1000: LR=9.16e-05, Loss=7.26e-03 BER=2.85e-03 FER=2.88e-02
2025-09-29 11:06:59,062 | INFO | Epoch 95 Train Time 60.348421812057495s

2025-09-29 11:07:58,924 | INFO | Training epoch 96, Batch 1000/1000: LR=9.14e-05, Loss=7.05e-03 BER=2.78e-03 FER=2.89e-02
2025-09-29 11:07:59,503 | INFO | Epoch 96 Train Time 60.44042491912842s

2025-09-29 11:08:59,641 | INFO | Training epoch 97, Batch 1000/1000: LR=9.13e-05, Loss=6.81e-03 BER=2.66e-03 FER=2.76e-02
2025-09-29 11:09:00,148 | INFO | Epoch 97 Train Time 60.6438353061676s

2025-09-29 11:09:59,897 | INFO | Training epoch 98, Batch 1000/1000: LR=9.11e-05, Loss=6.84e-03 BER=2.71e-03 FER=2.80e-02
2025-09-29 11:10:00,346 | INFO | Epoch 98 Train Time 60.19675016403198s

2025-09-29 11:11:00,881 | INFO | Training epoch 99, Batch 1000/1000: LR=9.09e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.84e-02
2025-09-29 11:11:01,736 | INFO | Epoch 99 Train Time 61.387044191360474s

2025-09-29 11:12:03,532 | INFO | Training epoch 100, Batch 1000/1000: LR=9.07e-05, Loss=7.19e-03 BER=2.83e-03 FER=2.88e-02
2025-09-29 11:12:03,991 | INFO | Epoch 100 Train Time 62.251036405563354s

2025-09-29 11:13:06,066 | INFO | Training epoch 101, Batch 1000/1000: LR=9.05e-05, Loss=6.96e-03 BER=2.78e-03 FER=2.84e-02
2025-09-29 11:13:06,523 | INFO | Epoch 101 Train Time 62.53121876716614s

2025-09-29 11:14:07,080 | INFO | Training epoch 102, Batch 1000/1000: LR=9.04e-05, Loss=6.65e-03 BER=2.62e-03 FER=2.76e-02
2025-09-29 11:14:07,535 | INFO | Epoch 102 Train Time 61.010584592819214s

2025-09-29 11:14:07,536 | INFO | [P1] saving best_model with loss 0.006654 at epoch 102
2025-09-29 11:15:07,643 | INFO | Training epoch 103, Batch 1000/1000: LR=9.02e-05, Loss=7.01e-03 BER=2.75e-03 FER=2.83e-02
2025-09-29 11:15:08,203 | INFO | Epoch 103 Train Time 60.61681580543518s

2025-09-29 11:16:08,101 | INFO | Training epoch 104, Batch 1000/1000: LR=9.00e-05, Loss=6.90e-03 BER=2.72e-03 FER=2.86e-02
2025-09-29 11:16:08,569 | INFO | Epoch 104 Train Time 60.36472845077515s

2025-09-29 11:17:09,681 | INFO | Training epoch 105, Batch 1000/1000: LR=8.98e-05, Loss=6.84e-03 BER=2.69e-03 FER=2.84e-02
2025-09-29 11:17:10,123 | INFO | Epoch 105 Train Time 61.55257749557495s

2025-09-29 11:18:11,299 | INFO | Training epoch 106, Batch 1000/1000: LR=8.96e-05, Loss=6.77e-03 BER=2.65e-03 FER=2.79e-02
2025-09-29 11:18:11,767 | INFO | Epoch 106 Train Time 61.64334583282471s

2025-09-29 11:19:11,973 | INFO | Training epoch 107, Batch 1000/1000: LR=8.94e-05, Loss=6.58e-03 BER=2.61e-03 FER=2.72e-02
2025-09-29 11:19:12,428 | INFO | Epoch 107 Train Time 60.659955978393555s

2025-09-29 11:19:12,429 | INFO | [P1] saving best_model with loss 0.006576 at epoch 107
2025-09-29 11:20:14,351 | INFO | Training epoch 108, Batch 1000/1000: LR=8.92e-05, Loss=6.98e-03 BER=2.77e-03 FER=2.87e-02
2025-09-29 11:20:14,790 | INFO | Epoch 108 Train Time 62.31388211250305s

2025-09-29 11:21:17,431 | INFO | Training epoch 109, Batch 1000/1000: LR=8.90e-05, Loss=6.89e-03 BER=2.73e-03 FER=2.84e-02
2025-09-29 11:21:17,878 | INFO | Epoch 109 Train Time 63.08654975891113s

2025-09-29 11:22:19,502 | INFO | Training epoch 110, Batch 1000/1000: LR=8.88e-05, Loss=6.98e-03 BER=2.78e-03 FER=2.86e-02
2025-09-29 11:22:19,959 | INFO | Epoch 110 Train Time 62.080384731292725s

2025-09-29 11:23:20,868 | INFO | Training epoch 111, Batch 1000/1000: LR=8.86e-05, Loss=6.69e-03 BER=2.66e-03 FER=2.78e-02
2025-09-29 11:23:21,452 | INFO | Epoch 111 Train Time 61.49145555496216s

2025-09-29 11:24:21,351 | INFO | Training epoch 112, Batch 1000/1000: LR=8.84e-05, Loss=6.92e-03 BER=2.71e-03 FER=2.80e-02
2025-09-29 11:24:21,810 | INFO | Epoch 112 Train Time 60.35742998123169s

2025-09-29 11:25:21,609 | INFO | Training epoch 113, Batch 1000/1000: LR=8.82e-05, Loss=6.95e-03 BER=2.71e-03 FER=2.79e-02
2025-09-29 11:25:22,068 | INFO | Epoch 113 Train Time 60.25709915161133s

2025-09-29 11:26:22,606 | INFO | Training epoch 114, Batch 1000/1000: LR=8.80e-05, Loss=6.93e-03 BER=2.73e-03 FER=2.87e-02
2025-09-29 11:26:23,068 | INFO | Epoch 114 Train Time 60.99894952774048s

2025-09-29 11:27:23,444 | INFO | Training epoch 115, Batch 1000/1000: LR=8.78e-05, Loss=6.74e-03 BER=2.69e-03 FER=2.82e-02
2025-09-29 11:27:23,899 | INFO | Epoch 115 Train Time 60.830034017562866s

2025-09-29 11:28:23,598 | INFO | Training epoch 116, Batch 1000/1000: LR=8.76e-05, Loss=6.72e-03 BER=2.65e-03 FER=2.78e-02
2025-09-29 11:28:24,083 | INFO | Epoch 116 Train Time 60.182310581207275s

2025-09-29 11:29:23,614 | INFO | Training epoch 117, Batch 1000/1000: LR=8.74e-05, Loss=6.87e-03 BER=2.73e-03 FER=2.82e-02
2025-09-29 11:29:24,054 | INFO | Epoch 117 Train Time 59.97020697593689s

2025-09-29 11:30:23,694 | INFO | Training epoch 118, Batch 1000/1000: LR=8.72e-05, Loss=6.97e-03 BER=2.76e-03 FER=2.87e-02
2025-09-29 11:30:24,180 | INFO | Epoch 118 Train Time 60.12360072135925s

2025-09-29 11:31:23,667 | INFO | Training epoch 119, Batch 1000/1000: LR=8.70e-05, Loss=6.86e-03 BER=2.70e-03 FER=2.78e-02
2025-09-29 11:31:24,111 | INFO | Epoch 119 Train Time 59.930684089660645s

2025-09-29 11:32:23,978 | INFO | Training epoch 120, Batch 1000/1000: LR=8.68e-05, Loss=7.02e-03 BER=2.78e-03 FER=2.87e-02
2025-09-29 11:32:24,432 | INFO | Epoch 120 Train Time 60.320006370544434s

2025-09-29 11:33:25,894 | INFO | Training epoch 121, Batch 1000/1000: LR=8.66e-05, Loss=6.68e-03 BER=2.66e-03 FER=2.78e-02
2025-09-29 11:33:26,580 | INFO | Epoch 121 Train Time 62.14625835418701s

2025-09-29 11:34:27,282 | INFO | Training epoch 122, Batch 1000/1000: LR=8.64e-05, Loss=6.86e-03 BER=2.71e-03 FER=2.83e-02
2025-09-29 11:34:27,724 | INFO | Epoch 122 Train Time 61.14298629760742s

2025-09-29 11:35:29,176 | INFO | Training epoch 123, Batch 1000/1000: LR=8.62e-05, Loss=6.72e-03 BER=2.66e-03 FER=2.77e-02
2025-09-29 11:35:29,645 | INFO | Epoch 123 Train Time 61.920815229415894s

2025-09-29 11:36:29,964 | INFO | Training epoch 124, Batch 1000/1000: LR=8.59e-05, Loss=6.92e-03 BER=2.74e-03 FER=2.83e-02
2025-09-29 11:36:30,437 | INFO | Epoch 124 Train Time 60.790969371795654s

2025-09-29 11:37:31,294 | INFO | Training epoch 125, Batch 1000/1000: LR=8.57e-05, Loss=6.79e-03 BER=2.68e-03 FER=2.81e-02
2025-09-29 11:37:31,739 | INFO | Epoch 125 Train Time 61.30072093009949s

2025-09-29 11:38:31,557 | INFO | Training epoch 126, Batch 1000/1000: LR=8.55e-05, Loss=6.69e-03 BER=2.62e-03 FER=2.75e-02
2025-09-29 11:38:32,024 | INFO | Epoch 126 Train Time 60.28430223464966s

2025-09-29 11:39:31,629 | INFO | Training epoch 127, Batch 1000/1000: LR=8.53e-05, Loss=6.88e-03 BER=2.71e-03 FER=2.81e-02
2025-09-29 11:39:32,074 | INFO | Epoch 127 Train Time 60.04834723472595s

2025-09-29 11:40:33,175 | INFO | Training epoch 128, Batch 1000/1000: LR=8.51e-05, Loss=7.00e-03 BER=2.72e-03 FER=2.84e-02
2025-09-29 11:40:33,647 | INFO | Epoch 128 Train Time 61.56995701789856s

2025-09-29 11:41:35,001 | INFO | Training epoch 129, Batch 1000/1000: LR=8.48e-05, Loss=6.78e-03 BER=2.68e-03 FER=2.79e-02
2025-09-29 11:41:35,460 | INFO | Epoch 129 Train Time 61.812010765075684s

2025-09-29 11:42:35,432 | INFO | Training epoch 130, Batch 1000/1000: LR=8.46e-05, Loss=6.79e-03 BER=2.64e-03 FER=2.74e-02
2025-09-29 11:42:35,901 | INFO | Epoch 130 Train Time 60.439255714416504s

2025-09-29 11:43:35,969 | INFO | Training epoch 131, Batch 1000/1000: LR=8.44e-05, Loss=6.82e-03 BER=2.66e-03 FER=2.78e-02
2025-09-29 11:43:36,412 | INFO | Epoch 131 Train Time 60.510188817977905s

2025-09-29 11:44:36,810 | INFO | Training epoch 132, Batch 1000/1000: LR=8.42e-05, Loss=6.83e-03 BER=2.66e-03 FER=2.76e-02
2025-09-29 11:44:37,441 | INFO | Epoch 132 Train Time 61.0280396938324s

2025-09-29 11:45:38,296 | INFO | Training epoch 133, Batch 1000/1000: LR=8.39e-05, Loss=7.00e-03 BER=2.77e-03 FER=2.86e-02
2025-09-29 11:45:38,741 | INFO | Epoch 133 Train Time 61.29877710342407s

2025-09-29 11:46:38,678 | INFO | Training epoch 134, Batch 1000/1000: LR=8.37e-05, Loss=6.77e-03 BER=2.67e-03 FER=2.71e-02
2025-09-29 11:46:39,125 | INFO | Epoch 134 Train Time 60.38260555267334s

2025-09-29 11:47:41,261 | INFO | Training epoch 135, Batch 1000/1000: LR=8.35e-05, Loss=6.57e-03 BER=2.60e-03 FER=2.72e-02
2025-09-29 11:47:41,777 | INFO | Epoch 135 Train Time 62.65160274505615s

2025-09-29 11:47:41,778 | INFO | [P1] saving best_model with loss 0.006570 at epoch 135
2025-09-29 11:48:42,113 | INFO | Training epoch 136, Batch 1000/1000: LR=8.32e-05, Loss=6.67e-03 BER=2.62e-03 FER=2.74e-02
2025-09-29 11:48:42,592 | INFO | Epoch 136 Train Time 60.762441873550415s

2025-09-29 11:49:42,751 | INFO | Training epoch 137, Batch 1000/1000: LR=8.30e-05, Loss=6.85e-03 BER=2.67e-03 FER=2.77e-02
2025-09-29 11:49:43,212 | INFO | Epoch 137 Train Time 60.61845922470093s

2025-09-29 11:50:43,843 | INFO | Training epoch 138, Batch 1000/1000: LR=8.28e-05, Loss=6.87e-03 BER=2.69e-03 FER=2.76e-02
2025-09-29 11:50:44,295 | INFO | Epoch 138 Train Time 61.08202934265137s

2025-09-29 11:51:44,529 | INFO | Training epoch 139, Batch 1000/1000: LR=8.25e-05, Loss=6.79e-03 BER=2.67e-03 FER=2.75e-02
2025-09-29 11:51:44,971 | INFO | Epoch 139 Train Time 60.67583346366882s

2025-09-29 11:52:44,708 | INFO | Training epoch 140, Batch 1000/1000: LR=8.23e-05, Loss=6.83e-03 BER=2.69e-03 FER=2.79e-02
2025-09-29 11:52:45,166 | INFO | Epoch 140 Train Time 60.19313454627991s

2025-09-29 11:53:45,837 | INFO | Training epoch 141, Batch 1000/1000: LR=8.21e-05, Loss=6.74e-03 BER=2.64e-03 FER=2.78e-02
2025-09-29 11:53:46,298 | INFO | Epoch 141 Train Time 61.131622314453125s

2025-09-29 11:54:45,656 | INFO | Training epoch 142, Batch 1000/1000: LR=8.18e-05, Loss=6.57e-03 BER=2.59e-03 FER=2.69e-02
2025-09-29 11:54:46,104 | INFO | Epoch 142 Train Time 59.80444073677063s

2025-09-29 11:54:46,104 | INFO | [P1] saving best_model with loss 0.006566 at epoch 142
2025-09-29 11:55:46,261 | INFO | Training epoch 143, Batch 1000/1000: LR=8.16e-05, Loss=6.83e-03 BER=2.69e-03 FER=2.82e-02
2025-09-29 11:55:46,709 | INFO | Epoch 143 Train Time 60.5308153629303s

2025-09-29 11:56:46,457 | INFO | Training epoch 144, Batch 1000/1000: LR=8.13e-05, Loss=6.68e-03 BER=2.65e-03 FER=2.74e-02
2025-09-29 11:56:46,919 | INFO | Epoch 144 Train Time 60.20961284637451s

2025-09-29 11:57:47,626 | INFO | Training epoch 145, Batch 1000/1000: LR=8.11e-05, Loss=6.79e-03 BER=2.65e-03 FER=2.77e-02
2025-09-29 11:57:48,080 | INFO | Epoch 145 Train Time 61.16005754470825s

2025-09-29 11:58:48,981 | INFO | Training epoch 146, Batch 1000/1000: LR=8.08e-05, Loss=6.73e-03 BER=2.67e-03 FER=2.81e-02
2025-09-29 11:58:49,443 | INFO | Epoch 146 Train Time 61.361711502075195s

2025-09-29 11:59:51,001 | INFO | Training epoch 147, Batch 1000/1000: LR=8.06e-05, Loss=6.83e-03 BER=2.69e-03 FER=2.79e-02
2025-09-29 11:59:51,443 | INFO | Epoch 147 Train Time 61.99922037124634s

2025-09-29 12:00:51,024 | INFO | Training epoch 148, Batch 1000/1000: LR=8.03e-05, Loss=6.78e-03 BER=2.69e-03 FER=2.77e-02
2025-09-29 12:00:51,485 | INFO | Epoch 148 Train Time 60.04113149642944s

2025-09-29 12:01:50,585 | INFO | Training epoch 149, Batch 1000/1000: LR=8.01e-05, Loss=6.80e-03 BER=2.68e-03 FER=2.76e-02
2025-09-29 12:01:51,362 | INFO | Epoch 149 Train Time 59.87552046775818s

2025-09-29 12:02:54,411 | INFO | Training epoch 150, Batch 1000/1000: LR=7.98e-05, Loss=6.86e-03 BER=2.70e-03 FER=2.77e-02
2025-09-29 12:02:54,881 | INFO | Epoch 150 Train Time 63.51645588874817s

2025-09-29 12:03:55,185 | INFO | Training epoch 151, Batch 1000/1000: LR=7.96e-05, Loss=6.91e-03 BER=2.73e-03 FER=2.85e-02
2025-09-29 12:03:55,634 | INFO | Epoch 151 Train Time 60.75257325172424s

2025-09-29 12:04:57,098 | INFO | Training epoch 152, Batch 1000/1000: LR=7.93e-05, Loss=6.75e-03 BER=2.65e-03 FER=2.74e-02
2025-09-29 12:04:57,538 | INFO | Epoch 152 Train Time 61.902950048446655s

2025-09-29 12:05:57,911 | INFO | Training epoch 153, Batch 1000/1000: LR=7.91e-05, Loss=6.58e-03 BER=2.60e-03 FER=2.71e-02
2025-09-29 12:05:58,357 | INFO | Epoch 153 Train Time 60.81768202781677s

2025-09-29 12:06:57,769 | INFO | Training epoch 154, Batch 1000/1000: LR=7.88e-05, Loss=6.98e-03 BER=2.76e-03 FER=2.85e-02
2025-09-29 12:06:58,214 | INFO | Epoch 154 Train Time 59.85574650764465s

2025-09-29 12:07:58,090 | INFO | Training epoch 155, Batch 1000/1000: LR=7.86e-05, Loss=6.87e-03 BER=2.72e-03 FER=2.79e-02
2025-09-29 12:07:58,560 | INFO | Epoch 155 Train Time 60.34513020515442s

2025-09-29 12:09:00,907 | INFO | Training epoch 156, Batch 1000/1000: LR=7.83e-05, Loss=6.65e-03 BER=2.61e-03 FER=2.73e-02
2025-09-29 12:09:01,386 | INFO | Epoch 156 Train Time 62.82481575012207s

2025-09-29 12:10:01,997 | INFO | Training epoch 157, Batch 1000/1000: LR=7.81e-05, Loss=6.65e-03 BER=2.63e-03 FER=2.74e-02
2025-09-29 12:10:02,461 | INFO | Epoch 157 Train Time 61.07407355308533s

2025-09-29 12:11:02,949 | INFO | Training epoch 158, Batch 1000/1000: LR=7.78e-05, Loss=6.89e-03 BER=2.72e-03 FER=2.79e-02
2025-09-29 12:11:03,402 | INFO | Epoch 158 Train Time 60.93976187705994s

2025-09-29 12:12:06,055 | INFO | Training epoch 159, Batch 1000/1000: LR=7.75e-05, Loss=6.61e-03 BER=2.58e-03 FER=2.73e-02
2025-09-29 12:12:06,520 | INFO | Epoch 159 Train Time 63.11751365661621s

2025-09-29 12:13:07,418 | INFO | Training epoch 160, Batch 1000/1000: LR=7.73e-05, Loss=6.86e-03 BER=2.71e-03 FER=2.78e-02
2025-09-29 12:13:07,897 | INFO | Epoch 160 Train Time 61.375061988830566s

2025-09-29 12:14:07,264 | INFO | Training epoch 161, Batch 1000/1000: LR=7.70e-05, Loss=6.69e-03 BER=2.63e-03 FER=2.69e-02
2025-09-29 12:14:07,706 | INFO | Epoch 161 Train Time 59.80727291107178s

2025-09-29 12:15:09,920 | INFO | Training epoch 162, Batch 1000/1000: LR=7.68e-05, Loss=6.75e-03 BER=2.66e-03 FER=2.73e-02
2025-09-29 12:15:10,365 | INFO | Epoch 162 Train Time 62.65799355506897s

2025-09-29 12:16:11,149 | INFO | Training epoch 163, Batch 1000/1000: LR=7.65e-05, Loss=6.78e-03 BER=2.67e-03 FER=2.78e-02
2025-09-29 12:16:11,602 | INFO | Epoch 163 Train Time 61.23665237426758s

2025-09-29 12:17:12,069 | INFO | Training epoch 164, Batch 1000/1000: LR=7.62e-05, Loss=6.93e-03 BER=2.76e-03 FER=2.82e-02
2025-09-29 12:17:12,519 | INFO | Epoch 164 Train Time 60.91438388824463s

2025-09-29 12:18:13,452 | INFO | Training epoch 165, Batch 1000/1000: LR=7.60e-05, Loss=6.64e-03 BER=2.63e-03 FER=2.74e-02
2025-09-29 12:18:13,954 | INFO | Epoch 165 Train Time 61.43443584442139s

2025-09-29 12:19:16,135 | INFO | Training epoch 166, Batch 1000/1000: LR=7.57e-05, Loss=6.93e-03 BER=2.74e-03 FER=2.82e-02
2025-09-29 12:19:16,585 | INFO | Epoch 166 Train Time 62.62941884994507s

2025-09-29 12:20:19,232 | INFO | Training epoch 167, Batch 1000/1000: LR=7.54e-05, Loss=6.88e-03 BER=2.71e-03 FER=2.82e-02
2025-09-29 12:20:19,697 | INFO | Epoch 167 Train Time 63.11117935180664s

2025-09-29 12:21:20,230 | INFO | Training epoch 168, Batch 1000/1000: LR=7.52e-05, Loss=6.65e-03 BER=2.61e-03 FER=2.72e-02
2025-09-29 12:21:20,732 | INFO | Epoch 168 Train Time 61.03458642959595s

2025-09-29 12:22:21,387 | INFO | Training epoch 169, Batch 1000/1000: LR=7.49e-05, Loss=6.86e-03 BER=2.74e-03 FER=2.83e-02
2025-09-29 12:22:21,845 | INFO | Epoch 169 Train Time 61.11161136627197s

2025-09-29 12:23:22,704 | INFO | Training epoch 170, Batch 1000/1000: LR=7.46e-05, Loss=6.73e-03 BER=2.68e-03 FER=2.77e-02
2025-09-29 12:23:23,151 | INFO | Epoch 170 Train Time 61.304845571517944s

2025-09-29 12:24:23,278 | INFO | Training epoch 171, Batch 1000/1000: LR=7.43e-05, Loss=6.78e-03 BER=2.63e-03 FER=2.75e-02
2025-09-29 12:24:23,738 | INFO | Epoch 171 Train Time 60.58624529838562s

2025-09-29 12:25:24,570 | INFO | Training epoch 172, Batch 1000/1000: LR=7.41e-05, Loss=6.62e-03 BER=2.62e-03 FER=2.69e-02
2025-09-29 12:25:25,216 | INFO | Epoch 172 Train Time 61.47679400444031s

2025-09-29 12:26:26,688 | INFO | Training epoch 173, Batch 1000/1000: LR=7.38e-05, Loss=6.87e-03 BER=2.71e-03 FER=2.80e-02
2025-09-29 12:26:27,143 | INFO | Epoch 173 Train Time 61.92601656913757s

2025-09-29 12:27:27,686 | INFO | Training epoch 174, Batch 1000/1000: LR=7.35e-05, Loss=6.61e-03 BER=2.66e-03 FER=2.73e-02
2025-09-29 12:27:28,154 | INFO | Epoch 174 Train Time 61.010273933410645s

2025-09-29 12:28:30,101 | INFO | Training epoch 175, Batch 1000/1000: LR=7.32e-05, Loss=6.96e-03 BER=2.72e-03 FER=2.78e-02
2025-09-29 12:28:30,545 | INFO | Epoch 175 Train Time 62.38839840888977s

2025-09-29 12:29:31,090 | INFO | Training epoch 176, Batch 1000/1000: LR=7.30e-05, Loss=6.73e-03 BER=2.66e-03 FER=2.72e-02
2025-09-29 12:29:31,552 | INFO | Epoch 176 Train Time 61.00511169433594s

2025-09-29 12:30:32,069 | INFO | Training epoch 177, Batch 1000/1000: LR=7.27e-05, Loss=6.70e-03 BER=2.66e-03 FER=2.75e-02
2025-09-29 12:30:32,540 | INFO | Epoch 177 Train Time 60.98589754104614s

2025-09-29 12:31:33,619 | INFO | Training epoch 178, Batch 1000/1000: LR=7.24e-05, Loss=6.70e-03 BER=2.65e-03 FER=2.75e-02
2025-09-29 12:31:34,338 | INFO | Epoch 178 Train Time 61.79628539085388s

2025-09-29 12:32:35,207 | INFO | Training epoch 179, Batch 1000/1000: LR=7.21e-05, Loss=6.76e-03 BER=2.66e-03 FER=2.74e-02
2025-09-29 12:32:35,670 | INFO | Epoch 179 Train Time 61.330519914627075s

2025-09-29 12:33:35,013 | INFO | Training epoch 180, Batch 1000/1000: LR=7.19e-05, Loss=6.78e-03 BER=2.66e-03 FER=2.75e-02
2025-09-29 12:33:35,647 | INFO | Epoch 180 Train Time 59.975629806518555s

2025-09-29 12:34:36,341 | INFO | Training epoch 181, Batch 1000/1000: LR=7.16e-05, Loss=6.53e-03 BER=2.57e-03 FER=2.69e-02
2025-09-29 12:34:36,792 | INFO | Epoch 181 Train Time 61.14406180381775s

2025-09-29 12:34:36,792 | INFO | [P1] saving best_model with loss 0.006534 at epoch 181
2025-09-29 12:35:36,707 | INFO | Training epoch 182, Batch 1000/1000: LR=7.13e-05, Loss=6.59e-03 BER=2.60e-03 FER=2.69e-02
2025-09-29 12:35:37,171 | INFO | Epoch 182 Train Time 60.32612466812134s

2025-09-29 12:36:38,429 | INFO | Training epoch 183, Batch 1000/1000: LR=7.10e-05, Loss=6.64e-03 BER=2.62e-03 FER=2.69e-02
2025-09-29 12:36:39,197 | INFO | Epoch 183 Train Time 62.025697469711304s

2025-09-29 12:37:39,994 | INFO | Training epoch 184, Batch 1000/1000: LR=7.07e-05, Loss=6.66e-03 BER=2.62e-03 FER=2.73e-02
2025-09-29 12:37:40,441 | INFO | Epoch 184 Train Time 61.24269890785217s

2025-09-29 12:38:39,871 | INFO | Training epoch 185, Batch 1000/1000: LR=7.04e-05, Loss=6.61e-03 BER=2.60e-03 FER=2.70e-02
2025-09-29 12:38:40,316 | INFO | Epoch 185 Train Time 59.87305974960327s

2025-09-29 12:39:41,737 | INFO | Training epoch 186, Batch 1000/1000: LR=7.02e-05, Loss=6.62e-03 BER=2.60e-03 FER=2.73e-02
2025-09-29 12:39:42,189 | INFO | Epoch 186 Train Time 61.871408462524414s

2025-09-29 12:40:43,238 | INFO | Training epoch 187, Batch 1000/1000: LR=6.99e-05, Loss=6.55e-03 BER=2.56e-03 FER=2.66e-02
2025-09-29 12:40:43,695 | INFO | Epoch 187 Train Time 61.50510382652283s

2025-09-29 12:41:43,942 | INFO | Training epoch 188, Batch 1000/1000: LR=6.96e-05, Loss=6.84e-03 BER=2.67e-03 FER=2.80e-02
2025-09-29 12:41:44,373 | INFO | Epoch 188 Train Time 60.676525592803955s

2025-09-29 12:42:45,464 | INFO | Training epoch 189, Batch 1000/1000: LR=6.93e-05, Loss=6.73e-03 BER=2.64e-03 FER=2.72e-02
2025-09-29 12:42:45,914 | INFO | Epoch 189 Train Time 61.53901648521423s

2025-09-29 12:43:46,077 | INFO | Training epoch 190, Batch 1000/1000: LR=6.90e-05, Loss=6.71e-03 BER=2.64e-03 FER=2.71e-02
2025-09-29 12:43:46,559 | INFO | Epoch 190 Train Time 60.64389657974243s

2025-09-29 12:44:47,401 | INFO | Training epoch 191, Batch 1000/1000: LR=6.87e-05, Loss=6.64e-03 BER=2.62e-03 FER=2.72e-02
2025-09-29 12:44:47,871 | INFO | Epoch 191 Train Time 61.31118369102478s

2025-09-29 12:45:48,778 | INFO | Training epoch 192, Batch 1000/1000: LR=6.84e-05, Loss=6.76e-03 BER=2.64e-03 FER=2.71e-02
2025-09-29 12:45:49,391 | INFO | Epoch 192 Train Time 61.518242835998535s

2025-09-29 12:46:49,593 | INFO | Training epoch 193, Batch 1000/1000: LR=6.81e-05, Loss=6.76e-03 BER=2.69e-03 FER=2.77e-02
2025-09-29 12:46:50,059 | INFO | Epoch 193 Train Time 60.66699242591858s

2025-09-29 12:47:51,695 | INFO | Training epoch 194, Batch 1000/1000: LR=6.79e-05, Loss=6.55e-03 BER=2.55e-03 FER=2.68e-02
2025-09-29 12:47:52,148 | INFO | Epoch 194 Train Time 62.08816862106323s

2025-09-29 12:48:53,739 | INFO | Training epoch 195, Batch 1000/1000: LR=6.76e-05, Loss=6.67e-03 BER=2.65e-03 FER=2.75e-02
2025-09-29 12:48:54,194 | INFO | Epoch 195 Train Time 62.04501962661743s

2025-09-29 12:49:57,315 | INFO | Training epoch 196, Batch 1000/1000: LR=6.73e-05, Loss=6.79e-03 BER=2.69e-03 FER=2.76e-02
2025-09-29 12:49:57,764 | INFO | Epoch 196 Train Time 63.569117307662964s

2025-09-29 12:50:59,574 | INFO | Training epoch 197, Batch 1000/1000: LR=6.70e-05, Loss=6.69e-03 BER=2.63e-03 FER=2.73e-02
2025-09-29 12:51:00,028 | INFO | Epoch 197 Train Time 62.26273512840271s

2025-09-29 12:52:00,863 | INFO | Training epoch 198, Batch 1000/1000: LR=6.67e-05, Loss=6.72e-03 BER=2.65e-03 FER=2.74e-02
2025-09-29 12:52:01,315 | INFO | Epoch 198 Train Time 61.284677028656006s

2025-09-29 12:53:02,670 | INFO | Training epoch 199, Batch 1000/1000: LR=6.64e-05, Loss=6.85e-03 BER=2.69e-03 FER=2.79e-02
2025-09-29 12:53:03,140 | INFO | Epoch 199 Train Time 61.824814558029175s

2025-09-29 12:54:03,756 | INFO | Training epoch 200, Batch 1000/1000: LR=6.61e-05, Loss=6.68e-03 BER=2.67e-03 FER=2.73e-02
2025-09-29 12:54:04,211 | INFO | Epoch 200 Train Time 61.068732261657715s

2025-09-29 12:55:05,547 | INFO | Training epoch 201, Batch 1000/1000: LR=6.58e-05, Loss=6.63e-03 BER=2.59e-03 FER=2.71e-02
2025-09-29 12:55:06,006 | INFO | Epoch 201 Train Time 61.79374599456787s

2025-09-29 12:56:08,323 | INFO | Training epoch 202, Batch 1000/1000: LR=6.55e-05, Loss=6.46e-03 BER=2.55e-03 FER=2.67e-02
2025-09-29 12:56:08,781 | INFO | Epoch 202 Train Time 62.77440547943115s

2025-09-29 12:56:08,782 | INFO | [P1] saving best_model with loss 0.006456 at epoch 202
2025-09-29 12:57:09,342 | INFO | Training epoch 203, Batch 1000/1000: LR=6.52e-05, Loss=6.76e-03 BER=2.66e-03 FER=2.77e-02
2025-09-29 12:57:09,805 | INFO | Epoch 203 Train Time 60.97335171699524s

2025-09-29 12:58:10,507 | INFO | Training epoch 204, Batch 1000/1000: LR=6.49e-05, Loss=6.85e-03 BER=2.66e-03 FER=2.75e-02
2025-09-29 12:58:10,956 | INFO | Epoch 204 Train Time 61.14834952354431s

2025-09-29 12:59:11,777 | INFO | Training epoch 205, Batch 1000/1000: LR=6.46e-05, Loss=6.58e-03 BER=2.59e-03 FER=2.69e-02
2025-09-29 12:59:12,238 | INFO | Epoch 205 Train Time 61.281869411468506s

2025-09-29 13:00:12,601 | INFO | Training epoch 206, Batch 1000/1000: LR=6.43e-05, Loss=6.70e-03 BER=2.66e-03 FER=2.71e-02
2025-09-29 13:00:13,054 | INFO | Epoch 206 Train Time 60.81365728378296s

2025-09-29 13:01:14,216 | INFO | Training epoch 207, Batch 1000/1000: LR=6.40e-05, Loss=6.76e-03 BER=2.64e-03 FER=2.76e-02
2025-09-29 13:01:14,666 | INFO | Epoch 207 Train Time 61.609914779663086s

2025-09-29 13:02:16,245 | INFO | Training epoch 208, Batch 1000/1000: LR=6.37e-05, Loss=6.72e-03 BER=2.63e-03 FER=2.72e-02
2025-09-29 13:02:16,678 | INFO | Epoch 208 Train Time 62.011266231536865s

2025-09-29 13:03:17,738 | INFO | Training epoch 209, Batch 1000/1000: LR=6.34e-05, Loss=6.50e-03 BER=2.57e-03 FER=2.68e-02
2025-09-29 13:03:18,201 | INFO | Epoch 209 Train Time 61.521448850631714s

2025-09-29 13:04:18,240 | INFO | Training epoch 210, Batch 1000/1000: LR=6.31e-05, Loss=6.71e-03 BER=2.63e-03 FER=2.72e-02
2025-09-29 13:04:18,910 | INFO | Epoch 210 Train Time 60.70850610733032s

2025-09-29 13:05:20,978 | INFO | Training epoch 211, Batch 1000/1000: LR=6.28e-05, Loss=6.73e-03 BER=2.67e-03 FER=2.73e-02
2025-09-29 13:05:21,418 | INFO | Epoch 211 Train Time 62.50513792037964s

2025-09-29 13:06:22,110 | INFO | Training epoch 212, Batch 1000/1000: LR=6.25e-05, Loss=6.77e-03 BER=2.70e-03 FER=2.79e-02
2025-09-29 13:06:22,581 | INFO | Epoch 212 Train Time 61.162479400634766s

2025-09-29 13:07:24,618 | INFO | Training epoch 213, Batch 1000/1000: LR=6.22e-05, Loss=6.67e-03 BER=2.61e-03 FER=2.71e-02
2025-09-29 13:07:25,065 | INFO | Epoch 213 Train Time 62.48279023170471s

2025-09-29 13:08:25,221 | INFO | Training epoch 214, Batch 1000/1000: LR=6.19e-05, Loss=6.69e-03 BER=2.64e-03 FER=2.73e-02
2025-09-29 13:08:25,663 | INFO | Epoch 214 Train Time 60.59704041481018s

2025-09-29 13:09:26,716 | INFO | Training epoch 215, Batch 1000/1000: LR=6.16e-05, Loss=6.92e-03 BER=2.71e-03 FER=2.80e-02
2025-09-29 13:09:27,185 | INFO | Epoch 215 Train Time 61.52096343040466s

2025-09-29 13:10:28,629 | INFO | Training epoch 216, Batch 1000/1000: LR=6.13e-05, Loss=6.56e-03 BER=2.59e-03 FER=2.65e-02
2025-09-29 13:10:29,065 | INFO | Epoch 216 Train Time 61.87839412689209s

2025-09-29 13:11:30,024 | INFO | Training epoch 217, Batch 1000/1000: LR=6.10e-05, Loss=6.69e-03 BER=2.66e-03 FER=2.75e-02
2025-09-29 13:11:30,465 | INFO | Epoch 217 Train Time 61.398356676101685s

2025-09-29 13:12:33,288 | INFO | Training epoch 218, Batch 1000/1000: LR=6.07e-05, Loss=6.58e-03 BER=2.61e-03 FER=2.67e-02
2025-09-29 13:12:33,743 | INFO | Epoch 218 Train Time 63.27718138694763s

2025-09-29 13:13:35,848 | INFO | Training epoch 219, Batch 1000/1000: LR=6.04e-05, Loss=6.54e-03 BER=2.56e-03 FER=2.69e-02
2025-09-29 13:13:36,309 | INFO | Epoch 219 Train Time 62.56545090675354s

2025-09-29 13:14:35,809 | INFO | Training epoch 220, Batch 1000/1000: LR=6.01e-05, Loss=6.56e-03 BER=2.59e-03 FER=2.67e-02
2025-09-29 13:14:36,250 | INFO | Epoch 220 Train Time 59.93979549407959s

2025-09-29 13:15:37,805 | INFO | Training epoch 221, Batch 1000/1000: LR=5.98e-05, Loss=6.72e-03 BER=2.63e-03 FER=2.76e-02
2025-09-29 13:15:38,270 | INFO | Epoch 221 Train Time 62.01760005950928s

2025-09-29 13:16:39,312 | INFO | Training epoch 222, Batch 1000/1000: LR=5.95e-05, Loss=6.58e-03 BER=2.60e-03 FER=2.67e-02
2025-09-29 13:16:39,771 | INFO | Epoch 222 Train Time 61.50025224685669s

2025-09-29 13:17:40,813 | INFO | Training epoch 223, Batch 1000/1000: LR=5.92e-05, Loss=6.80e-03 BER=2.68e-03 FER=2.74e-02
2025-09-29 13:17:41,266 | INFO | Epoch 223 Train Time 61.49338674545288s

2025-09-29 13:18:41,592 | INFO | Training epoch 224, Batch 1000/1000: LR=5.89e-05, Loss=6.54e-03 BER=2.56e-03 FER=2.65e-02
2025-09-29 13:18:42,051 | INFO | Epoch 224 Train Time 60.78357410430908s

2025-09-29 13:19:43,139 | INFO | Training epoch 225, Batch 1000/1000: LR=5.86e-05, Loss=6.78e-03 BER=2.67e-03 FER=2.75e-02
2025-09-29 13:19:43,600 | INFO | Epoch 225 Train Time 61.5474967956543s

2025-09-29 13:20:49,951 | INFO | Training epoch 226, Batch 1000/1000: LR=5.82e-05, Loss=6.66e-03 BER=2.64e-03 FER=2.68e-02
2025-09-29 13:20:50,401 | INFO | Epoch 226 Train Time 66.80043864250183s

2025-09-29 13:21:51,504 | INFO | Training epoch 227, Batch 1000/1000: LR=5.79e-05, Loss=6.69e-03 BER=2.64e-03 FER=2.77e-02
2025-09-29 13:21:51,942 | INFO | Epoch 227 Train Time 61.539794921875s

2025-09-29 13:22:52,969 | INFO | Training epoch 228, Batch 1000/1000: LR=5.76e-05, Loss=6.45e-03 BER=2.55e-03 FER=2.60e-02
2025-09-29 13:22:53,499 | INFO | Epoch 228 Train Time 61.55575704574585s

2025-09-29 13:22:53,499 | INFO | [P1] saving best_model with loss 0.006447 at epoch 228
2025-09-29 13:23:54,001 | INFO | Training epoch 229, Batch 1000/1000: LR=5.73e-05, Loss=6.65e-03 BER=2.64e-03 FER=2.71e-02
2025-09-29 13:23:54,468 | INFO | Epoch 229 Train Time 60.91795372962952s

2025-09-29 13:24:54,454 | INFO | Training epoch 230, Batch 1000/1000: LR=5.70e-05, Loss=6.64e-03 BER=2.64e-03 FER=2.72e-02
2025-09-29 13:24:54,919 | INFO | Epoch 230 Train Time 60.449113845825195s

2025-09-29 13:25:55,906 | INFO | Training epoch 231, Batch 1000/1000: LR=5.67e-05, Loss=6.74e-03 BER=2.68e-03 FER=2.71e-02
2025-09-29 13:25:56,350 | INFO | Epoch 231 Train Time 61.43039512634277s

2025-09-29 13:26:56,946 | INFO | Training epoch 232, Batch 1000/1000: LR=5.64e-05, Loss=6.73e-03 BER=2.64e-03 FER=2.73e-02
2025-09-29 13:26:57,402 | INFO | Epoch 232 Train Time 61.05086970329285s

2025-09-29 13:27:57,996 | INFO | Training epoch 233, Batch 1000/1000: LR=5.61e-05, Loss=6.63e-03 BER=2.60e-03 FER=2.66e-02
2025-09-29 13:27:58,431 | INFO | Epoch 233 Train Time 61.028003215789795s

2025-09-29 13:28:59,930 | INFO | Training epoch 234, Batch 1000/1000: LR=5.58e-05, Loss=6.67e-03 BER=2.67e-03 FER=2.69e-02
2025-09-29 13:29:00,420 | INFO | Epoch 234 Train Time 61.98816156387329s

2025-09-29 13:30:01,828 | INFO | Training epoch 235, Batch 1000/1000: LR=5.55e-05, Loss=6.41e-03 BER=2.51e-03 FER=2.61e-02
2025-09-29 13:30:02,290 | INFO | Epoch 235 Train Time 61.867332220077515s

2025-09-29 13:30:02,291 | INFO | [P1] saving best_model with loss 0.006408 at epoch 235
2025-09-29 13:31:02,265 | INFO | Training epoch 236, Batch 1000/1000: LR=5.52e-05, Loss=6.76e-03 BER=2.70e-03 FER=2.73e-02
2025-09-29 13:31:02,721 | INFO | Epoch 236 Train Time 60.37902331352234s

2025-09-29 13:32:03,412 | INFO | Training epoch 237, Batch 1000/1000: LR=5.48e-05, Loss=6.70e-03 BER=2.65e-03 FER=2.70e-02
2025-09-29 13:32:03,854 | INFO | Epoch 237 Train Time 61.130122423172s

2025-09-29 13:33:04,425 | INFO | Training epoch 238, Batch 1000/1000: LR=5.45e-05, Loss=6.92e-03 BER=2.72e-03 FER=2.76e-02
2025-09-29 13:33:04,902 | INFO | Epoch 238 Train Time 61.0460638999939s

2025-09-29 13:34:05,097 | INFO | Training epoch 239, Batch 1000/1000: LR=5.42e-05, Loss=6.69e-03 BER=2.62e-03 FER=2.67e-02
2025-09-29 13:34:05,560 | INFO | Epoch 239 Train Time 60.657379388809204s

2025-09-29 13:35:06,027 | INFO | Training epoch 240, Batch 1000/1000: LR=5.39e-05, Loss=6.77e-03 BER=2.66e-03 FER=2.71e-02
2025-09-29 13:35:06,463 | INFO | Epoch 240 Train Time 60.90180563926697s

2025-09-29 13:36:08,152 | INFO | Training epoch 241, Batch 1000/1000: LR=5.36e-05, Loss=6.67e-03 BER=2.65e-03 FER=2.74e-02
2025-09-29 13:36:08,663 | INFO | Epoch 241 Train Time 62.19841957092285s

2025-09-29 13:37:09,800 | INFO | Training epoch 242, Batch 1000/1000: LR=5.33e-05, Loss=6.57e-03 BER=2.62e-03 FER=2.69e-02
2025-09-29 13:37:10,239 | INFO | Epoch 242 Train Time 61.57508611679077s

2025-09-29 13:38:11,510 | INFO | Training epoch 243, Batch 1000/1000: LR=5.30e-05, Loss=6.59e-03 BER=2.61e-03 FER=2.69e-02
2025-09-29 13:38:11,977 | INFO | Epoch 243 Train Time 61.73577070236206s

2025-09-29 13:39:12,930 | INFO | Training epoch 244, Batch 1000/1000: LR=5.27e-05, Loss=6.58e-03 BER=2.61e-03 FER=2.69e-02
2025-09-29 13:39:13,379 | INFO | Epoch 244 Train Time 61.4008207321167s

2025-09-29 13:40:13,759 | INFO | Training epoch 245, Batch 1000/1000: LR=5.24e-05, Loss=6.68e-03 BER=2.65e-03 FER=2.72e-02
2025-09-29 13:40:14,406 | INFO | Epoch 245 Train Time 61.02580189704895s

2025-09-29 13:41:14,178 | INFO | Training epoch 246, Batch 1000/1000: LR=5.21e-05, Loss=6.73e-03 BER=2.62e-03 FER=2.70e-02
2025-09-29 13:41:14,625 | INFO | Epoch 246 Train Time 60.218356132507324s

2025-09-29 13:42:15,350 | INFO | Training epoch 247, Batch 1000/1000: LR=5.17e-05, Loss=6.82e-03 BER=2.71e-03 FER=2.79e-02
2025-09-29 13:42:15,805 | INFO | Epoch 247 Train Time 61.178635358810425s

2025-09-29 13:43:17,015 | INFO | Training epoch 248, Batch 1000/1000: LR=5.14e-05, Loss=6.58e-03 BER=2.62e-03 FER=2.69e-02
2025-09-29 13:43:17,462 | INFO | Epoch 248 Train Time 61.65625762939453s

2025-09-29 13:44:16,727 | INFO | Training epoch 249, Batch 1000/1000: LR=5.11e-05, Loss=6.51e-03 BER=2.53e-03 FER=2.64e-02
2025-09-29 13:44:17,169 | INFO | Epoch 249 Train Time 59.70597243309021s

2025-09-29 13:45:18,614 | INFO | Training epoch 250, Batch 1000/1000: LR=5.08e-05, Loss=6.49e-03 BER=2.59e-03 FER=2.68e-02
2025-09-29 13:45:19,111 | INFO | Epoch 250 Train Time 61.94104242324829s

2025-09-29 13:46:19,566 | INFO | Training epoch 251, Batch 1000/1000: LR=5.05e-05, Loss=6.54e-03 BER=2.56e-03 FER=2.63e-02
2025-09-29 13:46:20,029 | INFO | Epoch 251 Train Time 60.91704034805298s

2025-09-29 13:47:21,093 | INFO | Training epoch 252, Batch 1000/1000: LR=5.02e-05, Loss=6.68e-03 BER=2.64e-03 FER=2.72e-02
2025-09-29 13:47:21,583 | INFO | Epoch 252 Train Time 61.55150580406189s

2025-09-29 13:48:21,970 | INFO | Training epoch 253, Batch 1000/1000: LR=4.99e-05, Loss=6.70e-03 BER=2.66e-03 FER=2.70e-02
2025-09-29 13:48:22,429 | INFO | Epoch 253 Train Time 60.84492325782776s

2025-09-29 13:49:23,442 | INFO | Training epoch 254, Batch 1000/1000: LR=4.96e-05, Loss=6.59e-03 BER=2.56e-03 FER=2.67e-02
2025-09-29 13:49:23,884 | INFO | Epoch 254 Train Time 61.454671144485474s

2025-09-29 13:50:23,699 | INFO | Training epoch 255, Batch 1000/1000: LR=4.93e-05, Loss=6.37e-03 BER=2.52e-03 FER=2.58e-02
2025-09-29 13:50:24,169 | INFO | Epoch 255 Train Time 60.28311371803284s

2025-09-29 13:50:24,170 | INFO | [P1] saving best_model with loss 0.006370 at epoch 255
2025-09-29 13:51:25,734 | INFO | Training epoch 256, Batch 1000/1000: LR=4.89e-05, Loss=6.65e-03 BER=2.63e-03 FER=2.70e-02
2025-09-29 13:51:26,229 | INFO | Epoch 256 Train Time 62.00736880302429s

2025-09-29 13:52:29,050 | INFO | Training epoch 257, Batch 1000/1000: LR=4.86e-05, Loss=6.36e-03 BER=2.51e-03 FER=2.57e-02
2025-09-29 13:52:29,514 | INFO | Epoch 257 Train Time 63.282066106796265s

2025-09-29 13:52:29,514 | INFO | [P1] saving best_model with loss 0.006363 at epoch 257
2025-09-29 13:53:30,371 | INFO | Training epoch 258, Batch 1000/1000: LR=4.83e-05, Loss=6.48e-03 BER=2.60e-03 FER=2.64e-02
2025-09-29 13:53:30,811 | INFO | Epoch 258 Train Time 61.2493782043457s

2025-09-29 13:54:31,563 | INFO | Training epoch 259, Batch 1000/1000: LR=4.80e-05, Loss=6.70e-03 BER=2.65e-03 FER=2.71e-02
2025-09-29 13:54:32,020 | INFO | Epoch 259 Train Time 61.20787811279297s

2025-09-29 13:55:33,333 | INFO | Training epoch 260, Batch 1000/1000: LR=4.77e-05, Loss=6.44e-03 BER=2.51e-03 FER=2.62e-02
2025-09-29 13:55:33,788 | INFO | Epoch 260 Train Time 61.76631546020508s

2025-09-29 13:56:35,360 | INFO | Training epoch 261, Batch 1000/1000: LR=4.74e-05, Loss=6.68e-03 BER=2.65e-03 FER=2.70e-02
2025-09-29 13:56:35,835 | INFO | Epoch 261 Train Time 62.04645895957947s

2025-09-29 13:57:37,364 | INFO | Training epoch 262, Batch 1000/1000: LR=4.71e-05, Loss=6.36e-03 BER=2.53e-03 FER=2.60e-02
2025-09-29 13:57:37,813 | INFO | Epoch 262 Train Time 61.97663187980652s

2025-09-29 13:57:37,814 | INFO | [P1] saving best_model with loss 0.006357 at epoch 262
2025-09-29 13:58:39,172 | INFO | Training epoch 263, Batch 1000/1000: LR=4.68e-05, Loss=6.49e-03 BER=2.57e-03 FER=2.65e-02
2025-09-29 13:58:39,620 | INFO | Epoch 263 Train Time 61.753756523132324s

2025-09-29 13:59:39,890 | INFO | Training epoch 264, Batch 1000/1000: LR=4.65e-05, Loss=6.53e-03 BER=2.58e-03 FER=2.69e-02
2025-09-29 13:59:40,352 | INFO | Epoch 264 Train Time 60.731340408325195s

2025-09-29 14:00:41,927 | INFO | Training epoch 265, Batch 1000/1000: LR=4.62e-05, Loss=6.58e-03 BER=2.56e-03 FER=2.63e-02
2025-09-29 14:00:42,376 | INFO | Epoch 265 Train Time 62.02215933799744s

2025-09-29 14:01:44,152 | INFO | Training epoch 266, Batch 1000/1000: LR=4.58e-05, Loss=6.51e-03 BER=2.55e-03 FER=2.64e-02
2025-09-29 14:01:44,745 | INFO | Epoch 266 Train Time 62.36797070503235s

2025-09-29 14:02:45,586 | INFO | Training epoch 267, Batch 1000/1000: LR=4.55e-05, Loss=6.67e-03 BER=2.63e-03 FER=2.68e-02
2025-09-29 14:02:46,036 | INFO | Epoch 267 Train Time 61.29017639160156s

2025-09-29 14:03:47,770 | INFO | Training epoch 268, Batch 1000/1000: LR=4.52e-05, Loss=6.66e-03 BER=2.63e-03 FER=2.71e-02
2025-09-29 14:03:48,225 | INFO | Epoch 268 Train Time 62.18802618980408s

2025-09-29 14:04:49,054 | INFO | Training epoch 269, Batch 1000/1000: LR=4.49e-05, Loss=6.62e-03 BER=2.64e-03 FER=2.73e-02
2025-09-29 14:04:49,514 | INFO | Epoch 269 Train Time 61.28821396827698s

2025-09-29 14:05:50,888 | INFO | Training epoch 270, Batch 1000/1000: LR=4.46e-05, Loss=6.57e-03 BER=2.58e-03 FER=2.66e-02
2025-09-29 14:05:51,553 | INFO | Epoch 270 Train Time 62.03653573989868s

2025-09-29 14:06:52,291 | INFO | Training epoch 271, Batch 1000/1000: LR=4.43e-05, Loss=6.56e-03 BER=2.59e-03 FER=2.66e-02
2025-09-29 14:06:52,753 | INFO | Epoch 271 Train Time 61.19946765899658s

2025-09-29 14:07:53,561 | INFO | Training epoch 272, Batch 1000/1000: LR=4.40e-05, Loss=6.75e-03 BER=2.65e-03 FER=2.71e-02
2025-09-29 14:07:54,015 | INFO | Epoch 272 Train Time 61.26069116592407s

2025-09-29 14:08:54,317 | INFO | Training epoch 273, Batch 1000/1000: LR=4.37e-05, Loss=6.34e-03 BER=2.49e-03 FER=2.61e-02
2025-09-29 14:08:54,763 | INFO | Epoch 273 Train Time 60.7463219165802s

2025-09-29 14:08:54,764 | INFO | [P1] saving best_model with loss 0.006338 at epoch 273
2025-09-29 14:09:57,109 | INFO | Training epoch 274, Batch 1000/1000: LR=4.34e-05, Loss=6.71e-03 BER=2.66e-03 FER=2.70e-02
2025-09-29 14:09:57,625 | INFO | Epoch 274 Train Time 62.81085562705994s

2025-09-29 14:10:58,959 | INFO | Training epoch 275, Batch 1000/1000: LR=4.31e-05, Loss=6.38e-03 BER=2.54e-03 FER=2.61e-02
2025-09-29 14:10:59,416 | INFO | Epoch 275 Train Time 61.78988265991211s

2025-09-29 14:12:00,206 | INFO | Training epoch 276, Batch 1000/1000: LR=4.28e-05, Loss=6.58e-03 BER=2.58e-03 FER=2.66e-02
2025-09-29 14:12:00,681 | INFO | Epoch 276 Train Time 61.263347864151s

2025-09-29 14:13:02,899 | INFO | Training epoch 277, Batch 1000/1000: LR=4.24e-05, Loss=6.50e-03 BER=2.55e-03 FER=2.62e-02
2025-09-29 14:13:03,390 | INFO | Epoch 277 Train Time 62.707294940948486s

2025-09-29 14:14:05,362 | INFO | Training epoch 278, Batch 1000/1000: LR=4.21e-05, Loss=6.43e-03 BER=2.55e-03 FER=2.63e-02
2025-09-29 14:14:05,811 | INFO | Epoch 278 Train Time 62.41947889328003s

2025-09-29 14:15:06,556 | INFO | Training epoch 279, Batch 1000/1000: LR=4.18e-05, Loss=6.53e-03 BER=2.58e-03 FER=2.68e-02
2025-09-29 14:15:07,024 | INFO | Epoch 279 Train Time 61.212167501449585s

2025-09-29 14:16:08,822 | INFO | Training epoch 280, Batch 1000/1000: LR=4.15e-05, Loss=6.72e-03 BER=2.67e-03 FER=2.75e-02
2025-09-29 14:16:09,278 | INFO | Epoch 280 Train Time 62.25261116027832s

2025-09-29 14:17:10,314 | INFO | Training epoch 281, Batch 1000/1000: LR=4.12e-05, Loss=6.48e-03 BER=2.56e-03 FER=2.60e-02
2025-09-29 14:17:10,797 | INFO | Epoch 281 Train Time 61.51681351661682s

2025-09-29 14:18:11,890 | INFO | Training epoch 282, Batch 1000/1000: LR=4.09e-05, Loss=6.79e-03 BER=2.67e-03 FER=2.74e-02
2025-09-29 14:18:12,351 | INFO | Epoch 282 Train Time 61.55238461494446s

2025-09-29 14:19:12,784 | INFO | Training epoch 283, Batch 1000/1000: LR=4.06e-05, Loss=6.65e-03 BER=2.62e-03 FER=2.70e-02
2025-09-29 14:19:13,237 | INFO | Epoch 283 Train Time 60.88447070121765s

2025-09-29 14:20:15,989 | INFO | Training epoch 284, Batch 1000/1000: LR=4.03e-05, Loss=6.31e-03 BER=2.49e-03 FER=2.58e-02
2025-09-29 14:20:16,439 | INFO | Epoch 284 Train Time 63.20198178291321s

2025-09-29 14:20:16,439 | INFO | [P1] saving best_model with loss 0.006311 at epoch 284
2025-09-29 14:21:18,329 | INFO | Training epoch 285, Batch 1000/1000: LR=4.00e-05, Loss=6.40e-03 BER=2.53e-03 FER=2.61e-02
2025-09-29 14:21:18,771 | INFO | Epoch 285 Train Time 62.27333354949951s

2025-09-29 14:22:20,016 | INFO | Training epoch 286, Batch 1000/1000: LR=3.97e-05, Loss=6.65e-03 BER=2.63e-03 FER=2.71e-02
2025-09-29 14:22:20,491 | INFO | Epoch 286 Train Time 61.71902775764465s

2025-09-29 14:23:24,832 | INFO | Training epoch 287, Batch 1000/1000: LR=3.94e-05, Loss=6.63e-03 BER=2.63e-03 FER=2.68e-02
2025-09-29 14:23:25,288 | INFO | Epoch 287 Train Time 64.79529666900635s

2025-09-29 14:24:26,252 | INFO | Training epoch 288, Batch 1000/1000: LR=3.91e-05, Loss=6.51e-03 BER=2.56e-03 FER=2.62e-02
2025-09-29 14:24:26,706 | INFO | Epoch 288 Train Time 61.41734290122986s

2025-09-29 14:25:27,820 | INFO | Training epoch 289, Batch 1000/1000: LR=3.88e-05, Loss=6.47e-03 BER=2.56e-03 FER=2.61e-02
2025-09-29 14:25:28,282 | INFO | Epoch 289 Train Time 61.57537031173706s

2025-09-29 14:26:29,538 | INFO | Training epoch 290, Batch 1000/1000: LR=3.85e-05, Loss=6.27e-03 BER=2.48e-03 FER=2.58e-02
2025-09-29 14:26:30,006 | INFO | Epoch 290 Train Time 61.72232937812805s

2025-09-29 14:26:30,006 | INFO | [P1] saving best_model with loss 0.006266 at epoch 290
2025-09-29 14:27:30,260 | INFO | Training epoch 291, Batch 1000/1000: LR=3.82e-05, Loss=6.43e-03 BER=2.53e-03 FER=2.64e-02
2025-09-29 14:27:30,710 | INFO | Epoch 291 Train Time 60.640464305877686s

2025-09-29 14:28:32,226 | INFO | Training epoch 292, Batch 1000/1000: LR=3.79e-05, Loss=6.40e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 14:28:32,681 | INFO | Epoch 292 Train Time 61.970314264297485s

2025-09-29 14:29:34,055 | INFO | Training epoch 293, Batch 1000/1000: LR=3.76e-05, Loss=6.31e-03 BER=2.48e-03 FER=2.60e-02
2025-09-29 14:29:34,592 | INFO | Epoch 293 Train Time 61.90943479537964s

2025-09-29 14:30:34,333 | INFO | Training epoch 294, Batch 1000/1000: LR=3.73e-05, Loss=6.69e-03 BER=2.63e-03 FER=2.66e-02
2025-09-29 14:30:34,805 | INFO | Epoch 294 Train Time 60.21248269081116s

2025-09-29 14:31:34,878 | INFO | Training epoch 295, Batch 1000/1000: LR=3.70e-05, Loss=6.47e-03 BER=2.57e-03 FER=2.61e-02
2025-09-29 14:31:35,370 | INFO | Epoch 295 Train Time 60.563854455947876s

2025-09-29 14:32:35,685 | INFO | Training epoch 296, Batch 1000/1000: LR=3.67e-05, Loss=6.39e-03 BER=2.52e-03 FER=2.60e-02
2025-09-29 14:32:36,172 | INFO | Epoch 296 Train Time 60.800278425216675s

2025-09-29 14:33:36,597 | INFO | Training epoch 297, Batch 1000/1000: LR=3.64e-05, Loss=6.43e-03 BER=2.53e-03 FER=2.59e-02
2025-09-29 14:33:37,136 | INFO | Epoch 297 Train Time 60.96123242378235s

2025-09-29 14:34:38,311 | INFO | Training epoch 298, Batch 1000/1000: LR=3.61e-05, Loss=6.68e-03 BER=2.68e-03 FER=2.73e-02
2025-09-29 14:34:39,052 | INFO | Epoch 298 Train Time 61.91376566886902s

2025-09-29 14:35:39,461 | INFO | Training epoch 299, Batch 1000/1000: LR=3.58e-05, Loss=6.58e-03 BER=2.61e-03 FER=2.66e-02
2025-09-29 14:35:39,907 | INFO | Epoch 299 Train Time 60.85477066040039s

2025-09-29 14:36:40,045 | INFO | Training epoch 300, Batch 1000/1000: LR=3.55e-05, Loss=6.55e-03 BER=2.58e-03 FER=2.63e-02
2025-09-29 14:36:40,505 | INFO | Epoch 300 Train Time 60.59716320037842s

2025-09-29 14:37:41,681 | INFO | Training epoch 301, Batch 1000/1000: LR=3.52e-05, Loss=6.56e-03 BER=2.57e-03 FER=2.62e-02
2025-09-29 14:37:42,167 | INFO | Epoch 301 Train Time 61.66066598892212s

2025-09-29 14:38:43,377 | INFO | Training epoch 302, Batch 1000/1000: LR=3.49e-05, Loss=6.57e-03 BER=2.62e-03 FER=2.69e-02
2025-09-29 14:38:43,821 | INFO | Epoch 302 Train Time 61.65141725540161s

2025-09-29 14:39:43,567 | INFO | Training epoch 303, Batch 1000/1000: LR=3.46e-05, Loss=6.61e-03 BER=2.64e-03 FER=2.70e-02
2025-09-29 14:39:44,013 | INFO | Epoch 303 Train Time 60.19044852256775s

2025-09-29 14:40:44,991 | INFO | Training epoch 304, Batch 1000/1000: LR=3.43e-05, Loss=6.41e-03 BER=2.51e-03 FER=2.57e-02
2025-09-29 14:40:45,440 | INFO | Epoch 304 Train Time 61.42627573013306s

2025-09-29 14:41:46,260 | INFO | Training epoch 305, Batch 1000/1000: LR=3.40e-05, Loss=6.47e-03 BER=2.56e-03 FER=2.64e-02
2025-09-29 14:41:46,711 | INFO | Epoch 305 Train Time 61.26876497268677s

2025-09-29 14:42:47,707 | INFO | Training epoch 306, Batch 1000/1000: LR=3.37e-05, Loss=6.42e-03 BER=2.51e-03 FER=2.56e-02
2025-09-29 14:42:48,406 | INFO | Epoch 306 Train Time 61.692888021469116s

2025-09-29 14:43:49,721 | INFO | Training epoch 307, Batch 1000/1000: LR=3.34e-05, Loss=6.55e-03 BER=2.60e-03 FER=2.65e-02
2025-09-29 14:43:50,213 | INFO | Epoch 307 Train Time 61.80541682243347s

2025-09-29 14:44:51,037 | INFO | Training epoch 308, Batch 1000/1000: LR=3.31e-05, Loss=6.51e-03 BER=2.61e-03 FER=2.67e-02
2025-09-29 14:44:51,496 | INFO | Epoch 308 Train Time 61.282819986343384s

2025-09-29 14:45:52,672 | INFO | Training epoch 309, Batch 1000/1000: LR=3.29e-05, Loss=6.21e-03 BER=2.45e-03 FER=2.55e-02
2025-09-29 14:45:53,123 | INFO | Epoch 309 Train Time 61.62563133239746s

2025-09-29 14:45:53,124 | INFO | [P1] saving best_model with loss 0.006208 at epoch 309
2025-09-29 14:46:56,368 | INFO | Training epoch 310, Batch 1000/1000: LR=3.26e-05, Loss=6.61e-03 BER=2.57e-03 FER=2.64e-02
2025-09-29 14:46:56,809 | INFO | Epoch 310 Train Time 63.631476402282715s

2025-09-29 14:47:57,758 | INFO | Training epoch 311, Batch 1000/1000: LR=3.23e-05, Loss=6.53e-03 BER=2.60e-03 FER=2.66e-02
2025-09-29 14:47:58,213 | INFO | Epoch 311 Train Time 61.40302300453186s

2025-09-29 14:48:59,327 | INFO | Training epoch 312, Batch 1000/1000: LR=3.20e-05, Loss=6.47e-03 BER=2.54e-03 FER=2.60e-02
2025-09-29 14:48:59,774 | INFO | Epoch 312 Train Time 61.56014132499695s

2025-09-29 14:50:00,644 | INFO | Training epoch 313, Batch 1000/1000: LR=3.17e-05, Loss=6.47e-03 BER=2.57e-03 FER=2.68e-02
2025-09-29 14:50:01,144 | INFO | Epoch 313 Train Time 61.368173360824585s

2025-09-29 14:51:01,150 | INFO | Training epoch 314, Batch 1000/1000: LR=3.14e-05, Loss=6.57e-03 BER=2.61e-03 FER=2.65e-02
2025-09-29 14:51:01,643 | INFO | Epoch 314 Train Time 60.49866771697998s

2025-09-29 14:52:03,193 | INFO | Training epoch 315, Batch 1000/1000: LR=3.11e-05, Loss=6.27e-03 BER=2.45e-03 FER=2.54e-02
2025-09-29 14:52:03,679 | INFO | Epoch 315 Train Time 62.03456687927246s

2025-09-29 14:53:04,164 | INFO | Training epoch 316, Batch 1000/1000: LR=3.08e-05, Loss=6.66e-03 BER=2.66e-03 FER=2.70e-02
2025-09-29 14:53:04,633 | INFO | Epoch 316 Train Time 60.952834129333496s

2025-09-29 14:54:07,385 | INFO | Training epoch 317, Batch 1000/1000: LR=3.06e-05, Loss=6.57e-03 BER=2.59e-03 FER=2.64e-02
2025-09-29 14:54:07,860 | INFO | Epoch 317 Train Time 63.226576805114746s

2025-09-29 14:55:11,303 | INFO | Training epoch 318, Batch 1000/1000: LR=3.03e-05, Loss=6.49e-03 BER=2.56e-03 FER=2.59e-02
2025-09-29 14:55:11,926 | INFO | Epoch 318 Train Time 64.06494235992432s

2025-09-29 14:56:13,452 | INFO | Training epoch 319, Batch 1000/1000: LR=3.00e-05, Loss=6.40e-03 BER=2.55e-03 FER=2.59e-02
2025-09-29 14:56:14,124 | INFO | Epoch 319 Train Time 62.1967351436615s

2025-09-29 14:57:14,384 | INFO | Training epoch 320, Batch 1000/1000: LR=2.97e-05, Loss=6.57e-03 BER=2.61e-03 FER=2.67e-02
2025-09-29 14:57:14,840 | INFO | Epoch 320 Train Time 60.715455293655396s

2025-09-29 14:58:15,918 | INFO | Training epoch 321, Batch 1000/1000: LR=2.94e-05, Loss=6.48e-03 BER=2.56e-03 FER=2.60e-02
2025-09-29 14:58:16,368 | INFO | Epoch 321 Train Time 61.52614688873291s

2025-09-29 14:59:17,075 | INFO | Training epoch 322, Batch 1000/1000: LR=2.91e-05, Loss=6.42e-03 BER=2.51e-03 FER=2.59e-02
2025-09-29 14:59:17,527 | INFO | Epoch 322 Train Time 61.158039808273315s

2025-09-29 15:00:18,975 | INFO | Training epoch 323, Batch 1000/1000: LR=2.89e-05, Loss=6.43e-03 BER=2.50e-03 FER=2.54e-02
2025-09-29 15:00:19,434 | INFO | Epoch 323 Train Time 61.90517210960388s

2025-09-29 15:01:20,787 | INFO | Training epoch 324, Batch 1000/1000: LR=2.86e-05, Loss=6.37e-03 BER=2.54e-03 FER=2.57e-02
2025-09-29 15:01:21,273 | INFO | Epoch 324 Train Time 61.83780264854431s

2025-09-29 15:02:21,422 | INFO | Training epoch 325, Batch 1000/1000: LR=2.83e-05, Loss=6.47e-03 BER=2.56e-03 FER=2.59e-02
2025-09-29 15:02:21,884 | INFO | Epoch 325 Train Time 60.61008644104004s

2025-09-29 15:03:22,769 | INFO | Training epoch 326, Batch 1000/1000: LR=2.80e-05, Loss=6.29e-03 BER=2.47e-03 FER=2.51e-02
2025-09-29 15:03:23,224 | INFO | Epoch 326 Train Time 61.33914589881897s

2025-09-29 15:04:23,077 | INFO | Training epoch 327, Batch 1000/1000: LR=2.78e-05, Loss=6.33e-03 BER=2.48e-03 FER=2.56e-02
2025-09-29 15:04:23,539 | INFO | Epoch 327 Train Time 60.314440965652466s

2025-09-29 15:05:24,845 | INFO | Training epoch 328, Batch 1000/1000: LR=2.75e-05, Loss=6.46e-03 BER=2.58e-03 FER=2.59e-02
2025-09-29 15:05:25,305 | INFO | Epoch 328 Train Time 61.76394820213318s

2025-09-29 15:06:26,939 | INFO | Training epoch 329, Batch 1000/1000: LR=2.72e-05, Loss=6.54e-03 BER=2.62e-03 FER=2.69e-02
2025-09-29 15:06:27,547 | INFO | Epoch 329 Train Time 62.24115037918091s

2025-09-29 15:07:29,078 | INFO | Training epoch 330, Batch 1000/1000: LR=2.69e-05, Loss=6.51e-03 BER=2.55e-03 FER=2.62e-02
2025-09-29 15:07:29,532 | INFO | Epoch 330 Train Time 61.982725858688354s

2025-09-29 15:08:30,844 | INFO | Training epoch 331, Batch 1000/1000: LR=2.67e-05, Loss=6.28e-03 BER=2.48e-03 FER=2.58e-02
2025-09-29 15:08:31,282 | INFO | Epoch 331 Train Time 61.749117851257324s

2025-09-29 15:09:32,777 | INFO | Training epoch 332, Batch 1000/1000: LR=2.64e-05, Loss=6.22e-03 BER=2.47e-03 FER=2.55e-02
2025-09-29 15:09:33,250 | INFO | Epoch 332 Train Time 61.96638774871826s

2025-09-29 15:10:34,598 | INFO | Training epoch 333, Batch 1000/1000: LR=2.61e-05, Loss=6.41e-03 BER=2.51e-03 FER=2.57e-02
2025-09-29 15:10:35,068 | INFO | Epoch 333 Train Time 61.8171489238739s

2025-09-29 15:11:36,337 | INFO | Training epoch 334, Batch 1000/1000: LR=2.58e-05, Loss=6.48e-03 BER=2.57e-03 FER=2.60e-02
2025-09-29 15:11:36,782 | INFO | Epoch 334 Train Time 61.71359586715698s

2025-09-29 15:12:37,973 | INFO | Training epoch 335, Batch 1000/1000: LR=2.56e-05, Loss=6.54e-03 BER=2.58e-03 FER=2.65e-02
2025-09-29 15:12:38,428 | INFO | Epoch 335 Train Time 61.644224643707275s

2025-09-29 15:13:40,707 | INFO | Training epoch 336, Batch 1000/1000: LR=2.53e-05, Loss=6.52e-03 BER=2.58e-03 FER=2.66e-02
2025-09-29 15:13:41,164 | INFO | Epoch 336 Train Time 62.73565125465393s

2025-09-29 15:14:41,878 | INFO | Training epoch 337, Batch 1000/1000: LR=2.50e-05, Loss=6.51e-03 BER=2.60e-03 FER=2.65e-02
2025-09-29 15:14:42,394 | INFO | Epoch 337 Train Time 61.22784471511841s

2025-09-29 15:15:43,361 | INFO | Training epoch 338, Batch 1000/1000: LR=2.48e-05, Loss=6.31e-03 BER=2.49e-03 FER=2.57e-02
2025-09-29 15:15:43,805 | INFO | Epoch 338 Train Time 61.40972971916199s

2025-09-29 15:16:44,681 | INFO | Training epoch 339, Batch 1000/1000: LR=2.45e-05, Loss=6.37e-03 BER=2.53e-03 FER=2.59e-02
2025-09-29 15:16:45,129 | INFO | Epoch 339 Train Time 61.32281994819641s

2025-09-29 15:17:45,020 | INFO | Training epoch 340, Batch 1000/1000: LR=2.42e-05, Loss=6.31e-03 BER=2.46e-03 FER=2.54e-02
2025-09-29 15:17:45,511 | INFO | Epoch 340 Train Time 60.38083839416504s

2025-09-29 15:18:45,889 | INFO | Training epoch 341, Batch 1000/1000: LR=2.40e-05, Loss=6.36e-03 BER=2.53e-03 FER=2.57e-02
2025-09-29 15:18:46,504 | INFO | Epoch 341 Train Time 60.99270057678223s

2025-09-29 15:19:49,269 | INFO | Training epoch 342, Batch 1000/1000: LR=2.37e-05, Loss=6.36e-03 BER=2.51e-03 FER=2.55e-02
2025-09-29 15:19:49,714 | INFO | Epoch 342 Train Time 63.20909857749939s

2025-09-29 15:20:50,973 | INFO | Training epoch 343, Batch 1000/1000: LR=2.35e-05, Loss=6.23e-03 BER=2.47e-03 FER=2.60e-02
2025-09-29 15:20:51,444 | INFO | Epoch 343 Train Time 61.72832012176514s

2025-09-29 15:21:52,383 | INFO | Training epoch 344, Batch 1000/1000: LR=2.32e-05, Loss=6.34e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 15:21:52,827 | INFO | Epoch 344 Train Time 61.382519245147705s

2025-09-29 15:22:52,933 | INFO | Training epoch 345, Batch 1000/1000: LR=2.29e-05, Loss=6.54e-03 BER=2.57e-03 FER=2.64e-02
2025-09-29 15:22:53,392 | INFO | Epoch 345 Train Time 60.562504053115845s

2025-09-29 15:23:53,902 | INFO | Training epoch 346, Batch 1000/1000: LR=2.27e-05, Loss=6.30e-03 BER=2.50e-03 FER=2.54e-02
2025-09-29 15:23:54,357 | INFO | Epoch 346 Train Time 60.96436166763306s

2025-09-29 15:24:54,532 | INFO | Training epoch 347, Batch 1000/1000: LR=2.24e-05, Loss=6.46e-03 BER=2.60e-03 FER=2.62e-02
2025-09-29 15:24:54,966 | INFO | Epoch 347 Train Time 60.607093811035156s

2025-09-29 15:25:57,664 | INFO | Training epoch 348, Batch 1000/1000: LR=2.22e-05, Loss=6.59e-03 BER=2.61e-03 FER=2.68e-02
2025-09-29 15:25:58,159 | INFO | Epoch 348 Train Time 63.190975189208984s

2025-09-29 15:26:59,030 | INFO | Training epoch 349, Batch 1000/1000: LR=2.19e-05, Loss=6.32e-03 BER=2.52e-03 FER=2.55e-02
2025-09-29 15:26:59,494 | INFO | Epoch 349 Train Time 61.333115100860596s

2025-09-29 15:28:00,408 | INFO | Training epoch 350, Batch 1000/1000: LR=2.17e-05, Loss=6.34e-03 BER=2.55e-03 FER=2.59e-02
2025-09-29 15:28:00,860 | INFO | Epoch 350 Train Time 61.36466121673584s

2025-09-29 15:29:02,008 | INFO | Training epoch 351, Batch 1000/1000: LR=2.14e-05, Loss=6.46e-03 BER=2.54e-03 FER=2.66e-02
2025-09-29 15:29:02,481 | INFO | Epoch 351 Train Time 61.61904764175415s

2025-09-29 15:30:02,909 | INFO | Training epoch 352, Batch 1000/1000: LR=2.12e-05, Loss=6.15e-03 BER=2.46e-03 FER=2.51e-02
2025-09-29 15:30:03,348 | INFO | Epoch 352 Train Time 60.86525797843933s

2025-09-29 15:30:03,348 | INFO | [P1] saving best_model with loss 0.006154 at epoch 352
2025-09-29 15:31:05,728 | INFO | Training epoch 353, Batch 1000/1000: LR=2.09e-05, Loss=6.46e-03 BER=2.54e-03 FER=2.59e-02
2025-09-29 15:31:06,170 | INFO | Epoch 353 Train Time 62.76459217071533s

2025-09-29 15:32:07,780 | INFO | Training epoch 354, Batch 1000/1000: LR=2.07e-05, Loss=6.48e-03 BER=2.57e-03 FER=2.63e-02
2025-09-29 15:32:08,237 | INFO | Epoch 354 Train Time 62.066282987594604s

2025-09-29 15:33:08,734 | INFO | Training epoch 355, Batch 1000/1000: LR=2.04e-05, Loss=6.43e-03 BER=2.54e-03 FER=2.59e-02
2025-09-29 15:33:09,196 | INFO | Epoch 355 Train Time 60.957098722457886s

2025-09-29 15:34:09,957 | INFO | Training epoch 356, Batch 1000/1000: LR=2.02e-05, Loss=6.50e-03 BER=2.57e-03 FER=2.61e-02
2025-09-29 15:34:10,467 | INFO | Epoch 356 Train Time 61.26969599723816s

2025-09-29 15:35:11,521 | INFO | Training epoch 357, Batch 1000/1000: LR=1.99e-05, Loss=6.19e-03 BER=2.46e-03 FER=2.55e-02
2025-09-29 15:35:12,213 | INFO | Epoch 357 Train Time 61.74492120742798s

2025-09-29 15:36:14,039 | INFO | Training epoch 358, Batch 1000/1000: LR=1.97e-05, Loss=6.45e-03 BER=2.54e-03 FER=2.65e-02
2025-09-29 15:36:14,498 | INFO | Epoch 358 Train Time 62.28473210334778s

2025-09-29 15:37:14,819 | INFO | Training epoch 359, Batch 1000/1000: LR=1.94e-05, Loss=6.43e-03 BER=2.53e-03 FER=2.57e-02
2025-09-29 15:37:15,263 | INFO | Epoch 359 Train Time 60.763439416885376s

2025-09-29 15:38:16,260 | INFO | Training epoch 360, Batch 1000/1000: LR=1.92e-05, Loss=6.51e-03 BER=2.58e-03 FER=2.64e-02
2025-09-29 15:38:16,721 | INFO | Epoch 360 Train Time 61.4572913646698s

2025-09-29 15:39:17,494 | INFO | Training epoch 361, Batch 1000/1000: LR=1.89e-05, Loss=6.38e-03 BER=2.52e-03 FER=2.56e-02
2025-09-29 15:39:17,941 | INFO | Epoch 361 Train Time 61.219202756881714s

2025-09-29 15:40:19,047 | INFO | Training epoch 362, Batch 1000/1000: LR=1.87e-05, Loss=6.47e-03 BER=2.56e-03 FER=2.63e-02
2025-09-29 15:40:19,509 | INFO | Epoch 362 Train Time 61.56650996208191s

2025-09-29 15:41:18,996 | INFO | Training epoch 363, Batch 1000/1000: LR=1.85e-05, Loss=6.39e-03 BER=2.53e-03 FER=2.59e-02
2025-09-29 15:41:19,455 | INFO | Epoch 363 Train Time 59.94512128829956s

2025-09-29 15:42:19,604 | INFO | Training epoch 364, Batch 1000/1000: LR=1.82e-05, Loss=6.41e-03 BER=2.53e-03 FER=2.58e-02
2025-09-29 15:42:20,061 | INFO | Epoch 364 Train Time 60.60479998588562s

2025-09-29 15:43:20,148 | INFO | Training epoch 365, Batch 1000/1000: LR=1.80e-05, Loss=6.01e-03 BER=2.39e-03 FER=2.50e-02
2025-09-29 15:43:20,607 | INFO | Epoch 365 Train Time 60.54484748840332s

2025-09-29 15:43:20,608 | INFO | [P1] saving best_model with loss 0.006009 at epoch 365
2025-09-29 15:44:22,192 | INFO | Training epoch 366, Batch 1000/1000: LR=1.78e-05, Loss=6.42e-03 BER=2.54e-03 FER=2.58e-02
2025-09-29 15:44:22,662 | INFO | Epoch 366 Train Time 61.99676537513733s

2025-09-29 15:45:23,142 | INFO | Training epoch 367, Batch 1000/1000: LR=1.75e-05, Loss=6.42e-03 BER=2.54e-03 FER=2.62e-02
2025-09-29 15:45:23,613 | INFO | Epoch 367 Train Time 60.95003318786621s

2025-09-29 15:46:23,997 | INFO | Training epoch 368, Batch 1000/1000: LR=1.73e-05, Loss=6.28e-03 BER=2.46e-03 FER=2.53e-02
2025-09-29 15:46:24,453 | INFO | Epoch 368 Train Time 60.83814334869385s

2025-09-29 15:47:25,859 | INFO | Training epoch 369, Batch 1000/1000: LR=1.71e-05, Loss=6.50e-03 BER=2.57e-03 FER=2.65e-02
2025-09-29 15:47:26,322 | INFO | Epoch 369 Train Time 61.867319107055664s

2025-09-29 15:48:27,148 | INFO | Training epoch 370, Batch 1000/1000: LR=1.68e-05, Loss=6.12e-03 BER=2.42e-03 FER=2.51e-02
2025-09-29 15:48:27,593 | INFO | Epoch 370 Train Time 61.270267963409424s

2025-09-29 15:49:28,390 | INFO | Training epoch 371, Batch 1000/1000: LR=1.66e-05, Loss=6.17e-03 BER=2.43e-03 FER=2.50e-02
2025-09-29 15:49:29,079 | INFO | Epoch 371 Train Time 61.48380923271179s

2025-09-29 15:50:30,661 | INFO | Training epoch 372, Batch 1000/1000: LR=1.64e-05, Loss=6.33e-03 BER=2.49e-03 FER=2.53e-02
2025-09-29 15:50:31,101 | INFO | Epoch 372 Train Time 62.02122902870178s

2025-09-29 15:51:31,510 | INFO | Training epoch 373, Batch 1000/1000: LR=1.62e-05, Loss=6.38e-03 BER=2.56e-03 FER=2.64e-02
2025-09-29 15:51:31,954 | INFO | Epoch 373 Train Time 60.852200508117676s

2025-09-29 15:52:32,707 | INFO | Training epoch 374, Batch 1000/1000: LR=1.59e-05, Loss=6.27e-03 BER=2.46e-03 FER=2.53e-02
2025-09-29 15:52:33,151 | INFO | Epoch 374 Train Time 61.19562077522278s

2025-09-29 15:53:33,233 | INFO | Training epoch 375, Batch 1000/1000: LR=1.57e-05, Loss=6.46e-03 BER=2.57e-03 FER=2.59e-02
2025-09-29 15:53:33,689 | INFO | Epoch 375 Train Time 60.53662157058716s

2025-09-29 15:54:33,869 | INFO | Training epoch 376, Batch 1000/1000: LR=1.55e-05, Loss=6.54e-03 BER=2.60e-03 FER=2.66e-02
2025-09-29 15:54:34,307 | INFO | Epoch 376 Train Time 60.617409229278564s

2025-09-29 15:55:35,619 | INFO | Training epoch 377, Batch 1000/1000: LR=1.53e-05, Loss=6.55e-03 BER=2.60e-03 FER=2.66e-02
2025-09-29 15:55:36,078 | INFO | Epoch 377 Train Time 61.770272731781006s

2025-09-29 15:56:36,617 | INFO | Training epoch 378, Batch 1000/1000: LR=1.51e-05, Loss=6.26e-03 BER=2.48e-03 FER=2.57e-02
2025-09-29 15:56:37,071 | INFO | Epoch 378 Train Time 60.99162697792053s

2025-09-29 15:57:42,016 | INFO | Training epoch 379, Batch 1000/1000: LR=1.48e-05, Loss=6.20e-03 BER=2.44e-03 FER=2.52e-02
2025-09-29 15:57:42,507 | INFO | Epoch 379 Train Time 65.4348304271698s

2025-09-29 15:58:44,084 | INFO | Training epoch 380, Batch 1000/1000: LR=1.46e-05, Loss=6.49e-03 BER=2.56e-03 FER=2.62e-02
2025-09-29 15:58:44,538 | INFO | Epoch 380 Train Time 62.02933883666992s

2025-09-29 15:59:45,194 | INFO | Training epoch 381, Batch 1000/1000: LR=1.44e-05, Loss=6.25e-03 BER=2.47e-03 FER=2.53e-02
2025-09-29 15:59:45,659 | INFO | Epoch 381 Train Time 61.118841886520386s

2025-09-29 16:00:47,240 | INFO | Training epoch 382, Batch 1000/1000: LR=1.42e-05, Loss=6.43e-03 BER=2.56e-03 FER=2.64e-02
2025-09-29 16:00:47,726 | INFO | Epoch 382 Train Time 62.06576728820801s

2025-09-29 16:01:49,415 | INFO | Training epoch 383, Batch 1000/1000: LR=1.40e-05, Loss=6.18e-03 BER=2.44e-03 FER=2.56e-02
2025-09-29 16:01:49,942 | INFO | Epoch 383 Train Time 62.21547341346741s

2025-09-29 16:02:52,108 | INFO | Training epoch 384, Batch 1000/1000: LR=1.38e-05, Loss=6.29e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 16:02:52,553 | INFO | Epoch 384 Train Time 62.61011242866516s

2025-09-29 16:03:53,672 | INFO | Training epoch 385, Batch 1000/1000: LR=1.36e-05, Loss=6.32e-03 BER=2.53e-03 FER=2.57e-02
2025-09-29 16:03:54,119 | INFO | Epoch 385 Train Time 61.565274715423584s

2025-09-29 16:04:55,407 | INFO | Training epoch 386, Batch 1000/1000: LR=1.34e-05, Loss=6.41e-03 BER=2.55e-03 FER=2.59e-02
2025-09-29 16:04:55,866 | INFO | Epoch 386 Train Time 61.745418071746826s

2025-09-29 16:05:58,730 | INFO | Training epoch 387, Batch 1000/1000: LR=1.32e-05, Loss=6.51e-03 BER=2.56e-03 FER=2.64e-02
2025-09-29 16:05:59,171 | INFO | Epoch 387 Train Time 63.30411648750305s

2025-09-29 16:07:00,048 | INFO | Training epoch 388, Batch 1000/1000: LR=1.30e-05, Loss=6.26e-03 BER=2.51e-03 FER=2.57e-02
2025-09-29 16:07:00,489 | INFO | Epoch 388 Train Time 61.3166561126709s

2025-09-29 16:08:01,928 | INFO | Training epoch 389, Batch 1000/1000: LR=1.28e-05, Loss=6.53e-03 BER=2.60e-03 FER=2.64e-02
2025-09-29 16:08:02,395 | INFO | Epoch 389 Train Time 61.9050018787384s

2025-09-29 16:09:03,483 | INFO | Training epoch 390, Batch 1000/1000: LR=1.26e-05, Loss=6.41e-03 BER=2.53e-03 FER=2.61e-02
2025-09-29 16:09:03,942 | INFO | Epoch 390 Train Time 61.545987129211426s

2025-09-29 16:10:04,704 | INFO | Training epoch 391, Batch 1000/1000: LR=1.24e-05, Loss=6.39e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 16:10:05,155 | INFO | Epoch 391 Train Time 61.21220517158508s

2025-09-29 16:11:06,505 | INFO | Training epoch 392, Batch 1000/1000: LR=1.22e-05, Loss=6.41e-03 BER=2.54e-03 FER=2.58e-02
2025-09-29 16:11:06,977 | INFO | Epoch 392 Train Time 61.82040476799011s

2025-09-29 16:12:07,339 | INFO | Training epoch 393, Batch 1000/1000: LR=1.20e-05, Loss=6.49e-03 BER=2.54e-03 FER=2.60e-02
2025-09-29 16:12:07,824 | INFO | Epoch 393 Train Time 60.84678316116333s

2025-09-29 16:13:09,372 | INFO | Training epoch 394, Batch 1000/1000: LR=1.18e-05, Loss=6.48e-03 BER=2.56e-03 FER=2.59e-02
2025-09-29 16:13:09,826 | INFO | Epoch 394 Train Time 61.99929189682007s

2025-09-29 16:14:12,043 | INFO | Training epoch 395, Batch 1000/1000: LR=1.16e-05, Loss=6.36e-03 BER=2.55e-03 FER=2.57e-02
2025-09-29 16:14:12,489 | INFO | Epoch 395 Train Time 62.662046909332275s

2025-09-29 16:15:13,815 | INFO | Training epoch 396, Batch 1000/1000: LR=1.14e-05, Loss=6.33e-03 BER=2.50e-03 FER=2.55e-02
2025-09-29 16:15:14,282 | INFO | Epoch 396 Train Time 61.792619466781616s

2025-09-29 16:16:14,975 | INFO | Training epoch 397, Batch 1000/1000: LR=1.12e-05, Loss=6.23e-03 BER=2.48e-03 FER=2.55e-02
2025-09-29 16:16:15,417 | INFO | Epoch 397 Train Time 61.13313961029053s

2025-09-29 16:17:15,741 | INFO | Training epoch 398, Batch 1000/1000: LR=1.10e-05, Loss=6.21e-03 BER=2.45e-03 FER=2.53e-02
2025-09-29 16:17:16,199 | INFO | Epoch 398 Train Time 60.781158685684204s

2025-09-29 16:18:17,515 | INFO | Training epoch 399, Batch 1000/1000: LR=1.08e-05, Loss=6.23e-03 BER=2.47e-03 FER=2.51e-02
2025-09-29 16:18:17,970 | INFO | Epoch 399 Train Time 61.77024269104004s

2025-09-29 16:19:19,563 | INFO | Training epoch 400, Batch 1000/1000: LR=1.06e-05, Loss=6.36e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 16:19:20,007 | INFO | Epoch 400 Train Time 62.0356388092041s

2025-09-29 16:20:22,891 | INFO | Training epoch 401, Batch 1000/1000: LR=1.05e-05, Loss=6.20e-03 BER=2.47e-03 FER=2.53e-02
2025-09-29 16:20:23,372 | INFO | Epoch 401 Train Time 63.36451530456543s

2025-09-29 16:21:24,827 | INFO | Training epoch 402, Batch 1000/1000: LR=1.03e-05, Loss=6.35e-03 BER=2.53e-03 FER=2.55e-02
2025-09-29 16:21:25,443 | INFO | Epoch 402 Train Time 62.0684916973114s

2025-09-29 16:22:25,670 | INFO | Training epoch 403, Batch 1000/1000: LR=1.01e-05, Loss=6.50e-03 BER=2.59e-03 FER=2.62e-02
2025-09-29 16:22:26,115 | INFO | Epoch 403 Train Time 60.66944599151611s

2025-09-29 16:23:25,781 | INFO | Training epoch 404, Batch 1000/1000: LR=9.91e-06, Loss=6.25e-03 BER=2.50e-03 FER=2.58e-02
2025-09-29 16:23:26,218 | INFO | Epoch 404 Train Time 60.10225796699524s

2025-09-29 16:24:26,563 | INFO | Training epoch 405, Batch 1000/1000: LR=9.74e-06, Loss=6.49e-03 BER=2.55e-03 FER=2.61e-02
2025-09-29 16:24:27,018 | INFO | Epoch 405 Train Time 60.79641628265381s

2025-09-29 16:25:28,881 | INFO | Training epoch 406, Batch 1000/1000: LR=9.56e-06, Loss=6.32e-03 BER=2.51e-03 FER=2.54e-02
2025-09-29 16:25:29,330 | INFO | Epoch 406 Train Time 62.31104588508606s

2025-09-29 16:26:31,222 | INFO | Training epoch 407, Batch 1000/1000: LR=9.39e-06, Loss=6.19e-03 BER=2.47e-03 FER=2.49e-02
2025-09-29 16:26:31,692 | INFO | Epoch 407 Train Time 62.36100697517395s

2025-09-29 16:27:32,866 | INFO | Training epoch 408, Batch 1000/1000: LR=9.21e-06, Loss=6.47e-03 BER=2.54e-03 FER=2.58e-02
2025-09-29 16:27:33,357 | INFO | Epoch 408 Train Time 61.664374351501465s

2025-09-29 16:28:34,960 | INFO | Training epoch 409, Batch 1000/1000: LR=9.04e-06, Loss=6.44e-03 BER=2.59e-03 FER=2.62e-02
2025-09-29 16:28:35,455 | INFO | Epoch 409 Train Time 62.097039461135864s

2025-09-29 16:29:40,270 | INFO | Training epoch 410, Batch 1000/1000: LR=8.87e-06, Loss=6.33e-03 BER=2.53e-03 FER=2.60e-02
2025-09-29 16:29:40,745 | INFO | Epoch 410 Train Time 65.28652620315552s

2025-09-29 16:30:41,262 | INFO | Training epoch 411, Batch 1000/1000: LR=8.71e-06, Loss=6.34e-03 BER=2.50e-03 FER=2.55e-02
2025-09-29 16:30:41,708 | INFO | Epoch 411 Train Time 60.96101760864258s

2025-09-29 16:31:42,759 | INFO | Training epoch 412, Batch 1000/1000: LR=8.54e-06, Loss=6.24e-03 BER=2.43e-03 FER=2.52e-02
2025-09-29 16:31:43,224 | INFO | Epoch 412 Train Time 61.51407265663147s

2025-09-29 16:32:44,088 | INFO | Training epoch 413, Batch 1000/1000: LR=8.38e-06, Loss=6.24e-03 BER=2.47e-03 FER=2.49e-02
2025-09-29 16:32:44,690 | INFO | Epoch 413 Train Time 61.46528601646423s

2025-09-29 16:33:45,274 | INFO | Training epoch 414, Batch 1000/1000: LR=8.21e-06, Loss=6.43e-03 BER=2.54e-03 FER=2.59e-02
2025-09-29 16:33:45,750 | INFO | Epoch 414 Train Time 61.05860233306885s

2025-09-29 16:34:46,010 | INFO | Training epoch 415, Batch 1000/1000: LR=8.05e-06, Loss=6.10e-03 BER=2.40e-03 FER=2.47e-02
2025-09-29 16:34:46,606 | INFO | Epoch 415 Train Time 60.85513210296631s

2025-09-29 16:35:48,787 | INFO | Training epoch 416, Batch 1000/1000: LR=7.89e-06, Loss=6.38e-03 BER=2.48e-03 FER=2.57e-02
2025-09-29 16:35:49,240 | INFO | Epoch 416 Train Time 62.63260817527771s

2025-09-29 16:36:49,583 | INFO | Training epoch 417, Batch 1000/1000: LR=7.74e-06, Loss=6.49e-03 BER=2.57e-03 FER=2.63e-02
2025-09-29 16:36:50,027 | INFO | Epoch 417 Train Time 60.785847663879395s

2025-09-29 16:37:51,372 | INFO | Training epoch 418, Batch 1000/1000: LR=7.58e-06, Loss=6.32e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 16:37:52,097 | INFO | Epoch 418 Train Time 62.06867575645447s

2025-09-29 16:38:52,742 | INFO | Training epoch 419, Batch 1000/1000: LR=7.43e-06, Loss=6.31e-03 BER=2.51e-03 FER=2.56e-02
2025-09-29 16:38:53,250 | INFO | Epoch 419 Train Time 61.152286767959595s

2025-09-29 16:39:53,603 | INFO | Training epoch 420, Batch 1000/1000: LR=7.27e-06, Loss=6.41e-03 BER=2.53e-03 FER=2.55e-02
2025-09-29 16:39:54,060 | INFO | Epoch 420 Train Time 60.80806231498718s

2025-09-29 16:40:54,704 | INFO | Training epoch 421, Batch 1000/1000: LR=7.12e-06, Loss=6.44e-03 BER=2.55e-03 FER=2.60e-02
2025-09-29 16:40:55,164 | INFO | Epoch 421 Train Time 61.10162973403931s

2025-09-29 16:41:55,200 | INFO | Training epoch 422, Batch 1000/1000: LR=6.97e-06, Loss=6.33e-03 BER=2.52e-03 FER=2.56e-02
2025-09-29 16:41:55,840 | INFO | Epoch 422 Train Time 60.67363381385803s

2025-09-29 16:42:57,526 | INFO | Training epoch 423, Batch 1000/1000: LR=6.83e-06, Loss=6.25e-03 BER=2.46e-03 FER=2.56e-02
2025-09-29 16:42:57,970 | INFO | Epoch 423 Train Time 62.129263162612915s

2025-09-29 16:43:58,007 | INFO | Training epoch 424, Batch 1000/1000: LR=6.68e-06, Loss=6.17e-03 BER=2.45e-03 FER=2.51e-02
2025-09-29 16:43:58,445 | INFO | Epoch 424 Train Time 60.473623752593994s

2025-09-29 16:44:59,799 | INFO | Training epoch 425, Batch 1000/1000: LR=6.54e-06, Loss=6.45e-03 BER=2.56e-03 FER=2.59e-02
2025-09-29 16:45:00,253 | INFO | Epoch 425 Train Time 61.806111574172974s

2025-09-29 16:46:01,639 | INFO | Training epoch 426, Batch 1000/1000: LR=6.40e-06, Loss=6.26e-03 BER=2.48e-03 FER=2.51e-02
2025-09-29 16:46:02,106 | INFO | Epoch 426 Train Time 61.851712703704834s

2025-09-29 16:47:03,491 | INFO | Training epoch 427, Batch 1000/1000: LR=6.25e-06, Loss=6.33e-03 BER=2.50e-03 FER=2.57e-02
2025-09-29 16:47:03,953 | INFO | Epoch 427 Train Time 61.84524321556091s

2025-09-29 16:48:04,174 | INFO | Training epoch 428, Batch 1000/1000: LR=6.12e-06, Loss=6.27e-03 BER=2.49e-03 FER=2.54e-02
2025-09-29 16:48:04,627 | INFO | Epoch 428 Train Time 60.673553705215454s

2025-09-29 16:49:07,275 | INFO | Training epoch 429, Batch 1000/1000: LR=5.98e-06, Loss=6.23e-03 BER=2.48e-03 FER=2.53e-02
2025-09-29 16:49:07,985 | INFO | Epoch 429 Train Time 63.35706281661987s

2025-09-29 16:50:09,502 | INFO | Training epoch 430, Batch 1000/1000: LR=5.84e-06, Loss=6.21e-03 BER=2.47e-03 FER=2.52e-02
2025-09-29 16:50:09,966 | INFO | Epoch 430 Train Time 61.97987604141235s

2025-09-29 16:51:10,999 | INFO | Training epoch 431, Batch 1000/1000: LR=5.71e-06, Loss=6.47e-03 BER=2.56e-03 FER=2.60e-02
2025-09-29 16:51:11,446 | INFO | Epoch 431 Train Time 61.47889280319214s

2025-09-29 16:52:13,204 | INFO | Training epoch 432, Batch 1000/1000: LR=5.58e-06, Loss=6.62e-03 BER=2.62e-03 FER=2.67e-02
2025-09-29 16:52:13,688 | INFO | Epoch 432 Train Time 62.24108266830444s

2025-09-29 16:53:16,393 | INFO | Training epoch 433, Batch 1000/1000: LR=5.45e-06, Loss=6.36e-03 BER=2.54e-03 FER=2.59e-02
2025-09-29 16:53:16,846 | INFO | Epoch 433 Train Time 63.156880378723145s

2025-09-29 16:54:18,383 | INFO | Training epoch 434, Batch 1000/1000: LR=5.32e-06, Loss=6.65e-03 BER=2.64e-03 FER=2.68e-02
2025-09-29 16:54:18,895 | INFO | Epoch 434 Train Time 62.04705572128296s

2025-09-29 16:55:20,376 | INFO | Training epoch 435, Batch 1000/1000: LR=5.20e-06, Loss=6.31e-03 BER=2.52e-03 FER=2.54e-02
2025-09-29 16:55:20,834 | INFO | Epoch 435 Train Time 61.93785548210144s

2025-09-29 16:56:23,739 | INFO | Training epoch 436, Batch 1000/1000: LR=5.07e-06, Loss=6.33e-03 BER=2.51e-03 FER=2.58e-02
2025-09-29 16:56:24,396 | INFO | Epoch 436 Train Time 63.56123900413513s

2025-09-29 16:57:26,506 | INFO | Training epoch 437, Batch 1000/1000: LR=4.95e-06, Loss=6.38e-03 BER=2.53e-03 FER=2.60e-02
2025-09-29 16:57:26,952 | INFO | Epoch 437 Train Time 62.55485558509827s

2025-09-29 16:58:29,679 | INFO | Training epoch 438, Batch 1000/1000: LR=4.83e-06, Loss=6.16e-03 BER=2.44e-03 FER=2.52e-02
2025-09-29 16:58:30,267 | INFO | Epoch 438 Train Time 63.31351399421692s

2025-09-29 16:59:34,001 | INFO | Training epoch 439, Batch 1000/1000: LR=4.71e-06, Loss=6.39e-03 BER=2.53e-03 FER=2.59e-02
2025-09-29 16:59:34,762 | INFO | Epoch 439 Train Time 64.49472856521606s

2025-09-29 17:00:39,062 | INFO | Training epoch 440, Batch 1000/1000: LR=4.59e-06, Loss=6.47e-03 BER=2.58e-03 FER=2.59e-02
2025-09-29 17:00:39,562 | INFO | Epoch 440 Train Time 64.7988657951355s

2025-09-29 17:01:46,117 | INFO | Training epoch 441, Batch 1000/1000: LR=4.48e-06, Loss=6.05e-03 BER=2.42e-03 FER=2.45e-02
2025-09-29 17:01:46,607 | INFO | Epoch 441 Train Time 67.0436737537384s

2025-09-29 17:02:50,654 | INFO | Training epoch 442, Batch 1000/1000: LR=4.36e-06, Loss=6.05e-03 BER=2.38e-03 FER=2.45e-02
2025-09-29 17:02:51,119 | INFO | Epoch 442 Train Time 64.51063251495361s

2025-09-29 17:03:54,689 | INFO | Training epoch 443, Batch 1000/1000: LR=4.25e-06, Loss=6.42e-03 BER=2.55e-03 FER=2.60e-02
2025-09-29 17:03:55,138 | INFO | Epoch 443 Train Time 64.01724076271057s

2025-09-29 17:04:59,862 | INFO | Training epoch 444, Batch 1000/1000: LR=4.14e-06, Loss=6.33e-03 BER=2.52e-03 FER=2.56e-02
2025-09-29 17:05:00,329 | INFO | Epoch 444 Train Time 65.1894690990448s

2025-09-29 17:06:04,973 | INFO | Training epoch 445, Batch 1000/1000: LR=4.03e-06, Loss=6.31e-03 BER=2.48e-03 FER=2.53e-02
2025-09-29 17:06:05,529 | INFO | Epoch 445 Train Time 65.198810338974s

2025-09-29 17:07:10,099 | INFO | Training epoch 446, Batch 1000/1000: LR=3.93e-06, Loss=6.29e-03 BER=2.51e-03 FER=2.54e-02
2025-09-29 17:07:10,562 | INFO | Epoch 446 Train Time 65.03041791915894s

2025-09-29 17:08:15,077 | INFO | Training epoch 447, Batch 1000/1000: LR=3.82e-06, Loss=6.46e-03 BER=2.53e-03 FER=2.56e-02
2025-09-29 17:08:15,589 | INFO | Epoch 447 Train Time 65.02623224258423s

2025-09-29 17:09:19,958 | INFO | Training epoch 448, Batch 1000/1000: LR=3.72e-06, Loss=6.18e-03 BER=2.43e-03 FER=2.47e-02
2025-09-29 17:09:20,423 | INFO | Epoch 448 Train Time 64.8314802646637s

2025-09-29 17:10:25,148 | INFO | Training epoch 449, Batch 1000/1000: LR=3.62e-06, Loss=6.38e-03 BER=2.51e-03 FER=2.57e-02
2025-09-29 17:10:25,617 | INFO | Epoch 449 Train Time 65.1926121711731s

2025-09-29 17:11:29,739 | INFO | Training epoch 450, Batch 1000/1000: LR=3.52e-06, Loss=6.54e-03 BER=2.64e-03 FER=2.66e-02
2025-09-29 17:11:30,202 | INFO | Epoch 450 Train Time 64.58350491523743s

2025-09-29 17:12:36,039 | INFO | Training epoch 451, Batch 1000/1000: LR=3.42e-06, Loss=6.28e-03 BER=2.49e-03 FER=2.54e-02
2025-09-29 17:12:36,501 | INFO | Epoch 451 Train Time 66.29777240753174s

2025-09-29 17:13:43,178 | INFO | Training epoch 452, Batch 1000/1000: LR=3.33e-06, Loss=6.38e-03 BER=2.53e-03 FER=2.56e-02
2025-09-29 17:13:43,642 | INFO | Epoch 452 Train Time 67.13812470436096s

2025-09-29 17:14:50,455 | INFO | Training epoch 453, Batch 1000/1000: LR=3.23e-06, Loss=6.19e-03 BER=2.45e-03 FER=2.49e-02
2025-09-29 17:14:50,920 | INFO | Epoch 453 Train Time 67.27763843536377s

2025-09-29 17:15:56,575 | INFO | Training epoch 454, Batch 1000/1000: LR=3.14e-06, Loss=6.25e-03 BER=2.50e-03 FER=2.52e-02
2025-09-29 17:15:57,028 | INFO | Epoch 454 Train Time 66.10729837417603s

2025-09-29 17:17:02,950 | INFO | Training epoch 455, Batch 1000/1000: LR=3.05e-06, Loss=6.33e-03 BER=2.51e-03 FER=2.56e-02
2025-09-29 17:17:03,393 | INFO | Epoch 455 Train Time 66.36306428909302s

2025-09-29 17:18:09,629 | INFO | Training epoch 456, Batch 1000/1000: LR=2.97e-06, Loss=6.38e-03 BER=2.52e-03 FER=2.57e-02
2025-09-29 17:18:10,085 | INFO | Epoch 456 Train Time 66.69116830825806s

2025-09-29 17:19:18,822 | INFO | Training epoch 457, Batch 1000/1000: LR=2.88e-06, Loss=6.27e-03 BER=2.49e-03 FER=2.56e-02
2025-09-29 17:19:19,289 | INFO | Epoch 457 Train Time 69.20369362831116s

2025-09-29 17:20:28,304 | INFO | Training epoch 458, Batch 1000/1000: LR=2.80e-06, Loss=6.33e-03 BER=2.50e-03 FER=2.56e-02
2025-09-29 17:20:28,778 | INFO | Epoch 458 Train Time 69.48665118217468s

2025-09-29 17:21:37,086 | INFO | Training epoch 459, Batch 1000/1000: LR=2.71e-06, Loss=6.13e-03 BER=2.42e-03 FER=2.49e-02
2025-09-29 17:21:37,734 | INFO | Epoch 459 Train Time 68.95506000518799s

2025-09-29 17:22:44,758 | INFO | Training epoch 460, Batch 1000/1000: LR=2.63e-06, Loss=6.60e-03 BER=2.61e-03 FER=2.64e-02
2025-09-29 17:22:45,221 | INFO | Epoch 460 Train Time 67.48437857627869s

2025-09-29 17:23:52,878 | INFO | Training epoch 461, Batch 1000/1000: LR=2.56e-06, Loss=6.41e-03 BER=2.54e-03 FER=2.59e-02
2025-09-29 17:23:53,361 | INFO | Epoch 461 Train Time 68.1399474143982s

2025-09-29 17:25:00,291 | INFO | Training epoch 462, Batch 1000/1000: LR=2.48e-06, Loss=6.59e-03 BER=2.62e-03 FER=2.64e-02
2025-09-29 17:25:00,752 | INFO | Epoch 462 Train Time 67.38907814025879s

2025-09-29 17:26:06,911 | INFO | Training epoch 463, Batch 1000/1000: LR=2.40e-06, Loss=6.33e-03 BER=2.50e-03 FER=2.60e-02
2025-09-29 17:26:07,372 | INFO | Epoch 463 Train Time 66.61897659301758s

2025-09-29 17:27:14,602 | INFO | Training epoch 464, Batch 1000/1000: LR=2.33e-06, Loss=6.25e-03 BER=2.47e-03 FER=2.54e-02
2025-09-29 17:27:15,073 | INFO | Epoch 464 Train Time 67.70012927055359s

2025-09-29 17:28:23,231 | INFO | Training epoch 465, Batch 1000/1000: LR=2.26e-06, Loss=6.40e-03 BER=2.53e-03 FER=2.58e-02
2025-09-29 17:28:23,709 | INFO | Epoch 465 Train Time 68.63568425178528s

2025-09-29 17:29:41,892 | INFO | Training epoch 466, Batch 1000/1000: LR=2.19e-06, Loss=6.35e-03 BER=2.52e-03 FER=2.55e-02
2025-09-29 17:29:43,672 | INFO | Epoch 466 Train Time 79.96150302886963s

2025-09-29 17:31:03,093 | INFO | Training epoch 467, Batch 1000/1000: LR=2.13e-06, Loss=6.35e-03 BER=2.52e-03 FER=2.57e-02
2025-09-29 17:31:03,618 | INFO | Epoch 467 Train Time 79.93979692459106s

2025-09-29 17:32:42,826 | INFO | Training epoch 468, Batch 1000/1000: LR=2.06e-06, Loss=6.38e-03 BER=2.53e-03 FER=2.60e-02
2025-09-29 17:32:43,363 | INFO | Epoch 468 Train Time 99.74451327323914s

2025-09-29 17:33:52,120 | INFO | Training epoch 469, Batch 1000/1000: LR=2.00e-06, Loss=6.11e-03 BER=2.41e-03 FER=2.49e-02
2025-09-29 17:33:52,637 | INFO | Epoch 469 Train Time 69.27186822891235s

2025-09-29 17:34:57,134 | INFO | Training epoch 470, Batch 1000/1000: LR=1.94e-06, Loss=6.18e-03 BER=2.45e-03 FER=2.51e-02
2025-09-29 17:34:57,642 | INFO | Epoch 470 Train Time 65.00331425666809s

2025-09-29 17:36:02,757 | INFO | Training epoch 471, Batch 1000/1000: LR=1.88e-06, Loss=6.05e-03 BER=2.38e-03 FER=2.47e-02
2025-09-29 17:36:03,232 | INFO | Epoch 471 Train Time 65.58904552459717s

2025-09-29 17:37:08,635 | INFO | Training epoch 472, Batch 1000/1000: LR=1.82e-06, Loss=6.29e-03 BER=2.48e-03 FER=2.52e-02
2025-09-29 17:37:09,126 | INFO | Epoch 472 Train Time 65.89229774475098s

2025-09-29 17:38:14,967 | INFO | Training epoch 473, Batch 1000/1000: LR=1.76e-06, Loss=6.16e-03 BER=2.44e-03 FER=2.51e-02
2025-09-29 17:38:15,452 | INFO | Epoch 473 Train Time 66.32299327850342s

2025-09-29 17:39:20,343 | INFO | Training epoch 474, Batch 1000/1000: LR=1.71e-06, Loss=6.30e-03 BER=2.46e-03 FER=2.52e-02
2025-09-29 17:39:20,813 | INFO | Epoch 474 Train Time 65.3603208065033s

2025-09-29 17:40:25,065 | INFO | Training epoch 475, Batch 1000/1000: LR=1.66e-06, Loss=6.31e-03 BER=2.49e-03 FER=2.58e-02
2025-09-29 17:40:25,544 | INFO | Epoch 475 Train Time 64.73022699356079s

2025-09-29 17:41:31,710 | INFO | Training epoch 476, Batch 1000/1000: LR=1.61e-06, Loss=6.60e-03 BER=2.61e-03 FER=2.68e-02
2025-09-29 17:41:32,189 | INFO | Epoch 476 Train Time 66.64403915405273s

2025-09-29 17:42:38,589 | INFO | Training epoch 477, Batch 1000/1000: LR=1.56e-06, Loss=6.42e-03 BER=2.56e-03 FER=2.58e-02
2025-09-29 17:42:39,098 | INFO | Epoch 477 Train Time 66.90790605545044s

2025-09-29 17:43:45,876 | INFO | Training epoch 478, Batch 1000/1000: LR=1.52e-06, Loss=6.29e-03 BER=2.47e-03 FER=2.54e-02
2025-09-29 17:43:46,388 | INFO | Epoch 478 Train Time 67.28746199607849s

2025-09-29 17:44:55,192 | INFO | Training epoch 479, Batch 1000/1000: LR=1.47e-06, Loss=6.25e-03 BER=2.46e-03 FER=2.53e-02
2025-09-29 17:44:55,676 | INFO | Epoch 479 Train Time 69.28627967834473s

2025-09-29 17:46:01,832 | INFO | Training epoch 480, Batch 1000/1000: LR=1.43e-06, Loss=6.18e-03 BER=2.41e-03 FER=2.47e-02
2025-09-29 17:46:02,335 | INFO | Epoch 480 Train Time 66.65808773040771s

2025-09-29 17:47:09,295 | INFO | Training epoch 481, Batch 1000/1000: LR=1.39e-06, Loss=6.28e-03 BER=2.49e-03 FER=2.50e-02
2025-09-29 17:47:09,770 | INFO | Epoch 481 Train Time 67.43314003944397s

2025-09-29 17:48:15,868 | INFO | Training epoch 482, Batch 1000/1000: LR=1.35e-06, Loss=6.19e-03 BER=2.48e-03 FER=2.54e-02
2025-09-29 17:48:16,386 | INFO | Epoch 482 Train Time 66.61519265174866s

2025-09-29 17:49:25,377 | INFO | Training epoch 483, Batch 1000/1000: LR=1.32e-06, Loss=6.55e-03 BER=2.59e-03 FER=2.60e-02
2025-09-29 17:49:25,838 | INFO | Epoch 483 Train Time 69.44994688034058s

2025-09-29 17:50:33,746 | INFO | Training epoch 484, Batch 1000/1000: LR=1.28e-06, Loss=6.24e-03 BER=2.47e-03 FER=2.55e-02
2025-09-29 17:50:34,213 | INFO | Epoch 484 Train Time 68.37392902374268s

2025-09-29 17:51:40,298 | INFO | Training epoch 485, Batch 1000/1000: LR=1.25e-06, Loss=6.13e-03 BER=2.47e-03 FER=2.54e-02
2025-09-29 17:51:40,761 | INFO | Epoch 485 Train Time 66.54639220237732s

2025-09-29 17:52:46,726 | INFO | Training epoch 486, Batch 1000/1000: LR=1.22e-06, Loss=6.14e-03 BER=2.42e-03 FER=2.46e-02
2025-09-29 17:52:47,220 | INFO | Epoch 486 Train Time 66.458256483078s

2025-09-29 17:53:53,865 | INFO | Training epoch 487, Batch 1000/1000: LR=1.19e-06, Loss=6.26e-03 BER=2.46e-03 FER=2.51e-02
2025-09-29 17:53:54,328 | INFO | Epoch 487 Train Time 67.10682034492493s

2025-09-29 17:55:01,047 | INFO | Training epoch 488, Batch 1000/1000: LR=1.17e-06, Loss=6.14e-03 BER=2.42e-03 FER=2.49e-02
2025-09-29 17:55:01,650 | INFO | Epoch 488 Train Time 67.32084012031555s

2025-09-29 17:56:09,431 | INFO | Training epoch 489, Batch 1000/1000: LR=1.14e-06, Loss=6.18e-03 BER=2.46e-03 FER=2.49e-02
2025-09-29 17:56:09,985 | INFO | Epoch 489 Train Time 68.33453106880188s

2025-09-29 17:57:20,532 | INFO | Training epoch 490, Batch 1000/1000: LR=1.12e-06, Loss=6.45e-03 BER=2.58e-03 FER=2.63e-02
2025-09-29 17:57:21,083 | INFO | Epoch 490 Train Time 71.09706401824951s

2025-09-29 17:58:31,100 | INFO | Training epoch 491, Batch 1000/1000: LR=1.10e-06, Loss=6.35e-03 BER=2.53e-03 FER=2.58e-02
2025-09-29 17:58:31,575 | INFO | Epoch 491 Train Time 70.49107599258423s

2025-09-29 17:59:40,302 | INFO | Training epoch 492, Batch 1000/1000: LR=1.08e-06, Loss=6.29e-03 BER=2.49e-03 FER=2.53e-02
2025-09-29 17:59:40,782 | INFO | Epoch 492 Train Time 69.20447444915771s

2025-09-29 18:00:50,201 | INFO | Training epoch 493, Batch 1000/1000: LR=1.06e-06, Loss=6.43e-03 BER=2.54e-03 FER=2.62e-02
2025-09-29 18:00:50,733 | INFO | Epoch 493 Train Time 69.94184684753418s

2025-09-29 18:01:59,531 | INFO | Training epoch 494, Batch 1000/1000: LR=1.05e-06, Loss=6.21e-03 BER=2.45e-03 FER=2.51e-02
2025-09-29 18:01:59,986 | INFO | Epoch 494 Train Time 69.25208377838135s

2025-09-29 18:03:07,343 | INFO | Training epoch 495, Batch 1000/1000: LR=1.04e-06, Loss=6.28e-03 BER=2.49e-03 FER=2.50e-02
2025-09-29 18:03:07,806 | INFO | Epoch 495 Train Time 67.8194887638092s

2025-09-29 18:04:16,411 | INFO | Training epoch 496, Batch 1000/1000: LR=1.02e-06, Loss=6.31e-03 BER=2.52e-03 FER=2.53e-02
2025-09-29 18:04:16,872 | INFO | Epoch 496 Train Time 69.06414723396301s

2025-09-29 18:05:28,468 | INFO | Training epoch 497, Batch 1000/1000: LR=1.02e-06, Loss=6.27e-03 BER=2.48e-03 FER=2.53e-02
2025-09-29 18:05:29,042 | INFO | Epoch 497 Train Time 72.16918706893921s

2025-09-29 18:06:40,431 | INFO | Training epoch 498, Batch 1000/1000: LR=1.01e-06, Loss=6.48e-03 BER=2.57e-03 FER=2.60e-02
2025-09-29 18:06:40,945 | INFO | Epoch 498 Train Time 71.90178680419922s

2025-09-29 18:07:51,471 | INFO | Training epoch 499, Batch 1000/1000: LR=1.00e-06, Loss=6.42e-03 BER=2.56e-03 FER=2.60e-02
2025-09-29 18:07:51,954 | INFO | Epoch 499 Train Time 71.00779032707214s

2025-09-29 18:09:00,399 | INFO | Training epoch 500, Batch 1000/1000: LR=1.00e-06, Loss=6.14e-03 BER=2.43e-03 FER=2.51e-02
2025-09-29 18:09:00,948 | INFO | Epoch 500 Train Time 68.99267101287842s

2025-09-29 18:09:00,972 | INFO | Checkpoint saved: runs\20250929_092917\stage1_fp32__LDPC_n49_k24__Ndec6_d64_h8.pth
2025-09-29 18:09:00,995 | INFO | Checkpoint saved: runs\20250929_092917\stage1_fp32__LDPC_n49_k24__Ndec6_d64_h8__e500_loss0.006139.pth
2025-09-29 18:10:10,968 | INFO | FER count threshold reached for EbN0:4
2025-09-29 18:10:11,538 | INFO | Test EbN0=4, BER=2.47e-03
2025-09-29 18:11:19,929 | INFO | FER count threshold reached for EbN0:5
2025-09-29 18:11:20,417 | INFO | Test EbN0=5, BER=1.81e-04
2025-09-29 18:22:38,776 | INFO | FER count threshold reached for EbN0:6
2025-09-29 18:22:39,231 | INFO | Test EbN0=6, BER=6.74e-06
2025-09-29 18:22:39,231 | INFO | 
Test Loss 4: 6.4337e-03 5: 5.1746e-04 6: 2.1148e-05
2025-09-29 18:22:39,231 | INFO | Test FER 4: 2.6776e-02 5: 2.3617e-03 6: 1.0132e-04
2025-09-29 18:22:39,232 | INFO | Test BER 4: 2.4695e-03 5: 1.8120e-04 6: 6.7354e-06
2025-09-29 18:22:39,233 | INFO | Test -ln(BER) 4: 6.0038e+00 5: 8.6159e+00 6: 1.1908e+01
2025-09-29 18:22:39,233 | INFO | # of testing samples: [100352.0, 100352.0, 996864.0]
 Test Time 818.2379367351532 s

2025-09-29 18:22:39,455 | INFO | Loaded checkpoint: runs\20250929_092917\stage1_fp32__LDPC_n49_k24__Ndec6_d64_h8.pth (strict=False)
2025-09-29 18:24:53,161 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=2.92e-02 BER=1.02e-02 FER=1.74e-01
2025-09-29 18:24:53,634 | INFO | Epoch 1 Train Time 134.1775724887848s

2025-09-29 18:24:53,635 | INFO | [P2] saving best_model (QAT) with loss 0.029174 at epoch 1
2025-09-29 18:27:09,909 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=1.56e-02 BER=5.64e-03 FER=8.00e-02
2025-09-29 18:27:10,377 | INFO | Epoch 2 Train Time 136.6796395778656s

2025-09-29 18:27:10,378 | INFO | [P2] saving best_model (QAT) with loss 0.015622 at epoch 2
2025-09-29 18:29:27,422 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=1.28e-02 BER=4.65e-03 FER=6.01e-02
2025-09-29 18:29:27,911 | INFO | Epoch 3 Train Time 137.47110152244568s

2025-09-29 18:29:27,912 | INFO | [P2] saving best_model (QAT) with loss 0.012758 at epoch 3
2025-09-29 18:31:47,531 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=1.16e-02 BER=4.29e-03 FER=5.22e-02
2025-09-29 18:31:48,091 | INFO | Epoch 4 Train Time 140.11391830444336s

2025-09-29 18:31:48,092 | INFO | [P2] saving best_model (QAT) with loss 0.011581 at epoch 4
2025-09-29 18:34:03,182 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=1.10e-02 BER=4.08e-03 FER=4.83e-02
2025-09-29 18:34:03,642 | INFO | Epoch 5 Train Time 135.47744250297546s

2025-09-29 18:34:03,643 | INFO | [P2] saving best_model (QAT) with loss 0.011012 at epoch 5
2025-09-29 18:36:18,873 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=1.04e-02 BER=3.89e-03 FER=4.60e-02
2025-09-29 18:36:19,352 | INFO | Epoch 6 Train Time 135.63138914108276s

2025-09-29 18:36:19,353 | INFO | [P2] saving best_model (QAT) with loss 0.010427 at epoch 6
2025-09-29 18:38:48,582 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=9.76e-03 BER=3.67e-03 FER=4.28e-02
2025-09-29 18:38:49,034 | INFO | Epoch 7 Train Time 149.6029016971588s

2025-09-29 18:38:49,035 | INFO | [P2] saving best_model (QAT) with loss 0.009756 at epoch 7
2025-09-29 18:41:01,911 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=9.80e-03 BER=3.72e-03 FER=4.25e-02
2025-09-29 18:41:02,376 | INFO | Epoch 8 Train Time 133.27671027183533s

2025-09-29 18:43:15,207 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=9.21e-03 BER=3.52e-03 FER=4.06e-02
2025-09-29 18:43:15,657 | INFO | Epoch 9 Train Time 133.27971172332764s

2025-09-29 18:43:15,658 | INFO | [P2] saving best_model (QAT) with loss 0.009213 at epoch 9
2025-09-29 18:45:29,570 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=9.13e-03 BER=3.50e-03 FER=3.99e-02
2025-09-29 18:45:30,021 | INFO | Epoch 10 Train Time 134.2992763519287s

2025-09-29 18:45:30,022 | INFO | [P2] saving best_model (QAT) with loss 0.009131 at epoch 10
2025-09-29 18:47:42,013 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=9.06e-03 BER=3.52e-03 FER=3.98e-02
2025-09-29 18:47:42,476 | INFO | Epoch 11 Train Time 132.3925280570984s

2025-09-29 18:47:42,477 | INFO | [P2] saving best_model (QAT) with loss 0.009065 at epoch 11
2025-09-29 18:49:55,308 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=8.92e-03 BER=3.43e-03 FER=3.87e-02
2025-09-29 18:49:55,762 | INFO | Epoch 12 Train Time 133.22561407089233s

2025-09-29 18:49:55,764 | INFO | [P2] saving best_model (QAT) with loss 0.008918 at epoch 12
2025-09-29 18:52:08,474 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=8.73e-03 BER=3.37e-03 FER=3.73e-02
2025-09-29 18:52:08,918 | INFO | Epoch 13 Train Time 133.0912640094757s

2025-09-29 18:52:08,919 | INFO | [P2] saving best_model (QAT) with loss 0.008731 at epoch 13
2025-09-29 18:54:22,118 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=8.73e-03 BER=3.36e-03 FER=3.75e-02
2025-09-29 18:54:22,591 | INFO | Epoch 14 Train Time 133.60354018211365s

2025-09-29 18:54:22,591 | INFO | [P2] saving best_model (QAT) with loss 0.008727 at epoch 14
2025-09-29 18:56:34,717 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=8.51e-03 BER=3.29e-03 FER=3.69e-02
2025-09-29 18:56:35,170 | INFO | Epoch 15 Train Time 132.52047038078308s

2025-09-29 18:56:35,171 | INFO | [P2] saving best_model (QAT) with loss 0.008513 at epoch 15
2025-09-29 18:58:50,032 | INFO | Training epoch 16, Batch 1000/1000: LR=1.00e-04, Loss=8.45e-03 BER=3.28e-03 FER=3.64e-02
2025-09-29 18:58:50,511 | INFO | Epoch 16 Train Time 135.27126502990723s

2025-09-29 18:58:50,511 | INFO | [P2] saving best_model (QAT) with loss 0.008453 at epoch 16
2025-09-29 19:01:01,742 | INFO | Training epoch 17, Batch 1000/1000: LR=1.00e-04, Loss=8.44e-03 BER=3.26e-03 FER=3.56e-02
2025-09-29 19:01:02,208 | INFO | Epoch 17 Train Time 131.6365351676941s

2025-09-29 19:01:02,208 | INFO | [P2] saving best_model (QAT) with loss 0.008443 at epoch 17
2025-09-29 19:03:15,049 | INFO | Training epoch 18, Batch 1000/1000: LR=1.00e-04, Loss=8.41e-03 BER=3.29e-03 FER=3.58e-02
2025-09-29 19:03:15,498 | INFO | Epoch 18 Train Time 133.22964787483215s

2025-09-29 19:03:15,500 | INFO | [P2] saving best_model (QAT) with loss 0.008412 at epoch 18
2025-09-29 19:05:27,166 | INFO | Training epoch 19, Batch 1000/1000: LR=1.00e-04, Loss=8.33e-03 BER=3.20e-03 FER=3.53e-02
2025-09-29 19:05:27,625 | INFO | Epoch 19 Train Time 132.0573353767395s

2025-09-29 19:05:27,625 | INFO | [P2] saving best_model (QAT) with loss 0.008327 at epoch 19
2025-09-29 19:07:40,982 | INFO | Training epoch 20, Batch 1000/1000: LR=1.00e-04, Loss=8.33e-03 BER=3.23e-03 FER=3.56e-02
2025-09-29 19:07:41,442 | INFO | Epoch 20 Train Time 133.74884748458862s

2025-09-29 19:07:41,443 | INFO | [P2] saving best_model (QAT) with loss 0.008325 at epoch 20
2025-09-29 19:10:01,810 | INFO | Training epoch 21, Batch 1000/1000: LR=1.00e-04, Loss=8.32e-03 BER=3.22e-03 FER=3.54e-02
2025-09-29 19:10:02,298 | INFO | Epoch 21 Train Time 140.77988076210022s

2025-09-29 19:10:02,299 | INFO | [P2] saving best_model (QAT) with loss 0.008319 at epoch 21
2025-09-29 19:12:25,007 | INFO | Training epoch 22, Batch 1000/1000: LR=1.00e-04, Loss=8.51e-03 BER=3.33e-03 FER=3.63e-02
2025-09-29 19:12:25,485 | INFO | Epoch 22 Train Time 143.11138200759888s

2025-09-29 19:14:37,971 | INFO | Training epoch 23, Batch 1000/1000: LR=9.99e-05, Loss=8.22e-03 BER=3.21e-03 FER=3.51e-02
2025-09-29 19:14:38,398 | INFO | Epoch 23 Train Time 132.91103315353394s

2025-09-29 19:14:38,398 | INFO | [P2] saving best_model (QAT) with loss 0.008219 at epoch 23
2025-09-29 19:16:52,320 | INFO | Training epoch 24, Batch 1000/1000: LR=9.99e-05, Loss=8.27e-03 BER=3.23e-03 FER=3.57e-02
2025-09-29 19:16:52,773 | INFO | Epoch 24 Train Time 134.302499294281s

2025-09-29 19:19:09,043 | INFO | Training epoch 25, Batch 1000/1000: LR=9.99e-05, Loss=8.29e-03 BER=3.24e-03 FER=3.56e-02
2025-09-29 19:19:09,520 | INFO | Epoch 25 Train Time 136.74476146697998s

2025-09-29 19:21:23,068 | INFO | Training epoch 26, Batch 1000/1000: LR=9.99e-05, Loss=8.14e-03 BER=3.17e-03 FER=3.45e-02
2025-09-29 19:21:23,534 | INFO | Epoch 26 Train Time 134.01245522499084s

2025-09-29 19:21:23,535 | INFO | [P2] saving best_model (QAT) with loss 0.008138 at epoch 26
2025-09-29 19:23:36,729 | INFO | Training epoch 27, Batch 1000/1000: LR=9.99e-05, Loss=8.12e-03 BER=3.14e-03 FER=3.47e-02
2025-09-29 19:23:37,176 | INFO | Epoch 27 Train Time 133.56145524978638s

2025-09-29 19:23:37,177 | INFO | [P2] saving best_model (QAT) with loss 0.008116 at epoch 27
2025-09-29 19:25:50,471 | INFO | Training epoch 28, Batch 1000/1000: LR=9.99e-05, Loss=8.22e-03 BER=3.18e-03 FER=3.49e-02
2025-09-29 19:25:50,942 | INFO | Epoch 28 Train Time 133.7018325328827s

2025-09-29 19:28:04,982 | INFO | Training epoch 29, Batch 1000/1000: LR=9.99e-05, Loss=8.15e-03 BER=3.16e-03 FER=3.46e-02
2025-09-29 19:28:05,463 | INFO | Epoch 29 Train Time 134.51985955238342s

2025-09-29 19:30:21,810 | INFO | Training epoch 30, Batch 1000/1000: LR=9.99e-05, Loss=8.04e-03 BER=3.14e-03 FER=3.39e-02
2025-09-29 19:30:22,369 | INFO | Epoch 30 Train Time 136.90428137779236s

2025-09-29 19:30:22,370 | INFO | [P2] saving best_model (QAT) with loss 0.008042 at epoch 30
2025-09-29 19:32:38,050 | INFO | Training epoch 31, Batch 1000/1000: LR=9.99e-05, Loss=8.12e-03 BER=3.15e-03 FER=3.45e-02
2025-09-29 19:32:38,535 | INFO | Epoch 31 Train Time 136.07793974876404s

2025-09-29 19:34:53,220 | INFO | Training epoch 32, Batch 1000/1000: LR=9.99e-05, Loss=7.71e-03 BER=3.00e-03 FER=3.30e-02
2025-09-29 19:34:53,660 | INFO | Epoch 32 Train Time 135.12302422523499s

2025-09-29 19:34:53,661 | INFO | [P2] saving best_model (QAT) with loss 0.007709 at epoch 32
2025-09-29 19:37:07,354 | INFO | Training epoch 33, Batch 1000/1000: LR=9.99e-05, Loss=8.17e-03 BER=3.19e-03 FER=3.47e-02
2025-09-29 19:37:07,813 | INFO | Epoch 33 Train Time 134.08663034439087s

2025-09-29 19:39:22,666 | INFO | Training epoch 34, Batch 1000/1000: LR=9.99e-05, Loss=8.23e-03 BER=3.21e-03 FER=3.49e-02
2025-09-29 19:39:23,111 | INFO | Epoch 34 Train Time 135.29675316810608s

2025-09-29 19:41:39,001 | INFO | Training epoch 35, Batch 1000/1000: LR=9.99e-05, Loss=8.00e-03 BER=3.15e-03 FER=3.41e-02
2025-09-29 19:41:39,495 | INFO | Epoch 35 Train Time 136.3827347755432s

2025-09-29 19:44:07,396 | INFO | Training epoch 36, Batch 1000/1000: LR=9.99e-05, Loss=8.03e-03 BER=3.12e-03 FER=3.42e-02
2025-09-29 19:44:07,849 | INFO | Epoch 36 Train Time 148.35284757614136s

2025-09-29 19:46:20,444 | INFO | Training epoch 37, Batch 1000/1000: LR=9.99e-05, Loss=7.94e-03 BER=3.08e-03 FER=3.35e-02
2025-09-29 19:46:20,907 | INFO | Epoch 37 Train Time 133.05780744552612s

2025-09-29 19:48:35,747 | INFO | Training epoch 38, Batch 1000/1000: LR=9.99e-05, Loss=8.12e-03 BER=3.16e-03 FER=3.45e-02
2025-09-29 19:48:36,202 | INFO | Epoch 38 Train Time 135.29344987869263s

2025-09-29 19:50:49,240 | INFO | Training epoch 39, Batch 1000/1000: LR=9.98e-05, Loss=8.15e-03 BER=3.16e-03 FER=3.44e-02
2025-09-29 19:50:49,727 | INFO | Epoch 39 Train Time 133.52293968200684s

2025-09-29 19:53:04,157 | INFO | Training epoch 40, Batch 1000/1000: LR=9.98e-05, Loss=7.91e-03 BER=3.07e-03 FER=3.34e-02
2025-09-29 19:53:04,620 | INFO | Epoch 40 Train Time 134.89170336723328s

2025-09-29 19:55:18,585 | INFO | Training epoch 41, Batch 1000/1000: LR=9.98e-05, Loss=7.93e-03 BER=3.09e-03 FER=3.37e-02
2025-09-29 19:55:19,038 | INFO | Epoch 41 Train Time 134.41771578788757s

2025-09-29 19:57:33,214 | INFO | Training epoch 42, Batch 1000/1000: LR=9.98e-05, Loss=7.80e-03 BER=3.05e-03 FER=3.37e-02
2025-09-29 19:57:33,668 | INFO | Epoch 42 Train Time 134.62871146202087s

2025-09-29 19:59:47,509 | INFO | Training epoch 43, Batch 1000/1000: LR=9.98e-05, Loss=8.01e-03 BER=3.13e-03 FER=3.41e-02
2025-09-29 19:59:47,964 | INFO | Epoch 43 Train Time 134.29490327835083s

2025-09-29 20:02:01,880 | INFO | Training epoch 44, Batch 1000/1000: LR=9.98e-05, Loss=8.00e-03 BER=3.10e-03 FER=3.35e-02
2025-09-29 20:02:02,341 | INFO | Epoch 44 Train Time 134.3761866092682s

2025-09-29 20:04:15,757 | INFO | Training epoch 45, Batch 1000/1000: LR=9.98e-05, Loss=7.94e-03 BER=3.11e-03 FER=3.35e-02
2025-09-29 20:04:16,216 | INFO | Epoch 45 Train Time 133.8738512992859s

2025-09-29 20:06:30,367 | INFO | Training epoch 46, Batch 1000/1000: LR=9.98e-05, Loss=8.00e-03 BER=3.12e-03 FER=3.38e-02
2025-09-29 20:06:30,829 | INFO | Epoch 46 Train Time 134.61212134361267s

2025-09-29 20:08:44,347 | INFO | Training epoch 47, Batch 1000/1000: LR=9.98e-05, Loss=7.94e-03 BER=3.11e-03 FER=3.37e-02
2025-09-29 20:08:44,797 | INFO | Epoch 47 Train Time 133.9659128189087s

2025-09-29 20:10:58,456 | INFO | Training epoch 48, Batch 1000/1000: LR=9.98e-05, Loss=7.83e-03 BER=3.06e-03 FER=3.35e-02
2025-09-29 20:10:58,916 | INFO | Epoch 48 Train Time 134.11797308921814s

2025-09-29 20:13:12,731 | INFO | Training epoch 49, Batch 1000/1000: LR=9.98e-05, Loss=8.01e-03 BER=3.12e-03 FER=3.34e-02
2025-09-29 20:13:13,183 | INFO | Epoch 49 Train Time 134.26492619514465s

2025-09-29 20:15:40,973 | INFO | Training epoch 50, Batch 1000/1000: LR=9.97e-05, Loss=7.94e-03 BER=3.13e-03 FER=3.37e-02
2025-09-29 20:15:41,467 | INFO | Epoch 50 Train Time 148.28209829330444s

2025-09-29 20:17:54,461 | INFO | Training epoch 51, Batch 1000/1000: LR=9.97e-05, Loss=8.27e-03 BER=3.20e-03 FER=3.45e-02
2025-09-29 20:17:54,922 | INFO | Epoch 51 Train Time 133.45361351966858s

2025-09-29 20:20:10,770 | INFO | Training epoch 52, Batch 1000/1000: LR=9.97e-05, Loss=8.23e-03 BER=3.23e-03 FER=3.51e-02
2025-09-29 20:20:11,237 | INFO | Epoch 52 Train Time 136.31277108192444s

2025-09-29 20:22:24,014 | INFO | Training epoch 53, Batch 1000/1000: LR=9.97e-05, Loss=8.19e-03 BER=3.18e-03 FER=3.49e-02
2025-09-29 20:22:24,458 | INFO | Epoch 53 Train Time 133.22018790245056s

2025-09-29 20:24:37,639 | INFO | Training epoch 54, Batch 1000/1000: LR=9.97e-05, Loss=7.77e-03 BER=3.03e-03 FER=3.31e-02
2025-09-29 20:24:38,092 | INFO | Epoch 54 Train Time 133.63256192207336s

2025-09-29 20:26:49,428 | INFO | Training epoch 55, Batch 1000/1000: LR=9.97e-05, Loss=7.92e-03 BER=3.06e-03 FER=3.34e-02
2025-09-29 20:26:49,906 | INFO | Epoch 55 Train Time 131.8122661113739s

2025-09-29 20:29:02,408 | INFO | Training epoch 56, Batch 1000/1000: LR=9.97e-05, Loss=7.71e-03 BER=2.99e-03 FER=3.28e-02
2025-09-29 20:29:02,875 | INFO | Epoch 56 Train Time 132.96737146377563s

2025-09-29 20:31:18,734 | INFO | Training epoch 57, Batch 1000/1000: LR=9.97e-05, Loss=8.10e-03 BER=3.11e-03 FER=3.38e-02
2025-09-29 20:31:19,182 | INFO | Epoch 57 Train Time 136.30668878555298s

2025-09-29 20:33:31,880 | INFO | Training epoch 58, Batch 1000/1000: LR=9.96e-05, Loss=7.98e-03 BER=3.09e-03 FER=3.33e-02
2025-09-29 20:33:32,339 | INFO | Epoch 58 Train Time 133.15600872039795s

2025-09-29 20:35:44,575 | INFO | Training epoch 59, Batch 1000/1000: LR=9.96e-05, Loss=8.07e-03 BER=3.11e-03 FER=3.41e-02
2025-09-29 20:35:45,048 | INFO | Epoch 59 Train Time 132.70818376541138s

2025-09-29 20:37:58,722 | INFO | Training epoch 60, Batch 1000/1000: LR=9.96e-05, Loss=8.11e-03 BER=3.14e-03 FER=3.40e-02
2025-09-29 20:37:59,172 | INFO | Epoch 60 Train Time 134.12173628807068s

2025-09-29 20:40:16,657 | INFO | Training epoch 61, Batch 1000/1000: LR=9.96e-05, Loss=7.77e-03 BER=3.02e-03 FER=3.28e-02
2025-09-29 20:40:17,143 | INFO | Epoch 61 Train Time 137.9693157672882s

2025-09-29 20:42:33,648 | INFO | Training epoch 62, Batch 1000/1000: LR=9.96e-05, Loss=7.82e-03 BER=3.04e-03 FER=3.33e-02
2025-09-29 20:42:34,157 | INFO | Epoch 62 Train Time 137.0129792690277s

2025-09-29 20:44:58,389 | INFO | Training epoch 63, Batch 1000/1000: LR=9.96e-05, Loss=8.08e-03 BER=3.14e-03 FER=3.38e-02
2025-09-29 20:44:58,875 | INFO | Epoch 63 Train Time 144.71648812294006s

2025-09-29 20:47:31,637 | INFO | Training epoch 64, Batch 1000/1000: LR=9.96e-05, Loss=7.89e-03 BER=3.07e-03 FER=3.30e-02
2025-09-29 20:47:32,198 | INFO | Epoch 64 Train Time 153.32003808021545s

2025-09-29 20:49:59,418 | INFO | Training epoch 65, Batch 1000/1000: LR=9.96e-05, Loss=8.10e-03 BER=3.16e-03 FER=3.43e-02
2025-09-29 20:49:59,907 | INFO | Epoch 65 Train Time 147.70788288116455s

2025-09-29 20:52:20,013 | INFO | Training epoch 66, Batch 1000/1000: LR=9.95e-05, Loss=7.77e-03 BER=3.01e-03 FER=3.31e-02
2025-09-29 20:52:20,521 | INFO | Epoch 66 Train Time 140.61278557777405s

2025-09-29 20:54:39,214 | INFO | Training epoch 67, Batch 1000/1000: LR=9.95e-05, Loss=7.90e-03 BER=3.09e-03 FER=3.32e-02
2025-09-29 20:54:39,693 | INFO | Epoch 67 Train Time 139.17088198661804s

2025-09-29 20:57:00,146 | INFO | Training epoch 68, Batch 1000/1000: LR=9.95e-05, Loss=7.86e-03 BER=3.07e-03 FER=3.36e-02
2025-09-29 20:57:00,728 | INFO | Epoch 68 Train Time 141.03221464157104s

2025-09-29 20:59:21,584 | INFO | Training epoch 69, Batch 1000/1000: LR=9.95e-05, Loss=7.86e-03 BER=3.05e-03 FER=3.31e-02
2025-09-29 20:59:22,141 | INFO | Epoch 69 Train Time 141.41222739219666s

2025-09-29 21:01:43,852 | INFO | Training epoch 70, Batch 1000/1000: LR=9.95e-05, Loss=7.96e-03 BER=3.08e-03 FER=3.31e-02
2025-09-29 21:01:44,314 | INFO | Epoch 70 Train Time 142.17208671569824s

2025-09-29 21:04:05,035 | INFO | Training epoch 71, Batch 1000/1000: LR=9.95e-05, Loss=7.90e-03 BER=3.06e-03 FER=3.31e-02
2025-09-29 21:04:05,557 | INFO | Epoch 71 Train Time 141.24236011505127s

2025-09-29 21:06:26,526 | INFO | Training epoch 72, Batch 1000/1000: LR=9.95e-05, Loss=7.75e-03 BER=3.04e-03 FER=3.29e-02
2025-09-29 21:06:27,211 | INFO | Epoch 72 Train Time 141.65045070648193s

2025-09-29 21:08:45,924 | INFO | Training epoch 73, Batch 1000/1000: LR=9.94e-05, Loss=7.91e-03 BER=3.10e-03 FER=3.35e-02
2025-09-29 21:08:46,420 | INFO | Epoch 73 Train Time 139.20687246322632s

2025-09-29 21:11:05,659 | INFO | Training epoch 74, Batch 1000/1000: LR=9.94e-05, Loss=7.99e-03 BER=3.11e-03 FER=3.31e-02
2025-09-29 21:11:06,140 | INFO | Epoch 74 Train Time 139.71792578697205s

2025-09-29 21:13:25,719 | INFO | Training epoch 75, Batch 1000/1000: LR=9.94e-05, Loss=7.76e-03 BER=3.05e-03 FER=3.33e-02
2025-09-29 21:13:26,324 | INFO | Epoch 75 Train Time 140.18368268013s

2025-09-29 21:15:47,443 | INFO | Training epoch 76, Batch 1000/1000: LR=9.94e-05, Loss=8.03e-03 BER=3.12e-03 FER=3.34e-02
2025-09-29 21:15:47,933 | INFO | Epoch 76 Train Time 141.60530614852905s

2025-09-29 21:18:07,730 | INFO | Training epoch 77, Batch 1000/1000: LR=9.94e-05, Loss=7.85e-03 BER=3.02e-03 FER=3.23e-02
2025-09-29 21:18:08,232 | INFO | Epoch 77 Train Time 140.29782247543335s

2025-09-29 21:20:45,601 | INFO | Training epoch 78, Batch 1000/1000: LR=9.94e-05, Loss=8.20e-03 BER=3.18e-03 FER=3.41e-02
2025-09-29 21:20:46,134 | INFO | Epoch 78 Train Time 157.90048575401306s

2025-09-29 21:23:10,834 | INFO | Training epoch 79, Batch 1000/1000: LR=9.93e-05, Loss=7.87e-03 BER=3.05e-03 FER=3.33e-02
2025-09-29 21:23:11,316 | INFO | Epoch 79 Train Time 145.1788785457611s

2025-09-29 21:25:33,730 | INFO | Training epoch 80, Batch 1000/1000: LR=9.93e-05, Loss=7.82e-03 BER=3.04e-03 FER=3.32e-02
2025-09-29 21:25:34,209 | INFO | Epoch 80 Train Time 142.89147090911865s

2025-09-29 21:27:54,885 | INFO | Training epoch 81, Batch 1000/1000: LR=9.93e-05, Loss=8.09e-03 BER=3.18e-03 FER=3.39e-02
2025-09-29 21:27:55,562 | INFO | Epoch 81 Train Time 141.3520860671997s

2025-09-29 21:30:16,835 | INFO | Training epoch 82, Batch 1000/1000: LR=9.93e-05, Loss=8.04e-03 BER=3.12e-03 FER=3.37e-02
2025-09-29 21:30:17,306 | INFO | Epoch 82 Train Time 141.7429165840149s

2025-09-29 21:32:30,212 | INFO | Training epoch 83, Batch 1000/1000: LR=9.93e-05, Loss=8.21e-03 BER=3.18e-03 FER=3.43e-02
2025-09-29 21:32:30,668 | INFO | Epoch 83 Train Time 133.3610806465149s

2025-09-29 21:34:43,566 | INFO | Training epoch 84, Batch 1000/1000: LR=9.93e-05, Loss=7.88e-03 BER=3.06e-03 FER=3.33e-02
2025-09-29 21:34:44,021 | INFO | Epoch 84 Train Time 133.35191893577576s

2025-09-29 21:36:57,227 | INFO | Training epoch 85, Batch 1000/1000: LR=9.92e-05, Loss=8.10e-03 BER=3.15e-03 FER=3.39e-02
2025-09-29 21:36:57,691 | INFO | Epoch 85 Train Time 133.66847109794617s

2025-09-29 21:39:11,091 | INFO | Training epoch 86, Batch 1000/1000: LR=9.92e-05, Loss=7.89e-03 BER=3.08e-03 FER=3.32e-02
2025-09-29 21:39:11,558 | INFO | Epoch 86 Train Time 133.8660228252411s

2025-09-29 21:41:24,228 | INFO | Training epoch 87, Batch 1000/1000: LR=9.92e-05, Loss=7.88e-03 BER=3.07e-03 FER=3.36e-02
2025-09-29 21:41:24,676 | INFO | Epoch 87 Train Time 133.11598539352417s

2025-09-29 21:43:37,805 | INFO | Training epoch 88, Batch 1000/1000: LR=9.92e-05, Loss=7.93e-03 BER=3.07e-03 FER=3.33e-02
2025-09-29 21:43:38,273 | INFO | Epoch 88 Train Time 133.59609293937683s

2025-09-29 21:45:50,939 | INFO | Training epoch 89, Batch 1000/1000: LR=9.92e-05, Loss=7.96e-03 BER=3.09e-03 FER=3.33e-02
2025-09-29 21:45:51,408 | INFO | Epoch 89 Train Time 133.13408780097961s

2025-09-29 21:48:06,191 | INFO | Training epoch 90, Batch 1000/1000: LR=9.91e-05, Loss=8.07e-03 BER=3.14e-03 FER=3.36e-02
2025-09-29 21:48:06,659 | INFO | Epoch 90 Train Time 135.249981880188s

2025-09-29 21:50:19,645 | INFO | Training epoch 91, Batch 1000/1000: LR=9.91e-05, Loss=8.04e-03 BER=3.14e-03 FER=3.39e-02
2025-09-29 21:50:20,107 | INFO | Epoch 91 Train Time 133.4469747543335s

2025-09-29 21:52:40,250 | INFO | Training epoch 92, Batch 1000/1000: LR=9.91e-05, Loss=7.72e-03 BER=3.01e-03 FER=3.30e-02
2025-09-29 21:52:40,759 | INFO | Epoch 92 Train Time 140.6507852077484s

2025-09-29 21:55:52,912 | INFO | Training epoch 93, Batch 1000/1000: LR=9.91e-05, Loss=7.92e-03 BER=3.09e-03 FER=3.33e-02
2025-09-29 21:55:53,382 | INFO | Epoch 93 Train Time 192.62053656578064s

2025-09-29 21:59:04,030 | INFO | Training epoch 94, Batch 1000/1000: LR=9.91e-05, Loss=7.76e-03 BER=3.02e-03 FER=3.26e-02
2025-09-29 21:59:04,526 | INFO | Epoch 94 Train Time 191.1422779560089s

2025-09-29 22:03:09,343 | INFO | Training epoch 95, Batch 1000/1000: LR=9.90e-05, Loss=7.90e-03 BER=3.07e-03 FER=3.25e-02
2025-09-29 22:03:09,851 | INFO | Epoch 95 Train Time 245.32426476478577s

2025-09-29 22:06:28,338 | INFO | Training epoch 96, Batch 1000/1000: LR=9.90e-05, Loss=7.70e-03 BER=2.97e-03 FER=3.23e-02
2025-09-29 22:06:28,823 | INFO | Epoch 96 Train Time 198.9702935218811s

2025-09-29 22:06:28,823 | INFO | [P2] saving best_model (QAT) with loss 0.007702 at epoch 96
2025-09-29 22:09:23,900 | INFO | Training epoch 97, Batch 1000/1000: LR=9.90e-05, Loss=7.65e-03 BER=2.96e-03 FER=3.24e-02
2025-09-29 22:09:24,355 | INFO | Epoch 97 Train Time 175.45108938217163s

2025-09-29 22:09:24,355 | INFO | [P2] saving best_model (QAT) with loss 0.007650 at epoch 97
2025-09-29 22:12:32,982 | INFO | Training epoch 98, Batch 1000/1000: LR=9.90e-05, Loss=7.88e-03 BER=3.02e-03 FER=3.28e-02
2025-09-29 22:12:33,468 | INFO | Epoch 98 Train Time 189.04837083816528s

2025-09-29 22:15:33,284 | INFO | Training epoch 99, Batch 1000/1000: LR=9.90e-05, Loss=7.89e-03 BER=3.09e-03 FER=3.36e-02
2025-09-29 22:15:33,766 | INFO | Epoch 99 Train Time 180.2966456413269s

2025-09-29 22:18:39,216 | INFO | Training epoch 100, Batch 1000/1000: LR=9.89e-05, Loss=7.69e-03 BER=2.99e-03 FER=3.24e-02
2025-09-29 22:18:39,709 | INFO | Epoch 100 Train Time 185.9416754245758s

2025-09-29 22:21:45,430 | INFO | Training epoch 101, Batch 1000/1000: LR=9.89e-05, Loss=7.84e-03 BER=3.04e-03 FER=3.35e-02
2025-09-29 22:21:45,927 | INFO | Epoch 101 Train Time 186.21708250045776s

2025-09-29 22:24:45,907 | INFO | Training epoch 102, Batch 1000/1000: LR=9.89e-05, Loss=7.61e-03 BER=2.97e-03 FER=3.17e-02
2025-09-29 22:24:46,455 | INFO | Epoch 102 Train Time 180.52627182006836s

2025-09-29 22:24:46,456 | INFO | [P2] saving best_model (QAT) with loss 0.007611 at epoch 102
2025-09-29 22:27:40,659 | INFO | Training epoch 103, Batch 1000/1000: LR=9.89e-05, Loss=7.68e-03 BER=3.01e-03 FER=3.26e-02
2025-09-29 22:27:41,159 | INFO | Epoch 103 Train Time 174.6172378063202s

2025-09-29 22:30:29,009 | INFO | Training epoch 104, Batch 1000/1000: LR=9.89e-05, Loss=8.01e-03 BER=3.11e-03 FER=3.35e-02
2025-09-29 22:30:29,571 | INFO | Epoch 104 Train Time 168.41006898880005s

2025-09-29 22:33:20,130 | INFO | Training epoch 105, Batch 1000/1000: LR=9.88e-05, Loss=7.87e-03 BER=3.07e-03 FER=3.30e-02
2025-09-29 22:33:20,579 | INFO | Epoch 105 Train Time 171.00589752197266s

2025-09-29 22:36:14,973 | INFO | Training epoch 106, Batch 1000/1000: LR=9.88e-05, Loss=7.95e-03 BER=3.11e-03 FER=3.33e-02
2025-09-29 22:36:15,432 | INFO | Epoch 106 Train Time 174.85212349891663s

2025-09-29 22:39:51,593 | INFO | Training epoch 107, Batch 1000/1000: LR=9.88e-05, Loss=7.94e-03 BER=3.11e-03 FER=3.30e-02
2025-09-29 22:39:52,070 | INFO | Epoch 107 Train Time 216.6359362602234s

2025-09-29 22:44:57,760 | INFO | Training epoch 108, Batch 1000/1000: LR=9.88e-05, Loss=7.89e-03 BER=3.09e-03 FER=3.31e-02
2025-09-29 22:44:58,230 | INFO | Epoch 108 Train Time 306.158979177475s

2025-09-29 22:50:46,767 | INFO | Training epoch 109, Batch 1000/1000: LR=9.87e-05, Loss=7.94e-03 BER=3.12e-03 FER=3.35e-02
2025-09-29 22:50:47,261 | INFO | Epoch 109 Train Time 349.02988862991333s

2025-09-29 22:57:05,313 | INFO | Training epoch 110, Batch 1000/1000: LR=9.87e-05, Loss=7.70e-03 BER=2.98e-03 FER=3.21e-02
2025-09-29 22:57:05,787 | INFO | Epoch 110 Train Time 378.5243604183197s

2025-09-29 23:03:14,638 | INFO | Training epoch 111, Batch 1000/1000: LR=9.87e-05, Loss=7.80e-03 BER=3.03e-03 FER=3.26e-02
2025-09-29 23:03:15,123 | INFO | Epoch 111 Train Time 369.3341510295868s

2025-09-29 23:09:16,502 | INFO | Training epoch 112, Batch 1000/1000: LR=9.87e-05, Loss=7.92e-03 BER=3.09e-03 FER=3.31e-02
2025-09-29 23:09:16,972 | INFO | Epoch 112 Train Time 361.84798645973206s

2025-09-29 23:15:38,222 | INFO | Training epoch 113, Batch 1000/1000: LR=9.86e-05, Loss=8.09e-03 BER=3.16e-03 FER=3.39e-02
2025-09-29 23:15:38,680 | INFO | Epoch 113 Train Time 381.7071330547333s

2025-09-29 23:22:06,492 | INFO | Training epoch 114, Batch 1000/1000: LR=9.86e-05, Loss=7.77e-03 BER=3.05e-03 FER=3.26e-02
2025-09-29 23:22:06,976 | INFO | Epoch 114 Train Time 388.2933192253113s

2025-09-29 23:27:28,624 | INFO | Training epoch 115, Batch 1000/1000: LR=9.86e-05, Loss=7.73e-03 BER=3.01e-03 FER=3.29e-02
2025-09-29 23:27:29,082 | INFO | Epoch 115 Train Time 322.10462713241577s

2025-09-29 23:32:23,586 | INFO | Training epoch 116, Batch 1000/1000: LR=9.86e-05, Loss=7.68e-03 BER=3.01e-03 FER=3.26e-02
2025-09-29 23:32:24,096 | INFO | Epoch 116 Train Time 295.0113916397095s

2025-09-29 23:36:59,638 | INFO | Training epoch 117, Batch 1000/1000: LR=9.85e-05, Loss=7.74e-03 BER=3.03e-03 FER=3.25e-02
2025-09-29 23:37:00,132 | INFO | Epoch 117 Train Time 276.03452944755554s

2025-09-29 23:41:43,689 | INFO | Training epoch 118, Batch 1000/1000: LR=9.85e-05, Loss=7.90e-03 BER=3.09e-03 FER=3.37e-02
2025-09-29 23:41:44,176 | INFO | Epoch 118 Train Time 284.0423822402954s

2025-09-29 23:46:44,459 | INFO | Training epoch 119, Batch 1000/1000: LR=9.85e-05, Loss=7.69e-03 BER=3.00e-03 FER=3.25e-02
2025-09-29 23:46:44,912 | INFO | Epoch 119 Train Time 300.73503017425537s

2025-09-29 23:51:44,254 | INFO | Training epoch 120, Batch 1000/1000: LR=9.85e-05, Loss=7.96e-03 BER=3.11e-03 FER=3.31e-02
2025-09-29 23:51:44,723 | INFO | Epoch 120 Train Time 299.8106064796448s

2025-09-29 23:56:26,482 | INFO | Training epoch 121, Batch 1000/1000: LR=9.84e-05, Loss=7.96e-03 BER=3.09e-03 FER=3.25e-02
2025-09-29 23:56:26,966 | INFO | Epoch 121 Train Time 282.2420165538788s

2025-09-30 00:00:55,495 | INFO | Training epoch 122, Batch 1000/1000: LR=9.84e-05, Loss=7.93e-03 BER=3.08e-03 FER=3.30e-02
2025-09-30 00:00:55,964 | INFO | Epoch 122 Train Time 268.9971489906311s

2025-09-30 00:05:38,255 | INFO | Training epoch 123, Batch 1000/1000: LR=9.84e-05, Loss=7.97e-03 BER=3.08e-03 FER=3.31e-02
2025-09-30 00:05:38,764 | INFO | Epoch 123 Train Time 282.7988910675049s

2025-09-30 00:10:40,609 | INFO | Training epoch 124, Batch 1000/1000: LR=9.84e-05, Loss=7.88e-03 BER=3.07e-03 FER=3.31e-02
2025-09-30 00:10:41,080 | INFO | Epoch 124 Train Time 302.3139679431915s

2025-09-30 00:15:37,543 | INFO | Training epoch 125, Batch 1000/1000: LR=9.83e-05, Loss=7.91e-03 BER=3.11e-03 FER=3.37e-02
2025-09-30 00:15:38,016 | INFO | Epoch 125 Train Time 296.93563961982727s

2025-09-30 00:20:15,520 | INFO | Training epoch 126, Batch 1000/1000: LR=9.83e-05, Loss=7.88e-03 BER=3.06e-03 FER=3.30e-02
2025-09-30 00:20:15,981 | INFO | Epoch 126 Train Time 277.9613745212555s

2025-09-30 00:24:46,531 | INFO | Training epoch 127, Batch 1000/1000: LR=9.83e-05, Loss=7.81e-03 BER=3.06e-03 FER=3.30e-02
2025-09-30 00:24:46,995 | INFO | Epoch 127 Train Time 271.01368165016174s

2025-09-30 00:29:27,097 | INFO | Training epoch 128, Batch 1000/1000: LR=9.83e-05, Loss=7.92e-03 BER=3.06e-03 FER=3.32e-02
2025-09-30 00:29:27,573 | INFO | Epoch 128 Train Time 280.5765287876129s

2025-09-30 00:34:19,203 | INFO | Training epoch 129, Batch 1000/1000: LR=9.82e-05, Loss=7.80e-03 BER=3.01e-03 FER=3.23e-02
2025-09-30 00:34:19,689 | INFO | Epoch 129 Train Time 292.11521196365356s

2025-09-30 00:38:56,125 | INFO | Training epoch 130, Batch 1000/1000: LR=9.82e-05, Loss=7.71e-03 BER=3.05e-03 FER=3.28e-02
2025-09-30 00:38:56,593 | INFO | Epoch 130 Train Time 276.90269351005554s

2025-09-30 00:43:18,282 | INFO | Training epoch 131, Batch 1000/1000: LR=9.82e-05, Loss=8.01e-03 BER=3.16e-03 FER=3.38e-02
2025-09-30 00:43:18,750 | INFO | Epoch 131 Train Time 262.1549119949341s

2025-09-30 00:47:57,718 | INFO | Training epoch 132, Batch 1000/1000: LR=9.81e-05, Loss=7.69e-03 BER=2.99e-03 FER=3.24e-02
2025-09-30 00:47:58,187 | INFO | Epoch 132 Train Time 279.4365448951721s

2025-09-30 00:52:55,181 | INFO | Training epoch 133, Batch 1000/1000: LR=9.81e-05, Loss=7.91e-03 BER=3.12e-03 FER=3.35e-02
2025-09-30 00:52:55,631 | INFO | Epoch 133 Train Time 297.4419734477997s

2025-09-30 00:57:45,499 | INFO | Training epoch 134, Batch 1000/1000: LR=9.81e-05, Loss=8.04e-03 BER=3.13e-03 FER=3.36e-02
2025-09-30 00:57:45,972 | INFO | Epoch 134 Train Time 290.3394751548767s

2025-09-30 01:02:15,705 | INFO | Training epoch 135, Batch 1000/1000: LR=9.81e-05, Loss=7.71e-03 BER=3.04e-03 FER=3.21e-02
2025-09-30 01:02:16,160 | INFO | Epoch 135 Train Time 270.1866796016693s

2025-09-30 01:06:50,658 | INFO | Training epoch 136, Batch 1000/1000: LR=9.80e-05, Loss=7.87e-03 BER=3.06e-03 FER=3.30e-02
2025-09-30 01:06:51,140 | INFO | Epoch 136 Train Time 274.9790847301483s

2025-09-30 01:11:38,453 | INFO | Training epoch 137, Batch 1000/1000: LR=9.80e-05, Loss=7.92e-03 BER=3.10e-03 FER=3.31e-02
2025-09-30 01:11:38,950 | INFO | Epoch 137 Train Time 287.80878138542175s

2025-09-30 01:16:37,789 | INFO | Training epoch 138, Batch 1000/1000: LR=9.80e-05, Loss=7.71e-03 BER=2.98e-03 FER=3.25e-02
2025-09-30 01:16:38,252 | INFO | Epoch 138 Train Time 299.29982590675354s

2025-09-30 01:21:07,092 | INFO | Training epoch 139, Batch 1000/1000: LR=9.79e-05, Loss=7.89e-03 BER=3.08e-03 FER=3.36e-02
2025-09-30 01:21:07,563 | INFO | Epoch 139 Train Time 269.3086009025574s

2025-09-30 01:25:22,678 | INFO | Training epoch 140, Batch 1000/1000: LR=9.79e-05, Loss=7.45e-03 BER=2.90e-03 FER=3.14e-02
2025-09-30 01:25:23,163 | INFO | Epoch 140 Train Time 255.5989966392517s

2025-09-30 01:25:23,164 | INFO | [P2] saving best_model (QAT) with loss 0.007447 at epoch 140
2025-09-30 01:29:46,424 | INFO | Training epoch 141, Batch 1000/1000: LR=9.79e-05, Loss=7.81e-03 BER=3.03e-03 FER=3.29e-02
2025-09-30 01:29:46,901 | INFO | Epoch 141 Train Time 263.6781485080719s

2025-09-30 01:34:22,717 | INFO | Training epoch 142, Batch 1000/1000: LR=9.79e-05, Loss=8.06e-03 BER=3.16e-03 FER=3.34e-02
2025-09-30 01:34:23,194 | INFO | Epoch 142 Train Time 276.2906348705292s

2025-09-30 01:39:06,741 | INFO | Training epoch 143, Batch 1000/1000: LR=9.78e-05, Loss=7.77e-03 BER=3.02e-03 FER=3.26e-02
2025-09-30 01:39:07,230 | INFO | Epoch 143 Train Time 284.0352158546448s

2025-09-30 01:43:37,472 | INFO | Training epoch 144, Batch 1000/1000: LR=9.78e-05, Loss=7.65e-03 BER=2.98e-03 FER=3.24e-02
2025-09-30 01:43:37,988 | INFO | Epoch 144 Train Time 270.7555191516876s

2025-09-30 01:48:10,787 | INFO | Training epoch 145, Batch 1000/1000: LR=9.78e-05, Loss=7.69e-03 BER=3.00e-03 FER=3.24e-02
2025-09-30 01:48:11,260 | INFO | Epoch 145 Train Time 273.2712199687958s

2025-09-30 01:53:02,752 | INFO | Training epoch 146, Batch 1000/1000: LR=9.77e-05, Loss=8.07e-03 BER=3.13e-03 FER=3.36e-02
2025-09-30 01:53:03,225 | INFO | Epoch 146 Train Time 291.962628364563s

2025-09-30 01:57:52,186 | INFO | Training epoch 147, Batch 1000/1000: LR=9.77e-05, Loss=8.12e-03 BER=3.17e-03 FER=3.39e-02
2025-09-30 01:57:52,682 | INFO | Epoch 147 Train Time 289.4567377567291s

2025-09-30 02:02:26,018 | INFO | Training epoch 148, Batch 1000/1000: LR=9.77e-05, Loss=8.06e-03 BER=3.17e-03 FER=3.39e-02
2025-09-30 02:02:26,520 | INFO | Epoch 148 Train Time 273.83627939224243s

2025-09-30 02:06:59,570 | INFO | Training epoch 149, Batch 1000/1000: LR=9.76e-05, Loss=7.71e-03 BER=2.99e-03 FER=3.28e-02
2025-09-30 02:07:00,128 | INFO | Epoch 149 Train Time 273.6068983078003s

2025-09-30 02:11:57,550 | INFO | Training epoch 150, Batch 1000/1000: LR=9.76e-05, Loss=7.73e-03 BER=3.00e-03 FER=3.21e-02
2025-09-30 02:11:58,035 | INFO | Epoch 150 Train Time 297.90485095977783s

2025-09-30 02:16:45,489 | INFO | Training epoch 151, Batch 1000/1000: LR=9.76e-05, Loss=7.75e-03 BER=3.04e-03 FER=3.26e-02
2025-09-30 02:16:46,003 | INFO | Epoch 151 Train Time 287.96564984321594s

2025-09-30 02:21:15,747 | INFO | Training epoch 152, Batch 1000/1000: LR=9.75e-05, Loss=7.76e-03 BER=3.05e-03 FER=3.30e-02
2025-09-30 02:21:16,229 | INFO | Epoch 152 Train Time 270.22555112838745s

2025-09-30 02:25:43,930 | INFO | Training epoch 153, Batch 1000/1000: LR=9.75e-05, Loss=7.82e-03 BER=3.07e-03 FER=3.27e-02
2025-09-30 02:25:44,384 | INFO | Epoch 153 Train Time 268.1535141468048s

2025-09-30 02:30:30,072 | INFO | Training epoch 154, Batch 1000/1000: LR=9.75e-05, Loss=7.59e-03 BER=2.96e-03 FER=3.14e-02
2025-09-30 02:30:30,542 | INFO | Epoch 154 Train Time 286.1574749946594s

2025-09-30 02:35:27,429 | INFO | Training epoch 155, Batch 1000/1000: LR=9.74e-05, Loss=7.56e-03 BER=2.96e-03 FER=3.22e-02
2025-09-30 02:35:27,898 | INFO | Epoch 155 Train Time 297.35432982444763s

2025-09-30 02:40:01,792 | INFO | Training epoch 156, Batch 1000/1000: LR=9.74e-05, Loss=7.69e-03 BER=3.00e-03 FER=3.21e-02
2025-09-30 02:40:02,291 | INFO | Epoch 156 Train Time 274.39191794395447s

2025-09-30 02:44:31,490 | INFO | Training epoch 157, Batch 1000/1000: LR=9.74e-05, Loss=7.85e-03 BER=3.11e-03 FER=3.30e-02
2025-09-30 02:44:31,963 | INFO | Epoch 157 Train Time 269.6716094017029s

2025-09-30 02:49:08,424 | INFO | Training epoch 158, Batch 1000/1000: LR=9.73e-05, Loss=8.07e-03 BER=3.15e-03 FER=3.39e-02
2025-09-30 02:49:08,937 | INFO | Epoch 158 Train Time 276.972708940506s

2025-09-30 02:54:01,439 | INFO | Training epoch 159, Batch 1000/1000: LR=9.73e-05, Loss=7.73e-03 BER=2.99e-03 FER=3.24e-02
2025-09-30 02:54:01,905 | INFO | Epoch 159 Train Time 292.9670181274414s

2025-09-30 02:58:33,388 | INFO | Training epoch 160, Batch 1000/1000: LR=9.73e-05, Loss=7.68e-03 BER=2.98e-03 FER=3.20e-02
2025-09-30 02:58:33,885 | INFO | Epoch 160 Train Time 271.978364944458s

2025-09-30 03:02:53,894 | INFO | Training epoch 161, Batch 1000/1000: LR=9.72e-05, Loss=7.84e-03 BER=3.05e-03 FER=3.30e-02
2025-09-30 03:02:54,353 | INFO | Epoch 161 Train Time 260.4674994945526s

2025-09-30 03:07:33,335 | INFO | Training epoch 162, Batch 1000/1000: LR=9.72e-05, Loss=7.90e-03 BER=3.08e-03 FER=3.30e-02
2025-09-30 03:07:33,806 | INFO | Epoch 162 Train Time 279.4522817134857s

2025-09-30 03:12:21,007 | INFO | Training epoch 163, Batch 1000/1000: LR=9.72e-05, Loss=7.79e-03 BER=3.01e-03 FER=3.24e-02
2025-09-30 03:12:21,525 | INFO | Epoch 163 Train Time 287.7166111469269s

2025-09-30 03:16:50,370 | INFO | Training epoch 164, Batch 1000/1000: LR=9.71e-05, Loss=7.65e-03 BER=2.98e-03 FER=3.25e-02
2025-09-30 03:16:50,843 | INFO | Epoch 164 Train Time 269.31659173965454s

2025-09-30 03:21:14,142 | INFO | Training epoch 165, Batch 1000/1000: LR=9.71e-05, Loss=7.99e-03 BER=3.10e-03 FER=3.27e-02
2025-09-30 03:21:14,682 | INFO | Epoch 165 Train Time 263.8384771347046s

2025-09-30 03:25:39,133 | INFO | Training epoch 166, Batch 1000/1000: LR=9.71e-05, Loss=7.92e-03 BER=3.07e-03 FER=3.32e-02
2025-09-30 03:25:39,599 | INFO | Epoch 166 Train Time 264.91466641426086s

2025-09-30 03:30:15,326 | INFO | Training epoch 167, Batch 1000/1000: LR=9.70e-05, Loss=7.93e-03 BER=3.14e-03 FER=3.33e-02
2025-09-30 03:30:15,836 | INFO | Epoch 167 Train Time 276.2357454299927s

2025-09-30 03:34:38,628 | INFO | Training epoch 168, Batch 1000/1000: LR=9.70e-05, Loss=7.76e-03 BER=3.04e-03 FER=3.28e-02
2025-09-30 03:34:39,092 | INFO | Epoch 168 Train Time 263.2548859119415s

2025-09-30 03:38:58,368 | INFO | Training epoch 169, Batch 1000/1000: LR=9.70e-05, Loss=7.72e-03 BER=2.99e-03 FER=3.20e-02
2025-09-30 03:38:58,891 | INFO | Epoch 169 Train Time 259.79643845558167s

2025-09-30 03:43:50,420 | INFO | Training epoch 170, Batch 1000/1000: LR=9.69e-05, Loss=8.04e-03 BER=3.13e-03 FER=3.38e-02
2025-09-30 03:43:50,950 | INFO | Epoch 170 Train Time 292.0570721626282s

2025-09-30 03:48:37,667 | INFO | Training epoch 171, Batch 1000/1000: LR=9.69e-05, Loss=7.61e-03 BER=2.93e-03 FER=3.18e-02
2025-09-30 03:48:38,131 | INFO | Epoch 171 Train Time 287.1784715652466s

2025-09-30 03:52:59,021 | INFO | Training epoch 172, Batch 1000/1000: LR=9.69e-05, Loss=7.70e-03 BER=3.01e-03 FER=3.24e-02
2025-09-30 03:52:59,481 | INFO | Epoch 172 Train Time 261.3495616912842s

2025-09-30 03:57:14,564 | INFO | Training epoch 173, Batch 1000/1000: LR=9.68e-05, Loss=7.95e-03 BER=3.12e-03 FER=3.32e-02
2025-09-30 03:57:15,072 | INFO | Epoch 173 Train Time 255.59038090705872s

2025-09-30 04:01:40,779 | INFO | Training epoch 174, Batch 1000/1000: LR=9.68e-05, Loss=7.89e-03 BER=3.05e-03 FER=3.26e-02
2025-09-30 04:01:41,246 | INFO | Epoch 174 Train Time 266.17173957824707s

2025-09-30 04:06:16,134 | INFO | Training epoch 175, Batch 1000/1000: LR=9.67e-05, Loss=8.10e-03 BER=3.17e-03 FER=3.36e-02
2025-09-30 04:06:16,603 | INFO | Epoch 175 Train Time 275.3560109138489s

2025-09-30 04:10:43,577 | INFO | Training epoch 176, Batch 1000/1000: LR=9.67e-05, Loss=7.68e-03 BER=3.00e-03 FER=3.19e-02
2025-09-30 04:10:44,062 | INFO | Epoch 176 Train Time 267.457457780838s

2025-09-30 04:14:55,644 | INFO | Training epoch 177, Batch 1000/1000: LR=9.67e-05, Loss=7.92e-03 BER=3.10e-03 FER=3.27e-02
2025-09-30 04:14:56,128 | INFO | Epoch 177 Train Time 252.06408619880676s

2025-09-30 04:19:18,508 | INFO | Training epoch 178, Batch 1000/1000: LR=9.66e-05, Loss=7.95e-03 BER=3.08e-03 FER=3.33e-02
2025-09-30 04:19:18,988 | INFO | Epoch 178 Train Time 262.85966181755066s

2025-09-30 04:23:52,030 | INFO | Training epoch 179, Batch 1000/1000: LR=9.66e-05, Loss=7.62e-03 BER=2.97e-03 FER=3.22e-02
2025-09-30 04:23:52,513 | INFO | Epoch 179 Train Time 273.52321004867554s

2025-09-30 04:28:24,413 | INFO | Training epoch 180, Batch 1000/1000: LR=9.66e-05, Loss=7.80e-03 BER=3.06e-03 FER=3.24e-02
2025-09-30 04:28:24,936 | INFO | Epoch 180 Train Time 272.4220805168152s

2025-09-30 04:32:45,750 | INFO | Training epoch 181, Batch 1000/1000: LR=9.65e-05, Loss=7.71e-03 BER=3.03e-03 FER=3.22e-02
2025-09-30 04:32:46,285 | INFO | Epoch 181 Train Time 261.34678649902344s

2025-09-30 04:37:12,804 | INFO | Training epoch 182, Batch 1000/1000: LR=9.65e-05, Loss=7.83e-03 BER=3.05e-03 FER=3.33e-02
2025-09-30 04:37:13,286 | INFO | Epoch 182 Train Time 267.0000169277191s

2025-09-30 04:41:54,879 | INFO | Training epoch 183, Batch 1000/1000: LR=9.64e-05, Loss=7.96e-03 BER=3.09e-03 FER=3.34e-02
2025-09-30 04:41:55,362 | INFO | Epoch 183 Train Time 282.0735013484955s

2025-09-30 04:46:37,631 | INFO | Training epoch 184, Batch 1000/1000: LR=9.64e-05, Loss=7.69e-03 BER=2.98e-03 FER=3.19e-02
2025-09-30 04:46:38,092 | INFO | Epoch 184 Train Time 282.72819423675537s

2025-09-30 04:51:03,655 | INFO | Training epoch 185, Batch 1000/1000: LR=9.64e-05, Loss=7.85e-03 BER=3.06e-03 FER=3.28e-02
2025-09-30 04:51:04,108 | INFO | Epoch 185 Train Time 266.01514434814453s

2025-09-30 04:55:29,430 | INFO | Training epoch 186, Batch 1000/1000: LR=9.63e-05, Loss=7.53e-03 BER=2.94e-03 FER=3.20e-02
2025-09-30 04:55:29,886 | INFO | Epoch 186 Train Time 265.77683901786804s

2025-09-30 05:00:00,590 | INFO | Training epoch 187, Batch 1000/1000: LR=9.63e-05, Loss=7.59e-03 BER=2.94e-03 FER=3.18e-02
2025-09-30 05:00:01,055 | INFO | Epoch 187 Train Time 271.1672043800354s

2025-09-30 05:04:46,769 | INFO | Training epoch 188, Batch 1000/1000: LR=9.63e-05, Loss=7.79e-03 BER=3.04e-03 FER=3.22e-02
2025-09-30 05:04:47,289 | INFO | Epoch 188 Train Time 286.23294973373413s

2025-09-30 05:09:27,640 | INFO | Training epoch 189, Batch 1000/1000: LR=9.62e-05, Loss=7.70e-03 BER=3.02e-03 FER=3.26e-02
2025-09-30 05:09:28,150 | INFO | Epoch 189 Train Time 280.85918402671814s

2025-09-30 05:13:50,453 | INFO | Training epoch 190, Batch 1000/1000: LR=9.62e-05, Loss=7.89e-03 BER=3.04e-03 FER=3.22e-02
2025-09-30 05:13:50,930 | INFO | Epoch 190 Train Time 262.77840209007263s

2025-09-30 05:18:25,487 | INFO | Training epoch 191, Batch 1000/1000: LR=9.61e-05, Loss=7.93e-03 BER=3.07e-03 FER=3.28e-02
2025-09-30 05:18:25,944 | INFO | Epoch 191 Train Time 275.01326179504395s

2025-09-30 05:23:08,323 | INFO | Training epoch 192, Batch 1000/1000: LR=9.61e-05, Loss=7.76e-03 BER=3.05e-03 FER=3.26e-02
2025-09-30 05:23:08,787 | INFO | Epoch 192 Train Time 282.84181690216064s

2025-09-30 05:27:25,429 | INFO | Training epoch 193, Batch 1000/1000: LR=9.61e-05, Loss=7.82e-03 BER=3.04e-03 FER=3.27e-02
2025-09-30 05:27:25,910 | INFO | Epoch 193 Train Time 257.12208890914917s

2025-09-30 05:31:36,218 | INFO | Training epoch 194, Batch 1000/1000: LR=9.60e-05, Loss=7.60e-03 BER=2.97e-03 FER=3.16e-02
2025-09-30 05:31:36,708 | INFO | Epoch 194 Train Time 250.79668426513672s

2025-09-30 05:36:13,573 | INFO | Training epoch 195, Batch 1000/1000: LR=9.60e-05, Loss=7.78e-03 BER=3.03e-03 FER=3.25e-02
2025-09-30 05:36:14,067 | INFO | Epoch 195 Train Time 277.3565945625305s

2025-09-30 05:40:59,927 | INFO | Training epoch 196, Batch 1000/1000: LR=9.59e-05, Loss=7.82e-03 BER=3.03e-03 FER=3.24e-02
2025-09-30 05:41:00,483 | INFO | Epoch 196 Train Time 286.41542315483093s

2025-09-30 05:45:33,438 | INFO | Training epoch 197, Batch 1000/1000: LR=9.59e-05, Loss=7.88e-03 BER=3.11e-03 FER=3.28e-02
2025-09-30 05:45:33,946 | INFO | Epoch 197 Train Time 273.4604902267456s

2025-09-30 05:50:02,149 | INFO | Training epoch 198, Batch 1000/1000: LR=9.58e-05, Loss=7.55e-03 BER=2.91e-03 FER=3.14e-02
2025-09-30 05:50:02,610 | INFO | Epoch 198 Train Time 268.66183590888977s

2025-09-30 05:54:29,105 | INFO | Training epoch 199, Batch 1000/1000: LR=9.58e-05, Loss=7.93e-03 BER=3.13e-03 FER=3.33e-02
2025-09-30 05:54:29,566 | INFO | Epoch 199 Train Time 266.9559075832367s

2025-09-30 05:59:05,967 | INFO | Training epoch 200, Batch 1000/1000: LR=9.58e-05, Loss=7.97e-03 BER=3.11e-03 FER=3.33e-02
2025-09-30 05:59:06,434 | INFO | Epoch 200 Train Time 276.8666217327118s

2025-09-30 06:03:25,293 | INFO | Training epoch 201, Batch 1000/1000: LR=9.57e-05, Loss=7.82e-03 BER=3.03e-03 FER=3.27e-02
2025-09-30 06:03:25,752 | INFO | Epoch 201 Train Time 259.31736040115356s

2025-09-30 06:07:41,309 | INFO | Training epoch 202, Batch 1000/1000: LR=9.57e-05, Loss=7.72e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 06:07:41,844 | INFO | Epoch 202 Train Time 256.08981108665466s

2025-09-30 06:12:16,000 | INFO | Training epoch 203, Batch 1000/1000: LR=9.56e-05, Loss=7.55e-03 BER=2.98e-03 FER=3.20e-02
2025-09-30 06:12:16,485 | INFO | Epoch 203 Train Time 274.6400582790375s

2025-09-30 06:16:56,624 | INFO | Training epoch 204, Batch 1000/1000: LR=9.56e-05, Loss=7.82e-03 BER=3.05e-03 FER=3.23e-02
2025-09-30 06:16:57,077 | INFO | Epoch 204 Train Time 280.5904564857483s

2025-09-30 06:21:35,418 | INFO | Training epoch 205, Batch 1000/1000: LR=9.56e-05, Loss=7.86e-03 BER=3.08e-03 FER=3.29e-02
2025-09-30 06:21:35,951 | INFO | Epoch 205 Train Time 278.87274408340454s

2025-09-30 06:25:56,421 | INFO | Training epoch 206, Batch 1000/1000: LR=9.55e-05, Loss=7.87e-03 BER=3.07e-03 FER=3.26e-02
2025-09-30 06:25:56,894 | INFO | Epoch 206 Train Time 260.9424877166748s

2025-09-30 06:30:18,634 | INFO | Training epoch 207, Batch 1000/1000: LR=9.55e-05, Loss=7.72e-03 BER=2.99e-03 FER=3.22e-02
2025-09-30 06:30:19,102 | INFO | Epoch 207 Train Time 262.2074086666107s

2025-09-30 06:34:53,773 | INFO | Training epoch 208, Batch 1000/1000: LR=9.54e-05, Loss=7.67e-03 BER=2.99e-03 FER=3.20e-02
2025-09-30 06:34:54,243 | INFO | Epoch 208 Train Time 275.1400480270386s

2025-09-30 06:39:16,617 | INFO | Training epoch 209, Batch 1000/1000: LR=9.54e-05, Loss=7.67e-03 BER=3.01e-03 FER=3.23e-02
2025-09-30 06:39:17,078 | INFO | Epoch 209 Train Time 262.83276295661926s

2025-09-30 06:43:29,860 | INFO | Training epoch 210, Batch 1000/1000: LR=9.53e-05, Loss=7.62e-03 BER=2.97e-03 FER=3.20e-02
2025-09-30 06:43:30,320 | INFO | Epoch 210 Train Time 253.24129557609558s

2025-09-30 06:48:11,123 | INFO | Training epoch 211, Batch 1000/1000: LR=9.53e-05, Loss=7.71e-03 BER=2.99e-03 FER=3.19e-02
2025-09-30 06:48:11,585 | INFO | Epoch 211 Train Time 281.2638792991638s

2025-09-30 06:53:18,208 | INFO | Training epoch 212, Batch 1000/1000: LR=9.52e-05, Loss=7.64e-03 BER=2.98e-03 FER=3.16e-02
2025-09-30 06:53:18,697 | INFO | Epoch 212 Train Time 307.1102776527405s

2025-09-30 06:58:51,384 | INFO | Training epoch 213, Batch 1000/1000: LR=9.52e-05, Loss=7.78e-03 BER=3.03e-03 FER=3.25e-02
2025-09-30 06:58:51,909 | INFO | Epoch 213 Train Time 333.211626291275s

2025-09-30 07:04:48,131 | INFO | Training epoch 214, Batch 1000/1000: LR=9.52e-05, Loss=7.84e-03 BER=3.07e-03 FER=3.26e-02
2025-09-30 07:04:48,620 | INFO | Epoch 214 Train Time 356.70868706703186s

2025-09-30 07:10:50,069 | INFO | Training epoch 215, Batch 1000/1000: LR=9.51e-05, Loss=7.77e-03 BER=3.01e-03 FER=3.23e-02
2025-09-30 07:10:50,568 | INFO | Epoch 215 Train Time 361.94709825515747s

2025-09-30 07:16:43,652 | INFO | Training epoch 216, Batch 1000/1000: LR=9.51e-05, Loss=7.72e-03 BER=3.03e-03 FER=3.25e-02
2025-09-30 07:16:44,128 | INFO | Epoch 216 Train Time 353.55885672569275s

2025-09-30 07:22:40,248 | INFO | Training epoch 217, Batch 1000/1000: LR=9.50e-05, Loss=7.74e-03 BER=3.06e-03 FER=3.28e-02
2025-09-30 07:22:40,730 | INFO | Epoch 217 Train Time 356.6008870601654s

2025-09-30 07:28:36,551 | INFO | Training epoch 218, Batch 1000/1000: LR=9.50e-05, Loss=7.63e-03 BER=2.98e-03 FER=3.21e-02
2025-09-30 07:28:37,046 | INFO | Epoch 218 Train Time 356.3139486312866s

2025-09-30 07:34:25,075 | INFO | Training epoch 219, Batch 1000/1000: LR=9.49e-05, Loss=7.88e-03 BER=3.07e-03 FER=3.31e-02
2025-09-30 07:34:25,534 | INFO | Epoch 219 Train Time 348.4867670536041s

2025-09-30 07:40:18,680 | INFO | Training epoch 220, Batch 1000/1000: LR=9.49e-05, Loss=7.53e-03 BER=2.94e-03 FER=3.19e-02
2025-09-30 07:40:19,185 | INFO | Epoch 220 Train Time 353.64900946617126s

2025-09-30 07:46:17,950 | INFO | Training epoch 221, Batch 1000/1000: LR=9.48e-05, Loss=7.93e-03 BER=3.10e-03 FER=3.25e-02
2025-09-30 07:46:18,476 | INFO | Epoch 221 Train Time 359.2906582355499s

2025-09-30 07:52:17,213 | INFO | Training epoch 222, Batch 1000/1000: LR=9.48e-05, Loss=7.83e-03 BER=3.08e-03 FER=3.28e-02
2025-09-30 07:52:17,695 | INFO | Epoch 222 Train Time 359.21681332588196s

2025-09-30 07:58:27,464 | INFO | Training epoch 223, Batch 1000/1000: LR=9.47e-05, Loss=7.79e-03 BER=3.01e-03 FER=3.19e-02
2025-09-30 07:58:27,944 | INFO | Epoch 223 Train Time 370.2479169368744s

2025-09-30 08:04:41,586 | INFO | Training epoch 224, Batch 1000/1000: LR=9.47e-05, Loss=7.65e-03 BER=2.99e-03 FER=3.25e-02
2025-09-30 08:04:42,070 | INFO | Epoch 224 Train Time 374.12446570396423s

2025-09-30 08:11:03,593 | INFO | Training epoch 225, Batch 1000/1000: LR=9.47e-05, Loss=7.91e-03 BER=3.07e-03 FER=3.29e-02
2025-09-30 08:11:04,062 | INFO | Epoch 225 Train Time 381.98898935317993s

2025-09-30 08:17:31,339 | INFO | Training epoch 226, Batch 1000/1000: LR=9.46e-05, Loss=7.73e-03 BER=3.04e-03 FER=3.22e-02
2025-09-30 08:17:31,819 | INFO | Epoch 226 Train Time 387.75587487220764s

2025-09-30 08:23:59,241 | INFO | Training epoch 227, Batch 1000/1000: LR=9.46e-05, Loss=7.69e-03 BER=3.03e-03 FER=3.22e-02
2025-09-30 08:23:59,751 | INFO | Epoch 227 Train Time 387.931067943573s

2025-09-30 08:30:25,103 | INFO | Training epoch 228, Batch 1000/1000: LR=9.45e-05, Loss=7.63e-03 BER=2.99e-03 FER=3.21e-02
2025-09-30 08:30:25,566 | INFO | Epoch 228 Train Time 385.81317949295044s

2025-09-30 08:36:50,944 | INFO | Training epoch 229, Batch 1000/1000: LR=9.45e-05, Loss=7.52e-03 BER=2.92e-03 FER=3.16e-02
2025-09-30 08:36:51,426 | INFO | Epoch 229 Train Time 385.8591527938843s

2025-09-30 08:43:19,133 | INFO | Training epoch 230, Batch 1000/1000: LR=9.44e-05, Loss=7.56e-03 BER=2.97e-03 FER=3.18e-02
2025-09-30 08:43:19,608 | INFO | Epoch 230 Train Time 388.18037700653076s

2025-09-30 08:49:49,022 | INFO | Training epoch 231, Batch 1000/1000: LR=9.44e-05, Loss=7.73e-03 BER=3.00e-03 FER=3.18e-02
2025-09-30 08:49:49,485 | INFO | Epoch 231 Train Time 389.8751666545868s

2025-09-30 08:56:18,511 | INFO | Training epoch 232, Batch 1000/1000: LR=9.43e-05, Loss=7.66e-03 BER=3.01e-03 FER=3.19e-02
2025-09-30 08:56:18,977 | INFO | Epoch 232 Train Time 389.4916818141937s

2025-09-30 09:02:49,348 | INFO | Training epoch 233, Batch 1000/1000: LR=9.43e-05, Loss=7.85e-03 BER=3.05e-03 FER=3.23e-02
2025-09-30 09:02:49,829 | INFO | Epoch 233 Train Time 390.84996700286865s

2025-09-30 09:05:33,104 | INFO | Training epoch 234, Batch 1000/1000: LR=9.42e-05, Loss=7.93e-03 BER=3.10e-03 FER=3.27e-02
2025-09-30 09:05:33,599 | INFO | Epoch 234 Train Time 163.76859951019287s

2025-09-30 09:07:59,935 | INFO | Training epoch 235, Batch 1000/1000: LR=9.42e-05, Loss=7.68e-03 BER=3.00e-03 FER=3.21e-02
2025-09-30 09:08:00,439 | INFO | Epoch 235 Train Time 146.83874893188477s

2025-09-30 09:10:13,628 | INFO | Training epoch 236, Batch 1000/1000: LR=9.41e-05, Loss=7.74e-03 BER=3.01e-03 FER=3.21e-02
2025-09-30 09:10:14,084 | INFO | Epoch 236 Train Time 133.64312410354614s

2025-09-30 09:12:29,385 | INFO | Training epoch 237, Batch 1000/1000: LR=9.41e-05, Loss=7.74e-03 BER=3.03e-03 FER=3.24e-02
2025-09-30 09:12:29,890 | INFO | Epoch 237 Train Time 135.80508971214294s

2025-09-30 09:15:14,696 | INFO | Training epoch 238, Batch 1000/1000: LR=9.40e-05, Loss=7.77e-03 BER=3.05e-03 FER=3.21e-02
2025-09-30 09:15:15,255 | INFO | Epoch 238 Train Time 165.36386728286743s

2025-09-30 09:17:45,276 | INFO | Training epoch 239, Batch 1000/1000: LR=9.40e-05, Loss=7.72e-03 BER=2.98e-03 FER=3.20e-02
2025-09-30 09:17:45,757 | INFO | Epoch 239 Train Time 150.49885773658752s

2025-09-30 09:20:01,815 | INFO | Training epoch 240, Batch 1000/1000: LR=9.39e-05, Loss=7.59e-03 BER=2.96e-03 FER=3.16e-02
2025-09-30 09:20:02,319 | INFO | Epoch 240 Train Time 136.56177639961243s

2025-09-30 09:22:13,968 | INFO | Training epoch 241, Batch 1000/1000: LR=9.39e-05, Loss=7.75e-03 BER=3.00e-03 FER=3.19e-02
2025-09-30 09:22:14,435 | INFO | Epoch 241 Train Time 132.11373615264893s

2025-09-30 09:24:27,076 | INFO | Training epoch 242, Batch 1000/1000: LR=9.38e-05, Loss=7.61e-03 BER=2.96e-03 FER=3.20e-02
2025-09-30 09:24:27,541 | INFO | Epoch 242 Train Time 133.10448288917542s

2025-09-30 09:26:41,320 | INFO | Training epoch 243, Batch 1000/1000: LR=9.38e-05, Loss=7.58e-03 BER=2.96e-03 FER=3.22e-02
2025-09-30 09:26:41,800 | INFO | Epoch 243 Train Time 134.25796055793762s

2025-09-30 09:28:51,347 | INFO | Training epoch 244, Batch 1000/1000: LR=9.37e-05, Loss=7.81e-03 BER=3.07e-03 FER=3.27e-02
2025-09-30 09:28:51,805 | INFO | Epoch 244 Train Time 130.0047426223755s

2025-09-30 09:31:06,080 | INFO | Training epoch 245, Batch 1000/1000: LR=9.37e-05, Loss=7.44e-03 BER=2.88e-03 FER=3.12e-02
2025-09-30 09:31:06,540 | INFO | Epoch 245 Train Time 134.7338628768921s

2025-09-30 09:31:06,541 | INFO | [P2] saving best_model (QAT) with loss 0.007437 at epoch 245
2025-09-30 09:33:20,298 | INFO | Training epoch 246, Batch 1000/1000: LR=9.36e-05, Loss=7.56e-03 BER=2.94e-03 FER=3.16e-02
2025-09-30 09:33:20,765 | INFO | Epoch 246 Train Time 134.16447401046753s

2025-09-30 09:35:48,158 | INFO | Training epoch 247, Batch 1000/1000: LR=9.36e-05, Loss=7.68e-03 BER=3.01e-03 FER=3.17e-02
2025-09-30 09:35:48,643 | INFO | Epoch 247 Train Time 147.87717866897583s

2025-09-30 09:38:07,865 | INFO | Training epoch 248, Batch 1000/1000: LR=9.35e-05, Loss=7.65e-03 BER=2.96e-03 FER=3.19e-02
2025-09-30 09:38:08,330 | INFO | Epoch 248 Train Time 139.68642210960388s

2025-09-30 09:40:21,528 | INFO | Training epoch 249, Batch 1000/1000: LR=9.35e-05, Loss=7.69e-03 BER=3.02e-03 FER=3.20e-02
2025-09-30 09:40:22,008 | INFO | Epoch 249 Train Time 133.67684316635132s

2025-09-30 09:42:33,382 | INFO | Training epoch 250, Batch 1000/1000: LR=9.34e-05, Loss=7.74e-03 BER=3.02e-03 FER=3.21e-02
2025-09-30 09:42:33,888 | INFO | Epoch 250 Train Time 131.878821849823s

2025-09-30 09:44:44,780 | INFO | Training epoch 251, Batch 1000/1000: LR=9.34e-05, Loss=7.66e-03 BER=2.97e-03 FER=3.17e-02
2025-09-30 09:44:45,236 | INFO | Epoch 251 Train Time 131.3459231853485s

2025-09-30 09:47:01,130 | INFO | Training epoch 252, Batch 1000/1000: LR=9.33e-05, Loss=7.91e-03 BER=3.07e-03 FER=3.29e-02
2025-09-30 09:47:01,610 | INFO | Epoch 252 Train Time 136.37335729599s

2025-09-30 09:49:16,106 | INFO | Training epoch 253, Batch 1000/1000: LR=9.33e-05, Loss=7.40e-03 BER=2.92e-03 FER=3.09e-02
2025-09-30 09:49:16,709 | INFO | Epoch 253 Train Time 135.09707951545715s

2025-09-30 09:49:16,710 | INFO | [P2] saving best_model (QAT) with loss 0.007401 at epoch 253
2025-09-30 09:51:30,621 | INFO | Training epoch 254, Batch 1000/1000: LR=9.32e-05, Loss=7.72e-03 BER=3.05e-03 FER=3.20e-02
2025-09-30 09:51:31,116 | INFO | Epoch 254 Train Time 134.33010745048523s

2025-09-30 09:53:46,670 | INFO | Training epoch 255, Batch 1000/1000: LR=9.32e-05, Loss=7.74e-03 BER=3.03e-03 FER=3.22e-02
2025-09-30 09:53:47,138 | INFO | Epoch 255 Train Time 136.0202157497406s

2025-09-30 09:56:07,943 | INFO | Training epoch 256, Batch 1000/1000: LR=9.31e-05, Loss=7.53e-03 BER=2.93e-03 FER=3.16e-02
2025-09-30 09:56:08,468 | INFO | Epoch 256 Train Time 141.32885479927063s

2025-09-30 09:58:25,668 | INFO | Training epoch 257, Batch 1000/1000: LR=9.31e-05, Loss=7.79e-03 BER=3.02e-03 FER=3.25e-02
2025-09-30 09:58:26,200 | INFO | Epoch 257 Train Time 137.72949934005737s

2025-09-30 10:00:52,646 | INFO | Training epoch 258, Batch 1000/1000: LR=9.30e-05, Loss=7.51e-03 BER=2.91e-03 FER=3.15e-02
2025-09-30 10:00:53,125 | INFO | Epoch 258 Train Time 146.92370009422302s

2025-09-30 10:03:32,468 | INFO | Training epoch 259, Batch 1000/1000: LR=9.29e-05, Loss=7.74e-03 BER=3.02e-03 FER=3.27e-02
2025-09-30 10:03:32,935 | INFO | Epoch 259 Train Time 159.80843448638916s

2025-09-30 10:05:47,543 | INFO | Training epoch 260, Batch 1000/1000: LR=9.29e-05, Loss=7.72e-03 BER=3.04e-03 FER=3.22e-02
2025-09-30 10:05:48,029 | INFO | Epoch 260 Train Time 135.09347558021545s

2025-09-30 10:08:02,076 | INFO | Training epoch 261, Batch 1000/1000: LR=9.28e-05, Loss=7.61e-03 BER=2.97e-03 FER=3.21e-02
2025-09-30 10:08:02,561 | INFO | Epoch 261 Train Time 134.53071665763855s

2025-09-30 10:10:15,120 | INFO | Training epoch 262, Batch 1000/1000: LR=9.28e-05, Loss=7.79e-03 BER=3.02e-03 FER=3.25e-02
2025-09-30 10:10:15,605 | INFO | Epoch 262 Train Time 133.04256200790405s

2025-09-30 10:12:27,574 | INFO | Training epoch 263, Batch 1000/1000: LR=9.27e-05, Loss=7.71e-03 BER=2.99e-03 FER=3.17e-02
2025-09-30 10:12:28,044 | INFO | Epoch 263 Train Time 132.43805074691772s

2025-09-30 10:14:38,907 | INFO | Training epoch 264, Batch 1000/1000: LR=9.27e-05, Loss=7.47e-03 BER=2.93e-03 FER=3.16e-02
2025-09-30 10:14:39,363 | INFO | Epoch 264 Train Time 131.31823754310608s

2025-09-30 10:16:47,145 | INFO | Training epoch 265, Batch 1000/1000: LR=9.26e-05, Loss=7.62e-03 BER=2.96e-03 FER=3.23e-02
2025-09-30 10:16:47,594 | INFO | Epoch 265 Train Time 128.22983598709106s

2025-09-30 10:18:57,057 | INFO | Training epoch 266, Batch 1000/1000: LR=9.26e-05, Loss=7.72e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 10:18:57,577 | INFO | Epoch 266 Train Time 129.98228788375854s

2025-09-30 10:21:22,033 | INFO | Training epoch 267, Batch 1000/1000: LR=9.25e-05, Loss=8.06e-03 BER=3.16e-03 FER=3.30e-02
2025-09-30 10:21:22,515 | INFO | Epoch 267 Train Time 144.9356198310852s

2025-09-30 10:23:33,608 | INFO | Training epoch 268, Batch 1000/1000: LR=9.25e-05, Loss=7.82e-03 BER=3.06e-03 FER=3.26e-02
2025-09-30 10:23:34,062 | INFO | Epoch 268 Train Time 131.5458722114563s

2025-09-30 10:25:45,726 | INFO | Training epoch 269, Batch 1000/1000: LR=9.24e-05, Loss=7.74e-03 BER=3.02e-03 FER=3.23e-02
2025-09-30 10:25:46,202 | INFO | Epoch 269 Train Time 132.1397750377655s

2025-09-30 10:28:11,186 | INFO | Training epoch 270, Batch 1000/1000: LR=9.23e-05, Loss=7.67e-03 BER=3.01e-03 FER=3.17e-02
2025-09-30 10:28:11,747 | INFO | Epoch 270 Train Time 145.542959690094s

2025-09-30 10:30:54,940 | INFO | Training epoch 271, Batch 1000/1000: LR=9.23e-05, Loss=7.78e-03 BER=3.03e-03 FER=3.27e-02
2025-09-30 10:30:55,458 | INFO | Epoch 271 Train Time 163.71007561683655s

2025-09-30 10:33:35,635 | INFO | Training epoch 272, Batch 1000/1000: LR=9.22e-05, Loss=7.75e-03 BER=3.00e-03 FER=3.24e-02
2025-09-30 10:33:36,214 | INFO | Epoch 272 Train Time 160.75406193733215s

2025-09-30 10:36:26,968 | INFO | Training epoch 273, Batch 1000/1000: LR=9.22e-05, Loss=7.58e-03 BER=2.94e-03 FER=3.15e-02
2025-09-30 10:36:27,540 | INFO | Epoch 273 Train Time 171.3253676891327s

2025-09-30 10:39:00,154 | INFO | Training epoch 274, Batch 1000/1000: LR=9.21e-05, Loss=7.86e-03 BER=3.07e-03 FER=3.26e-02
2025-09-30 10:39:00,786 | INFO | Epoch 274 Train Time 153.2455062866211s

2025-09-30 10:41:46,766 | INFO | Training epoch 275, Batch 1000/1000: LR=9.21e-05, Loss=7.52e-03 BER=2.96e-03 FER=3.16e-02
2025-09-30 10:41:47,326 | INFO | Epoch 275 Train Time 166.53834533691406s

2025-09-30 10:44:14,235 | INFO | Training epoch 276, Batch 1000/1000: LR=9.20e-05, Loss=7.71e-03 BER=3.01e-03 FER=3.22e-02
2025-09-30 10:44:14,698 | INFO | Epoch 276 Train Time 147.37131452560425s

2025-09-30 10:46:25,928 | INFO | Training epoch 277, Batch 1000/1000: LR=9.20e-05, Loss=7.74e-03 BER=3.02e-03 FER=3.23e-02
2025-09-30 10:46:26,413 | INFO | Epoch 277 Train Time 131.71412992477417s

2025-09-30 10:48:39,741 | INFO | Training epoch 278, Batch 1000/1000: LR=9.19e-05, Loss=7.71e-03 BER=2.99e-03 FER=3.20e-02
2025-09-30 10:48:40,210 | INFO | Epoch 278 Train Time 133.7945168018341s

2025-09-30 10:50:51,332 | INFO | Training epoch 279, Batch 1000/1000: LR=9.18e-05, Loss=7.57e-03 BER=2.97e-03 FER=3.18e-02
2025-09-30 10:50:51,816 | INFO | Epoch 279 Train Time 131.60510849952698s

2025-09-30 10:53:03,079 | INFO | Training epoch 280, Batch 1000/1000: LR=9.18e-05, Loss=7.60e-03 BER=2.99e-03 FER=3.15e-02
2025-09-30 10:53:03,557 | INFO | Epoch 280 Train Time 131.74038100242615s

2025-09-30 10:55:14,085 | INFO | Training epoch 281, Batch 1000/1000: LR=9.17e-05, Loss=7.60e-03 BER=2.99e-03 FER=3.19e-02
2025-09-30 10:55:14,552 | INFO | Epoch 281 Train Time 130.99340295791626s

2025-09-30 10:57:28,057 | INFO | Training epoch 282, Batch 1000/1000: LR=9.17e-05, Loss=7.63e-03 BER=2.99e-03 FER=3.20e-02
2025-09-30 10:57:28,532 | INFO | Epoch 282 Train Time 133.9782099723816s

2025-09-30 10:59:43,380 | INFO | Training epoch 283, Batch 1000/1000: LR=9.16e-05, Loss=7.72e-03 BER=3.01e-03 FER=3.24e-02
2025-09-30 10:59:43,850 | INFO | Epoch 283 Train Time 135.3171045780182s

2025-09-30 11:01:53,967 | INFO | Training epoch 284, Batch 1000/1000: LR=9.16e-05, Loss=7.52e-03 BER=2.96e-03 FER=3.16e-02
2025-09-30 11:01:54,431 | INFO | Epoch 284 Train Time 130.57898664474487s

2025-09-30 11:04:03,965 | INFO | Training epoch 285, Batch 1000/1000: LR=9.15e-05, Loss=7.81e-03 BER=3.06e-03 FER=3.29e-02
2025-09-30 11:04:04,430 | INFO | Epoch 285 Train Time 129.99799847602844s

2025-09-30 11:06:20,581 | INFO | Training epoch 286, Batch 1000/1000: LR=9.14e-05, Loss=7.54e-03 BER=2.94e-03 FER=3.16e-02
2025-09-30 11:06:21,042 | INFO | Epoch 286 Train Time 136.61111497879028s

2025-09-30 11:08:33,459 | INFO | Training epoch 287, Batch 1000/1000: LR=9.14e-05, Loss=7.62e-03 BER=2.98e-03 FER=3.16e-02
2025-09-30 11:08:33,991 | INFO | Epoch 287 Train Time 132.94800925254822s

2025-09-30 11:10:44,210 | INFO | Training epoch 288, Batch 1000/1000: LR=9.13e-05, Loss=7.65e-03 BER=2.99e-03 FER=3.18e-02
2025-09-30 11:10:44,713 | INFO | Epoch 288 Train Time 130.71896290779114s

2025-09-30 11:12:55,284 | INFO | Training epoch 289, Batch 1000/1000: LR=9.13e-05, Loss=7.64e-03 BER=2.96e-03 FER=3.14e-02
2025-09-30 11:12:55,789 | INFO | Epoch 289 Train Time 131.07488322257996s

2025-09-30 11:15:12,395 | INFO | Training epoch 290, Batch 1000/1000: LR=9.12e-05, Loss=7.57e-03 BER=2.95e-03 FER=3.18e-02
2025-09-30 11:15:12,879 | INFO | Epoch 290 Train Time 137.08934950828552s

2025-09-30 11:17:21,921 | INFO | Training epoch 291, Batch 1000/1000: LR=9.11e-05, Loss=7.56e-03 BER=2.93e-03 FER=3.16e-02
2025-09-30 11:17:22,380 | INFO | Epoch 291 Train Time 129.49867582321167s

2025-09-30 11:19:30,674 | INFO | Training epoch 292, Batch 1000/1000: LR=9.11e-05, Loss=7.67e-03 BER=3.01e-03 FER=3.24e-02
2025-09-30 11:19:31,170 | INFO | Epoch 292 Train Time 128.78836512565613s

2025-09-30 11:21:48,528 | INFO | Training epoch 293, Batch 1000/1000: LR=9.10e-05, Loss=7.53e-03 BER=2.94e-03 FER=3.14e-02
2025-09-30 11:21:48,984 | INFO | Epoch 293 Train Time 137.8137776851654s

2025-09-30 11:23:56,774 | INFO | Training epoch 294, Batch 1000/1000: LR=9.10e-05, Loss=7.85e-03 BER=3.04e-03 FER=3.26e-02
2025-09-30 11:23:57,231 | INFO | Epoch 294 Train Time 128.24442410469055s

2025-09-30 11:26:04,555 | INFO | Training epoch 295, Batch 1000/1000: LR=9.09e-05, Loss=7.35e-03 BER=2.87e-03 FER=3.10e-02
2025-09-30 11:26:05,025 | INFO | Epoch 295 Train Time 127.79317665100098s

2025-09-30 11:26:05,026 | INFO | [P2] saving best_model (QAT) with loss 0.007351 at epoch 295
2025-09-30 11:28:16,362 | INFO | Training epoch 296, Batch 1000/1000: LR=9.08e-05, Loss=7.58e-03 BER=2.95e-03 FER=3.11e-02
2025-09-30 11:28:16,834 | INFO | Epoch 296 Train Time 131.73105382919312s

2025-09-30 11:30:28,658 | INFO | Training epoch 297, Batch 1000/1000: LR=9.08e-05, Loss=7.92e-03 BER=3.07e-03 FER=3.29e-02
2025-09-30 11:30:29,124 | INFO | Epoch 297 Train Time 132.2888584136963s

2025-09-30 11:32:38,026 | INFO | Training epoch 298, Batch 1000/1000: LR=9.07e-05, Loss=7.95e-03 BER=3.13e-03 FER=3.28e-02
2025-09-30 11:32:38,501 | INFO | Epoch 298 Train Time 129.37550115585327s

2025-09-30 11:34:47,756 | INFO | Training epoch 299, Batch 1000/1000: LR=9.07e-05, Loss=7.59e-03 BER=2.95e-03 FER=3.19e-02
2025-09-30 11:34:48,246 | INFO | Epoch 299 Train Time 129.74403762817383s

2025-09-30 11:36:58,842 | INFO | Training epoch 300, Batch 1000/1000: LR=9.06e-05, Loss=7.90e-03 BER=3.11e-03 FER=3.34e-02
2025-09-30 11:36:59,316 | INFO | Epoch 300 Train Time 131.0681495666504s

2025-09-30 11:39:08,725 | INFO | Training epoch 301, Batch 1000/1000: LR=9.05e-05, Loss=7.45e-03 BER=2.94e-03 FER=3.14e-02
2025-09-30 11:39:09,176 | INFO | Epoch 301 Train Time 129.8587884902954s

2025-09-30 11:41:14,810 | INFO | Training epoch 302, Batch 1000/1000: LR=9.05e-05, Loss=7.60e-03 BER=2.97e-03 FER=3.16e-02
2025-09-30 11:41:15,274 | INFO | Epoch 302 Train Time 126.0972306728363s

2025-09-30 11:43:20,779 | INFO | Training epoch 303, Batch 1000/1000: LR=9.04e-05, Loss=7.62e-03 BER=2.96e-03 FER=3.14e-02
2025-09-30 11:43:21,223 | INFO | Epoch 303 Train Time 125.94707345962524s

2025-09-30 11:45:26,252 | INFO | Training epoch 304, Batch 1000/1000: LR=9.04e-05, Loss=7.63e-03 BER=2.99e-03 FER=3.21e-02
2025-09-30 11:45:26,709 | INFO | Epoch 304 Train Time 125.48486590385437s

2025-09-30 11:47:31,957 | INFO | Training epoch 305, Batch 1000/1000: LR=9.03e-05, Loss=7.63e-03 BER=2.99e-03 FER=3.21e-02
2025-09-30 11:47:32,396 | INFO | Epoch 305 Train Time 125.68458819389343s

2025-09-30 11:49:37,438 | INFO | Training epoch 306, Batch 1000/1000: LR=9.02e-05, Loss=7.40e-03 BER=2.87e-03 FER=3.13e-02
2025-09-30 11:49:37,904 | INFO | Epoch 306 Train Time 125.5076642036438s

2025-09-30 11:51:43,179 | INFO | Training epoch 307, Batch 1000/1000: LR=9.02e-05, Loss=7.63e-03 BER=2.99e-03 FER=3.18e-02
2025-09-30 11:51:43,627 | INFO | Epoch 307 Train Time 125.72201752662659s

2025-09-30 11:53:47,621 | INFO | Training epoch 308, Batch 1000/1000: LR=9.01e-05, Loss=7.59e-03 BER=2.95e-03 FER=3.17e-02
2025-09-30 11:53:48,082 | INFO | Epoch 308 Train Time 124.45367646217346s

2025-09-30 11:55:53,357 | INFO | Training epoch 309, Batch 1000/1000: LR=9.01e-05, Loss=7.56e-03 BER=2.99e-03 FER=3.20e-02
2025-09-30 11:55:53,843 | INFO | Epoch 309 Train Time 125.75980377197266s

2025-09-30 11:57:58,742 | INFO | Training epoch 310, Batch 1000/1000: LR=9.00e-05, Loss=7.38e-03 BER=2.87e-03 FER=3.09e-02
2025-09-30 11:57:59,198 | INFO | Epoch 310 Train Time 125.3538601398468s

2025-09-30 12:00:26,632 | INFO | Training epoch 311, Batch 1000/1000: LR=8.99e-05, Loss=7.58e-03 BER=2.96e-03 FER=3.21e-02
2025-09-30 12:00:27,088 | INFO | Epoch 311 Train Time 147.88961839675903s

2025-09-30 12:02:33,521 | INFO | Training epoch 312, Batch 1000/1000: LR=8.99e-05, Loss=7.68e-03 BER=3.02e-03 FER=3.15e-02
2025-09-30 12:02:33,984 | INFO | Epoch 312 Train Time 126.89329767227173s

2025-09-30 12:04:44,954 | INFO | Training epoch 313, Batch 1000/1000: LR=8.98e-05, Loss=7.57e-03 BER=2.94e-03 FER=3.17e-02
2025-09-30 12:04:45,420 | INFO | Epoch 313 Train Time 131.43539595603943s

2025-09-30 12:06:54,824 | INFO | Training epoch 314, Batch 1000/1000: LR=8.97e-05, Loss=7.52e-03 BER=2.92e-03 FER=3.16e-02
2025-09-30 12:06:55,307 | INFO | Epoch 314 Train Time 129.88603043556213s

2025-09-30 12:09:05,395 | INFO | Training epoch 315, Batch 1000/1000: LR=8.97e-05, Loss=7.47e-03 BER=2.91e-03 FER=3.11e-02
2025-09-30 12:09:05,844 | INFO | Epoch 315 Train Time 130.53614974021912s

2025-09-30 12:11:13,453 | INFO | Training epoch 316, Batch 1000/1000: LR=8.96e-05, Loss=7.71e-03 BER=3.02e-03 FER=3.22e-02
2025-09-30 12:11:13,909 | INFO | Epoch 316 Train Time 128.06402444839478s

2025-09-30 12:13:21,973 | INFO | Training epoch 317, Batch 1000/1000: LR=8.95e-05, Loss=7.65e-03 BER=2.95e-03 FER=3.15e-02
2025-09-30 12:13:22,439 | INFO | Epoch 317 Train Time 128.52847480773926s

2025-09-30 12:15:31,232 | INFO | Training epoch 318, Batch 1000/1000: LR=8.95e-05, Loss=7.74e-03 BER=3.01e-03 FER=3.24e-02
2025-09-30 12:15:31,708 | INFO | Epoch 318 Train Time 129.2686951160431s

2025-09-30 12:17:40,475 | INFO | Training epoch 319, Batch 1000/1000: LR=8.94e-05, Loss=7.64e-03 BER=2.97e-03 FER=3.16e-02
2025-09-30 12:17:40,939 | INFO | Epoch 319 Train Time 129.22921681404114s

2025-09-30 12:19:49,299 | INFO | Training epoch 320, Batch 1000/1000: LR=8.94e-05, Loss=7.63e-03 BER=2.94e-03 FER=3.15e-02
2025-09-30 12:19:49,848 | INFO | Epoch 320 Train Time 128.90691566467285s

2025-09-30 12:22:00,809 | INFO | Training epoch 321, Batch 1000/1000: LR=8.93e-05, Loss=7.60e-03 BER=2.96e-03 FER=3.17e-02
2025-09-30 12:22:01,272 | INFO | Epoch 321 Train Time 131.42226243019104s

2025-09-30 12:24:09,123 | INFO | Training epoch 322, Batch 1000/1000: LR=8.92e-05, Loss=7.56e-03 BER=2.96e-03 FER=3.16e-02
2025-09-30 12:24:09,585 | INFO | Epoch 322 Train Time 128.31178832054138s

2025-09-30 12:26:18,371 | INFO | Training epoch 323, Batch 1000/1000: LR=8.92e-05, Loss=7.75e-03 BER=3.02e-03 FER=3.20e-02
2025-09-30 12:26:18,823 | INFO | Epoch 323 Train Time 129.23648166656494s

2025-09-30 12:28:26,810 | INFO | Training epoch 324, Batch 1000/1000: LR=8.91e-05, Loss=7.74e-03 BER=3.00e-03 FER=3.18e-02
2025-09-30 12:28:27,269 | INFO | Epoch 324 Train Time 128.44515323638916s

2025-09-30 12:30:35,708 | INFO | Training epoch 325, Batch 1000/1000: LR=8.90e-05, Loss=7.72e-03 BER=3.00e-03 FER=3.24e-02
2025-09-30 12:30:36,169 | INFO | Epoch 325 Train Time 128.89865040779114s

2025-09-30 12:32:44,309 | INFO | Training epoch 326, Batch 1000/1000: LR=8.90e-05, Loss=7.51e-03 BER=2.96e-03 FER=3.17e-02
2025-09-30 12:32:44,807 | INFO | Epoch 326 Train Time 128.63716387748718s

2025-09-30 12:34:52,909 | INFO | Training epoch 327, Batch 1000/1000: LR=8.89e-05, Loss=7.74e-03 BER=3.04e-03 FER=3.21e-02
2025-09-30 12:34:53,376 | INFO | Epoch 327 Train Time 128.56635999679565s

2025-09-30 12:37:01,312 | INFO | Training epoch 328, Batch 1000/1000: LR=8.88e-05, Loss=7.74e-03 BER=3.05e-03 FER=3.22e-02
2025-09-30 12:37:01,757 | INFO | Epoch 328 Train Time 128.38022804260254s

2025-09-30 12:39:13,786 | INFO | Training epoch 329, Batch 1000/1000: LR=8.88e-05, Loss=7.69e-03 BER=3.04e-03 FER=3.18e-02
2025-09-30 12:39:14,238 | INFO | Epoch 329 Train Time 132.4793417453766s

2025-09-30 12:41:22,517 | INFO | Training epoch 330, Batch 1000/1000: LR=8.87e-05, Loss=7.55e-03 BER=2.98e-03 FER=3.15e-02
2025-09-30 12:41:22,980 | INFO | Epoch 330 Train Time 128.74045825004578s

2025-09-30 12:43:30,537 | INFO | Training epoch 331, Batch 1000/1000: LR=8.86e-05, Loss=7.89e-03 BER=3.08e-03 FER=3.23e-02
2025-09-30 12:43:30,991 | INFO | Epoch 331 Train Time 128.00937747955322s

2025-09-30 12:45:38,006 | INFO | Training epoch 332, Batch 1000/1000: LR=8.86e-05, Loss=7.71e-03 BER=2.98e-03 FER=3.12e-02
2025-09-30 12:45:38,468 | INFO | Epoch 332 Train Time 127.47584128379822s

2025-09-30 12:47:46,470 | INFO | Training epoch 333, Batch 1000/1000: LR=8.85e-05, Loss=7.65e-03 BER=3.03e-03 FER=3.18e-02
2025-09-30 12:47:46,943 | INFO | Epoch 333 Train Time 128.47432041168213s

2025-09-30 12:49:54,470 | INFO | Training epoch 334, Batch 1000/1000: LR=8.84e-05, Loss=7.78e-03 BER=3.03e-03 FER=3.23e-02
2025-09-30 12:49:54,954 | INFO | Epoch 334 Train Time 128.01015973091125s

2025-09-30 12:52:02,778 | INFO | Training epoch 335, Batch 1000/1000: LR=8.84e-05, Loss=7.75e-03 BER=3.05e-03 FER=3.23e-02
2025-09-30 12:52:03,230 | INFO | Epoch 335 Train Time 128.2742884159088s

2025-09-30 12:54:10,371 | INFO | Training epoch 336, Batch 1000/1000: LR=8.83e-05, Loss=7.54e-03 BER=2.96e-03 FER=3.12e-02
2025-09-30 12:54:10,843 | INFO | Epoch 336 Train Time 127.61269497871399s

2025-09-30 12:56:19,630 | INFO | Training epoch 337, Batch 1000/1000: LR=8.82e-05, Loss=7.70e-03 BER=2.98e-03 FER=3.16e-02
2025-09-30 12:56:20,103 | INFO | Epoch 337 Train Time 129.25833988189697s

2025-09-30 12:58:28,681 | INFO | Training epoch 338, Batch 1000/1000: LR=8.82e-05, Loss=7.67e-03 BER=3.03e-03 FER=3.21e-02
2025-09-30 12:58:29,136 | INFO | Epoch 338 Train Time 129.0325062274933s

2025-09-30 13:00:37,451 | INFO | Training epoch 339, Batch 1000/1000: LR=8.81e-05, Loss=7.69e-03 BER=2.99e-03 FER=3.19e-02
2025-09-30 13:00:37,897 | INFO | Epoch 339 Train Time 128.75847911834717s

2025-09-30 13:02:46,235 | INFO | Training epoch 340, Batch 1000/1000: LR=8.80e-05, Loss=7.72e-03 BER=3.03e-03 FER=3.22e-02
2025-09-30 13:02:46,711 | INFO | Epoch 340 Train Time 128.81353378295898s

2025-09-30 13:04:54,331 | INFO | Training epoch 341, Batch 1000/1000: LR=8.80e-05, Loss=7.61e-03 BER=2.98e-03 FER=3.21e-02
2025-09-30 13:04:54,786 | INFO | Epoch 341 Train Time 128.07407021522522s

2025-09-30 13:07:05,891 | INFO | Training epoch 342, Batch 1000/1000: LR=8.79e-05, Loss=7.59e-03 BER=2.97e-03 FER=3.14e-02
2025-09-30 13:07:06,362 | INFO | Epoch 342 Train Time 131.57455587387085s

2025-09-30 13:09:17,659 | INFO | Training epoch 343, Batch 1000/1000: LR=8.78e-05, Loss=7.83e-03 BER=3.06e-03 FER=3.22e-02
2025-09-30 13:09:18,208 | INFO | Epoch 343 Train Time 131.8449206352234s

2025-09-30 13:11:29,296 | INFO | Training epoch 344, Batch 1000/1000: LR=8.78e-05, Loss=7.53e-03 BER=2.95e-03 FER=3.14e-02
2025-09-30 13:11:29,752 | INFO | Epoch 344 Train Time 131.54282474517822s

2025-09-30 13:13:40,325 | INFO | Training epoch 345, Batch 1000/1000: LR=8.77e-05, Loss=7.81e-03 BER=3.04e-03 FER=3.18e-02
2025-09-30 13:13:40,819 | INFO | Epoch 345 Train Time 131.06550931930542s

2025-09-30 13:15:48,693 | INFO | Training epoch 346, Batch 1000/1000: LR=8.76e-05, Loss=7.71e-03 BER=3.04e-03 FER=3.20e-02
2025-09-30 13:15:49,157 | INFO | Epoch 346 Train Time 128.33642959594727s

2025-09-30 13:17:57,867 | INFO | Training epoch 347, Batch 1000/1000: LR=8.76e-05, Loss=7.61e-03 BER=2.99e-03 FER=3.17e-02
2025-09-30 13:17:58,356 | INFO | Epoch 347 Train Time 129.1977777481079s

2025-09-30 13:20:10,373 | INFO | Training epoch 348, Batch 1000/1000: LR=8.75e-05, Loss=7.46e-03 BER=2.93e-03 FER=3.17e-02
2025-09-30 13:20:10,854 | INFO | Epoch 348 Train Time 132.49665021896362s

2025-09-30 13:22:18,838 | INFO | Training epoch 349, Batch 1000/1000: LR=8.74e-05, Loss=7.75e-03 BER=3.03e-03 FER=3.17e-02
2025-09-30 13:22:19,296 | INFO | Epoch 349 Train Time 128.44110894203186s

2025-09-30 13:24:26,873 | INFO | Training epoch 350, Batch 1000/1000: LR=8.74e-05, Loss=7.52e-03 BER=2.94e-03 FER=3.14e-02
2025-09-30 13:24:27,341 | INFO | Epoch 350 Train Time 128.0440957546234s

2025-09-30 13:26:35,338 | INFO | Training epoch 351, Batch 1000/1000: LR=8.73e-05, Loss=7.47e-03 BER=2.89e-03 FER=3.13e-02
2025-09-30 13:26:35,799 | INFO | Epoch 351 Train Time 128.4567894935608s

2025-09-30 13:28:44,087 | INFO | Training epoch 352, Batch 1000/1000: LR=8.72e-05, Loss=7.54e-03 BER=2.95e-03 FER=3.16e-02
2025-09-30 13:28:44,542 | INFO | Epoch 352 Train Time 128.74180459976196s

2025-09-30 13:30:53,353 | INFO | Training epoch 353, Batch 1000/1000: LR=8.71e-05, Loss=7.53e-03 BER=2.93e-03 FER=3.14e-02
2025-09-30 13:30:53,822 | INFO | Epoch 353 Train Time 129.27876210212708s

2025-09-30 13:33:01,733 | INFO | Training epoch 354, Batch 1000/1000: LR=8.71e-05, Loss=7.60e-03 BER=2.97e-03 FER=3.15e-02
2025-09-30 13:33:02,212 | INFO | Epoch 354 Train Time 128.38869619369507s

2025-09-30 13:35:10,042 | INFO | Training epoch 355, Batch 1000/1000: LR=8.70e-05, Loss=7.77e-03 BER=3.04e-03 FER=3.22e-02
2025-09-30 13:35:10,510 | INFO | Epoch 355 Train Time 128.29561734199524s

2025-09-30 13:37:18,518 | INFO | Training epoch 356, Batch 1000/1000: LR=8.69e-05, Loss=7.82e-03 BER=3.05e-03 FER=3.24e-02
2025-09-30 13:37:18,971 | INFO | Epoch 356 Train Time 128.46031403541565s

2025-09-30 13:39:27,248 | INFO | Training epoch 357, Batch 1000/1000: LR=8.69e-05, Loss=7.62e-03 BER=2.99e-03 FER=3.12e-02
2025-09-30 13:39:27,706 | INFO | Epoch 357 Train Time 128.73367285728455s

2025-09-30 13:41:40,432 | INFO | Training epoch 358, Batch 1000/1000: LR=8.68e-05, Loss=7.63e-03 BER=3.00e-03 FER=3.17e-02
2025-09-30 13:41:40,889 | INFO | Epoch 358 Train Time 133.1822702884674s

2025-09-30 13:43:49,824 | INFO | Training epoch 359, Batch 1000/1000: LR=8.67e-05, Loss=7.61e-03 BER=2.99e-03 FER=3.16e-02
2025-09-30 13:43:50,289 | INFO | Epoch 359 Train Time 129.39778757095337s

2025-09-30 13:45:58,220 | INFO | Training epoch 360, Batch 1000/1000: LR=8.67e-05, Loss=7.61e-03 BER=2.95e-03 FER=3.13e-02
2025-09-30 13:45:58,690 | INFO | Epoch 360 Train Time 128.40032529830933s

2025-09-30 13:48:06,878 | INFO | Training epoch 361, Batch 1000/1000: LR=8.66e-05, Loss=7.54e-03 BER=2.92e-03 FER=3.11e-02
2025-09-30 13:48:07,342 | INFO | Epoch 361 Train Time 128.6505196094513s

2025-09-30 13:50:15,363 | INFO | Training epoch 362, Batch 1000/1000: LR=8.65e-05, Loss=7.56e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 13:50:15,820 | INFO | Epoch 362 Train Time 128.47756099700928s

2025-09-30 13:52:23,867 | INFO | Training epoch 363, Batch 1000/1000: LR=8.64e-05, Loss=7.64e-03 BER=3.02e-03 FER=3.22e-02
2025-09-30 13:52:24,326 | INFO | Epoch 363 Train Time 128.50306344032288s

2025-09-30 13:54:32,490 | INFO | Training epoch 364, Batch 1000/1000: LR=8.64e-05, Loss=7.71e-03 BER=2.99e-03 FER=3.22e-02
2025-09-30 13:54:32,942 | INFO | Epoch 364 Train Time 128.6164152622223s

2025-09-30 13:56:41,153 | INFO | Training epoch 365, Batch 1000/1000: LR=8.63e-05, Loss=7.52e-03 BER=2.93e-03 FER=3.13e-02
2025-09-30 13:56:41,605 | INFO | Epoch 365 Train Time 128.6621334552765s

2025-09-30 13:58:50,284 | INFO | Training epoch 366, Batch 1000/1000: LR=8.62e-05, Loss=7.65e-03 BER=3.00e-03 FER=3.16e-02
2025-09-30 13:58:50,742 | INFO | Epoch 366 Train Time 129.13590502738953s

2025-09-30 14:00:58,907 | INFO | Training epoch 367, Batch 1000/1000: LR=8.62e-05, Loss=7.34e-03 BER=2.87e-03 FER=3.03e-02
2025-09-30 14:00:59,370 | INFO | Epoch 367 Train Time 128.626060962677s

2025-09-30 14:00:59,371 | INFO | [P2] saving best_model (QAT) with loss 0.007344 at epoch 367
2025-09-30 14:03:07,773 | INFO | Training epoch 368, Batch 1000/1000: LR=8.61e-05, Loss=7.62e-03 BER=2.97e-03 FER=3.14e-02
2025-09-30 14:03:08,223 | INFO | Epoch 368 Train Time 128.78820967674255s

2025-09-30 14:05:16,523 | INFO | Training epoch 369, Batch 1000/1000: LR=8.60e-05, Loss=7.42e-03 BER=2.90e-03 FER=3.07e-02
2025-09-30 14:05:16,987 | INFO | Epoch 369 Train Time 128.76346826553345s

2025-09-30 14:07:27,915 | INFO | Training epoch 370, Batch 1000/1000: LR=8.59e-05, Loss=7.60e-03 BER=2.94e-03 FER=3.11e-02
2025-09-30 14:07:28,380 | INFO | Epoch 370 Train Time 131.39125537872314s

2025-09-30 14:09:36,474 | INFO | Training epoch 371, Batch 1000/1000: LR=8.59e-05, Loss=7.55e-03 BER=2.96e-03 FER=3.15e-02
2025-09-30 14:09:36,956 | INFO | Epoch 371 Train Time 128.5754542350769s

2025-09-30 14:11:47,723 | INFO | Training epoch 372, Batch 1000/1000: LR=8.58e-05, Loss=7.70e-03 BER=3.03e-03 FER=3.22e-02
2025-09-30 14:11:48,216 | INFO | Epoch 372 Train Time 131.25926208496094s

2025-09-30 14:13:58,912 | INFO | Training epoch 373, Batch 1000/1000: LR=8.57e-05, Loss=7.71e-03 BER=3.02e-03 FER=3.24e-02
2025-09-30 14:13:59,413 | INFO | Epoch 373 Train Time 131.1955544948578s

2025-09-30 14:16:07,460 | INFO | Training epoch 374, Batch 1000/1000: LR=8.56e-05, Loss=7.62e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 14:16:07,912 | INFO | Epoch 374 Train Time 128.49819946289062s

2025-09-30 14:18:16,663 | INFO | Training epoch 375, Batch 1000/1000: LR=8.56e-05, Loss=7.45e-03 BER=2.90e-03 FER=3.07e-02
2025-09-30 14:18:17,118 | INFO | Epoch 375 Train Time 129.20493292808533s

2025-09-30 14:20:28,455 | INFO | Training epoch 376, Batch 1000/1000: LR=8.55e-05, Loss=7.29e-03 BER=2.87e-03 FER=3.05e-02
2025-09-30 14:20:28,919 | INFO | Epoch 376 Train Time 131.79977226257324s

2025-09-30 14:20:28,920 | INFO | [P2] saving best_model (QAT) with loss 0.007287 at epoch 376
2025-09-30 14:22:36,620 | INFO | Training epoch 377, Batch 1000/1000: LR=8.54e-05, Loss=7.68e-03 BER=3.02e-03 FER=3.27e-02
2025-09-30 14:22:37,106 | INFO | Epoch 377 Train Time 128.1215443611145s

2025-09-30 14:24:45,107 | INFO | Training epoch 378, Batch 1000/1000: LR=8.54e-05, Loss=7.41e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 14:24:45,559 | INFO | Epoch 378 Train Time 128.45157957077026s

2025-09-30 14:26:54,218 | INFO | Training epoch 379, Batch 1000/1000: LR=8.53e-05, Loss=7.42e-03 BER=2.90e-03 FER=3.12e-02
2025-09-30 14:26:54,669 | INFO | Epoch 379 Train Time 129.10887265205383s

2025-09-30 14:29:02,696 | INFO | Training epoch 380, Batch 1000/1000: LR=8.52e-05, Loss=7.51e-03 BER=2.94e-03 FER=3.16e-02
2025-09-30 14:29:03,160 | INFO | Epoch 380 Train Time 128.49057173728943s

2025-09-30 14:31:11,780 | INFO | Training epoch 381, Batch 1000/1000: LR=8.51e-05, Loss=7.54e-03 BER=2.94e-03 FER=3.17e-02
2025-09-30 14:31:12,242 | INFO | Epoch 381 Train Time 129.08120369911194s

2025-09-30 14:33:20,476 | INFO | Training epoch 382, Batch 1000/1000: LR=8.51e-05, Loss=7.58e-03 BER=2.97e-03 FER=3.16e-02
2025-09-30 14:33:20,947 | INFO | Epoch 382 Train Time 128.70350575447083s

2025-09-30 14:35:30,323 | INFO | Training epoch 383, Batch 1000/1000: LR=8.50e-05, Loss=7.53e-03 BER=2.94e-03 FER=3.16e-02
2025-09-30 14:35:30,779 | INFO | Epoch 383 Train Time 129.83112049102783s

2025-09-30 14:37:38,778 | INFO | Training epoch 384, Batch 1000/1000: LR=8.49e-05, Loss=7.57e-03 BER=2.94e-03 FER=3.13e-02
2025-09-30 14:37:39,256 | INFO | Epoch 384 Train Time 128.47592782974243s

2025-09-30 14:39:47,744 | INFO | Training epoch 385, Batch 1000/1000: LR=8.48e-05, Loss=7.68e-03 BER=3.01e-03 FER=3.19e-02
2025-09-30 14:39:48,213 | INFO | Epoch 385 Train Time 128.9558482170105s

2025-09-30 14:41:55,888 | INFO | Training epoch 386, Batch 1000/1000: LR=8.48e-05, Loss=7.46e-03 BER=2.91e-03 FER=3.12e-02
2025-09-30 14:41:56,353 | INFO | Epoch 386 Train Time 128.139221906662s

2025-09-30 14:44:10,497 | INFO | Training epoch 387, Batch 1000/1000: LR=8.47e-05, Loss=7.66e-03 BER=3.00e-03 FER=3.19e-02
2025-09-30 14:44:10,959 | INFO | Epoch 387 Train Time 134.60458326339722s

2025-09-30 14:46:19,474 | INFO | Training epoch 388, Batch 1000/1000: LR=8.46e-05, Loss=7.50e-03 BER=2.94e-03 FER=3.10e-02
2025-09-30 14:46:19,926 | INFO | Epoch 388 Train Time 128.96580290794373s

2025-09-30 14:48:28,937 | INFO | Training epoch 389, Batch 1000/1000: LR=8.45e-05, Loss=7.40e-03 BER=2.88e-03 FER=3.06e-02
2025-09-30 14:48:29,388 | INFO | Epoch 389 Train Time 129.46029615402222s

2025-09-30 14:50:37,928 | INFO | Training epoch 390, Batch 1000/1000: LR=8.45e-05, Loss=7.84e-03 BER=3.07e-03 FER=3.25e-02
2025-09-30 14:50:38,383 | INFO | Epoch 390 Train Time 128.99479341506958s

2025-09-30 14:52:47,356 | INFO | Training epoch 391, Batch 1000/1000: LR=8.44e-05, Loss=7.64e-03 BER=2.96e-03 FER=3.16e-02
2025-09-30 14:52:47,825 | INFO | Epoch 391 Train Time 129.4402573108673s

2025-09-30 14:54:55,871 | INFO | Training epoch 392, Batch 1000/1000: LR=8.43e-05, Loss=7.61e-03 BER=2.97e-03 FER=3.18e-02
2025-09-30 14:54:56,332 | INFO | Epoch 392 Train Time 128.50629305839539s

2025-09-30 14:57:05,314 | INFO | Training epoch 393, Batch 1000/1000: LR=8.42e-05, Loss=7.93e-03 BER=3.11e-03 FER=3.28e-02
2025-09-30 14:57:05,805 | INFO | Epoch 393 Train Time 129.4717140197754s

2025-09-30 14:59:14,923 | INFO | Training epoch 394, Batch 1000/1000: LR=8.42e-05, Loss=7.62e-03 BER=2.97e-03 FER=3.20e-02
2025-09-30 14:59:15,383 | INFO | Epoch 394 Train Time 129.57609486579895s

2025-09-30 15:01:23,792 | INFO | Training epoch 395, Batch 1000/1000: LR=8.41e-05, Loss=7.67e-03 BER=3.00e-03 FER=3.20e-02
2025-09-30 15:01:24,240 | INFO | Epoch 395 Train Time 128.85654997825623s

2025-09-30 15:03:33,515 | INFO | Training epoch 396, Batch 1000/1000: LR=8.40e-05, Loss=7.53e-03 BER=2.92e-03 FER=3.10e-02
2025-09-30 15:03:34,001 | INFO | Epoch 396 Train Time 129.75963687896729s

2025-09-30 15:05:42,690 | INFO | Training epoch 397, Batch 1000/1000: LR=8.39e-05, Loss=7.57e-03 BER=2.98e-03 FER=3.17e-02
2025-09-30 15:05:43,130 | INFO | Epoch 397 Train Time 129.127694606781s

2025-09-30 15:07:54,285 | INFO | Training epoch 398, Batch 1000/1000: LR=8.39e-05, Loss=7.55e-03 BER=2.98e-03 FER=3.16e-02
2025-09-30 15:07:54,734 | INFO | Epoch 398 Train Time 131.60364508628845s

2025-09-30 15:10:04,296 | INFO | Training epoch 399, Batch 1000/1000: LR=8.38e-05, Loss=7.64e-03 BER=2.98e-03 FER=3.14e-02
2025-09-30 15:10:04,748 | INFO | Epoch 399 Train Time 130.0124955177307s

2025-09-30 15:12:13,049 | INFO | Training epoch 400, Batch 1000/1000: LR=8.37e-05, Loss=7.96e-03 BER=3.11e-03 FER=3.24e-02
2025-09-30 15:12:13,517 | INFO | Epoch 400 Train Time 128.7682466506958s

2025-09-30 15:14:26,139 | INFO | Training epoch 401, Batch 1000/1000: LR=8.36e-05, Loss=7.46e-03 BER=2.91e-03 FER=3.15e-02
2025-09-30 15:14:26,652 | INFO | Epoch 401 Train Time 133.13370656967163s

2025-09-30 15:16:36,528 | INFO | Training epoch 402, Batch 1000/1000: LR=8.35e-05, Loss=7.43e-03 BER=2.90e-03 FER=3.08e-02
2025-09-30 15:16:37,014 | INFO | Epoch 402 Train Time 130.35970664024353s

2025-09-30 15:18:46,052 | INFO | Training epoch 403, Batch 1000/1000: LR=8.35e-05, Loss=7.46e-03 BER=2.92e-03 FER=3.09e-02
2025-09-30 15:18:46,508 | INFO | Epoch 403 Train Time 129.4934630393982s

2025-09-30 15:20:58,421 | INFO | Training epoch 404, Batch 1000/1000: LR=8.34e-05, Loss=7.61e-03 BER=2.99e-03 FER=3.18e-02
2025-09-30 15:20:58,886 | INFO | Epoch 404 Train Time 132.37555384635925s

2025-09-30 15:23:07,527 | INFO | Training epoch 405, Batch 1000/1000: LR=8.33e-05, Loss=7.46e-03 BER=2.93e-03 FER=3.12e-02
2025-09-30 15:23:07,968 | INFO | Epoch 405 Train Time 129.080961227417s

2025-09-30 15:25:15,942 | INFO | Training epoch 406, Batch 1000/1000: LR=8.32e-05, Loss=7.47e-03 BER=2.89e-03 FER=3.08e-02
2025-09-30 15:25:16,397 | INFO | Epoch 406 Train Time 128.42894458770752s

2025-09-30 15:27:24,936 | INFO | Training epoch 407, Batch 1000/1000: LR=8.32e-05, Loss=7.63e-03 BER=3.04e-03 FER=3.25e-02
2025-09-30 15:27:25,387 | INFO | Epoch 407 Train Time 128.98821306228638s

2025-09-30 15:29:32,507 | INFO | Training epoch 408, Batch 1000/1000: LR=8.31e-05, Loss=7.72e-03 BER=3.03e-03 FER=3.24e-02
2025-09-30 15:29:32,972 | INFO | Epoch 408 Train Time 127.58474206924438s

2025-09-30 15:31:41,258 | INFO | Training epoch 409, Batch 1000/1000: LR=8.30e-05, Loss=7.73e-03 BER=3.02e-03 FER=3.20e-02
2025-09-30 15:31:41,724 | INFO | Epoch 409 Train Time 128.75091528892517s

2025-09-30 15:33:50,920 | INFO | Training epoch 410, Batch 1000/1000: LR=8.29e-05, Loss=7.64e-03 BER=3.01e-03 FER=3.21e-02
2025-09-30 15:33:51,380 | INFO | Epoch 410 Train Time 129.65469479560852s

2025-09-30 15:36:00,495 | INFO | Training epoch 411, Batch 1000/1000: LR=8.28e-05, Loss=7.59e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 15:36:00,953 | INFO | Epoch 411 Train Time 129.57183742523193s

2025-09-30 15:38:08,911 | INFO | Training epoch 412, Batch 1000/1000: LR=8.28e-05, Loss=7.63e-03 BER=2.98e-03 FER=3.15e-02
2025-09-30 15:38:09,369 | INFO | Epoch 412 Train Time 128.4152615070343s

2025-09-30 15:40:18,597 | INFO | Training epoch 413, Batch 1000/1000: LR=8.27e-05, Loss=7.42e-03 BER=2.92e-03 FER=3.10e-02
2025-09-30 15:40:19,063 | INFO | Epoch 413 Train Time 129.6926200389862s

2025-09-30 15:42:27,335 | INFO | Training epoch 414, Batch 1000/1000: LR=8.26e-05, Loss=7.42e-03 BER=2.90e-03 FER=3.10e-02
2025-09-30 15:42:27,785 | INFO | Epoch 414 Train Time 128.72047662734985s

2025-09-30 15:44:36,330 | INFO | Training epoch 415, Batch 1000/1000: LR=8.25e-05, Loss=7.72e-03 BER=3.02e-03 FER=3.16e-02
2025-09-30 15:44:36,783 | INFO | Epoch 415 Train Time 128.99660730361938s

2025-09-30 15:46:52,480 | INFO | Training epoch 416, Batch 1000/1000: LR=8.25e-05, Loss=7.56e-03 BER=2.95e-03 FER=3.11e-02
2025-09-30 15:46:52,928 | INFO | Epoch 416 Train Time 136.1441946029663s

2025-09-30 15:49:01,472 | INFO | Training epoch 417, Batch 1000/1000: LR=8.24e-05, Loss=7.60e-03 BER=2.99e-03 FER=3.14e-02
2025-09-30 15:49:01,946 | INFO | Epoch 417 Train Time 129.01724815368652s

2025-09-30 15:51:10,622 | INFO | Training epoch 418, Batch 1000/1000: LR=8.23e-05, Loss=7.51e-03 BER=2.95e-03 FER=3.11e-02
2025-09-30 15:51:11,120 | INFO | Epoch 418 Train Time 129.17309737205505s

2025-09-30 15:53:20,384 | INFO | Training epoch 419, Batch 1000/1000: LR=8.22e-05, Loss=7.57e-03 BER=2.93e-03 FER=3.12e-02
2025-09-30 15:53:20,844 | INFO | Epoch 419 Train Time 129.72180485725403s

2025-09-30 15:55:29,390 | INFO | Training epoch 420, Batch 1000/1000: LR=8.21e-05, Loss=7.49e-03 BER=2.93e-03 FER=3.13e-02
2025-09-30 15:55:29,834 | INFO | Epoch 420 Train Time 128.98889803886414s

2025-09-30 15:57:38,262 | INFO | Training epoch 421, Batch 1000/1000: LR=8.21e-05, Loss=7.36e-03 BER=2.87e-03 FER=3.05e-02
2025-09-30 15:57:38,710 | INFO | Epoch 421 Train Time 128.87528085708618s

2025-09-30 15:59:48,670 | INFO | Training epoch 422, Batch 1000/1000: LR=8.20e-05, Loss=7.51e-03 BER=2.95e-03 FER=3.12e-02
2025-09-30 15:59:49,133 | INFO | Epoch 422 Train Time 130.42173552513123s

2025-09-30 16:01:57,537 | INFO | Training epoch 423, Batch 1000/1000: LR=8.19e-05, Loss=7.30e-03 BER=2.87e-03 FER=3.06e-02
2025-09-30 16:01:57,981 | INFO | Epoch 423 Train Time 128.84695053100586s

2025-09-30 16:04:06,296 | INFO | Training epoch 424, Batch 1000/1000: LR=8.18e-05, Loss=7.69e-03 BER=3.00e-03 FER=3.19e-02
2025-09-30 16:04:06,750 | INFO | Epoch 424 Train Time 128.7677516937256s

2025-09-30 16:06:16,534 | INFO | Training epoch 425, Batch 1000/1000: LR=8.17e-05, Loss=7.37e-03 BER=2.89e-03 FER=3.09e-02
2025-09-30 16:06:16,977 | INFO | Epoch 425 Train Time 130.2256374359131s

2025-09-30 16:08:28,838 | INFO | Training epoch 426, Batch 1000/1000: LR=8.17e-05, Loss=7.50e-03 BER=2.91e-03 FER=3.09e-02
2025-09-30 16:08:29,329 | INFO | Epoch 426 Train Time 132.35174703598022s

2025-09-30 16:10:38,855 | INFO | Training epoch 427, Batch 1000/1000: LR=8.16e-05, Loss=7.52e-03 BER=2.95e-03 FER=3.09e-02
2025-09-30 16:10:39,295 | INFO | Epoch 427 Train Time 129.96426439285278s

2025-09-30 16:12:48,136 | INFO | Training epoch 428, Batch 1000/1000: LR=8.15e-05, Loss=7.46e-03 BER=2.89e-03 FER=3.10e-02
2025-09-30 16:12:48,581 | INFO | Epoch 428 Train Time 129.28478479385376s

2025-09-30 16:14:58,578 | INFO | Training epoch 429, Batch 1000/1000: LR=8.14e-05, Loss=7.61e-03 BER=3.00e-03 FER=3.15e-02
2025-09-30 16:14:59,035 | INFO | Epoch 429 Train Time 130.4530951976776s

2025-09-30 16:17:10,758 | INFO | Training epoch 430, Batch 1000/1000: LR=8.13e-05, Loss=7.47e-03 BER=2.93e-03 FER=3.12e-02
2025-09-30 16:17:11,248 | INFO | Epoch 430 Train Time 132.21154856681824s

2025-09-30 16:19:23,174 | INFO | Training epoch 431, Batch 1000/1000: LR=8.12e-05, Loss=7.70e-03 BER=3.02e-03 FER=3.20e-02
2025-09-30 16:19:23,636 | INFO | Epoch 431 Train Time 132.3856475353241s

2025-09-30 16:21:35,705 | INFO | Training epoch 432, Batch 1000/1000: LR=8.12e-05, Loss=7.69e-03 BER=3.00e-03 FER=3.17e-02
2025-09-30 16:21:36,169 | INFO | Epoch 432 Train Time 132.5312943458557s

2025-09-30 16:23:45,556 | INFO | Training epoch 433, Batch 1000/1000: LR=8.11e-05, Loss=7.60e-03 BER=2.97e-03 FER=3.17e-02
2025-09-30 16:23:46,006 | INFO | Epoch 433 Train Time 129.83620858192444s

2025-09-30 16:25:54,963 | INFO | Training epoch 434, Batch 1000/1000: LR=8.10e-05, Loss=7.57e-03 BER=2.96e-03 FER=3.10e-02
2025-09-30 16:25:55,420 | INFO | Epoch 434 Train Time 129.4131829738617s

2025-09-30 16:28:05,016 | INFO | Training epoch 435, Batch 1000/1000: LR=8.09e-05, Loss=7.32e-03 BER=2.86e-03 FER=3.07e-02
2025-09-30 16:28:05,480 | INFO | Epoch 435 Train Time 130.05756306648254s

2025-09-30 16:30:13,813 | INFO | Training epoch 436, Batch 1000/1000: LR=8.08e-05, Loss=7.62e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 16:30:14,267 | INFO | Epoch 436 Train Time 128.78577876091003s

2025-09-30 16:32:23,679 | INFO | Training epoch 437, Batch 1000/1000: LR=8.08e-05, Loss=7.63e-03 BER=3.00e-03 FER=3.15e-02
2025-09-30 16:32:24,120 | INFO | Epoch 437 Train Time 129.85222959518433s

2025-09-30 16:34:33,312 | INFO | Training epoch 438, Batch 1000/1000: LR=8.07e-05, Loss=7.46e-03 BER=2.89e-03 FER=3.03e-02
2025-09-30 16:34:33,765 | INFO | Epoch 438 Train Time 129.64235830307007s

2025-09-30 16:36:43,444 | INFO | Training epoch 439, Batch 1000/1000: LR=8.06e-05, Loss=7.30e-03 BER=2.87e-03 FER=3.06e-02
2025-09-30 16:36:43,913 | INFO | Epoch 439 Train Time 130.1469805240631s

2025-09-30 16:38:52,824 | INFO | Training epoch 440, Batch 1000/1000: LR=8.05e-05, Loss=7.79e-03 BER=3.05e-03 FER=3.22e-02
2025-09-30 16:38:53,262 | INFO | Epoch 440 Train Time 129.34760403633118s

2025-09-30 16:41:02,305 | INFO | Training epoch 441, Batch 1000/1000: LR=8.04e-05, Loss=7.46e-03 BER=2.91e-03 FER=3.12e-02
2025-09-30 16:41:02,744 | INFO | Epoch 441 Train Time 129.48055934906006s

2025-09-30 16:43:11,323 | INFO | Training epoch 442, Batch 1000/1000: LR=8.03e-05, Loss=7.42e-03 BER=2.90e-03 FER=3.08e-02
2025-09-30 16:43:11,812 | INFO | Epoch 442 Train Time 129.06719827651978s

2025-09-30 16:45:21,101 | INFO | Training epoch 443, Batch 1000/1000: LR=8.03e-05, Loss=7.44e-03 BER=2.91e-03 FER=3.12e-02
2025-09-30 16:45:21,551 | INFO | Epoch 443 Train Time 129.73678827285767s

2025-09-30 16:47:29,836 | INFO | Training epoch 444, Batch 1000/1000: LR=8.02e-05, Loss=7.30e-03 BER=2.84e-03 FER=3.04e-02
2025-09-30 16:47:30,284 | INFO | Epoch 444 Train Time 128.7313506603241s

2025-09-30 16:49:46,404 | INFO | Training epoch 445, Batch 1000/1000: LR=8.01e-05, Loss=7.49e-03 BER=2.92e-03 FER=3.10e-02
2025-09-30 16:49:46,903 | INFO | Epoch 445 Train Time 136.6168248653412s

2025-09-30 16:51:56,546 | INFO | Training epoch 446, Batch 1000/1000: LR=8.00e-05, Loss=7.70e-03 BER=3.03e-03 FER=3.16e-02
2025-09-30 16:51:57,027 | INFO | Epoch 446 Train Time 130.12266778945923s

2025-09-30 16:54:05,844 | INFO | Training epoch 447, Batch 1000/1000: LR=7.99e-05, Loss=7.57e-03 BER=2.98e-03 FER=3.12e-02
2025-09-30 16:54:06,305 | INFO | Epoch 447 Train Time 129.2772295475006s

2025-09-30 16:56:14,998 | INFO | Training epoch 448, Batch 1000/1000: LR=7.98e-05, Loss=7.67e-03 BER=2.96e-03 FER=3.12e-02
2025-09-30 16:56:15,456 | INFO | Epoch 448 Train Time 129.14939880371094s

2025-09-30 16:58:24,724 | INFO | Training epoch 449, Batch 1000/1000: LR=7.98e-05, Loss=7.54e-03 BER=2.98e-03 FER=3.15e-02
2025-09-30 16:58:25,198 | INFO | Epoch 449 Train Time 129.74072217941284s

2025-09-30 17:00:33,648 | INFO | Training epoch 450, Batch 1000/1000: LR=7.97e-05, Loss=7.60e-03 BER=2.97e-03 FER=3.15e-02
2025-09-30 17:00:34,101 | INFO | Epoch 450 Train Time 128.9027636051178s

2025-09-30 17:02:42,992 | INFO | Training epoch 451, Batch 1000/1000: LR=7.96e-05, Loss=7.34e-03 BER=2.83e-03 FER=3.02e-02
2025-09-30 17:02:43,469 | INFO | Epoch 451 Train Time 129.36637115478516s

2025-09-30 17:04:52,653 | INFO | Training epoch 452, Batch 1000/1000: LR=7.95e-05, Loss=7.52e-03 BER=2.98e-03 FER=3.12e-02
2025-09-30 17:04:53,116 | INFO | Epoch 452 Train Time 129.64665365219116s

2025-09-30 17:07:03,535 | INFO | Training epoch 453, Batch 1000/1000: LR=7.94e-05, Loss=7.50e-03 BER=2.94e-03 FER=3.11e-02
2025-09-30 17:07:04,001 | INFO | Epoch 453 Train Time 130.88396859169006s

2025-09-30 17:09:14,208 | INFO | Training epoch 454, Batch 1000/1000: LR=7.93e-05, Loss=7.40e-03 BER=2.88e-03 FER=3.09e-02
2025-09-30 17:09:14,661 | INFO | Epoch 454 Train Time 130.65889692306519s

2025-09-30 17:11:24,508 | INFO | Training epoch 455, Batch 1000/1000: LR=7.93e-05, Loss=7.48e-03 BER=2.93e-03 FER=3.10e-02
2025-09-30 17:11:24,968 | INFO | Epoch 455 Train Time 130.3051085472107s

2025-09-30 17:13:35,326 | INFO | Training epoch 456, Batch 1000/1000: LR=7.92e-05, Loss=7.49e-03 BER=2.95e-03 FER=3.13e-02
2025-09-30 17:13:35,777 | INFO | Epoch 456 Train Time 130.80813694000244s

2025-09-30 17:15:44,802 | INFO | Training epoch 457, Batch 1000/1000: LR=7.91e-05, Loss=7.43e-03 BER=2.95e-03 FER=3.12e-02
2025-09-30 17:15:45,264 | INFO | Epoch 457 Train Time 129.4850800037384s

2025-09-30 17:17:53,703 | INFO | Training epoch 458, Batch 1000/1000: LR=7.90e-05, Loss=7.28e-03 BER=2.84e-03 FER=3.06e-02
2025-09-30 17:17:54,171 | INFO | Epoch 458 Train Time 128.90603303909302s

2025-09-30 17:17:54,171 | INFO | [P2] saving best_model (QAT) with loss 0.007284 at epoch 458
2025-09-30 17:20:08,862 | INFO | Training epoch 459, Batch 1000/1000: LR=7.89e-05, Loss=7.51e-03 BER=2.95e-03 FER=3.14e-02
2025-09-30 17:20:09,395 | INFO | Epoch 459 Train Time 135.15356850624084s

2025-09-30 17:22:25,126 | INFO | Training epoch 460, Batch 1000/1000: LR=7.88e-05, Loss=7.59e-03 BER=2.98e-03 FER=3.12e-02
2025-09-30 17:22:25,589 | INFO | Epoch 460 Train Time 136.1916012763977s

2025-09-30 17:24:34,568 | INFO | Training epoch 461, Batch 1000/1000: LR=7.88e-05, Loss=7.58e-03 BER=2.94e-03 FER=3.11e-02
2025-09-30 17:24:35,038 | INFO | Epoch 461 Train Time 129.4476969242096s

2025-09-30 17:26:43,731 | INFO | Training epoch 462, Batch 1000/1000: LR=7.87e-05, Loss=7.54e-03 BER=2.95e-03 FER=3.15e-02
2025-09-30 17:26:44,205 | INFO | Epoch 462 Train Time 129.16598868370056s

2025-09-30 17:28:53,264 | INFO | Training epoch 463, Batch 1000/1000: LR=7.86e-05, Loss=7.52e-03 BER=2.91e-03 FER=3.09e-02
2025-09-30 17:28:53,741 | INFO | Epoch 463 Train Time 129.53450059890747s

2025-09-30 17:31:01,668 | INFO | Training epoch 464, Batch 1000/1000: LR=7.85e-05, Loss=7.67e-03 BER=2.98e-03 FER=3.16e-02
2025-09-30 17:31:02,121 | INFO | Epoch 464 Train Time 128.3780176639557s

2025-09-30 17:33:11,677 | INFO | Training epoch 465, Batch 1000/1000: LR=7.84e-05, Loss=7.57e-03 BER=2.95e-03 FER=3.15e-02
2025-09-30 17:33:12,141 | INFO | Epoch 465 Train Time 130.01862144470215s

2025-09-30 17:35:20,786 | INFO | Training epoch 466, Batch 1000/1000: LR=7.83e-05, Loss=7.40e-03 BER=2.90e-03 FER=3.10e-02
2025-09-30 17:35:21,252 | INFO | Epoch 466 Train Time 129.10974597930908s

2025-09-30 17:37:29,874 | INFO | Training epoch 467, Batch 1000/1000: LR=7.82e-05, Loss=7.36e-03 BER=2.88e-03 FER=3.07e-02
2025-09-30 17:37:30,330 | INFO | Epoch 467 Train Time 129.0773627758026s

2025-09-30 17:39:39,046 | INFO | Training epoch 468, Batch 1000/1000: LR=7.82e-05, Loss=7.35e-03 BER=2.85e-03 FER=3.04e-02
2025-09-30 17:39:39,514 | INFO | Epoch 468 Train Time 129.18253254890442s

2025-09-30 17:41:48,596 | INFO | Training epoch 469, Batch 1000/1000: LR=7.81e-05, Loss=7.40e-03 BER=2.89e-03 FER=3.11e-02
2025-09-30 17:41:49,074 | INFO | Epoch 469 Train Time 129.55900859832764s

2025-09-30 17:43:57,284 | INFO | Training epoch 470, Batch 1000/1000: LR=7.80e-05, Loss=7.59e-03 BER=2.98e-03 FER=3.16e-02
2025-09-30 17:43:57,743 | INFO | Epoch 470 Train Time 128.66839599609375s

2025-09-30 17:46:06,867 | INFO | Training epoch 471, Batch 1000/1000: LR=7.79e-05, Loss=7.69e-03 BER=3.00e-03 FER=3.20e-02
2025-09-30 17:46:07,342 | INFO | Epoch 471 Train Time 129.59743690490723s

2025-09-30 17:48:16,075 | INFO | Training epoch 472, Batch 1000/1000: LR=7.78e-05, Loss=7.56e-03 BER=2.95e-03 FER=3.12e-02
2025-09-30 17:48:16,536 | INFO | Epoch 472 Train Time 129.19344449043274s

2025-09-30 17:50:25,760 | INFO | Training epoch 473, Batch 1000/1000: LR=7.77e-05, Loss=7.39e-03 BER=2.89e-03 FER=3.07e-02
2025-09-30 17:50:26,224 | INFO | Epoch 473 Train Time 129.68648433685303s

2025-09-30 17:52:43,495 | INFO | Training epoch 474, Batch 1000/1000: LR=7.76e-05, Loss=7.41e-03 BER=2.88e-03 FER=3.09e-02
2025-09-30 17:52:44,051 | INFO | Epoch 474 Train Time 137.8252351284027s

2025-09-30 17:54:53,109 | INFO | Training epoch 475, Batch 1000/1000: LR=7.75e-05, Loss=7.54e-03 BER=2.96e-03 FER=3.12e-02
2025-09-30 17:54:53,554 | INFO | Epoch 475 Train Time 129.50149416923523s

2025-09-30 17:57:03,714 | INFO | Training epoch 476, Batch 1000/1000: LR=7.75e-05, Loss=7.35e-03 BER=2.85e-03 FER=3.03e-02
2025-09-30 17:57:04,178 | INFO | Epoch 476 Train Time 130.62217330932617s

2025-09-30 17:59:13,693 | INFO | Training epoch 477, Batch 1000/1000: LR=7.74e-05, Loss=7.53e-03 BER=2.93e-03 FER=3.15e-02
2025-09-30 17:59:14,145 | INFO | Epoch 477 Train Time 129.96619653701782s

2025-09-30 18:01:23,313 | INFO | Training epoch 478, Batch 1000/1000: LR=7.73e-05, Loss=7.33e-03 BER=2.89e-03 FER=3.07e-02
2025-09-30 18:01:23,792 | INFO | Epoch 478 Train Time 129.64582061767578s

2025-09-30 18:03:32,603 | INFO | Training epoch 479, Batch 1000/1000: LR=7.72e-05, Loss=7.45e-03 BER=2.93e-03 FER=3.15e-02
2025-09-30 18:03:33,063 | INFO | Epoch 479 Train Time 129.2697949409485s

2025-09-30 18:05:41,693 | INFO | Training epoch 480, Batch 1000/1000: LR=7.71e-05, Loss=7.64e-03 BER=3.00e-03 FER=3.15e-02
2025-09-30 18:05:42,150 | INFO | Epoch 480 Train Time 129.08572936058044s

2025-09-30 18:07:52,620 | INFO | Training epoch 481, Batch 1000/1000: LR=7.70e-05, Loss=7.46e-03 BER=2.92e-03 FER=3.12e-02
2025-09-30 18:07:53,157 | INFO | Epoch 481 Train Time 131.00557231903076s

2025-09-30 18:10:03,303 | INFO | Training epoch 482, Batch 1000/1000: LR=7.69e-05, Loss=7.28e-03 BER=2.83e-03 FER=3.08e-02
2025-09-30 18:10:03,758 | INFO | Epoch 482 Train Time 130.59918928146362s

2025-09-30 18:10:03,759 | INFO | [P2] saving best_model (QAT) with loss 0.007278 at epoch 482
2025-09-30 18:12:14,000 | INFO | Training epoch 483, Batch 1000/1000: LR=7.68e-05, Loss=7.52e-03 BER=2.93e-03 FER=3.11e-02
2025-09-30 18:12:14,469 | INFO | Epoch 483 Train Time 130.651385307312s

2025-09-30 18:14:23,243 | INFO | Training epoch 484, Batch 1000/1000: LR=7.68e-05, Loss=7.63e-03 BER=3.01e-03 FER=3.14e-02
2025-09-30 18:14:23,687 | INFO | Epoch 484 Train Time 129.2160725593567s

2025-09-30 18:16:32,897 | INFO | Training epoch 485, Batch 1000/1000: LR=7.67e-05, Loss=7.26e-03 BER=2.84e-03 FER=3.01e-02
2025-09-30 18:16:33,367 | INFO | Epoch 485 Train Time 129.67979836463928s

2025-09-30 18:16:33,367 | INFO | [P2] saving best_model (QAT) with loss 0.007261 at epoch 485
2025-09-30 18:18:45,330 | INFO | Training epoch 486, Batch 1000/1000: LR=7.66e-05, Loss=7.37e-03 BER=2.88e-03 FER=3.07e-02
2025-09-30 18:18:45,794 | INFO | Epoch 486 Train Time 132.36946487426758s

2025-09-30 18:20:55,090 | INFO | Training epoch 487, Batch 1000/1000: LR=7.65e-05, Loss=7.72e-03 BER=3.03e-03 FER=3.17e-02
2025-09-30 18:20:55,547 | INFO | Epoch 487 Train Time 129.75119829177856s

2025-09-30 18:23:06,073 | INFO | Training epoch 488, Batch 1000/1000: LR=7.64e-05, Loss=7.57e-03 BER=2.96e-03 FER=3.13e-02
2025-09-30 18:23:06,575 | INFO | Epoch 488 Train Time 131.02744102478027s

2025-09-30 18:25:24,521 | INFO | Training epoch 489, Batch 1000/1000: LR=7.63e-05, Loss=7.60e-03 BER=2.96e-03 FER=3.11e-02
2025-09-30 18:25:24,984 | INFO | Epoch 489 Train Time 138.40829396247864s

2025-09-30 18:27:33,572 | INFO | Training epoch 490, Batch 1000/1000: LR=7.62e-05, Loss=7.33e-03 BER=2.87e-03 FER=3.01e-02
2025-09-30 18:27:34,062 | INFO | Epoch 490 Train Time 129.0768346786499s

2025-09-30 18:29:42,894 | INFO | Training epoch 491, Batch 1000/1000: LR=7.61e-05, Loss=7.58e-03 BER=2.93e-03 FER=3.13e-02
2025-09-30 18:29:43,356 | INFO | Epoch 491 Train Time 129.29233741760254s

2025-09-30 18:31:51,907 | INFO | Training epoch 492, Batch 1000/1000: LR=7.61e-05, Loss=7.51e-03 BER=2.93e-03 FER=3.07e-02
2025-09-30 18:31:52,381 | INFO | Epoch 492 Train Time 129.02267146110535s

2025-09-30 18:34:01,667 | INFO | Training epoch 493, Batch 1000/1000: LR=7.60e-05, Loss=7.53e-03 BER=2.94e-03 FER=3.08e-02
2025-09-30 18:34:02,134 | INFO | Epoch 493 Train Time 129.75187849998474s

2025-09-30 18:36:10,894 | INFO | Training epoch 494, Batch 1000/1000: LR=7.59e-05, Loss=7.36e-03 BER=2.86e-03 FER=3.02e-02
2025-09-30 18:36:11,375 | INFO | Epoch 494 Train Time 129.23882913589478s

2025-09-30 18:38:20,595 | INFO | Training epoch 495, Batch 1000/1000: LR=7.58e-05, Loss=7.44e-03 BER=2.94e-03 FER=3.11e-02
2025-09-30 18:38:21,067 | INFO | Epoch 495 Train Time 129.6901512145996s

2025-09-30 18:40:29,675 | INFO | Training epoch 496, Batch 1000/1000: LR=7.57e-05, Loss=7.37e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 18:40:30,131 | INFO | Epoch 496 Train Time 129.06186866760254s

2025-09-30 18:43:00,014 | INFO | Training epoch 497, Batch 1000/1000: LR=7.56e-05, Loss=7.47e-03 BER=2.93e-03 FER=3.09e-02
2025-09-30 18:43:00,538 | INFO | Epoch 497 Train Time 150.40507984161377s

2025-09-30 18:45:16,153 | INFO | Training epoch 498, Batch 1000/1000: LR=7.55e-05, Loss=7.28e-03 BER=2.84e-03 FER=3.00e-02
2025-09-30 18:45:16,603 | INFO | Epoch 498 Train Time 136.06360912322998s

2025-09-30 18:47:31,439 | INFO | Training epoch 499, Batch 1000/1000: LR=7.54e-05, Loss=7.62e-03 BER=2.95e-03 FER=3.17e-02
2025-09-30 18:47:31,901 | INFO | Epoch 499 Train Time 135.29600524902344s

2025-09-30 18:49:55,862 | INFO | Training epoch 500, Batch 1000/1000: LR=7.53e-05, Loss=7.19e-03 BER=2.81e-03 FER=2.99e-02
2025-09-30 18:49:56,334 | INFO | Epoch 500 Train Time 144.43289709091187s

2025-09-30 18:49:56,335 | INFO | [P2] saving best_model (QAT) with loss 0.007186 at epoch 500
2025-09-30 18:52:09,633 | INFO | Training epoch 501, Batch 1000/1000: LR=7.52e-05, Loss=7.38e-03 BER=2.90e-03 FER=3.12e-02
2025-09-30 18:52:10,085 | INFO | Epoch 501 Train Time 133.6832616329193s

2025-09-30 18:54:23,137 | INFO | Training epoch 502, Batch 1000/1000: LR=7.52e-05, Loss=7.42e-03 BER=2.91e-03 FER=3.05e-02
2025-09-30 18:54:23,592 | INFO | Epoch 502 Train Time 133.50557327270508s

2025-09-30 18:56:46,077 | INFO | Training epoch 503, Batch 1000/1000: LR=7.51e-05, Loss=7.30e-03 BER=2.87e-03 FER=3.07e-02
2025-09-30 18:56:46,571 | INFO | Epoch 503 Train Time 142.9782898426056s

2025-09-30 18:58:59,568 | INFO | Training epoch 504, Batch 1000/1000: LR=7.50e-05, Loss=7.40e-03 BER=2.95e-03 FER=3.12e-02
2025-09-30 18:59:00,113 | INFO | Epoch 504 Train Time 133.54100704193115s

2025-09-30 19:01:11,405 | INFO | Training epoch 505, Batch 1000/1000: LR=7.49e-05, Loss=7.27e-03 BER=2.85e-03 FER=3.06e-02
2025-09-30 19:01:11,859 | INFO | Epoch 505 Train Time 131.7446892261505s

2025-09-30 19:03:21,289 | INFO | Training epoch 506, Batch 1000/1000: LR=7.48e-05, Loss=7.59e-03 BER=2.95e-03 FER=3.08e-02
2025-09-30 19:03:21,746 | INFO | Epoch 506 Train Time 129.88612151145935s

2025-09-30 19:05:31,657 | INFO | Training epoch 507, Batch 1000/1000: LR=7.47e-05, Loss=7.52e-03 BER=2.96e-03 FER=3.11e-02
2025-09-30 19:05:32,128 | INFO | Epoch 507 Train Time 130.38112330436707s

2025-09-30 19:07:43,460 | INFO | Training epoch 508, Batch 1000/1000: LR=7.46e-05, Loss=7.41e-03 BER=2.92e-03 FER=3.10e-02
2025-09-30 19:07:44,007 | INFO | Epoch 508 Train Time 131.87851762771606s

2025-09-30 19:09:54,604 | INFO | Training epoch 509, Batch 1000/1000: LR=7.45e-05, Loss=7.18e-03 BER=2.80e-03 FER=2.98e-02
2025-09-30 19:09:55,056 | INFO | Epoch 509 Train Time 131.04680967330933s

2025-09-30 19:09:55,056 | INFO | [P2] saving best_model (QAT) with loss 0.007180 at epoch 509
2025-09-30 19:12:05,494 | INFO | Training epoch 510, Batch 1000/1000: LR=7.44e-05, Loss=7.54e-03 BER=2.97e-03 FER=3.11e-02
2025-09-30 19:12:05,942 | INFO | Epoch 510 Train Time 130.8241322040558s

2025-09-30 19:14:16,388 | INFO | Training epoch 511, Batch 1000/1000: LR=7.43e-05, Loss=7.58e-03 BER=2.98e-03 FER=3.18e-02
2025-09-30 19:14:16,852 | INFO | Epoch 511 Train Time 130.90940499305725s

2025-09-30 19:16:27,609 | INFO | Training epoch 512, Batch 1000/1000: LR=7.43e-05, Loss=7.61e-03 BER=2.98e-03 FER=3.12e-02
2025-09-30 19:16:28,063 | INFO | Epoch 512 Train Time 131.20926451683044s

2025-09-30 19:18:39,074 | INFO | Training epoch 513, Batch 1000/1000: LR=7.42e-05, Loss=7.46e-03 BER=2.90e-03 FER=3.02e-02
2025-09-30 19:18:39,532 | INFO | Epoch 513 Train Time 131.46881437301636s

2025-09-30 19:20:56,292 | INFO | Training epoch 514, Batch 1000/1000: LR=7.41e-05, Loss=7.35e-03 BER=2.86e-03 FER=3.05e-02
2025-09-30 19:20:56,750 | INFO | Epoch 514 Train Time 137.216894865036s

2025-09-30 19:23:06,789 | INFO | Training epoch 515, Batch 1000/1000: LR=7.40e-05, Loss=7.59e-03 BER=2.96e-03 FER=3.11e-02
2025-09-30 19:23:07,241 | INFO | Epoch 515 Train Time 130.4900860786438s

2025-09-30 19:25:17,728 | INFO | Training epoch 516, Batch 1000/1000: LR=7.39e-05, Loss=7.40e-03 BER=2.88e-03 FER=3.06e-02
2025-09-30 19:25:18,177 | INFO | Epoch 516 Train Time 130.9331932067871s

2025-09-30 19:27:35,632 | INFO | Training epoch 517, Batch 1000/1000: LR=7.38e-05, Loss=7.41e-03 BER=2.90e-03 FER=3.08e-02
2025-09-30 19:27:36,115 | INFO | Epoch 517 Train Time 137.9361777305603s

2025-09-30 19:29:48,354 | INFO | Training epoch 518, Batch 1000/1000: LR=7.37e-05, Loss=7.33e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 19:29:48,841 | INFO | Epoch 518 Train Time 132.72553253173828s

2025-09-30 19:31:58,903 | INFO | Training epoch 519, Batch 1000/1000: LR=7.36e-05, Loss=7.29e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 19:31:59,367 | INFO | Epoch 519 Train Time 130.52331352233887s

2025-09-30 19:34:08,850 | INFO | Training epoch 520, Batch 1000/1000: LR=7.35e-05, Loss=7.40e-03 BER=2.88e-03 FER=3.07e-02
2025-09-30 19:34:09,309 | INFO | Epoch 520 Train Time 129.94164729118347s

2025-09-30 19:36:19,493 | INFO | Training epoch 521, Batch 1000/1000: LR=7.34e-05, Loss=7.41e-03 BER=2.90e-03 FER=3.10e-02
2025-09-30 19:36:19,965 | INFO | Epoch 521 Train Time 130.65455675125122s

2025-09-30 19:38:29,940 | INFO | Training epoch 522, Batch 1000/1000: LR=7.33e-05, Loss=7.55e-03 BER=2.96e-03 FER=3.08e-02
2025-09-30 19:38:30,401 | INFO | Epoch 522 Train Time 130.4355640411377s

2025-09-30 19:40:40,764 | INFO | Training epoch 523, Batch 1000/1000: LR=7.32e-05, Loss=7.66e-03 BER=2.96e-03 FER=3.14e-02
2025-09-30 19:40:41,225 | INFO | Epoch 523 Train Time 130.82246708869934s

2025-09-30 19:42:56,491 | INFO | Training epoch 524, Batch 1000/1000: LR=7.32e-05, Loss=7.34e-03 BER=2.86e-03 FER=2.99e-02
2025-09-30 19:42:57,018 | INFO | Epoch 524 Train Time 135.79250359535217s

2025-09-30 19:45:08,322 | INFO | Training epoch 525, Batch 1000/1000: LR=7.31e-05, Loss=7.71e-03 BER=3.01e-03 FER=3.17e-02
2025-09-30 19:45:08,796 | INFO | Epoch 525 Train Time 131.77644681930542s

2025-09-30 19:47:18,329 | INFO | Training epoch 526, Batch 1000/1000: LR=7.30e-05, Loss=7.36e-03 BER=2.89e-03 FER=3.04e-02
2025-09-30 19:47:18,811 | INFO | Epoch 526 Train Time 130.0127944946289s

2025-09-30 19:49:29,620 | INFO | Training epoch 527, Batch 1000/1000: LR=7.29e-05, Loss=7.47e-03 BER=2.90e-03 FER=3.09e-02
2025-09-30 19:49:30,085 | INFO | Epoch 527 Train Time 131.27311038970947s

2025-09-30 19:51:40,440 | INFO | Training epoch 528, Batch 1000/1000: LR=7.28e-05, Loss=7.59e-03 BER=2.96e-03 FER=3.14e-02
2025-09-30 19:51:40,900 | INFO | Epoch 528 Train Time 130.81400632858276s

2025-09-30 19:53:51,510 | INFO | Training epoch 529, Batch 1000/1000: LR=7.27e-05, Loss=7.14e-03 BER=2.80e-03 FER=2.98e-02
2025-09-30 19:53:51,966 | INFO | Epoch 529 Train Time 131.06516098976135s

2025-09-30 19:53:51,967 | INFO | [P2] saving best_model (QAT) with loss 0.007143 at epoch 529
2025-09-30 19:56:03,054 | INFO | Training epoch 530, Batch 1000/1000: LR=7.26e-05, Loss=7.55e-03 BER=2.96e-03 FER=3.09e-02
2025-09-30 19:56:03,513 | INFO | Epoch 530 Train Time 131.4719557762146s

2025-09-30 19:58:15,652 | INFO | Training epoch 531, Batch 1000/1000: LR=7.25e-05, Loss=7.29e-03 BER=2.83e-03 FER=3.00e-02
2025-09-30 19:58:16,118 | INFO | Epoch 531 Train Time 132.60315227508545s

2025-09-30 20:00:37,792 | INFO | Training epoch 532, Batch 1000/1000: LR=7.24e-05, Loss=7.19e-03 BER=2.81e-03 FER=3.01e-02
2025-09-30 20:00:38,298 | INFO | Epoch 532 Train Time 142.17898511886597s

2025-09-30 20:02:49,223 | INFO | Training epoch 533, Batch 1000/1000: LR=7.23e-05, Loss=7.49e-03 BER=2.95e-03 FER=3.08e-02
2025-09-30 20:02:49,686 | INFO | Epoch 533 Train Time 131.3871078491211s

2025-09-30 20:05:00,238 | INFO | Training epoch 534, Batch 1000/1000: LR=7.22e-05, Loss=7.55e-03 BER=2.96e-03 FER=3.13e-02
2025-09-30 20:05:00,699 | INFO | Epoch 534 Train Time 131.01239800453186s

2025-09-30 20:07:11,866 | INFO | Training epoch 535, Batch 1000/1000: LR=7.21e-05, Loss=7.32e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 20:07:12,324 | INFO | Epoch 535 Train Time 131.62376070022583s

2025-09-30 20:09:23,264 | INFO | Training epoch 536, Batch 1000/1000: LR=7.20e-05, Loss=7.34e-03 BER=2.88e-03 FER=3.08e-02
2025-09-30 20:09:23,724 | INFO | Epoch 536 Train Time 131.39951062202454s

2025-09-30 20:11:34,503 | INFO | Training epoch 537, Batch 1000/1000: LR=7.20e-05, Loss=7.60e-03 BER=2.97e-03 FER=3.15e-02
2025-09-30 20:11:34,971 | INFO | Epoch 537 Train Time 131.24521493911743s

2025-09-30 20:13:46,694 | INFO | Training epoch 538, Batch 1000/1000: LR=7.19e-05, Loss=7.37e-03 BER=2.90e-03 FER=3.06e-02
2025-09-30 20:13:47,151 | INFO | Epoch 538 Train Time 132.17943453788757s

2025-09-30 20:16:00,168 | INFO | Training epoch 539, Batch 1000/1000: LR=7.18e-05, Loss=7.11e-03 BER=2.79e-03 FER=2.98e-02
2025-09-30 20:16:00,633 | INFO | Epoch 539 Train Time 133.4809913635254s

2025-09-30 20:16:00,634 | INFO | [P2] saving best_model (QAT) with loss 0.007111 at epoch 539
2025-09-30 20:18:11,589 | INFO | Training epoch 540, Batch 1000/1000: LR=7.17e-05, Loss=7.30e-03 BER=2.87e-03 FER=3.01e-02
2025-09-30 20:18:12,049 | INFO | Epoch 540 Train Time 131.3498342037201s

2025-09-30 20:20:26,942 | INFO | Training epoch 541, Batch 1000/1000: LR=7.16e-05, Loss=7.21e-03 BER=2.84e-03 FER=3.02e-02
2025-09-30 20:20:27,400 | INFO | Epoch 541 Train Time 135.35053157806396s

2025-09-30 20:22:39,589 | INFO | Training epoch 542, Batch 1000/1000: LR=7.15e-05, Loss=7.51e-03 BER=2.94e-03 FER=3.10e-02
2025-09-30 20:22:40,032 | INFO | Epoch 542 Train Time 132.6294665336609s

2025-09-30 20:24:51,719 | INFO | Training epoch 543, Batch 1000/1000: LR=7.14e-05, Loss=7.53e-03 BER=2.97e-03 FER=3.12e-02
2025-09-30 20:24:52,183 | INFO | Epoch 543 Train Time 132.15011548995972s

2025-09-30 20:27:03,739 | INFO | Training epoch 544, Batch 1000/1000: LR=7.13e-05, Loss=7.51e-03 BER=2.93e-03 FER=3.08e-02
2025-09-30 20:27:04,185 | INFO | Epoch 544 Train Time 132.00061893463135s

2025-09-30 20:29:16,706 | INFO | Training epoch 545, Batch 1000/1000: LR=7.12e-05, Loss=7.56e-03 BER=2.97e-03 FER=3.15e-02
2025-09-30 20:29:17,155 | INFO | Epoch 545 Train Time 132.9687762260437s

2025-09-30 20:31:40,176 | INFO | Training epoch 546, Batch 1000/1000: LR=7.11e-05, Loss=7.46e-03 BER=2.92e-03 FER=3.09e-02
2025-09-30 20:31:40,643 | INFO | Epoch 546 Train Time 143.4866316318512s

2025-09-30 20:33:52,682 | INFO | Training epoch 547, Batch 1000/1000: LR=7.10e-05, Loss=7.23e-03 BER=2.82e-03 FER=2.97e-02
2025-09-30 20:33:53,138 | INFO | Epoch 547 Train Time 132.49412751197815s

2025-09-30 20:36:04,655 | INFO | Training epoch 548, Batch 1000/1000: LR=7.09e-05, Loss=7.71e-03 BER=3.01e-03 FER=3.15e-02
2025-09-30 20:36:05,116 | INFO | Epoch 548 Train Time 131.9766948223114s

2025-09-30 20:38:16,925 | INFO | Training epoch 549, Batch 1000/1000: LR=7.08e-05, Loss=7.49e-03 BER=2.91e-03 FER=3.06e-02
2025-09-30 20:38:17,410 | INFO | Epoch 549 Train Time 132.29168939590454s

2025-09-30 20:40:28,829 | INFO | Training epoch 550, Batch 1000/1000: LR=7.07e-05, Loss=7.40e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 20:40:29,290 | INFO | Epoch 550 Train Time 131.87929511070251s

2025-09-30 20:42:42,560 | INFO | Training epoch 551, Batch 1000/1000: LR=7.06e-05, Loss=7.54e-03 BER=2.97e-03 FER=3.10e-02
2025-09-30 20:42:43,009 | INFO | Epoch 551 Train Time 133.71777081489563s

2025-09-30 20:44:55,921 | INFO | Training epoch 552, Batch 1000/1000: LR=7.05e-05, Loss=7.39e-03 BER=2.85e-03 FER=3.00e-02
2025-09-30 20:44:56,374 | INFO | Epoch 552 Train Time 133.36396837234497s

2025-09-30 20:47:07,690 | INFO | Training epoch 553, Batch 1000/1000: LR=7.04e-05, Loss=7.33e-03 BER=2.87e-03 FER=3.10e-02
2025-09-30 20:47:08,160 | INFO | Epoch 553 Train Time 131.78429293632507s

2025-09-30 20:49:19,147 | INFO | Training epoch 554, Batch 1000/1000: LR=7.03e-05, Loss=7.58e-03 BER=2.95e-03 FER=3.12e-02
2025-09-30 20:49:19,615 | INFO | Epoch 554 Train Time 131.4543273448944s

2025-09-30 20:51:31,003 | INFO | Training epoch 555, Batch 1000/1000: LR=7.03e-05, Loss=7.34e-03 BER=2.88e-03 FER=3.04e-02
2025-09-30 20:51:31,473 | INFO | Epoch 555 Train Time 131.85627341270447s

2025-09-30 20:53:43,108 | INFO | Training epoch 556, Batch 1000/1000: LR=7.02e-05, Loss=7.49e-03 BER=2.91e-03 FER=3.11e-02
2025-09-30 20:53:43,557 | INFO | Epoch 556 Train Time 132.08315658569336s

2025-09-30 20:56:04,275 | INFO | Training epoch 557, Batch 1000/1000: LR=7.01e-05, Loss=7.14e-03 BER=2.78e-03 FER=2.99e-02
2025-09-30 20:56:04,805 | INFO | Epoch 557 Train Time 141.24608635902405s

2025-09-30 20:58:22,677 | INFO | Training epoch 558, Batch 1000/1000: LR=7.00e-05, Loss=7.34e-03 BER=2.89e-03 FER=3.09e-02
2025-09-30 20:58:23,156 | INFO | Epoch 558 Train Time 138.35065698623657s

2025-09-30 21:00:37,745 | INFO | Training epoch 559, Batch 1000/1000: LR=6.99e-05, Loss=7.57e-03 BER=2.98e-03 FER=3.11e-02
2025-09-30 21:00:38,204 | INFO | Epoch 559 Train Time 135.045343875885s

2025-09-30 21:02:58,645 | INFO | Training epoch 560, Batch 1000/1000: LR=6.98e-05, Loss=7.37e-03 BER=2.88e-03 FER=3.06e-02
2025-09-30 21:02:59,223 | INFO | Epoch 560 Train Time 141.01788020133972s

2025-09-30 21:05:20,595 | INFO | Training epoch 561, Batch 1000/1000: LR=6.97e-05, Loss=7.41e-03 BER=2.89e-03 FER=3.07e-02
2025-09-30 21:05:21,110 | INFO | Epoch 561 Train Time 141.8862020969391s

2025-09-30 21:07:37,882 | INFO | Training epoch 562, Batch 1000/1000: LR=6.96e-05, Loss=7.24e-03 BER=2.82e-03 FER=2.99e-02
2025-09-30 21:07:38,340 | INFO | Epoch 562 Train Time 137.2265431880951s

2025-09-30 21:09:56,880 | INFO | Training epoch 563, Batch 1000/1000: LR=6.95e-05, Loss=7.36e-03 BER=2.86e-03 FER=3.03e-02
2025-09-30 21:09:57,369 | INFO | Epoch 563 Train Time 139.02847361564636s

2025-09-30 21:12:15,697 | INFO | Training epoch 564, Batch 1000/1000: LR=6.94e-05, Loss=7.30e-03 BER=2.86e-03 FER=3.07e-02
2025-09-30 21:12:16,165 | INFO | Epoch 564 Train Time 138.7947075366974s

2025-09-30 21:14:27,709 | INFO | Training epoch 565, Batch 1000/1000: LR=6.93e-05, Loss=7.29e-03 BER=2.86e-03 FER=3.04e-02
2025-09-30 21:14:28,184 | INFO | Epoch 565 Train Time 132.01800203323364s

2025-09-30 21:16:41,567 | INFO | Training epoch 566, Batch 1000/1000: LR=6.92e-05, Loss=7.41e-03 BER=2.94e-03 FER=3.11e-02
2025-09-30 21:16:42,095 | INFO | Epoch 566 Train Time 133.90960144996643s

2025-09-30 21:18:56,555 | INFO | Training epoch 567, Batch 1000/1000: LR=6.91e-05, Loss=7.30e-03 BER=2.83e-03 FER=2.99e-02
2025-09-30 21:18:57,008 | INFO | Epoch 567 Train Time 134.91110706329346s

2025-09-30 21:21:19,518 | INFO | Training epoch 568, Batch 1000/1000: LR=6.90e-05, Loss=7.68e-03 BER=2.97e-03 FER=3.18e-02
2025-09-30 21:21:20,011 | INFO | Epoch 568 Train Time 143.00207376480103s

2025-09-30 21:23:36,309 | INFO | Training epoch 569, Batch 1000/1000: LR=6.89e-05, Loss=7.36e-03 BER=2.89e-03 FER=3.08e-02
2025-09-30 21:23:36,807 | INFO | Epoch 569 Train Time 136.79540419578552s

2025-09-30 21:25:48,733 | INFO | Training epoch 570, Batch 1000/1000: LR=6.88e-05, Loss=7.29e-03 BER=2.87e-03 FER=3.03e-02
2025-09-30 21:25:49,221 | INFO | Epoch 570 Train Time 132.41162133216858s

2025-09-30 21:28:06,909 | INFO | Training epoch 571, Batch 1000/1000: LR=6.87e-05, Loss=7.30e-03 BER=2.85e-03 FER=3.04e-02
2025-09-30 21:28:07,416 | INFO | Epoch 571 Train Time 138.1931185722351s

2025-09-30 21:30:24,183 | INFO | Training epoch 572, Batch 1000/1000: LR=6.86e-05, Loss=7.20e-03 BER=2.82e-03 FER=3.00e-02
2025-09-30 21:30:24,688 | INFO | Epoch 572 Train Time 137.2713966369629s

2025-09-30 21:32:38,444 | INFO | Training epoch 573, Batch 1000/1000: LR=6.85e-05, Loss=7.38e-03 BER=2.86e-03 FER=3.05e-02
2025-09-30 21:32:38,927 | INFO | Epoch 573 Train Time 134.23679208755493s

2025-09-30 21:34:59,662 | INFO | Training epoch 574, Batch 1000/1000: LR=6.84e-05, Loss=7.32e-03 BER=2.85e-03 FER=3.06e-02
2025-09-30 21:35:00,193 | INFO | Epoch 574 Train Time 141.26454401016235s

2025-09-30 21:37:27,168 | INFO | Training epoch 575, Batch 1000/1000: LR=6.83e-05, Loss=7.59e-03 BER=2.94e-03 FER=3.10e-02
2025-09-30 21:37:27,639 | INFO | Epoch 575 Train Time 147.44484281539917s

2025-09-30 21:39:44,457 | INFO | Training epoch 576, Batch 1000/1000: LR=6.82e-05, Loss=7.64e-03 BER=3.01e-03 FER=3.17e-02
2025-09-30 21:39:45,022 | INFO | Epoch 576 Train Time 137.38277196884155s

2025-09-30 21:42:02,892 | INFO | Training epoch 577, Batch 1000/1000: LR=6.81e-05, Loss=7.27e-03 BER=2.84e-03 FER=3.01e-02
2025-09-30 21:42:03,384 | INFO | Epoch 577 Train Time 138.36094045639038s

2025-09-30 21:44:21,734 | INFO | Training epoch 578, Batch 1000/1000: LR=6.80e-05, Loss=7.53e-03 BER=2.95e-03 FER=3.10e-02
2025-09-30 21:44:22,195 | INFO | Epoch 578 Train Time 138.80956625938416s

2025-09-30 21:46:35,373 | INFO | Training epoch 579, Batch 1000/1000: LR=6.79e-05, Loss=7.69e-03 BER=3.04e-03 FER=3.16e-02
2025-09-30 21:46:35,847 | INFO | Epoch 579 Train Time 133.65144038200378s

2025-09-30 21:48:50,295 | INFO | Training epoch 580, Batch 1000/1000: LR=6.79e-05, Loss=7.30e-03 BER=2.85e-03 FER=3.03e-02
2025-09-30 21:48:50,769 | INFO | Epoch 580 Train Time 134.92088341712952s

2025-09-30 21:51:05,476 | INFO | Training epoch 581, Batch 1000/1000: LR=6.78e-05, Loss=7.69e-03 BER=3.02e-03 FER=3.17e-02
2025-09-30 21:51:06,133 | INFO | Epoch 581 Train Time 135.36268830299377s

2025-09-30 21:53:25,526 | INFO | Training epoch 582, Batch 1000/1000: LR=6.77e-05, Loss=7.39e-03 BER=2.88e-03 FER=3.09e-02
2025-09-30 21:53:26,059 | INFO | Epoch 582 Train Time 139.92562222480774s

2025-09-30 21:55:43,656 | INFO | Training epoch 583, Batch 1000/1000: LR=6.76e-05, Loss=7.24e-03 BER=2.84e-03 FER=3.05e-02
2025-09-30 21:55:44,119 | INFO | Epoch 583 Train Time 138.0590364933014s

2025-09-30 21:58:03,994 | INFO | Training epoch 584, Batch 1000/1000: LR=6.75e-05, Loss=7.46e-03 BER=2.91e-03 FER=3.08e-02
2025-09-30 21:58:04,449 | INFO | Epoch 584 Train Time 140.32763600349426s

2025-09-30 22:00:22,615 | INFO | Training epoch 585, Batch 1000/1000: LR=6.74e-05, Loss=7.33e-03 BER=2.84e-03 FER=3.06e-02
2025-09-30 22:00:23,157 | INFO | Epoch 585 Train Time 138.70700860023499s

2025-09-30 22:02:41,419 | INFO | Training epoch 586, Batch 1000/1000: LR=6.73e-05, Loss=7.34e-03 BER=2.88e-03 FER=3.02e-02
2025-09-30 22:02:42,029 | INFO | Epoch 586 Train Time 138.86945056915283s

2025-09-30 22:05:00,542 | INFO | Training epoch 587, Batch 1000/1000: LR=6.72e-05, Loss=7.23e-03 BER=2.85e-03 FER=3.01e-02
2025-09-30 22:05:01,036 | INFO | Epoch 587 Train Time 139.0054485797882s

2025-09-30 22:07:30,019 | INFO | Training epoch 588, Batch 1000/1000: LR=6.71e-05, Loss=7.36e-03 BER=2.91e-03 FER=3.09e-02
2025-09-30 22:07:30,525 | INFO | Epoch 588 Train Time 149.4876480102539s

2025-09-30 22:09:55,505 | INFO | Training epoch 589, Batch 1000/1000: LR=6.70e-05, Loss=7.31e-03 BER=2.89e-03 FER=3.04e-02
2025-09-30 22:09:56,037 | INFO | Epoch 589 Train Time 145.51041722297668s

2025-09-30 22:12:42,413 | INFO | Training epoch 590, Batch 1000/1000: LR=6.69e-05, Loss=7.26e-03 BER=2.85e-03 FER=3.03e-02
2025-09-30 22:12:42,940 | INFO | Epoch 590 Train Time 166.8998372554779s

2025-09-30 22:15:19,627 | INFO | Training epoch 591, Batch 1000/1000: LR=6.68e-05, Loss=7.35e-03 BER=2.88e-03 FER=3.04e-02
2025-09-30 22:15:20,150 | INFO | Epoch 591 Train Time 157.20935606956482s

2025-09-30 22:17:48,830 | INFO | Training epoch 592, Batch 1000/1000: LR=6.67e-05, Loss=7.35e-03 BER=2.85e-03 FER=3.03e-02
2025-09-30 22:17:49,355 | INFO | Epoch 592 Train Time 149.2041471004486s

2025-09-30 22:20:21,916 | INFO | Training epoch 593, Batch 1000/1000: LR=6.66e-05, Loss=7.22e-03 BER=2.83e-03 FER=3.01e-02
2025-09-30 22:20:22,398 | INFO | Epoch 593 Train Time 153.0418655872345s

2025-09-30 22:22:51,832 | INFO | Training epoch 594, Batch 1000/1000: LR=6.65e-05, Loss=7.56e-03 BER=2.96e-03 FER=3.11e-02
2025-09-30 22:22:52,321 | INFO | Epoch 594 Train Time 149.92114353179932s

2025-09-30 22:25:21,111 | INFO | Training epoch 595, Batch 1000/1000: LR=6.64e-05, Loss=7.16e-03 BER=2.78e-03 FER=2.96e-02
2025-09-30 22:25:21,591 | INFO | Epoch 595 Train Time 149.26892924308777s

2025-09-30 22:27:49,315 | INFO | Training epoch 596, Batch 1000/1000: LR=6.63e-05, Loss=7.52e-03 BER=2.94e-03 FER=3.10e-02
2025-09-30 22:27:49,815 | INFO | Epoch 596 Train Time 148.21965312957764s

2025-09-30 22:30:19,032 | INFO | Training epoch 597, Batch 1000/1000: LR=6.62e-05, Loss=7.63e-03 BER=3.03e-03 FER=3.12e-02
2025-09-30 22:30:19,528 | INFO | Epoch 597 Train Time 149.7117145061493s

2025-09-30 22:32:37,090 | INFO | Training epoch 598, Batch 1000/1000: LR=6.61e-05, Loss=7.59e-03 BER=2.97e-03 FER=3.09e-02
2025-09-30 22:32:37,566 | INFO | Epoch 598 Train Time 138.03705644607544s

2025-09-30 22:34:48,264 | INFO | Training epoch 599, Batch 1000/1000: LR=6.60e-05, Loss=7.65e-03 BER=3.00e-03 FER=3.12e-02
2025-09-30 22:34:48,732 | INFO | Epoch 599 Train Time 131.16528868675232s

2025-09-30 22:36:58,915 | INFO | Training epoch 600, Batch 1000/1000: LR=6.59e-05, Loss=7.57e-03 BER=2.95e-03 FER=3.11e-02
2025-09-30 22:36:59,379 | INFO | Epoch 600 Train Time 130.6457211971283s

2025-09-30 22:39:12,553 | INFO | Training epoch 601, Batch 1000/1000: LR=6.58e-05, Loss=7.27e-03 BER=2.86e-03 FER=3.02e-02
2025-09-30 22:39:13,062 | INFO | Epoch 601 Train Time 133.68282270431519s

2025-09-30 22:41:33,509 | INFO | Training epoch 602, Batch 1000/1000: LR=6.57e-05, Loss=7.32e-03 BER=2.86e-03 FER=3.01e-02
2025-09-30 22:41:34,007 | INFO | Epoch 602 Train Time 140.94291615486145s

2025-09-30 22:43:50,276 | INFO | Training epoch 603, Batch 1000/1000: LR=6.56e-05, Loss=7.21e-03 BER=2.80e-03 FER=2.97e-02
2025-09-30 22:43:50,741 | INFO | Epoch 603 Train Time 136.7329661846161s

2025-09-30 22:46:00,755 | INFO | Training epoch 604, Batch 1000/1000: LR=6.55e-05, Loss=7.42e-03 BER=2.90e-03 FER=3.03e-02
2025-09-30 22:46:01,228 | INFO | Epoch 604 Train Time 130.4856195449829s

2025-09-30 22:48:11,584 | INFO | Training epoch 605, Batch 1000/1000: LR=6.54e-05, Loss=7.44e-03 BER=2.95e-03 FER=3.08e-02
2025-09-30 22:48:12,047 | INFO | Epoch 605 Train Time 130.8169801235199s

2025-09-30 22:50:21,875 | INFO | Training epoch 606, Batch 1000/1000: LR=6.53e-05, Loss=7.43e-03 BER=2.89e-03 FER=3.06e-02
2025-09-30 22:50:22,318 | INFO | Epoch 606 Train Time 130.26990914344788s

2025-09-30 22:52:32,604 | INFO | Training epoch 607, Batch 1000/1000: LR=6.52e-05, Loss=7.54e-03 BER=2.94e-03 FER=3.07e-02
2025-09-30 22:52:33,050 | INFO | Epoch 607 Train Time 130.73061323165894s

2025-09-30 22:54:42,433 | INFO | Training epoch 608, Batch 1000/1000: LR=6.51e-05, Loss=7.45e-03 BER=2.92e-03 FER=3.05e-02
2025-09-30 22:54:42,894 | INFO | Epoch 608 Train Time 129.84225630760193s

2025-09-30 22:56:53,676 | INFO | Training epoch 609, Batch 1000/1000: LR=6.50e-05, Loss=7.50e-03 BER=2.95e-03 FER=3.10e-02
2025-09-30 22:56:54,125 | INFO | Epoch 609 Train Time 131.2308042049408s

2025-09-30 22:59:04,054 | INFO | Training epoch 610, Batch 1000/1000: LR=6.49e-05, Loss=7.37e-03 BER=2.87e-03 FER=3.09e-02
2025-09-30 22:59:04,512 | INFO | Epoch 610 Train Time 130.3858025074005s

2025-09-30 23:01:15,623 | INFO | Training epoch 611, Batch 1000/1000: LR=6.48e-05, Loss=7.47e-03 BER=2.95e-03 FER=3.10e-02
2025-09-30 23:01:16,073 | INFO | Epoch 611 Train Time 131.5578281879425s

2025-09-30 23:03:26,710 | INFO | Training epoch 612, Batch 1000/1000: LR=6.47e-05, Loss=7.31e-03 BER=2.89e-03 FER=3.05e-02
2025-09-30 23:03:27,175 | INFO | Epoch 612 Train Time 131.1012086868286s

2025-09-30 23:05:38,140 | INFO | Training epoch 613, Batch 1000/1000: LR=6.46e-05, Loss=7.44e-03 BER=2.94e-03 FER=3.07e-02
2025-09-30 23:05:38,587 | INFO | Epoch 613 Train Time 131.41063976287842s

2025-09-30 23:07:48,936 | INFO | Training epoch 614, Batch 1000/1000: LR=6.45e-05, Loss=7.40e-03 BER=2.91e-03 FER=3.05e-02
2025-09-30 23:07:49,406 | INFO | Epoch 614 Train Time 130.81839513778687s

2025-09-30 23:09:59,823 | INFO | Training epoch 615, Batch 1000/1000: LR=6.44e-05, Loss=7.77e-03 BER=3.05e-03 FER=3.21e-02
2025-09-30 23:10:00,291 | INFO | Epoch 615 Train Time 130.8831808567047s

2025-09-30 23:12:20,416 | INFO | Training epoch 616, Batch 1000/1000: LR=6.43e-05, Loss=7.23e-03 BER=2.82e-03 FER=2.99e-02
2025-09-30 23:12:20,924 | INFO | Epoch 616 Train Time 140.63130140304565s

2025-09-30 23:14:34,416 | INFO | Training epoch 617, Batch 1000/1000: LR=6.42e-05, Loss=7.37e-03 BER=2.86e-03 FER=3.05e-02
2025-09-30 23:14:34,878 | INFO | Epoch 617 Train Time 133.9532208442688s

2025-09-30 23:16:46,399 | INFO | Training epoch 618, Batch 1000/1000: LR=6.41e-05, Loss=7.12e-03 BER=2.77e-03 FER=2.95e-02
2025-09-30 23:16:46,898 | INFO | Epoch 618 Train Time 132.018000125885s

2025-09-30 23:18:57,739 | INFO | Training epoch 619, Batch 1000/1000: LR=6.40e-05, Loss=7.40e-03 BER=2.92e-03 FER=3.05e-02
2025-09-30 23:18:58,181 | INFO | Epoch 619 Train Time 131.28136038780212s

2025-09-30 23:21:10,984 | INFO | Training epoch 620, Batch 1000/1000: LR=6.39e-05, Loss=7.34e-03 BER=2.90e-03 FER=3.07e-02
2025-09-30 23:21:11,464 | INFO | Epoch 620 Train Time 133.28120923042297s

2025-09-30 23:23:21,674 | INFO | Training epoch 621, Batch 1000/1000: LR=6.38e-05, Loss=7.38e-03 BER=2.89e-03 FER=3.07e-02
2025-09-30 23:23:22,147 | INFO | Epoch 621 Train Time 130.68077087402344s

2025-09-30 23:25:32,266 | INFO | Training epoch 622, Batch 1000/1000: LR=6.37e-05, Loss=7.33e-03 BER=2.87e-03 FER=3.01e-02
2025-09-30 23:25:32,726 | INFO | Epoch 622 Train Time 130.5786018371582s

2025-09-30 23:27:42,983 | INFO | Training epoch 623, Batch 1000/1000: LR=6.36e-05, Loss=7.58e-03 BER=3.00e-03 FER=3.14e-02
2025-09-30 23:27:43,436 | INFO | Epoch 623 Train Time 130.7078459262848s

2025-09-30 23:29:52,895 | INFO | Training epoch 624, Batch 1000/1000: LR=6.35e-05, Loss=7.40e-03 BER=2.91e-03 FER=3.06e-02
2025-09-30 23:29:53,345 | INFO | Epoch 624 Train Time 129.90790104866028s

2025-09-30 23:32:04,338 | INFO | Training epoch 625, Batch 1000/1000: LR=6.34e-05, Loss=7.50e-03 BER=2.92e-03 FER=3.09e-02
2025-09-30 23:32:04,799 | INFO | Epoch 625 Train Time 131.45240449905396s

2025-09-30 23:34:14,683 | INFO | Training epoch 626, Batch 1000/1000: LR=6.33e-05, Loss=7.24e-03 BER=2.86e-03 FER=3.03e-02
2025-09-30 23:34:15,151 | INFO | Epoch 626 Train Time 130.35138535499573s

2025-09-30 23:36:24,718 | INFO | Training epoch 627, Batch 1000/1000: LR=6.32e-05, Loss=7.36e-03 BER=2.88e-03 FER=3.03e-02
2025-09-30 23:36:25,196 | INFO | Epoch 627 Train Time 130.0436863899231s

2025-09-30 23:38:35,174 | INFO | Training epoch 628, Batch 1000/1000: LR=6.31e-05, Loss=7.63e-03 BER=3.01e-03 FER=3.14e-02
2025-09-30 23:38:35,628 | INFO | Epoch 628 Train Time 130.4317021369934s

2025-09-30 23:40:45,360 | INFO | Training epoch 629, Batch 1000/1000: LR=6.30e-05, Loss=7.21e-03 BER=2.83e-03 FER=3.01e-02
2025-09-30 23:40:45,811 | INFO | Epoch 629 Train Time 130.1819703578949s

2025-09-30 23:42:57,045 | INFO | Training epoch 630, Batch 1000/1000: LR=6.29e-05, Loss=7.42e-03 BER=2.88e-03 FER=3.03e-02
2025-09-30 23:42:57,585 | INFO | Epoch 630 Train Time 131.77243399620056s

2025-09-30 23:45:23,625 | INFO | Training epoch 631, Batch 1000/1000: LR=6.28e-05, Loss=7.32e-03 BER=2.82e-03 FER=2.97e-02
2025-09-30 23:45:24,086 | INFO | Epoch 631 Train Time 146.50050139427185s

2025-09-30 23:47:33,394 | INFO | Training epoch 632, Batch 1000/1000: LR=6.27e-05, Loss=7.42e-03 BER=2.89e-03 FER=3.07e-02
2025-09-30 23:47:33,885 | INFO | Epoch 632 Train Time 129.79744243621826s

2025-09-30 23:49:43,809 | INFO | Training epoch 633, Batch 1000/1000: LR=6.26e-05, Loss=7.25e-03 BER=2.85e-03 FER=3.04e-02
2025-09-30 23:49:44,276 | INFO | Epoch 633 Train Time 130.38940334320068s

2025-09-30 23:51:54,134 | INFO | Training epoch 634, Batch 1000/1000: LR=6.25e-05, Loss=7.30e-03 BER=2.85e-03 FER=3.00e-02
2025-09-30 23:51:54,587 | INFO | Epoch 634 Train Time 130.30980443954468s

2025-09-30 23:54:04,667 | INFO | Training epoch 635, Batch 1000/1000: LR=6.24e-05, Loss=7.55e-03 BER=2.96e-03 FER=3.08e-02
2025-09-30 23:54:05,119 | INFO | Epoch 635 Train Time 130.5309522151947s

2025-09-30 23:56:15,157 | INFO | Training epoch 636, Batch 1000/1000: LR=6.23e-05, Loss=7.61e-03 BER=2.98e-03 FER=3.13e-02
2025-09-30 23:56:15,594 | INFO | Epoch 636 Train Time 130.47365617752075s

2025-09-30 23:58:26,732 | INFO | Training epoch 637, Batch 1000/1000: LR=6.22e-05, Loss=7.33e-03 BER=2.88e-03 FER=3.07e-02
2025-09-30 23:58:27,202 | INFO | Epoch 637 Train Time 131.60748624801636s

2025-10-01 00:00:36,374 | INFO | Training epoch 638, Batch 1000/1000: LR=6.21e-05, Loss=7.46e-03 BER=2.94e-03 FER=3.07e-02
2025-10-01 00:00:36,839 | INFO | Epoch 638 Train Time 129.6353726387024s

2025-10-01 00:02:46,525 | INFO | Training epoch 639, Batch 1000/1000: LR=6.20e-05, Loss=7.40e-03 BER=2.91e-03 FER=3.07e-02
2025-10-01 00:02:47,042 | INFO | Epoch 639 Train Time 130.20238590240479s

2025-10-01 00:04:57,655 | INFO | Training epoch 640, Batch 1000/1000: LR=6.19e-05, Loss=7.27e-03 BER=2.82e-03 FER=3.02e-02
2025-10-01 00:04:58,128 | INFO | Epoch 640 Train Time 131.08464241027832s

2025-10-01 00:07:07,983 | INFO | Training epoch 641, Batch 1000/1000: LR=6.18e-05, Loss=7.33e-03 BER=2.87e-03 FER=3.03e-02
2025-10-01 00:07:08,429 | INFO | Epoch 641 Train Time 130.29971432685852s

2025-10-01 00:09:17,896 | INFO | Training epoch 642, Batch 1000/1000: LR=6.17e-05, Loss=7.49e-03 BER=2.97e-03 FER=3.10e-02
2025-10-01 00:09:18,346 | INFO | Epoch 642 Train Time 129.91427087783813s

2025-10-01 00:11:28,308 | INFO | Training epoch 643, Batch 1000/1000: LR=6.16e-05, Loss=7.33e-03 BER=2.84e-03 FER=3.03e-02
2025-10-01 00:11:28,771 | INFO | Epoch 643 Train Time 130.42320108413696s

2025-10-01 00:13:39,271 | INFO | Training epoch 644, Batch 1000/1000: LR=6.15e-05, Loss=7.29e-03 BER=2.88e-03 FER=3.03e-02
2025-10-01 00:13:39,728 | INFO | Epoch 644 Train Time 130.9548683166504s

2025-10-01 00:15:52,604 | INFO | Training epoch 645, Batch 1000/1000: LR=6.14e-05, Loss=7.54e-03 BER=2.97e-03 FER=3.10e-02
2025-10-01 00:15:53,098 | INFO | Epoch 645 Train Time 133.36953234672546s

2025-10-01 00:18:16,186 | INFO | Training epoch 646, Batch 1000/1000: LR=6.13e-05, Loss=7.58e-03 BER=2.95e-03 FER=3.07e-02
2025-10-01 00:18:16,660 | INFO | Epoch 646 Train Time 143.55918312072754s

2025-10-01 00:20:29,801 | INFO | Training epoch 647, Batch 1000/1000: LR=6.12e-05, Loss=7.24e-03 BER=2.84e-03 FER=3.00e-02
2025-10-01 00:20:30,262 | INFO | Epoch 647 Train Time 133.60144472122192s

2025-10-01 00:22:38,894 | INFO | Training epoch 648, Batch 1000/1000: LR=6.11e-05, Loss=7.23e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 00:22:39,361 | INFO | Epoch 648 Train Time 129.09753274917603s

2025-10-01 00:24:48,817 | INFO | Training epoch 649, Batch 1000/1000: LR=6.10e-05, Loss=7.33e-03 BER=2.90e-03 FER=3.07e-02
2025-10-01 00:24:49,283 | INFO | Epoch 649 Train Time 129.92157793045044s

2025-10-01 00:26:59,175 | INFO | Training epoch 650, Batch 1000/1000: LR=6.09e-05, Loss=7.39e-03 BER=2.89e-03 FER=3.05e-02
2025-10-01 00:26:59,619 | INFO | Epoch 650 Train Time 130.33470153808594s

2025-10-01 00:29:09,404 | INFO | Training epoch 651, Batch 1000/1000: LR=6.08e-05, Loss=7.36e-03 BER=2.87e-03 FER=3.04e-02
2025-10-01 00:29:09,865 | INFO | Epoch 651 Train Time 130.2453269958496s

2025-10-01 00:31:19,311 | INFO | Training epoch 652, Batch 1000/1000: LR=6.07e-05, Loss=7.32e-03 BER=2.89e-03 FER=3.04e-02
2025-10-01 00:31:19,771 | INFO | Epoch 652 Train Time 129.90517401695251s

2025-10-01 00:33:30,294 | INFO | Training epoch 653, Batch 1000/1000: LR=6.06e-05, Loss=7.19e-03 BER=2.84e-03 FER=2.98e-02
2025-10-01 00:33:30,745 | INFO | Epoch 653 Train Time 130.97229862213135s

2025-10-01 00:35:40,560 | INFO | Training epoch 654, Batch 1000/1000: LR=6.05e-05, Loss=7.16e-03 BER=2.81e-03 FER=2.99e-02
2025-10-01 00:35:40,996 | INFO | Epoch 654 Train Time 130.2506721019745s

2025-10-01 00:37:51,725 | INFO | Training epoch 655, Batch 1000/1000: LR=6.04e-05, Loss=7.46e-03 BER=2.93e-03 FER=3.06e-02
2025-10-01 00:37:52,205 | INFO | Epoch 655 Train Time 131.20773696899414s

2025-10-01 00:40:01,898 | INFO | Training epoch 656, Batch 1000/1000: LR=6.03e-05, Loss=7.54e-03 BER=2.97e-03 FER=3.11e-02
2025-10-01 00:40:02,361 | INFO | Epoch 656 Train Time 130.15415287017822s

2025-10-01 00:42:13,118 | INFO | Training epoch 657, Batch 1000/1000: LR=6.02e-05, Loss=7.36e-03 BER=2.89e-03 FER=3.05e-02
2025-10-01 00:42:13,567 | INFO | Epoch 657 Train Time 131.20422458648682s

2025-10-01 00:44:26,694 | INFO | Training epoch 658, Batch 1000/1000: LR=6.01e-05, Loss=7.33e-03 BER=2.87e-03 FER=3.03e-02
2025-10-01 00:44:27,150 | INFO | Epoch 658 Train Time 133.5816524028778s

2025-10-01 00:46:37,081 | INFO | Training epoch 659, Batch 1000/1000: LR=6.00e-05, Loss=7.31e-03 BER=2.88e-03 FER=3.00e-02
2025-10-01 00:46:37,543 | INFO | Epoch 659 Train Time 130.39139413833618s

2025-10-01 00:48:54,349 | INFO | Training epoch 660, Batch 1000/1000: LR=5.99e-05, Loss=7.18e-03 BER=2.81e-03 FER=2.96e-02
2025-10-01 00:48:54,847 | INFO | Epoch 660 Train Time 137.30381083488464s

2025-10-01 00:51:12,257 | INFO | Training epoch 661, Batch 1000/1000: LR=5.98e-05, Loss=7.33e-03 BER=2.88e-03 FER=3.01e-02
2025-10-01 00:51:12,703 | INFO | Epoch 661 Train Time 137.85392689704895s

2025-10-01 00:53:22,469 | INFO | Training epoch 662, Batch 1000/1000: LR=5.97e-05, Loss=7.30e-03 BER=2.87e-03 FER=3.06e-02
2025-10-01 00:53:22,904 | INFO | Epoch 662 Train Time 130.19976329803467s

2025-10-01 00:55:33,042 | INFO | Training epoch 663, Batch 1000/1000: LR=5.96e-05, Loss=7.23e-03 BER=2.80e-03 FER=2.96e-02
2025-10-01 00:55:33,489 | INFO | Epoch 663 Train Time 130.5839807987213s

2025-10-01 00:57:44,218 | INFO | Training epoch 664, Batch 1000/1000: LR=5.95e-05, Loss=7.24e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 00:57:44,686 | INFO | Epoch 664 Train Time 131.19584250450134s

2025-10-01 00:59:55,189 | INFO | Training epoch 665, Batch 1000/1000: LR=5.94e-05, Loss=7.32e-03 BER=2.86e-03 FER=3.04e-02
2025-10-01 00:59:55,665 | INFO | Epoch 665 Train Time 130.97733330726624s

2025-10-01 01:02:06,791 | INFO | Training epoch 666, Batch 1000/1000: LR=5.93e-05, Loss=7.21e-03 BER=2.84e-03 FER=3.00e-02
2025-10-01 01:02:07,254 | INFO | Epoch 666 Train Time 131.58743524551392s

2025-10-01 01:04:19,103 | INFO | Training epoch 667, Batch 1000/1000: LR=5.92e-05, Loss=7.37e-03 BER=2.91e-03 FER=3.05e-02
2025-10-01 01:04:19,566 | INFO | Epoch 667 Train Time 132.3110065460205s

2025-10-01 01:06:31,036 | INFO | Training epoch 668, Batch 1000/1000: LR=5.91e-05, Loss=6.97e-03 BER=2.73e-03 FER=2.89e-02
2025-10-01 01:06:31,507 | INFO | Epoch 668 Train Time 131.93880701065063s

2025-10-01 01:06:31,509 | INFO | [P2] saving best_model (QAT) with loss 0.006973 at epoch 668
2025-10-01 01:08:42,953 | INFO | Training epoch 669, Batch 1000/1000: LR=5.90e-05, Loss=7.17e-03 BER=2.84e-03 FER=2.97e-02
2025-10-01 01:08:43,396 | INFO | Epoch 669 Train Time 131.8159840106964s

2025-10-01 01:10:54,624 | INFO | Training epoch 670, Batch 1000/1000: LR=5.89e-05, Loss=7.09e-03 BER=2.79e-03 FER=2.96e-02
2025-10-01 01:10:55,088 | INFO | Epoch 670 Train Time 131.6898512840271s

2025-10-01 01:13:07,759 | INFO | Training epoch 671, Batch 1000/1000: LR=5.88e-05, Loss=7.57e-03 BER=2.97e-03 FER=3.15e-02
2025-10-01 01:13:08,214 | INFO | Epoch 671 Train Time 133.1244728565216s

2025-10-01 01:15:19,538 | INFO | Training epoch 672, Batch 1000/1000: LR=5.87e-05, Loss=7.26e-03 BER=2.86e-03 FER=3.03e-02
2025-10-01 01:15:20,009 | INFO | Epoch 672 Train Time 131.79392409324646s

2025-10-01 01:17:31,417 | INFO | Training epoch 673, Batch 1000/1000: LR=5.86e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.97e-02
2025-10-01 01:17:31,877 | INFO | Epoch 673 Train Time 131.86711311340332s

2025-10-01 01:19:42,990 | INFO | Training epoch 674, Batch 1000/1000: LR=5.84e-05, Loss=7.24e-03 BER=2.81e-03 FER=2.98e-02
2025-10-01 01:19:43,445 | INFO | Epoch 674 Train Time 131.56722497940063s

2025-10-01 01:22:14,947 | INFO | Training epoch 675, Batch 1000/1000: LR=5.83e-05, Loss=7.22e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 01:22:15,436 | INFO | Epoch 675 Train Time 151.98970007896423s

2025-10-01 01:24:26,807 | INFO | Training epoch 676, Batch 1000/1000: LR=5.82e-05, Loss=7.35e-03 BER=2.87e-03 FER=3.05e-02
2025-10-01 01:24:27,259 | INFO | Epoch 676 Train Time 131.82101249694824s

2025-10-01 01:26:38,603 | INFO | Training epoch 677, Batch 1000/1000: LR=5.81e-05, Loss=7.39e-03 BER=2.91e-03 FER=3.06e-02
2025-10-01 01:26:39,059 | INFO | Epoch 677 Train Time 131.79782795906067s

2025-10-01 01:28:51,321 | INFO | Training epoch 678, Batch 1000/1000: LR=5.80e-05, Loss=7.37e-03 BER=2.90e-03 FER=3.08e-02
2025-10-01 01:28:51,802 | INFO | Epoch 678 Train Time 132.7420299053192s

2025-10-01 01:31:03,100 | INFO | Training epoch 679, Batch 1000/1000: LR=5.79e-05, Loss=7.29e-03 BER=2.88e-03 FER=3.02e-02
2025-10-01 01:31:03,567 | INFO | Epoch 679 Train Time 131.76418781280518s

2025-10-01 01:33:14,244 | INFO | Training epoch 680, Batch 1000/1000: LR=5.78e-05, Loss=7.59e-03 BER=2.95e-03 FER=3.13e-02
2025-10-01 01:33:14,710 | INFO | Epoch 680 Train Time 131.14248085021973s

2025-10-01 01:35:24,918 | INFO | Training epoch 681, Batch 1000/1000: LR=5.77e-05, Loss=7.35e-03 BER=2.89e-03 FER=3.02e-02
2025-10-01 01:35:25,375 | INFO | Epoch 681 Train Time 130.66390562057495s

2025-10-01 01:37:35,956 | INFO | Training epoch 682, Batch 1000/1000: LR=5.76e-05, Loss=7.47e-03 BER=2.91e-03 FER=3.04e-02
2025-10-01 01:37:36,418 | INFO | Epoch 682 Train Time 131.0412528514862s

2025-10-01 01:39:47,373 | INFO | Training epoch 683, Batch 1000/1000: LR=5.75e-05, Loss=7.43e-03 BER=2.92e-03 FER=3.05e-02
2025-10-01 01:39:47,848 | INFO | Epoch 683 Train Time 131.42923307418823s

2025-10-01 01:41:58,132 | INFO | Training epoch 684, Batch 1000/1000: LR=5.74e-05, Loss=7.28e-03 BER=2.86e-03 FER=3.00e-02
2025-10-01 01:41:58,588 | INFO | Epoch 684 Train Time 130.739275932312s

2025-10-01 01:44:11,695 | INFO | Training epoch 685, Batch 1000/1000: LR=5.73e-05, Loss=7.37e-03 BER=2.87e-03 FER=3.03e-02
2025-10-01 01:44:12,182 | INFO | Epoch 685 Train Time 133.59334993362427s

2025-10-01 01:46:22,429 | INFO | Training epoch 686, Batch 1000/1000: LR=5.72e-05, Loss=7.29e-03 BER=2.86e-03 FER=2.99e-02
2025-10-01 01:46:22,914 | INFO | Epoch 686 Train Time 130.73096752166748s

2025-10-01 01:48:33,532 | INFO | Training epoch 687, Batch 1000/1000: LR=5.71e-05, Loss=7.35e-03 BER=2.88e-03 FER=3.03e-02
2025-10-01 01:48:34,013 | INFO | Epoch 687 Train Time 131.09673070907593s

2025-10-01 01:50:44,368 | INFO | Training epoch 688, Batch 1000/1000: LR=5.70e-05, Loss=7.12e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 01:50:44,860 | INFO | Epoch 688 Train Time 130.8447654247284s

2025-10-01 01:52:56,169 | INFO | Training epoch 689, Batch 1000/1000: LR=5.69e-05, Loss=7.23e-03 BER=2.82e-03 FER=2.97e-02
2025-10-01 01:52:56,621 | INFO | Epoch 689 Train Time 131.7601318359375s

2025-10-01 01:55:23,411 | INFO | Training epoch 690, Batch 1000/1000: LR=5.68e-05, Loss=7.39e-03 BER=2.88e-03 FER=3.04e-02
2025-10-01 01:55:23,862 | INFO | Epoch 690 Train Time 147.2396845817566s

2025-10-01 01:57:34,576 | INFO | Training epoch 691, Batch 1000/1000: LR=5.67e-05, Loss=7.24e-03 BER=2.83e-03 FER=3.00e-02
2025-10-01 01:57:35,023 | INFO | Epoch 691 Train Time 131.1602144241333s

2025-10-01 01:59:45,344 | INFO | Training epoch 692, Batch 1000/1000: LR=5.66e-05, Loss=7.20e-03 BER=2.81e-03 FER=2.93e-02
2025-10-01 01:59:45,790 | INFO | Epoch 692 Train Time 130.76549339294434s

2025-10-01 02:01:57,218 | INFO | Training epoch 693, Batch 1000/1000: LR=5.65e-05, Loss=7.29e-03 BER=2.83e-03 FER=2.98e-02
2025-10-01 02:01:57,672 | INFO | Epoch 693 Train Time 131.88164138793945s

2025-10-01 02:04:07,863 | INFO | Training epoch 694, Batch 1000/1000: LR=5.64e-05, Loss=7.17e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 02:04:08,328 | INFO | Epoch 694 Train Time 130.65495324134827s

2025-10-01 02:06:19,129 | INFO | Training epoch 695, Batch 1000/1000: LR=5.63e-05, Loss=7.14e-03 BER=2.78e-03 FER=2.96e-02
2025-10-01 02:06:19,590 | INFO | Epoch 695 Train Time 131.26128697395325s

2025-10-01 02:08:29,645 | INFO | Training epoch 696, Batch 1000/1000: LR=5.62e-05, Loss=7.31e-03 BER=2.88e-03 FER=3.05e-02
2025-10-01 02:08:30,103 | INFO | Epoch 696 Train Time 130.51129484176636s

2025-10-01 02:10:42,638 | INFO | Training epoch 697, Batch 1000/1000: LR=5.61e-05, Loss=7.46e-03 BER=2.95e-03 FER=3.06e-02
2025-10-01 02:10:43,113 | INFO | Epoch 697 Train Time 133.00907492637634s

2025-10-01 02:12:54,028 | INFO | Training epoch 698, Batch 1000/1000: LR=5.60e-05, Loss=7.26e-03 BER=2.85e-03 FER=3.01e-02
2025-10-01 02:12:54,485 | INFO | Epoch 698 Train Time 131.37149739265442s

2025-10-01 02:15:05,130 | INFO | Training epoch 699, Batch 1000/1000: LR=5.59e-05, Loss=7.45e-03 BER=2.91e-03 FER=3.04e-02
2025-10-01 02:15:05,577 | INFO | Epoch 699 Train Time 131.0910713672638s

2025-10-01 02:17:16,901 | INFO | Training epoch 700, Batch 1000/1000: LR=5.58e-05, Loss=7.25e-03 BER=2.85e-03 FER=3.00e-02
2025-10-01 02:17:17,357 | INFO | Epoch 700 Train Time 131.7789511680603s

2025-10-01 02:19:27,829 | INFO | Training epoch 701, Batch 1000/1000: LR=5.57e-05, Loss=7.07e-03 BER=2.73e-03 FER=2.92e-02
2025-10-01 02:19:28,268 | INFO | Epoch 701 Train Time 130.90959334373474s

2025-10-01 02:21:41,344 | INFO | Training epoch 702, Batch 1000/1000: LR=5.56e-05, Loss=7.37e-03 BER=2.88e-03 FER=3.01e-02
2025-10-01 02:21:41,806 | INFO | Epoch 702 Train Time 133.53566002845764s

2025-10-01 02:23:52,438 | INFO | Training epoch 703, Batch 1000/1000: LR=5.55e-05, Loss=7.46e-03 BER=2.92e-03 FER=3.03e-02
2025-10-01 02:23:52,909 | INFO | Epoch 703 Train Time 131.10225415229797s

2025-10-01 02:26:19,591 | INFO | Training epoch 704, Batch 1000/1000: LR=5.54e-05, Loss=7.46e-03 BER=2.92e-03 FER=3.09e-02
2025-10-01 02:26:20,274 | INFO | Epoch 704 Train Time 147.36410331726074s

2025-10-01 02:29:22,816 | INFO | Training epoch 705, Batch 1000/1000: LR=5.53e-05, Loss=7.18e-03 BER=2.80e-03 FER=2.97e-02
2025-10-01 02:29:23,344 | INFO | Epoch 705 Train Time 183.06681847572327s

2025-10-01 02:31:40,233 | INFO | Training epoch 706, Batch 1000/1000: LR=5.52e-05, Loss=7.16e-03 BER=2.81e-03 FER=3.01e-02
2025-10-01 02:31:40,693 | INFO | Epoch 706 Train Time 137.34684228897095s

2025-10-01 02:33:52,281 | INFO | Training epoch 707, Batch 1000/1000: LR=5.51e-05, Loss=7.36e-03 BER=2.90e-03 FER=3.06e-02
2025-10-01 02:33:52,733 | INFO | Epoch 707 Train Time 132.0399522781372s

2025-10-01 02:36:03,581 | INFO | Training epoch 708, Batch 1000/1000: LR=5.50e-05, Loss=7.37e-03 BER=2.87e-03 FER=2.98e-02
2025-10-01 02:36:04,024 | INFO | Epoch 708 Train Time 131.28901171684265s

2025-10-01 02:38:16,752 | INFO | Training epoch 709, Batch 1000/1000: LR=5.48e-05, Loss=7.19e-03 BER=2.81e-03 FER=2.99e-02
2025-10-01 02:38:17,208 | INFO | Epoch 709 Train Time 133.18374395370483s

2025-10-01 02:40:28,425 | INFO | Training epoch 710, Batch 1000/1000: LR=5.47e-05, Loss=7.38e-03 BER=2.88e-03 FER=3.03e-02
2025-10-01 02:40:28,863 | INFO | Epoch 710 Train Time 131.6523630619049s

2025-10-01 02:42:41,725 | INFO | Training epoch 711, Batch 1000/1000: LR=5.46e-05, Loss=7.34e-03 BER=2.87e-03 FER=3.03e-02
2025-10-01 02:42:42,210 | INFO | Epoch 711 Train Time 133.3460624217987s

2025-10-01 02:44:57,756 | INFO | Training epoch 712, Batch 1000/1000: LR=5.45e-05, Loss=7.37e-03 BER=2.90e-03 FER=3.05e-02
2025-10-01 02:44:58,205 | INFO | Epoch 712 Train Time 135.99364233016968s

2025-10-01 02:47:09,858 | INFO | Training epoch 713, Batch 1000/1000: LR=5.44e-05, Loss=7.33e-03 BER=2.85e-03 FER=3.02e-02
2025-10-01 02:47:10,314 | INFO | Epoch 713 Train Time 132.10767197608948s

2025-10-01 02:49:21,168 | INFO | Training epoch 714, Batch 1000/1000: LR=5.43e-05, Loss=7.38e-03 BER=2.88e-03 FER=3.04e-02
2025-10-01 02:49:21,633 | INFO | Epoch 714 Train Time 131.31825518608093s

2025-10-01 02:51:33,921 | INFO | Training epoch 715, Batch 1000/1000: LR=5.42e-05, Loss=7.38e-03 BER=2.85e-03 FER=3.02e-02
2025-10-01 02:51:34,376 | INFO | Epoch 715 Train Time 132.74207258224487s

2025-10-01 02:53:45,437 | INFO | Training epoch 716, Batch 1000/1000: LR=5.41e-05, Loss=7.00e-03 BER=2.72e-03 FER=2.88e-02
2025-10-01 02:53:45,894 | INFO | Epoch 716 Train Time 131.5160939693451s

2025-10-01 02:55:57,582 | INFO | Training epoch 717, Batch 1000/1000: LR=5.40e-05, Loss=7.16e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 02:55:58,038 | INFO | Epoch 717 Train Time 132.1416802406311s

2025-10-01 02:58:08,608 | INFO | Training epoch 718, Batch 1000/1000: LR=5.39e-05, Loss=7.35e-03 BER=2.89e-03 FER=3.05e-02
2025-10-01 02:58:09,072 | INFO | Epoch 718 Train Time 131.03325819969177s

2025-10-01 03:00:30,839 | INFO | Training epoch 719, Batch 1000/1000: LR=5.38e-05, Loss=7.32e-03 BER=2.88e-03 FER=3.04e-02
2025-10-01 03:00:31,337 | INFO | Epoch 719 Train Time 142.2633159160614s

2025-10-01 03:02:48,960 | INFO | Training epoch 720, Batch 1000/1000: LR=5.37e-05, Loss=7.32e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 03:02:49,408 | INFO | Epoch 720 Train Time 138.07059121131897s

2025-10-01 03:05:01,310 | INFO | Training epoch 721, Batch 1000/1000: LR=5.36e-05, Loss=7.45e-03 BER=2.94e-03 FER=3.09e-02
2025-10-01 03:05:01,759 | INFO | Epoch 721 Train Time 132.3492956161499s

2025-10-01 03:07:13,921 | INFO | Training epoch 722, Batch 1000/1000: LR=5.35e-05, Loss=7.24e-03 BER=2.84e-03 FER=3.03e-02
2025-10-01 03:07:14,358 | INFO | Epoch 722 Train Time 132.5984718799591s

2025-10-01 03:09:26,619 | INFO | Training epoch 723, Batch 1000/1000: LR=5.34e-05, Loss=7.22e-03 BER=2.81e-03 FER=3.00e-02
2025-10-01 03:09:27,053 | INFO | Epoch 723 Train Time 132.69349575042725s

2025-10-01 03:11:38,171 | INFO | Training epoch 724, Batch 1000/1000: LR=5.33e-05, Loss=7.31e-03 BER=2.89e-03 FER=3.00e-02
2025-10-01 03:11:38,626 | INFO | Epoch 724 Train Time 131.57233691215515s

2025-10-01 03:13:51,579 | INFO | Training epoch 725, Batch 1000/1000: LR=5.32e-05, Loss=7.49e-03 BER=2.92e-03 FER=3.05e-02
2025-10-01 03:13:52,029 | INFO | Epoch 725 Train Time 133.40169620513916s

2025-10-01 03:16:03,418 | INFO | Training epoch 726, Batch 1000/1000: LR=5.31e-05, Loss=7.29e-03 BER=2.85e-03 FER=2.99e-02
2025-10-01 03:16:03,897 | INFO | Epoch 726 Train Time 131.86486315727234s

2025-10-01 03:18:16,577 | INFO | Training epoch 727, Batch 1000/1000: LR=5.30e-05, Loss=7.17e-03 BER=2.83e-03 FER=3.01e-02
2025-10-01 03:18:17,040 | INFO | Epoch 727 Train Time 133.1424915790558s

2025-10-01 03:20:30,818 | INFO | Training epoch 728, Batch 1000/1000: LR=5.29e-05, Loss=7.16e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 03:20:31,273 | INFO | Epoch 728 Train Time 134.23219633102417s

2025-10-01 03:22:42,518 | INFO | Training epoch 729, Batch 1000/1000: LR=5.28e-05, Loss=7.41e-03 BER=2.91e-03 FER=3.05e-02
2025-10-01 03:22:42,971 | INFO | Epoch 729 Train Time 131.69628739356995s

2025-10-01 03:24:53,434 | INFO | Training epoch 730, Batch 1000/1000: LR=5.27e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.97e-02
2025-10-01 03:24:53,890 | INFO | Epoch 730 Train Time 130.91874599456787s

2025-10-01 03:27:05,187 | INFO | Training epoch 731, Batch 1000/1000: LR=5.26e-05, Loss=7.43e-03 BER=2.91e-03 FER=3.06e-02
2025-10-01 03:27:05,652 | INFO | Epoch 731 Train Time 131.76038885116577s

2025-10-01 03:29:16,144 | INFO | Training epoch 732, Batch 1000/1000: LR=5.25e-05, Loss=7.24e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 03:29:16,604 | INFO | Epoch 732 Train Time 130.95097708702087s

2025-10-01 03:31:27,157 | INFO | Training epoch 733, Batch 1000/1000: LR=5.24e-05, Loss=6.93e-03 BER=2.74e-03 FER=2.93e-02
2025-10-01 03:31:27,599 | INFO | Epoch 733 Train Time 130.99467968940735s

2025-10-01 03:31:27,600 | INFO | [P2] saving best_model (QAT) with loss 0.006933 at epoch 733
2025-10-01 03:33:52,219 | INFO | Training epoch 734, Batch 1000/1000: LR=5.23e-05, Loss=7.26e-03 BER=2.83e-03 FER=2.97e-02
2025-10-01 03:33:52,724 | INFO | Epoch 734 Train Time 145.06408786773682s

2025-10-01 03:36:06,306 | INFO | Training epoch 735, Batch 1000/1000: LR=5.22e-05, Loss=7.47e-03 BER=2.91e-03 FER=3.07e-02
2025-10-01 03:36:06,765 | INFO | Epoch 735 Train Time 134.0390694141388s

2025-10-01 03:38:17,228 | INFO | Training epoch 736, Batch 1000/1000: LR=5.21e-05, Loss=7.32e-03 BER=2.87e-03 FER=3.01e-02
2025-10-01 03:38:17,693 | INFO | Epoch 736 Train Time 130.9265694618225s

2025-10-01 03:40:29,511 | INFO | Training epoch 737, Batch 1000/1000: LR=5.20e-05, Loss=7.38e-03 BER=2.89e-03 FER=3.05e-02
2025-10-01 03:40:29,965 | INFO | Epoch 737 Train Time 132.27116584777832s

2025-10-01 03:42:41,038 | INFO | Training epoch 738, Batch 1000/1000: LR=5.18e-05, Loss=7.25e-03 BER=2.88e-03 FER=3.01e-02
2025-10-01 03:42:41,515 | INFO | Epoch 738 Train Time 131.5484185218811s

2025-10-01 03:44:57,349 | INFO | Training epoch 739, Batch 1000/1000: LR=5.17e-05, Loss=7.07e-03 BER=2.82e-03 FER=2.91e-02
2025-10-01 03:44:57,799 | INFO | Epoch 739 Train Time 136.28217339515686s

2025-10-01 03:47:09,604 | INFO | Training epoch 740, Batch 1000/1000: LR=5.16e-05, Loss=7.18e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 03:47:10,057 | INFO | Epoch 740 Train Time 132.25705814361572s

2025-10-01 03:49:21,311 | INFO | Training epoch 741, Batch 1000/1000: LR=5.15e-05, Loss=7.39e-03 BER=2.92e-03 FER=3.10e-02
2025-10-01 03:49:21,779 | INFO | Epoch 741 Train Time 131.72135162353516s

2025-10-01 03:51:33,123 | INFO | Training epoch 742, Batch 1000/1000: LR=5.14e-05, Loss=7.09e-03 BER=2.75e-03 FER=2.93e-02
2025-10-01 03:51:33,599 | INFO | Epoch 742 Train Time 131.81918501853943s

2025-10-01 03:53:44,895 | INFO | Training epoch 743, Batch 1000/1000: LR=5.13e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.94e-02
2025-10-01 03:53:45,351 | INFO | Epoch 743 Train Time 131.7506809234619s

2025-10-01 03:55:55,458 | INFO | Training epoch 744, Batch 1000/1000: LR=5.12e-05, Loss=7.18e-03 BER=2.80e-03 FER=3.00e-02
2025-10-01 03:55:55,914 | INFO | Epoch 744 Train Time 130.5615029335022s

2025-10-01 03:58:08,402 | INFO | Training epoch 745, Batch 1000/1000: LR=5.11e-05, Loss=7.30e-03 BER=2.84e-03 FER=2.97e-02
2025-10-01 03:58:08,863 | INFO | Epoch 745 Train Time 132.94849634170532s

2025-10-01 04:00:18,659 | INFO | Training epoch 746, Batch 1000/1000: LR=5.10e-05, Loss=7.24e-03 BER=2.84e-03 FER=2.99e-02
2025-10-01 04:00:19,117 | INFO | Epoch 746 Train Time 130.2530701160431s

2025-10-01 04:02:30,296 | INFO | Training epoch 747, Batch 1000/1000: LR=5.09e-05, Loss=7.27e-03 BER=2.85e-03 FER=2.99e-02
2025-10-01 04:02:30,754 | INFO | Epoch 747 Train Time 131.6354911327362s

2025-10-01 04:04:40,536 | INFO | Training epoch 748, Batch 1000/1000: LR=5.08e-05, Loss=7.32e-03 BER=2.89e-03 FER=2.99e-02
2025-10-01 04:04:40,999 | INFO | Epoch 748 Train Time 130.2437629699707s

2025-10-01 04:07:10,351 | INFO | Training epoch 749, Batch 1000/1000: LR=5.07e-05, Loss=7.25e-03 BER=2.82e-03 FER=2.99e-02
2025-10-01 04:07:10,802 | INFO | Epoch 749 Train Time 149.8007915019989s

2025-10-01 04:09:21,662 | INFO | Training epoch 750, Batch 1000/1000: LR=5.06e-05, Loss=7.38e-03 BER=2.89e-03 FER=3.07e-02
2025-10-01 04:09:22,108 | INFO | Epoch 750 Train Time 131.30508875846863s

2025-10-01 04:11:33,843 | INFO | Training epoch 751, Batch 1000/1000: LR=5.05e-05, Loss=7.05e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 04:11:34,296 | INFO | Epoch 751 Train Time 132.18661546707153s

2025-10-01 04:13:45,192 | INFO | Training epoch 752, Batch 1000/1000: LR=5.04e-05, Loss=7.30e-03 BER=2.87e-03 FER=2.96e-02
2025-10-01 04:13:45,637 | INFO | Epoch 752 Train Time 131.3399155139923s

2025-10-01 04:16:09,374 | INFO | Training epoch 753, Batch 1000/1000: LR=5.03e-05, Loss=6.89e-03 BER=2.71e-03 FER=2.88e-02
2025-10-01 04:16:09,917 | INFO | Epoch 753 Train Time 144.2794258594513s

2025-10-01 04:16:09,918 | INFO | [P2] saving best_model (QAT) with loss 0.006893 at epoch 753
2025-10-01 04:18:46,007 | INFO | Training epoch 754, Batch 1000/1000: LR=5.02e-05, Loss=7.13e-03 BER=2.81e-03 FER=2.95e-02
2025-10-01 04:18:46,515 | INFO | Epoch 754 Train Time 156.51060891151428s

2025-10-01 04:21:26,974 | INFO | Training epoch 755, Batch 1000/1000: LR=5.01e-05, Loss=7.38e-03 BER=2.91e-03 FER=3.06e-02
2025-10-01 04:21:27,429 | INFO | Epoch 755 Train Time 160.9120306968689s

2025-10-01 04:23:37,204 | INFO | Training epoch 756, Batch 1000/1000: LR=5.00e-05, Loss=7.29e-03 BER=2.88e-03 FER=3.00e-02
2025-10-01 04:23:37,668 | INFO | Epoch 756 Train Time 130.23843431472778s

2025-10-01 04:25:47,875 | INFO | Training epoch 757, Batch 1000/1000: LR=4.99e-05, Loss=7.07e-03 BER=2.76e-03 FER=2.97e-02
2025-10-01 04:25:48,334 | INFO | Epoch 757 Train Time 130.66424798965454s

2025-10-01 04:27:59,093 | INFO | Training epoch 758, Batch 1000/1000: LR=4.98e-05, Loss=7.30e-03 BER=2.84e-03 FER=2.99e-02
2025-10-01 04:27:59,562 | INFO | Epoch 758 Train Time 131.22765398025513s

2025-10-01 04:30:10,937 | INFO | Training epoch 759, Batch 1000/1000: LR=4.97e-05, Loss=7.21e-03 BER=2.83e-03 FER=3.01e-02
2025-10-01 04:30:11,389 | INFO | Epoch 759 Train Time 131.8251497745514s

2025-10-01 04:32:21,606 | INFO | Training epoch 760, Batch 1000/1000: LR=4.96e-05, Loss=7.36e-03 BER=2.89e-03 FER=3.02e-02
2025-10-01 04:32:22,073 | INFO | Epoch 760 Train Time 130.682386636734s

2025-10-01 04:34:33,199 | INFO | Training epoch 761, Batch 1000/1000: LR=4.95e-05, Loss=7.10e-03 BER=2.78e-03 FER=2.97e-02
2025-10-01 04:34:33,666 | INFO | Epoch 761 Train Time 131.59250903129578s

2025-10-01 04:36:43,280 | INFO | Training epoch 762, Batch 1000/1000: LR=4.94e-05, Loss=7.22e-03 BER=2.83e-03 FER=3.00e-02
2025-10-01 04:36:43,761 | INFO | Epoch 762 Train Time 130.09406542778015s

2025-10-01 04:39:07,377 | INFO | Training epoch 763, Batch 1000/1000: LR=4.93e-05, Loss=7.36e-03 BER=2.88e-03 FER=3.02e-02
2025-10-01 04:39:07,879 | INFO | Epoch 763 Train Time 144.11635065078735s

2025-10-01 04:41:24,837 | INFO | Training epoch 764, Batch 1000/1000: LR=4.92e-05, Loss=7.24e-03 BER=2.85e-03 FER=2.99e-02
2025-10-01 04:41:25,294 | INFO | Epoch 764 Train Time 137.41339492797852s

2025-10-01 04:43:37,267 | INFO | Training epoch 765, Batch 1000/1000: LR=4.90e-05, Loss=7.35e-03 BER=2.88e-03 FER=3.00e-02
2025-10-01 04:43:37,794 | INFO | Epoch 765 Train Time 132.49911427497864s

2025-10-01 04:45:53,495 | INFO | Training epoch 766, Batch 1000/1000: LR=4.89e-05, Loss=7.48e-03 BER=2.92e-03 FER=3.05e-02
2025-10-01 04:45:53,943 | INFO | Epoch 766 Train Time 136.14711260795593s

2025-10-01 04:48:04,859 | INFO | Training epoch 767, Batch 1000/1000: LR=4.88e-05, Loss=7.13e-03 BER=2.80e-03 FER=2.93e-02
2025-10-01 04:48:05,310 | INFO | Epoch 767 Train Time 131.36534309387207s

2025-10-01 04:50:15,113 | INFO | Training epoch 768, Batch 1000/1000: LR=4.87e-05, Loss=7.16e-03 BER=2.77e-03 FER=2.94e-02
2025-10-01 04:50:15,572 | INFO | Epoch 768 Train Time 130.26153087615967s

2025-10-01 04:52:27,809 | INFO | Training epoch 769, Batch 1000/1000: LR=4.86e-05, Loss=7.20e-03 BER=2.84e-03 FER=2.98e-02
2025-10-01 04:52:28,280 | INFO | Epoch 769 Train Time 132.70596623420715s

2025-10-01 04:54:37,970 | INFO | Training epoch 770, Batch 1000/1000: LR=4.85e-05, Loss=7.20e-03 BER=2.83e-03 FER=2.96e-02
2025-10-01 04:54:38,433 | INFO | Epoch 770 Train Time 130.15169644355774s

2025-10-01 04:56:49,593 | INFO | Training epoch 771, Batch 1000/1000: LR=4.84e-05, Loss=7.37e-03 BER=2.87e-03 FER=3.04e-02
2025-10-01 04:56:50,060 | INFO | Epoch 771 Train Time 131.62656664848328s

2025-10-01 04:59:00,716 | INFO | Training epoch 772, Batch 1000/1000: LR=4.83e-05, Loss=7.22e-03 BER=2.84e-03 FER=3.00e-02
2025-10-01 04:59:01,167 | INFO | Epoch 772 Train Time 131.1060926914215s

2025-10-01 05:01:11,694 | INFO | Training epoch 773, Batch 1000/1000: LR=4.82e-05, Loss=7.17e-03 BER=2.81e-03 FER=3.00e-02
2025-10-01 05:01:12,156 | INFO | Epoch 773 Train Time 130.98770403862s

2025-10-01 05:03:22,273 | INFO | Training epoch 774, Batch 1000/1000: LR=4.81e-05, Loss=7.20e-03 BER=2.85e-03 FER=2.96e-02
2025-10-01 05:03:22,730 | INFO | Epoch 774 Train Time 130.57241773605347s

2025-10-01 05:05:36,960 | INFO | Training epoch 775, Batch 1000/1000: LR=4.80e-05, Loss=7.21e-03 BER=2.80e-03 FER=2.99e-02
2025-10-01 05:05:37,412 | INFO | Epoch 775 Train Time 134.6809844970703s

2025-10-01 05:07:49,978 | INFO | Training epoch 776, Batch 1000/1000: LR=4.79e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.93e-02
2025-10-01 05:07:50,442 | INFO | Epoch 776 Train Time 133.0285017490387s

2025-10-01 05:10:04,578 | INFO | Training epoch 777, Batch 1000/1000: LR=4.78e-05, Loss=7.25e-03 BER=2.82e-03 FER=3.02e-02
2025-10-01 05:10:05,074 | INFO | Epoch 777 Train Time 134.63069105148315s

2025-10-01 05:12:33,022 | INFO | Training epoch 778, Batch 1000/1000: LR=4.77e-05, Loss=7.26e-03 BER=2.84e-03 FER=2.98e-02
2025-10-01 05:12:33,526 | INFO | Epoch 778 Train Time 148.4507668018341s

2025-10-01 05:14:52,789 | INFO | Training epoch 779, Batch 1000/1000: LR=4.76e-05, Loss=7.20e-03 BER=2.84e-03 FER=2.95e-02
2025-10-01 05:14:53,271 | INFO | Epoch 779 Train Time 139.74420762062073s

2025-10-01 05:17:03,097 | INFO | Training epoch 780, Batch 1000/1000: LR=4.75e-05, Loss=7.16e-03 BER=2.82e-03 FER=2.96e-02
2025-10-01 05:17:03,559 | INFO | Epoch 780 Train Time 130.28685331344604s

2025-10-01 05:19:17,327 | INFO | Training epoch 781, Batch 1000/1000: LR=4.74e-05, Loss=7.19e-03 BER=2.81e-03 FER=2.97e-02
2025-10-01 05:19:17,785 | INFO | Epoch 781 Train Time 134.2254936695099s

2025-10-01 05:21:27,900 | INFO | Training epoch 782, Batch 1000/1000: LR=4.73e-05, Loss=7.28e-03 BER=2.84e-03 FER=3.00e-02
2025-10-01 05:21:28,342 | INFO | Epoch 782 Train Time 130.556081533432s

2025-10-01 05:23:39,675 | INFO | Training epoch 783, Batch 1000/1000: LR=4.72e-05, Loss=7.36e-03 BER=2.87e-03 FER=3.02e-02
2025-10-01 05:23:40,136 | INFO | Epoch 783 Train Time 131.79333782196045s

2025-10-01 05:25:50,019 | INFO | Training epoch 784, Batch 1000/1000: LR=4.71e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.99e-02
2025-10-01 05:25:50,480 | INFO | Epoch 784 Train Time 130.342280626297s

2025-10-01 05:28:01,166 | INFO | Training epoch 785, Batch 1000/1000: LR=4.70e-05, Loss=7.24e-03 BER=2.83e-03 FER=2.94e-02
2025-10-01 05:28:01,623 | INFO | Epoch 785 Train Time 131.14217233657837s

2025-10-01 05:30:11,635 | INFO | Training epoch 786, Batch 1000/1000: LR=4.69e-05, Loss=7.29e-03 BER=2.85e-03 FER=3.02e-02
2025-10-01 05:30:12,084 | INFO | Epoch 786 Train Time 130.4602770805359s

2025-10-01 05:32:22,986 | INFO | Training epoch 787, Batch 1000/1000: LR=4.68e-05, Loss=7.36e-03 BER=2.88e-03 FER=3.04e-02
2025-10-01 05:32:23,428 | INFO | Epoch 787 Train Time 131.34213662147522s

2025-10-01 05:34:33,611 | INFO | Training epoch 788, Batch 1000/1000: LR=4.67e-05, Loss=7.22e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 05:34:34,078 | INFO | Epoch 788 Train Time 130.64954257011414s

2025-10-01 05:36:45,474 | INFO | Training epoch 789, Batch 1000/1000: LR=4.66e-05, Loss=7.24e-03 BER=2.82e-03 FER=2.97e-02
2025-10-01 05:36:45,930 | INFO | Epoch 789 Train Time 131.85031843185425s

2025-10-01 05:38:55,954 | INFO | Training epoch 790, Batch 1000/1000: LR=4.65e-05, Loss=7.14e-03 BER=2.81e-03 FER=2.91e-02
2025-10-01 05:38:56,426 | INFO | Epoch 790 Train Time 130.49512815475464s

2025-10-01 05:41:08,944 | INFO | Training epoch 791, Batch 1000/1000: LR=4.64e-05, Loss=7.33e-03 BER=2.89e-03 FER=3.00e-02
2025-10-01 05:41:09,418 | INFO | Epoch 791 Train Time 132.99099898338318s

2025-10-01 05:43:19,647 | INFO | Training epoch 792, Batch 1000/1000: LR=4.63e-05, Loss=7.37e-03 BER=2.93e-03 FER=3.08e-02
2025-10-01 05:43:20,102 | INFO | Epoch 792 Train Time 130.68361926078796s

2025-10-01 05:45:50,622 | INFO | Training epoch 793, Batch 1000/1000: LR=4.62e-05, Loss=7.31e-03 BER=2.88e-03 FER=2.98e-02
2025-10-01 05:45:51,147 | INFO | Epoch 793 Train Time 151.04346752166748s

2025-10-01 05:48:04,576 | INFO | Training epoch 794, Batch 1000/1000: LR=4.60e-05, Loss=7.25e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 05:48:05,039 | INFO | Epoch 794 Train Time 133.8900125026703s

2025-10-01 05:50:15,716 | INFO | Training epoch 795, Batch 1000/1000: LR=4.59e-05, Loss=7.10e-03 BER=2.74e-03 FER=2.92e-02
2025-10-01 05:50:16,162 | INFO | Epoch 795 Train Time 131.12248611450195s

2025-10-01 05:52:26,437 | INFO | Training epoch 796, Batch 1000/1000: LR=4.58e-05, Loss=7.08e-03 BER=2.80e-03 FER=2.93e-02
2025-10-01 05:52:26,905 | INFO | Epoch 796 Train Time 130.74163484573364s

2025-10-01 05:54:37,925 | INFO | Training epoch 797, Batch 1000/1000: LR=4.57e-05, Loss=7.28e-03 BER=2.86e-03 FER=3.05e-02
2025-10-01 05:54:38,399 | INFO | Epoch 797 Train Time 131.49263525009155s

2025-10-01 05:56:48,754 | INFO | Training epoch 798, Batch 1000/1000: LR=4.56e-05, Loss=7.20e-03 BER=2.77e-03 FER=2.92e-02
2025-10-01 05:56:49,217 | INFO | Epoch 798 Train Time 130.81681609153748s

2025-10-01 05:58:59,954 | INFO | Training epoch 799, Batch 1000/1000: LR=4.55e-05, Loss=7.21e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 05:59:00,413 | INFO | Epoch 799 Train Time 131.19415426254272s

2025-10-01 06:01:10,806 | INFO | Training epoch 800, Batch 1000/1000: LR=4.54e-05, Loss=7.23e-03 BER=2.83e-03 FER=2.98e-02
2025-10-01 06:01:11,255 | INFO | Epoch 800 Train Time 130.8415014743805s

2025-10-01 06:03:22,300 | INFO | Training epoch 801, Batch 1000/1000: LR=4.53e-05, Loss=7.16e-03 BER=2.81e-03 FER=2.96e-02
2025-10-01 06:03:22,755 | INFO | Epoch 801 Train Time 131.4988877773285s

2025-10-01 06:05:33,090 | INFO | Training epoch 802, Batch 1000/1000: LR=4.52e-05, Loss=7.48e-03 BER=2.91e-03 FER=3.08e-02
2025-10-01 06:05:33,566 | INFO | Epoch 802 Train Time 130.81009697914124s

2025-10-01 06:07:44,436 | INFO | Training epoch 803, Batch 1000/1000: LR=4.51e-05, Loss=7.28e-03 BER=2.85e-03 FER=2.97e-02
2025-10-01 06:07:44,908 | INFO | Epoch 803 Train Time 131.33971524238586s

2025-10-01 06:09:54,839 | INFO | Training epoch 804, Batch 1000/1000: LR=4.50e-05, Loss=7.36e-03 BER=2.90e-03 FER=3.00e-02
2025-10-01 06:09:55,289 | INFO | Epoch 804 Train Time 130.38055968284607s

2025-10-01 06:12:06,200 | INFO | Training epoch 805, Batch 1000/1000: LR=4.49e-05, Loss=7.14e-03 BER=2.79e-03 FER=2.96e-02
2025-10-01 06:12:06,675 | INFO | Epoch 805 Train Time 131.3844780921936s

2025-10-01 06:14:16,445 | INFO | Training epoch 806, Batch 1000/1000: LR=4.48e-05, Loss=7.42e-03 BER=2.93e-03 FER=3.07e-02
2025-10-01 06:14:16,915 | INFO | Epoch 806 Train Time 130.2381775379181s

2025-10-01 06:16:27,748 | INFO | Training epoch 807, Batch 1000/1000: LR=4.47e-05, Loss=7.14e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 06:16:28,200 | INFO | Epoch 807 Train Time 131.28408098220825s

2025-10-01 06:18:52,519 | INFO | Training epoch 808, Batch 1000/1000: LR=4.46e-05, Loss=7.30e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 06:18:53,003 | INFO | Epoch 808 Train Time 144.80163979530334s

2025-10-01 06:21:11,478 | INFO | Training epoch 809, Batch 1000/1000: LR=4.45e-05, Loss=7.37e-03 BER=2.91e-03 FER=3.05e-02
2025-10-01 06:21:11,930 | INFO | Epoch 809 Train Time 138.9261612892151s

2025-10-01 06:23:21,733 | INFO | Training epoch 810, Batch 1000/1000: LR=4.44e-05, Loss=7.02e-03 BER=2.77e-03 FER=2.95e-02
2025-10-01 06:23:22,184 | INFO | Epoch 810 Train Time 130.25351810455322s

2025-10-01 06:25:33,081 | INFO | Training epoch 811, Batch 1000/1000: LR=4.43e-05, Loss=7.03e-03 BER=2.76e-03 FER=2.89e-02
2025-10-01 06:25:33,535 | INFO | Epoch 811 Train Time 131.3501636981964s

2025-10-01 06:27:43,542 | INFO | Training epoch 812, Batch 1000/1000: LR=4.42e-05, Loss=7.27e-03 BER=2.79e-03 FER=2.98e-02
2025-10-01 06:27:44,076 | INFO | Epoch 812 Train Time 130.53944444656372s

2025-10-01 06:29:55,578 | INFO | Training epoch 813, Batch 1000/1000: LR=4.41e-05, Loss=7.11e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 06:29:56,025 | INFO | Epoch 813 Train Time 131.9478895664215s

2025-10-01 06:32:06,213 | INFO | Training epoch 814, Batch 1000/1000: LR=4.40e-05, Loss=6.97e-03 BER=2.72e-03 FER=2.88e-02
2025-10-01 06:32:06,676 | INFO | Epoch 814 Train Time 130.65023708343506s

2025-10-01 06:34:17,965 | INFO | Training epoch 815, Batch 1000/1000: LR=4.39e-05, Loss=7.32e-03 BER=2.91e-03 FER=3.03e-02
2025-10-01 06:34:18,420 | INFO | Epoch 815 Train Time 131.74183678627014s

2025-10-01 06:36:28,305 | INFO | Training epoch 816, Batch 1000/1000: LR=4.38e-05, Loss=7.16e-03 BER=2.82e-03 FER=2.97e-02
2025-10-01 06:36:28,744 | INFO | Epoch 816 Train Time 130.32242512702942s

2025-10-01 06:38:39,860 | INFO | Training epoch 817, Batch 1000/1000: LR=4.37e-05, Loss=7.27e-03 BER=2.88e-03 FER=2.98e-02
2025-10-01 06:38:40,318 | INFO | Epoch 817 Train Time 131.57277631759644s

2025-10-01 06:40:50,264 | INFO | Training epoch 818, Batch 1000/1000: LR=4.36e-05, Loss=7.29e-03 BER=2.87e-03 FER=3.01e-02
2025-10-01 06:40:50,721 | INFO | Epoch 818 Train Time 130.4017014503479s

2025-10-01 06:43:02,413 | INFO | Training epoch 819, Batch 1000/1000: LR=4.35e-05, Loss=7.29e-03 BER=2.85e-03 FER=3.05e-02
2025-10-01 06:43:02,870 | INFO | Epoch 819 Train Time 132.14832830429077s

2025-10-01 06:45:14,614 | INFO | Training epoch 820, Batch 1000/1000: LR=4.34e-05, Loss=7.32e-03 BER=2.87e-03 FER=2.97e-02
2025-10-01 06:45:15,107 | INFO | Epoch 820 Train Time 132.2357623577118s

2025-10-01 06:47:27,469 | INFO | Training epoch 821, Batch 1000/1000: LR=4.33e-05, Loss=7.09e-03 BER=2.76e-03 FER=2.89e-02
2025-10-01 06:47:27,925 | INFO | Epoch 821 Train Time 132.8161060810089s

2025-10-01 06:49:37,376 | INFO | Training epoch 822, Batch 1000/1000: LR=4.32e-05, Loss=7.23e-03 BER=2.86e-03 FER=2.97e-02
2025-10-01 06:49:37,836 | INFO | Epoch 822 Train Time 129.91011691093445s

2025-10-01 06:52:04,226 | INFO | Training epoch 823, Batch 1000/1000: LR=4.31e-05, Loss=7.24e-03 BER=2.83e-03 FER=2.96e-02
2025-10-01 06:52:04,716 | INFO | Epoch 823 Train Time 146.87905859947205s

2025-10-01 06:54:33,937 | INFO | Training epoch 824, Batch 1000/1000: LR=4.30e-05, Loss=7.08e-03 BER=2.77e-03 FER=2.91e-02
2025-10-01 06:54:34,428 | INFO | Epoch 824 Train Time 149.70961689949036s

2025-10-01 06:56:55,797 | INFO | Training epoch 825, Batch 1000/1000: LR=4.29e-05, Loss=7.36e-03 BER=2.88e-03 FER=3.02e-02
2025-10-01 06:56:56,250 | INFO | Epoch 825 Train Time 141.82038354873657s

2025-10-01 06:59:06,474 | INFO | Training epoch 826, Batch 1000/1000: LR=4.28e-05, Loss=7.29e-03 BER=2.86e-03 FER=2.97e-02
2025-10-01 06:59:06,932 | INFO | Epoch 826 Train Time 130.68082118034363s

2025-10-01 07:01:17,297 | INFO | Training epoch 827, Batch 1000/1000: LR=4.27e-05, Loss=7.14e-03 BER=2.81e-03 FER=3.00e-02
2025-10-01 07:01:17,753 | INFO | Epoch 827 Train Time 130.8207745552063s

2025-10-01 07:03:27,365 | INFO | Training epoch 828, Batch 1000/1000: LR=4.26e-05, Loss=6.95e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 07:03:27,831 | INFO | Epoch 828 Train Time 130.07654523849487s

2025-10-01 07:05:38,263 | INFO | Training epoch 829, Batch 1000/1000: LR=4.24e-05, Loss=7.36e-03 BER=2.88e-03 FER=3.00e-02
2025-10-01 07:05:38,723 | INFO | Epoch 829 Train Time 130.8910882472992s

2025-10-01 07:07:49,588 | INFO | Training epoch 830, Batch 1000/1000: LR=4.23e-05, Loss=7.13e-03 BER=2.77e-03 FER=2.93e-02
2025-10-01 07:07:50,034 | INFO | Epoch 830 Train Time 131.30969500541687s

2025-10-01 07:10:01,121 | INFO | Training epoch 831, Batch 1000/1000: LR=4.22e-05, Loss=7.24e-03 BER=2.79e-03 FER=2.96e-02
2025-10-01 07:10:01,583 | INFO | Epoch 831 Train Time 131.54816818237305s

2025-10-01 07:12:11,714 | INFO | Training epoch 832, Batch 1000/1000: LR=4.21e-05, Loss=7.23e-03 BER=2.82e-03 FER=3.03e-02
2025-10-01 07:12:12,172 | INFO | Epoch 832 Train Time 130.58821606636047s

2025-10-01 07:14:22,907 | INFO | Training epoch 833, Batch 1000/1000: LR=4.20e-05, Loss=7.32e-03 BER=2.84e-03 FER=2.95e-02
2025-10-01 07:14:23,340 | INFO | Epoch 833 Train Time 131.16700959205627s

2025-10-01 07:16:33,334 | INFO | Training epoch 834, Batch 1000/1000: LR=4.19e-05, Loss=7.01e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 07:16:33,810 | INFO | Epoch 834 Train Time 130.46879649162292s

2025-10-01 07:18:45,536 | INFO | Training epoch 835, Batch 1000/1000: LR=4.18e-05, Loss=7.11e-03 BER=2.80e-03 FER=2.93e-02
2025-10-01 07:18:45,997 | INFO | Epoch 835 Train Time 132.18564820289612s

2025-10-01 07:20:59,884 | INFO | Training epoch 836, Batch 1000/1000: LR=4.17e-05, Loss=7.29e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 07:21:00,355 | INFO | Epoch 836 Train Time 134.356849193573s

2025-10-01 07:23:12,356 | INFO | Training epoch 837, Batch 1000/1000: LR=4.16e-05, Loss=7.20e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 07:23:12,841 | INFO | Epoch 837 Train Time 132.48514604568481s

2025-10-01 07:25:23,186 | INFO | Training epoch 838, Batch 1000/1000: LR=4.15e-05, Loss=6.87e-03 BER=2.68e-03 FER=2.84e-02
2025-10-01 07:25:23,642 | INFO | Epoch 838 Train Time 130.79868078231812s

2025-10-01 07:25:23,642 | INFO | [P2] saving best_model (QAT) with loss 0.006866 at epoch 838
2025-10-01 07:27:46,333 | INFO | Training epoch 839, Batch 1000/1000: LR=4.14e-05, Loss=7.17e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 07:27:46,835 | INFO | Epoch 839 Train Time 143.13197541236877s

2025-10-01 07:30:15,854 | INFO | Training epoch 840, Batch 1000/1000: LR=4.13e-05, Loss=7.32e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 07:30:16,382 | INFO | Epoch 840 Train Time 149.5438632965088s

2025-10-01 07:32:47,805 | INFO | Training epoch 841, Batch 1000/1000: LR=4.12e-05, Loss=7.48e-03 BER=2.93e-03 FER=3.06e-02
2025-10-01 07:32:48,312 | INFO | Epoch 841 Train Time 151.9292676448822s

2025-10-01 07:35:03,761 | INFO | Training epoch 842, Batch 1000/1000: LR=4.11e-05, Loss=7.21e-03 BER=2.83e-03 FER=2.97e-02
2025-10-01 07:35:04,217 | INFO | Epoch 842 Train Time 135.90313339233398s

2025-10-01 07:37:16,964 | INFO | Training epoch 843, Batch 1000/1000: LR=4.10e-05, Loss=7.44e-03 BER=2.91e-03 FER=3.03e-02
2025-10-01 07:37:17,422 | INFO | Epoch 843 Train Time 133.20427584648132s

2025-10-01 07:39:27,493 | INFO | Training epoch 844, Batch 1000/1000: LR=4.09e-05, Loss=7.15e-03 BER=2.81e-03 FER=2.94e-02
2025-10-01 07:39:27,952 | INFO | Epoch 844 Train Time 130.52935028076172s

2025-10-01 07:41:39,241 | INFO | Training epoch 845, Batch 1000/1000: LR=4.08e-05, Loss=7.01e-03 BER=2.74e-03 FER=2.94e-02
2025-10-01 07:41:39,721 | INFO | Epoch 845 Train Time 131.76758861541748s

2025-10-01 07:43:50,566 | INFO | Training epoch 846, Batch 1000/1000: LR=4.07e-05, Loss=7.14e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 07:43:51,024 | INFO | Epoch 846 Train Time 131.3012044429779s

2025-10-01 07:46:07,049 | INFO | Training epoch 847, Batch 1000/1000: LR=4.06e-05, Loss=7.12e-03 BER=2.80e-03 FER=2.95e-02
2025-10-01 07:46:07,513 | INFO | Epoch 847 Train Time 136.48831987380981s

2025-10-01 07:48:18,509 | INFO | Training epoch 848, Batch 1000/1000: LR=4.05e-05, Loss=7.19e-03 BER=2.86e-03 FER=3.02e-02
2025-10-01 07:48:18,997 | INFO | Epoch 848 Train Time 131.48255324363708s

2025-10-01 07:50:29,830 | INFO | Training epoch 849, Batch 1000/1000: LR=4.04e-05, Loss=7.26e-03 BER=2.85e-03 FER=3.00e-02
2025-10-01 07:50:30,301 | INFO | Epoch 849 Train Time 131.30309104919434s

2025-10-01 07:52:40,994 | INFO | Training epoch 850, Batch 1000/1000: LR=4.03e-05, Loss=7.16e-03 BER=2.81e-03 FER=2.99e-02
2025-10-01 07:52:41,452 | INFO | Epoch 850 Train Time 131.15030884742737s

2025-10-01 07:54:52,741 | INFO | Training epoch 851, Batch 1000/1000: LR=4.02e-05, Loss=7.05e-03 BER=2.75e-03 FER=2.91e-02
2025-10-01 07:54:53,204 | INFO | Epoch 851 Train Time 131.74926853179932s

2025-10-01 07:57:04,759 | INFO | Training epoch 852, Batch 1000/1000: LR=4.01e-05, Loss=7.47e-03 BER=2.95e-03 FER=3.09e-02
2025-10-01 07:57:05,212 | INFO | Epoch 852 Train Time 132.00771236419678s

2025-10-01 07:59:16,014 | INFO | Training epoch 853, Batch 1000/1000: LR=4.00e-05, Loss=7.13e-03 BER=2.81e-03 FER=2.93e-02
2025-10-01 07:59:16,475 | INFO | Epoch 853 Train Time 131.26215529441833s

2025-10-01 08:01:27,004 | INFO | Training epoch 854, Batch 1000/1000: LR=3.99e-05, Loss=7.19e-03 BER=2.81e-03 FER=2.94e-02
2025-10-01 08:01:27,451 | INFO | Epoch 854 Train Time 130.97562742233276s

2025-10-01 08:03:38,964 | INFO | Training epoch 855, Batch 1000/1000: LR=3.98e-05, Loss=7.28e-03 BER=2.86e-03 FER=2.98e-02
2025-10-01 08:03:39,427 | INFO | Epoch 855 Train Time 131.97482180595398s

2025-10-01 08:05:49,687 | INFO | Training epoch 856, Batch 1000/1000: LR=3.97e-05, Loss=7.15e-03 BER=2.78e-03 FER=2.92e-02
2025-10-01 08:05:50,205 | INFO | Epoch 856 Train Time 130.77703380584717s

2025-10-01 08:08:02,263 | INFO | Training epoch 857, Batch 1000/1000: LR=3.96e-05, Loss=7.17e-03 BER=2.81e-03 FER=2.95e-02
2025-10-01 08:08:02,700 | INFO | Epoch 857 Train Time 132.4923334121704s

2025-10-01 08:10:14,655 | INFO | Training epoch 858, Batch 1000/1000: LR=3.95e-05, Loss=7.20e-03 BER=2.87e-03 FER=2.97e-02
2025-10-01 08:10:15,121 | INFO | Epoch 858 Train Time 132.41994071006775s

2025-10-01 08:12:27,145 | INFO | Training epoch 859, Batch 1000/1000: LR=3.94e-05, Loss=7.08e-03 BER=2.78e-03 FER=2.95e-02
2025-10-01 08:12:27,604 | INFO | Epoch 859 Train Time 132.48091626167297s

2025-10-01 08:14:37,625 | INFO | Training epoch 860, Batch 1000/1000: LR=3.93e-05, Loss=7.09e-03 BER=2.79e-03 FER=2.90e-02
2025-10-01 08:14:38,081 | INFO | Epoch 860 Train Time 130.4762237071991s

2025-10-01 08:16:49,011 | INFO | Training epoch 861, Batch 1000/1000: LR=3.92e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.90e-02
2025-10-01 08:16:49,449 | INFO | Epoch 861 Train Time 131.36573243141174s

2025-10-01 08:18:59,077 | INFO | Training epoch 862, Batch 1000/1000: LR=3.91e-05, Loss=6.98e-03 BER=2.73e-03 FER=2.92e-02
2025-10-01 08:18:59,549 | INFO | Epoch 862 Train Time 130.0981080532074s

2025-10-01 08:21:13,423 | INFO | Training epoch 863, Batch 1000/1000: LR=3.90e-05, Loss=6.95e-03 BER=2.73e-03 FER=2.90e-02
2025-10-01 08:21:13,867 | INFO | Epoch 863 Train Time 134.31734943389893s

2025-10-01 08:23:23,408 | INFO | Training epoch 864, Batch 1000/1000: LR=3.89e-05, Loss=7.33e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 08:23:23,899 | INFO | Epoch 864 Train Time 130.03076601028442s

2025-10-01 08:25:34,693 | INFO | Training epoch 865, Batch 1000/1000: LR=3.88e-05, Loss=7.14e-03 BER=2.79e-03 FER=2.93e-02
2025-10-01 08:25:35,151 | INFO | Epoch 865 Train Time 131.251282453537s

2025-10-01 08:27:45,546 | INFO | Training epoch 866, Batch 1000/1000: LR=3.87e-05, Loss=6.88e-03 BER=2.70e-03 FER=2.86e-02
2025-10-01 08:27:46,009 | INFO | Epoch 866 Train Time 130.85738062858582s

2025-10-01 08:29:57,647 | INFO | Training epoch 867, Batch 1000/1000: LR=3.86e-05, Loss=7.08e-03 BER=2.83e-03 FER=2.97e-02
2025-10-01 08:29:58,113 | INFO | Epoch 867 Train Time 132.10122418403625s

2025-10-01 08:32:13,429 | INFO | Training epoch 868, Batch 1000/1000: LR=3.85e-05, Loss=7.27e-03 BER=2.83e-03 FER=2.96e-02
2025-10-01 08:32:13,888 | INFO | Epoch 868 Train Time 135.77419114112854s

2025-10-01 08:34:27,556 | INFO | Training epoch 869, Batch 1000/1000: LR=3.84e-05, Loss=7.47e-03 BER=2.94e-03 FER=3.07e-02
2025-10-01 08:34:27,999 | INFO | Epoch 869 Train Time 134.10966420173645s

2025-10-01 08:36:40,051 | INFO | Training epoch 870, Batch 1000/1000: LR=3.83e-05, Loss=7.26e-03 BER=2.87e-03 FER=2.97e-02
2025-10-01 08:36:40,525 | INFO | Epoch 870 Train Time 132.52581906318665s

2025-10-01 08:38:51,939 | INFO | Training epoch 871, Batch 1000/1000: LR=3.82e-05, Loss=7.15e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 08:38:52,384 | INFO | Epoch 871 Train Time 131.8558976650238s

2025-10-01 08:41:04,161 | INFO | Training epoch 872, Batch 1000/1000: LR=3.81e-05, Loss=6.90e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 08:41:04,623 | INFO | Epoch 872 Train Time 132.23721504211426s

2025-10-01 08:43:13,977 | INFO | Training epoch 873, Batch 1000/1000: LR=3.80e-05, Loss=7.28e-03 BER=2.86e-03 FER=3.01e-02
2025-10-01 08:43:14,439 | INFO | Epoch 873 Train Time 129.8149025440216s

2025-10-01 08:45:24,788 | INFO | Training epoch 874, Batch 1000/1000: LR=3.79e-05, Loss=7.19e-03 BER=2.85e-03 FER=2.93e-02
2025-10-01 08:45:25,241 | INFO | Epoch 874 Train Time 130.8012068271637s

2025-10-01 08:47:37,958 | INFO | Training epoch 875, Batch 1000/1000: LR=3.78e-05, Loss=7.07e-03 BER=2.77e-03 FER=2.91e-02
2025-10-01 08:47:38,403 | INFO | Epoch 875 Train Time 133.16088604927063s

2025-10-01 08:49:47,369 | INFO | Training epoch 876, Batch 1000/1000: LR=3.77e-05, Loss=7.13e-03 BER=2.79e-03 FER=2.94e-02
2025-10-01 08:49:47,836 | INFO | Epoch 876 Train Time 129.4324653148651s

2025-10-01 08:51:58,430 | INFO | Training epoch 877, Batch 1000/1000: LR=3.76e-05, Loss=7.10e-03 BER=2.79e-03 FER=2.91e-02
2025-10-01 08:51:58,870 | INFO | Epoch 877 Train Time 131.03129291534424s

2025-10-01 08:54:08,648 | INFO | Training epoch 878, Batch 1000/1000: LR=3.75e-05, Loss=7.29e-03 BER=2.85e-03 FER=2.98e-02
2025-10-01 08:54:09,100 | INFO | Epoch 878 Train Time 130.2297670841217s

2025-10-01 08:56:18,531 | INFO | Training epoch 879, Batch 1000/1000: LR=3.74e-05, Loss=7.32e-03 BER=2.90e-03 FER=3.04e-02
2025-10-01 08:56:18,975 | INFO | Epoch 879 Train Time 129.87330889701843s

2025-10-01 08:58:27,639 | INFO | Training epoch 880, Batch 1000/1000: LR=3.73e-05, Loss=7.28e-03 BER=2.85e-03 FER=3.00e-02
2025-10-01 08:58:28,107 | INFO | Epoch 880 Train Time 129.13121581077576s

2025-10-01 09:00:37,148 | INFO | Training epoch 881, Batch 1000/1000: LR=3.72e-05, Loss=7.28e-03 BER=2.86e-03 FER=3.04e-02
2025-10-01 09:00:37,612 | INFO | Epoch 881 Train Time 129.50408101081848s

2025-10-01 09:02:46,159 | INFO | Training epoch 882, Batch 1000/1000: LR=3.71e-05, Loss=7.29e-03 BER=2.85e-03 FER=2.98e-02
2025-10-01 09:02:46,607 | INFO | Epoch 882 Train Time 128.99450826644897s

2025-10-01 09:05:28,434 | INFO | Training epoch 883, Batch 1000/1000: LR=3.70e-05, Loss=7.06e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 09:05:28,923 | INFO | Epoch 883 Train Time 162.3141872882843s

2025-10-01 09:07:41,634 | INFO | Training epoch 884, Batch 1000/1000: LR=3.69e-05, Loss=7.06e-03 BER=2.75e-03 FER=2.87e-02
2025-10-01 09:07:42,156 | INFO | Epoch 884 Train Time 133.23243308067322s

2025-10-01 09:09:54,746 | INFO | Training epoch 885, Batch 1000/1000: LR=3.68e-05, Loss=7.05e-03 BER=2.75e-03 FER=2.90e-02
2025-10-01 09:09:55,242 | INFO | Epoch 885 Train Time 133.08461809158325s

2025-10-01 09:12:08,543 | INFO | Training epoch 886, Batch 1000/1000: LR=3.67e-05, Loss=7.34e-03 BER=2.89e-03 FER=2.99e-02
2025-10-01 09:12:09,056 | INFO | Epoch 886 Train Time 133.81300449371338s

2025-10-01 09:14:21,487 | INFO | Training epoch 887, Batch 1000/1000: LR=3.66e-05, Loss=7.16e-03 BER=2.80e-03 FER=2.95e-02
2025-10-01 09:14:21,964 | INFO | Epoch 887 Train Time 132.9076006412506s

2025-10-01 09:16:32,853 | INFO | Training epoch 888, Batch 1000/1000: LR=3.65e-05, Loss=7.17e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 09:16:33,350 | INFO | Epoch 888 Train Time 131.38476848602295s

2025-10-01 09:18:48,578 | INFO | Training epoch 889, Batch 1000/1000: LR=3.64e-05, Loss=7.31e-03 BER=2.87e-03 FER=2.97e-02
2025-10-01 09:18:49,089 | INFO | Epoch 889 Train Time 135.73713421821594s

2025-10-01 09:21:00,829 | INFO | Training epoch 890, Batch 1000/1000: LR=3.63e-05, Loss=7.11e-03 BER=2.78e-03 FER=2.95e-02
2025-10-01 09:21:01,298 | INFO | Epoch 890 Train Time 132.20768475532532s

2025-10-01 09:23:11,103 | INFO | Training epoch 891, Batch 1000/1000: LR=3.62e-05, Loss=7.23e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 09:23:11,557 | INFO | Epoch 891 Train Time 130.25793933868408s

2025-10-01 09:25:19,624 | INFO | Training epoch 892, Batch 1000/1000: LR=3.61e-05, Loss=7.09e-03 BER=2.77e-03 FER=2.93e-02
2025-10-01 09:25:20,091 | INFO | Epoch 892 Train Time 128.53271770477295s

2025-10-01 09:27:28,298 | INFO | Training epoch 893, Batch 1000/1000: LR=3.60e-05, Loss=7.10e-03 BER=2.77e-03 FER=2.90e-02
2025-10-01 09:27:28,729 | INFO | Epoch 893 Train Time 128.63635182380676s

2025-10-01 09:29:46,817 | INFO | Training epoch 894, Batch 1000/1000: LR=3.59e-05, Loss=7.29e-03 BER=2.87e-03 FER=2.96e-02
2025-10-01 09:29:47,314 | INFO | Epoch 894 Train Time 138.58384895324707s

2025-10-01 09:32:02,625 | INFO | Training epoch 895, Batch 1000/1000: LR=3.58e-05, Loss=7.16e-03 BER=2.86e-03 FER=2.94e-02
2025-10-01 09:32:03,087 | INFO | Epoch 895 Train Time 135.7722487449646s

2025-10-01 09:34:16,347 | INFO | Training epoch 896, Batch 1000/1000: LR=3.57e-05, Loss=7.23e-03 BER=2.84e-03 FER=2.96e-02
2025-10-01 09:34:16,829 | INFO | Epoch 896 Train Time 133.74080276489258s

2025-10-01 09:36:36,484 | INFO | Training epoch 897, Batch 1000/1000: LR=3.56e-05, Loss=7.21e-03 BER=2.84e-03 FER=2.96e-02
2025-10-01 09:36:37,012 | INFO | Epoch 897 Train Time 140.18164348602295s

2025-10-01 09:38:54,889 | INFO | Training epoch 898, Batch 1000/1000: LR=3.55e-05, Loss=7.10e-03 BER=2.78e-03 FER=2.93e-02
2025-10-01 09:38:55,397 | INFO | Epoch 898 Train Time 138.38243126869202s

2025-10-01 09:41:11,425 | INFO | Training epoch 899, Batch 1000/1000: LR=3.54e-05, Loss=7.13e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 09:41:11,944 | INFO | Epoch 899 Train Time 136.5455298423767s

2025-10-01 09:43:25,222 | INFO | Training epoch 900, Batch 1000/1000: LR=3.53e-05, Loss=7.22e-03 BER=2.85e-03 FER=2.98e-02
2025-10-01 09:43:25,702 | INFO | Epoch 900 Train Time 133.75615549087524s

2025-10-01 09:45:40,316 | INFO | Training epoch 901, Batch 1000/1000: LR=3.52e-05, Loss=7.18e-03 BER=2.80e-03 FER=2.97e-02
2025-10-01 09:45:40,792 | INFO | Epoch 901 Train Time 135.0887496471405s

2025-10-01 09:47:56,839 | INFO | Training epoch 902, Batch 1000/1000: LR=3.51e-05, Loss=7.28e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 09:47:57,386 | INFO | Epoch 902 Train Time 136.59208583831787s

2025-10-01 09:50:12,911 | INFO | Training epoch 903, Batch 1000/1000: LR=3.50e-05, Loss=7.21e-03 BER=2.78e-03 FER=2.97e-02
2025-10-01 09:50:13,365 | INFO | Epoch 903 Train Time 135.97703099250793s

2025-10-01 09:52:27,099 | INFO | Training epoch 904, Batch 1000/1000: LR=3.49e-05, Loss=7.19e-03 BER=2.83e-03 FER=2.96e-02
2025-10-01 09:52:27,612 | INFO | Epoch 904 Train Time 134.24252772331238s

2025-10-01 09:54:42,735 | INFO | Training epoch 905, Batch 1000/1000: LR=3.48e-05, Loss=7.08e-03 BER=2.79e-03 FER=2.89e-02
2025-10-01 09:54:43,209 | INFO | Epoch 905 Train Time 135.59590888023376s

2025-10-01 09:56:57,873 | INFO | Training epoch 906, Batch 1000/1000: LR=3.47e-05, Loss=7.21e-03 BER=2.81e-03 FER=2.95e-02
2025-10-01 09:56:58,359 | INFO | Epoch 906 Train Time 135.14967226982117s

2025-10-01 09:59:13,900 | INFO | Training epoch 907, Batch 1000/1000: LR=3.46e-05, Loss=7.15e-03 BER=2.81e-03 FER=2.93e-02
2025-10-01 09:59:14,431 | INFO | Epoch 907 Train Time 136.06916213035583s

2025-10-01 10:01:27,468 | INFO | Training epoch 908, Batch 1000/1000: LR=3.45e-05, Loss=7.20e-03 BER=2.80e-03 FER=2.94e-02
2025-10-01 10:01:27,958 | INFO | Epoch 908 Train Time 133.5261025428772s

2025-10-01 10:03:43,105 | INFO | Training epoch 909, Batch 1000/1000: LR=3.44e-05, Loss=7.32e-03 BER=2.85e-03 FER=3.00e-02
2025-10-01 10:03:43,589 | INFO | Epoch 909 Train Time 135.629243850708s

2025-10-01 10:06:00,771 | INFO | Training epoch 910, Batch 1000/1000: LR=3.43e-05, Loss=7.22e-03 BER=2.85e-03 FER=2.95e-02
2025-10-01 10:06:01,235 | INFO | Epoch 910 Train Time 137.6446282863617s

2025-10-01 10:08:15,324 | INFO | Training epoch 911, Batch 1000/1000: LR=3.42e-05, Loss=7.28e-03 BER=2.87e-03 FER=2.99e-02
2025-10-01 10:08:15,792 | INFO | Epoch 911 Train Time 134.5565891265869s

2025-10-01 10:10:31,029 | INFO | Training epoch 912, Batch 1000/1000: LR=3.41e-05, Loss=7.19e-03 BER=2.79e-03 FER=2.91e-02
2025-10-01 10:10:31,520 | INFO | Epoch 912 Train Time 135.72677564620972s

2025-10-01 10:12:51,618 | INFO | Training epoch 913, Batch 1000/1000: LR=3.40e-05, Loss=7.25e-03 BER=2.85e-03 FER=2.98e-02
2025-10-01 10:12:52,095 | INFO | Epoch 913 Train Time 140.57336354255676s

2025-10-01 10:15:06,611 | INFO | Training epoch 914, Batch 1000/1000: LR=3.39e-05, Loss=7.18e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 10:15:07,084 | INFO | Epoch 914 Train Time 134.98830008506775s

2025-10-01 10:17:19,299 | INFO | Training epoch 915, Batch 1000/1000: LR=3.38e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 10:17:19,756 | INFO | Epoch 915 Train Time 132.67111539840698s

2025-10-01 10:19:33,323 | INFO | Training epoch 916, Batch 1000/1000: LR=3.37e-05, Loss=7.22e-03 BER=2.82e-03 FER=2.99e-02
2025-10-01 10:19:33,806 | INFO | Epoch 916 Train Time 134.04863786697388s

2025-10-01 10:21:49,513 | INFO | Training epoch 917, Batch 1000/1000: LR=3.36e-05, Loss=6.99e-03 BER=2.74e-03 FER=2.91e-02
2025-10-01 10:21:49,995 | INFO | Epoch 917 Train Time 136.1879847049713s

2025-10-01 10:24:06,299 | INFO | Training epoch 918, Batch 1000/1000: LR=3.35e-05, Loss=7.24e-03 BER=2.85e-03 FER=3.00e-02
2025-10-01 10:24:06,772 | INFO | Epoch 918 Train Time 136.7749662399292s

2025-10-01 10:26:20,891 | INFO | Training epoch 919, Batch 1000/1000: LR=3.34e-05, Loss=6.97e-03 BER=2.71e-03 FER=2.88e-02
2025-10-01 10:26:21,456 | INFO | Epoch 919 Train Time 134.68354105949402s

2025-10-01 10:28:34,658 | INFO | Training epoch 920, Batch 1000/1000: LR=3.33e-05, Loss=7.05e-03 BER=2.78e-03 FER=2.90e-02
2025-10-01 10:28:35,155 | INFO | Epoch 920 Train Time 133.69754242897034s

2025-10-01 10:30:48,190 | INFO | Training epoch 921, Batch 1000/1000: LR=3.32e-05, Loss=7.14e-03 BER=2.84e-03 FER=2.92e-02
2025-10-01 10:30:48,668 | INFO | Epoch 921 Train Time 133.51244449615479s

2025-10-01 10:33:07,512 | INFO | Training epoch 922, Batch 1000/1000: LR=3.31e-05, Loss=6.94e-03 BER=2.70e-03 FER=2.85e-02
2025-10-01 10:33:08,029 | INFO | Epoch 922 Train Time 139.3601357936859s

2025-10-01 10:35:18,176 | INFO | Training epoch 923, Batch 1000/1000: LR=3.31e-05, Loss=7.01e-03 BER=2.73e-03 FER=2.88e-02
2025-10-01 10:35:18,650 | INFO | Epoch 923 Train Time 130.61940383911133s

2025-10-01 10:37:28,425 | INFO | Training epoch 924, Batch 1000/1000: LR=3.30e-05, Loss=7.13e-03 BER=2.81e-03 FER=2.94e-02
2025-10-01 10:37:28,889 | INFO | Epoch 924 Train Time 130.2384672164917s

2025-10-01 10:39:39,377 | INFO | Training epoch 925, Batch 1000/1000: LR=3.29e-05, Loss=7.30e-03 BER=2.86e-03 FER=3.00e-02
2025-10-01 10:39:39,875 | INFO | Epoch 925 Train Time 130.98523592948914s

2025-10-01 10:41:51,112 | INFO | Training epoch 926, Batch 1000/1000: LR=3.28e-05, Loss=7.15e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 10:41:51,609 | INFO | Epoch 926 Train Time 131.73251914978027s

2025-10-01 10:44:11,007 | INFO | Training epoch 927, Batch 1000/1000: LR=3.27e-05, Loss=7.15e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 10:44:11,504 | INFO | Epoch 927 Train Time 139.8937213420868s

2025-10-01 10:46:27,677 | INFO | Training epoch 928, Batch 1000/1000: LR=3.26e-05, Loss=7.16e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 10:46:28,160 | INFO | Epoch 928 Train Time 136.654128074646s

2025-10-01 10:48:45,441 | INFO | Training epoch 929, Batch 1000/1000: LR=3.25e-05, Loss=7.13e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 10:48:45,909 | INFO | Epoch 929 Train Time 137.74692010879517s

2025-10-01 10:50:57,358 | INFO | Training epoch 930, Batch 1000/1000: LR=3.24e-05, Loss=7.07e-03 BER=2.74e-03 FER=2.85e-02
2025-10-01 10:50:57,897 | INFO | Epoch 930 Train Time 131.98630833625793s

2025-10-01 10:53:10,859 | INFO | Training epoch 931, Batch 1000/1000: LR=3.23e-05, Loss=7.21e-03 BER=2.80e-03 FER=2.90e-02
2025-10-01 10:53:11,330 | INFO | Epoch 931 Train Time 133.43220281600952s

2025-10-01 10:55:20,159 | INFO | Training epoch 932, Batch 1000/1000: LR=3.22e-05, Loss=7.03e-03 BER=2.76e-03 FER=2.93e-02
2025-10-01 10:55:20,629 | INFO | Epoch 932 Train Time 129.298006772995s

2025-10-01 10:57:33,684 | INFO | Training epoch 933, Batch 1000/1000: LR=3.21e-05, Loss=6.98e-03 BER=2.73e-03 FER=2.87e-02
2025-10-01 10:57:34,202 | INFO | Epoch 933 Train Time 133.57186913490295s

2025-10-01 10:59:45,176 | INFO | Training epoch 934, Batch 1000/1000: LR=3.20e-05, Loss=7.02e-03 BER=2.74e-03 FER=2.90e-02
2025-10-01 10:59:45,624 | INFO | Epoch 934 Train Time 131.42119121551514s

2025-10-01 11:01:57,088 | INFO | Training epoch 935, Batch 1000/1000: LR=3.19e-05, Loss=7.27e-03 BER=2.81e-03 FER=2.93e-02
2025-10-01 11:01:57,574 | INFO | Epoch 935 Train Time 131.94658946990967s

2025-10-01 11:04:12,569 | INFO | Training epoch 936, Batch 1000/1000: LR=3.18e-05, Loss=7.17e-03 BER=2.84e-03 FER=2.93e-02
2025-10-01 11:04:13,139 | INFO | Epoch 936 Train Time 135.56380486488342s

2025-10-01 11:06:30,217 | INFO | Training epoch 937, Batch 1000/1000: LR=3.17e-05, Loss=7.35e-03 BER=2.87e-03 FER=2.98e-02
2025-10-01 11:06:30,691 | INFO | Epoch 937 Train Time 137.54987406730652s

2025-10-01 11:08:46,242 | INFO | Training epoch 938, Batch 1000/1000: LR=3.16e-05, Loss=7.17e-03 BER=2.80e-03 FER=2.90e-02
2025-10-01 11:08:46,836 | INFO | Epoch 938 Train Time 136.14319491386414s

2025-10-01 11:11:05,741 | INFO | Training epoch 939, Batch 1000/1000: LR=3.15e-05, Loss=7.06e-03 BER=2.76e-03 FER=2.88e-02
2025-10-01 11:11:06,197 | INFO | Epoch 939 Train Time 139.36049246788025s

2025-10-01 11:13:22,536 | INFO | Training epoch 940, Batch 1000/1000: LR=3.14e-05, Loss=7.09e-03 BER=2.82e-03 FER=2.93e-02
2025-10-01 11:13:23,079 | INFO | Epoch 940 Train Time 136.87848210334778s

2025-10-01 11:15:39,263 | INFO | Training epoch 941, Batch 1000/1000: LR=3.13e-05, Loss=7.09e-03 BER=2.81e-03 FER=2.97e-02
2025-10-01 11:15:39,738 | INFO | Epoch 941 Train Time 136.65812277793884s

2025-10-01 11:17:50,796 | INFO | Training epoch 942, Batch 1000/1000: LR=3.12e-05, Loss=7.08e-03 BER=2.79e-03 FER=2.87e-02
2025-10-01 11:17:51,256 | INFO | Epoch 942 Train Time 131.51676440238953s

2025-10-01 11:20:05,385 | INFO | Training epoch 943, Batch 1000/1000: LR=3.11e-05, Loss=7.02e-03 BER=2.75e-03 FER=2.86e-02
2025-10-01 11:20:05,897 | INFO | Epoch 943 Train Time 134.63996005058289s

2025-10-01 11:22:21,090 | INFO | Training epoch 944, Batch 1000/1000: LR=3.10e-05, Loss=7.22e-03 BER=2.88e-03 FER=2.97e-02
2025-10-01 11:22:21,579 | INFO | Epoch 944 Train Time 135.68139004707336s

2025-10-01 11:24:36,697 | INFO | Training epoch 945, Batch 1000/1000: LR=3.09e-05, Loss=7.22e-03 BER=2.84e-03 FER=2.97e-02
2025-10-01 11:24:37,146 | INFO | Epoch 945 Train Time 135.5661895275116s

2025-10-01 11:26:49,505 | INFO | Training epoch 946, Batch 1000/1000: LR=3.08e-05, Loss=7.32e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 11:26:49,982 | INFO | Epoch 946 Train Time 132.83533382415771s

2025-10-01 11:29:06,894 | INFO | Training epoch 947, Batch 1000/1000: LR=3.07e-05, Loss=7.06e-03 BER=2.76e-03 FER=2.91e-02
2025-10-01 11:29:07,396 | INFO | Epoch 947 Train Time 137.4122281074524s

2025-10-01 11:31:22,642 | INFO | Training epoch 948, Batch 1000/1000: LR=3.07e-05, Loss=6.92e-03 BER=2.72e-03 FER=2.88e-02
2025-10-01 11:31:23,115 | INFO | Epoch 948 Train Time 135.7176263332367s

2025-10-01 11:33:36,772 | INFO | Training epoch 949, Batch 1000/1000: LR=3.06e-05, Loss=7.23e-03 BER=2.84e-03 FER=2.97e-02
2025-10-01 11:33:37,257 | INFO | Epoch 949 Train Time 134.14105534553528s

2025-10-01 11:36:05,823 | INFO | Training epoch 950, Batch 1000/1000: LR=3.05e-05, Loss=7.10e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 11:36:06,466 | INFO | Epoch 950 Train Time 149.2063672542572s

2025-10-01 11:38:43,876 | INFO | Training epoch 951, Batch 1000/1000: LR=3.04e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.87e-02
2025-10-01 11:38:44,444 | INFO | Epoch 951 Train Time 157.97726893424988s

2025-10-01 11:41:06,210 | INFO | Training epoch 952, Batch 1000/1000: LR=3.03e-05, Loss=7.25e-03 BER=2.84e-03 FER=2.94e-02
2025-10-01 11:41:06,705 | INFO | Epoch 952 Train Time 142.2597451210022s

2025-10-01 11:43:27,389 | INFO | Training epoch 953, Batch 1000/1000: LR=3.02e-05, Loss=7.10e-03 BER=2.78e-03 FER=2.94e-02
2025-10-01 11:43:27,913 | INFO | Epoch 953 Train Time 141.20659136772156s

2025-10-01 11:45:42,666 | INFO | Training epoch 954, Batch 1000/1000: LR=3.01e-05, Loss=7.02e-03 BER=2.78e-03 FER=2.89e-02
2025-10-01 11:45:43,136 | INFO | Epoch 954 Train Time 135.2212142944336s

2025-10-01 11:48:08,093 | INFO | Training epoch 955, Batch 1000/1000: LR=3.00e-05, Loss=6.97e-03 BER=2.71e-03 FER=2.86e-02
2025-10-01 11:48:08,624 | INFO | Epoch 955 Train Time 145.48670506477356s

2025-10-01 11:50:36,176 | INFO | Training epoch 956, Batch 1000/1000: LR=2.99e-05, Loss=6.99e-03 BER=2.74e-03 FER=2.88e-02
2025-10-01 11:50:36,719 | INFO | Epoch 956 Train Time 148.0937623977661s

2025-10-01 11:53:05,158 | INFO | Training epoch 957, Batch 1000/1000: LR=2.98e-05, Loss=7.03e-03 BER=2.76e-03 FER=2.91e-02
2025-10-01 11:53:05,726 | INFO | Epoch 957 Train Time 149.00547885894775s

2025-10-01 11:55:39,791 | INFO | Training epoch 958, Batch 1000/1000: LR=2.97e-05, Loss=7.30e-03 BER=2.86e-03 FER=3.02e-02
2025-10-01 11:55:40,424 | INFO | Epoch 958 Train Time 154.69601774215698s

2025-10-01 11:58:07,986 | INFO | Training epoch 959, Batch 1000/1000: LR=2.96e-05, Loss=7.19e-03 BER=2.81e-03 FER=2.94e-02
2025-10-01 11:58:08,572 | INFO | Epoch 959 Train Time 148.14606428146362s

2025-10-01 12:00:40,825 | INFO | Training epoch 960, Batch 1000/1000: LR=2.95e-05, Loss=7.09e-03 BER=2.75e-03 FER=2.91e-02
2025-10-01 12:00:41,375 | INFO | Epoch 960 Train Time 152.80187153816223s

2025-10-01 12:02:58,825 | INFO | Training epoch 961, Batch 1000/1000: LR=2.94e-05, Loss=7.08e-03 BER=2.77e-03 FER=2.89e-02
2025-10-01 12:02:59,261 | INFO | Epoch 961 Train Time 137.8839931488037s

2025-10-01 12:05:21,418 | INFO | Training epoch 962, Batch 1000/1000: LR=2.93e-05, Loss=7.35e-03 BER=2.91e-03 FER=3.07e-02
2025-10-01 12:05:21,934 | INFO | Epoch 962 Train Time 142.67113852500916s

2025-10-01 12:07:32,228 | INFO | Training epoch 963, Batch 1000/1000: LR=2.92e-05, Loss=7.14e-03 BER=2.79e-03 FER=2.92e-02
2025-10-01 12:07:32,701 | INFO | Epoch 963 Train Time 130.76526880264282s

2025-10-01 12:09:46,137 | INFO | Training epoch 964, Batch 1000/1000: LR=2.91e-05, Loss=7.25e-03 BER=2.81e-03 FER=2.97e-02
2025-10-01 12:09:46,601 | INFO | Epoch 964 Train Time 133.89878582954407s

2025-10-01 12:11:58,062 | INFO | Training epoch 965, Batch 1000/1000: LR=2.90e-05, Loss=7.10e-03 BER=2.79e-03 FER=2.89e-02
2025-10-01 12:11:58,546 | INFO | Epoch 965 Train Time 131.94386434555054s

2025-10-01 12:14:09,644 | INFO | Training epoch 966, Batch 1000/1000: LR=2.90e-05, Loss=7.00e-03 BER=2.77e-03 FER=2.92e-02
2025-10-01 12:14:10,097 | INFO | Epoch 966 Train Time 131.55063796043396s

2025-10-01 12:16:20,207 | INFO | Training epoch 967, Batch 1000/1000: LR=2.89e-05, Loss=7.09e-03 BER=2.80e-03 FER=2.89e-02
2025-10-01 12:16:20,681 | INFO | Epoch 967 Train Time 130.58234333992004s

2025-10-01 12:18:28,926 | INFO | Training epoch 968, Batch 1000/1000: LR=2.88e-05, Loss=7.07e-03 BER=2.81e-03 FER=2.98e-02
2025-10-01 12:18:29,371 | INFO | Epoch 968 Train Time 128.68944716453552s

2025-10-01 12:20:41,249 | INFO | Training epoch 969, Batch 1000/1000: LR=2.87e-05, Loss=7.01e-03 BER=2.73e-03 FER=2.94e-02
2025-10-01 12:20:41,693 | INFO | Epoch 969 Train Time 132.32124543190002s

2025-10-01 12:22:50,511 | INFO | Training epoch 970, Batch 1000/1000: LR=2.86e-05, Loss=7.05e-03 BER=2.79e-03 FER=2.91e-02
2025-10-01 12:22:50,989 | INFO | Epoch 970 Train Time 129.29511189460754s

2025-10-01 12:24:59,035 | INFO | Training epoch 971, Batch 1000/1000: LR=2.85e-05, Loss=7.04e-03 BER=2.74e-03 FER=2.90e-02
2025-10-01 12:24:59,481 | INFO | Epoch 971 Train Time 128.49059224128723s

2025-10-01 12:27:07,744 | INFO | Training epoch 972, Batch 1000/1000: LR=2.84e-05, Loss=7.31e-03 BER=2.87e-03 FER=2.95e-02
2025-10-01 12:27:08,208 | INFO | Epoch 972 Train Time 128.72560095787048s

2025-10-01 12:29:18,005 | INFO | Training epoch 973, Batch 1000/1000: LR=2.83e-05, Loss=7.09e-03 BER=2.76e-03 FER=2.89e-02
2025-10-01 12:29:18,459 | INFO | Epoch 973 Train Time 130.25032210350037s

2025-10-01 12:31:25,741 | INFO | Training epoch 974, Batch 1000/1000: LR=2.82e-05, Loss=7.13e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 12:31:26,210 | INFO | Epoch 974 Train Time 127.75000095367432s

2025-10-01 12:33:35,907 | INFO | Training epoch 975, Batch 1000/1000: LR=2.81e-05, Loss=7.17e-03 BER=2.77e-03 FER=2.87e-02
2025-10-01 12:33:36,382 | INFO | Epoch 975 Train Time 130.17024445533752s

2025-10-01 12:35:44,344 | INFO | Training epoch 976, Batch 1000/1000: LR=2.80e-05, Loss=6.80e-03 BER=2.67e-03 FER=2.85e-02
2025-10-01 12:35:44,813 | INFO | Epoch 976 Train Time 128.43020725250244s

2025-10-01 12:35:44,814 | INFO | [P2] saving best_model (QAT) with loss 0.006796 at epoch 976
2025-10-01 12:37:54,574 | INFO | Training epoch 977, Batch 1000/1000: LR=2.79e-05, Loss=7.02e-03 BER=2.77e-03 FER=2.91e-02
2025-10-01 12:37:55,034 | INFO | Epoch 977 Train Time 130.15609097480774s

2025-10-01 12:40:06,911 | INFO | Training epoch 978, Batch 1000/1000: LR=2.78e-05, Loss=6.94e-03 BER=2.72e-03 FER=2.86e-02
2025-10-01 12:40:07,375 | INFO | Epoch 978 Train Time 132.33936738967896s

2025-10-01 12:42:16,935 | INFO | Training epoch 979, Batch 1000/1000: LR=2.78e-05, Loss=6.90e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 12:42:17,404 | INFO | Epoch 979 Train Time 130.02817487716675s

2025-10-01 12:44:26,154 | INFO | Training epoch 980, Batch 1000/1000: LR=2.77e-05, Loss=7.17e-03 BER=2.81e-03 FER=2.94e-02
2025-10-01 12:44:26,611 | INFO | Epoch 980 Train Time 129.2064208984375s

2025-10-01 12:46:35,651 | INFO | Training epoch 981, Batch 1000/1000: LR=2.76e-05, Loss=7.27e-03 BER=2.86e-03 FER=2.95e-02
2025-10-01 12:46:36,123 | INFO | Epoch 981 Train Time 129.51019835472107s

2025-10-01 12:48:44,949 | INFO | Training epoch 982, Batch 1000/1000: LR=2.75e-05, Loss=7.30e-03 BER=2.88e-03 FER=2.96e-02
2025-10-01 12:48:45,418 | INFO | Epoch 982 Train Time 129.29407167434692s

2025-10-01 12:50:55,238 | INFO | Training epoch 983, Batch 1000/1000: LR=2.74e-05, Loss=7.14e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 12:50:55,687 | INFO | Epoch 983 Train Time 130.26799297332764s

2025-10-01 12:53:04,078 | INFO | Training epoch 984, Batch 1000/1000: LR=2.73e-05, Loss=7.12e-03 BER=2.79e-03 FER=2.91e-02
2025-10-01 12:53:04,532 | INFO | Epoch 984 Train Time 128.8440501689911s

2025-10-01 12:55:13,622 | INFO | Training epoch 985, Batch 1000/1000: LR=2.72e-05, Loss=7.03e-03 BER=2.76e-03 FER=2.90e-02
2025-10-01 12:55:14,080 | INFO | Epoch 985 Train Time 129.54673075675964s

2025-10-01 12:57:22,738 | INFO | Training epoch 986, Batch 1000/1000: LR=2.71e-05, Loss=7.09e-03 BER=2.79e-03 FER=2.94e-02
2025-10-01 12:57:23,190 | INFO | Epoch 986 Train Time 129.10833644866943s

2025-10-01 12:59:33,601 | INFO | Training epoch 987, Batch 1000/1000: LR=2.70e-05, Loss=7.03e-03 BER=2.78e-03 FER=2.85e-02
2025-10-01 12:59:34,045 | INFO | Epoch 987 Train Time 130.8529407978058s

2025-10-01 13:01:43,740 | INFO | Training epoch 988, Batch 1000/1000: LR=2.69e-05, Loss=6.86e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 13:01:44,205 | INFO | Epoch 988 Train Time 130.15904712677002s

2025-10-01 13:03:55,955 | INFO | Training epoch 989, Batch 1000/1000: LR=2.68e-05, Loss=7.07e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 13:03:56,407 | INFO | Epoch 989 Train Time 132.20036673545837s

2025-10-01 13:06:06,492 | INFO | Training epoch 990, Batch 1000/1000: LR=2.67e-05, Loss=7.00e-03 BER=2.77e-03 FER=2.91e-02
2025-10-01 13:06:06,945 | INFO | Epoch 990 Train Time 130.53667950630188s

2025-10-01 13:08:17,631 | INFO | Training epoch 991, Batch 1000/1000: LR=2.67e-05, Loss=7.21e-03 BER=2.85e-03 FER=2.99e-02
2025-10-01 13:08:18,111 | INFO | Epoch 991 Train Time 131.16523146629333s

2025-10-01 13:10:28,523 | INFO | Training epoch 992, Batch 1000/1000: LR=2.66e-05, Loss=7.05e-03 BER=2.77e-03 FER=2.92e-02
2025-10-01 13:10:28,970 | INFO | Epoch 992 Train Time 130.85767889022827s

2025-10-01 13:12:44,143 | INFO | Training epoch 993, Batch 1000/1000: LR=2.65e-05, Loss=7.06e-03 BER=2.78e-03 FER=2.92e-02
2025-10-01 13:12:44,634 | INFO | Epoch 993 Train Time 135.6631817817688s

2025-10-01 13:14:54,783 | INFO | Training epoch 994, Batch 1000/1000: LR=2.64e-05, Loss=7.22e-03 BER=2.83e-03 FER=2.94e-02
2025-10-01 13:14:55,246 | INFO | Epoch 994 Train Time 130.61064553260803s

2025-10-01 13:17:04,438 | INFO | Training epoch 995, Batch 1000/1000: LR=2.63e-05, Loss=7.02e-03 BER=2.77e-03 FER=2.86e-02
2025-10-01 13:17:04,890 | INFO | Epoch 995 Train Time 129.64348721504211s

2025-10-01 13:19:13,917 | INFO | Training epoch 996, Batch 1000/1000: LR=2.62e-05, Loss=6.90e-03 BER=2.73e-03 FER=2.86e-02
2025-10-01 13:19:14,382 | INFO | Epoch 996 Train Time 129.4899001121521s

2025-10-01 13:21:27,599 | INFO | Training epoch 997, Batch 1000/1000: LR=2.61e-05, Loss=7.11e-03 BER=2.76e-03 FER=2.87e-02
2025-10-01 13:21:28,057 | INFO | Epoch 997 Train Time 133.67368960380554s

2025-10-01 13:23:37,456 | INFO | Training epoch 998, Batch 1000/1000: LR=2.60e-05, Loss=7.15e-03 BER=2.81e-03 FER=2.95e-02
2025-10-01 13:23:37,910 | INFO | Epoch 998 Train Time 129.8512110710144s

2025-10-01 13:25:47,792 | INFO | Training epoch 999, Batch 1000/1000: LR=2.59e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.84e-02
2025-10-01 13:25:48,236 | INFO | Epoch 999 Train Time 130.3248827457428s

2025-10-01 13:27:57,707 | INFO | Training epoch 1000, Batch 1000/1000: LR=2.58e-05, Loss=7.16e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 13:27:58,177 | INFO | Epoch 1000 Train Time 129.9402048587799s

2025-10-01 13:30:07,485 | INFO | Training epoch 1001, Batch 1000/1000: LR=2.57e-05, Loss=7.11e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 13:30:07,940 | INFO | Epoch 1001 Train Time 129.76131176948547s

2025-10-01 13:32:16,033 | INFO | Training epoch 1002, Batch 1000/1000: LR=2.57e-05, Loss=7.16e-03 BER=2.82e-03 FER=2.98e-02
2025-10-01 13:32:16,488 | INFO | Epoch 1002 Train Time 128.5472583770752s

2025-10-01 13:34:27,798 | INFO | Training epoch 1003, Batch 1000/1000: LR=2.56e-05, Loss=6.97e-03 BER=2.72e-03 FER=2.86e-02
2025-10-01 13:34:28,247 | INFO | Epoch 1003 Train Time 131.7584068775177s

2025-10-01 13:36:36,814 | INFO | Training epoch 1004, Batch 1000/1000: LR=2.55e-05, Loss=7.06e-03 BER=2.76e-03 FER=2.88e-02
2025-10-01 13:36:37,272 | INFO | Epoch 1004 Train Time 129.02363395690918s

2025-10-01 13:38:47,499 | INFO | Training epoch 1005, Batch 1000/1000: LR=2.54e-05, Loss=6.94e-03 BER=2.75e-03 FER=2.85e-02
2025-10-01 13:38:47,945 | INFO | Epoch 1005 Train Time 130.67206501960754s

2025-10-01 13:40:56,391 | INFO | Training epoch 1006, Batch 1000/1000: LR=2.53e-05, Loss=7.02e-03 BER=2.74e-03 FER=2.85e-02
2025-10-01 13:40:56,866 | INFO | Epoch 1006 Train Time 128.9199481010437s

2025-10-01 13:43:12,008 | INFO | Training epoch 1007, Batch 1000/1000: LR=2.52e-05, Loss=6.98e-03 BER=2.74e-03 FER=2.90e-02
2025-10-01 13:43:12,452 | INFO | Epoch 1007 Train Time 135.58387780189514s

2025-10-01 13:45:20,962 | INFO | Training epoch 1008, Batch 1000/1000: LR=2.51e-05, Loss=6.93e-03 BER=2.73e-03 FER=2.87e-02
2025-10-01 13:45:21,397 | INFO | Epoch 1008 Train Time 128.94449830055237s

2025-10-01 13:47:30,855 | INFO | Training epoch 1009, Batch 1000/1000: LR=2.50e-05, Loss=6.96e-03 BER=2.77e-03 FER=2.87e-02
2025-10-01 13:47:31,320 | INFO | Epoch 1009 Train Time 129.92158675193787s

2025-10-01 13:49:40,255 | INFO | Training epoch 1010, Batch 1000/1000: LR=2.49e-05, Loss=7.02e-03 BER=2.77e-03 FER=2.90e-02
2025-10-01 13:49:40,717 | INFO | Epoch 1010 Train Time 129.39613151550293s

2025-10-01 13:51:50,664 | INFO | Training epoch 1011, Batch 1000/1000: LR=2.49e-05, Loss=6.89e-03 BER=2.72e-03 FER=2.87e-02
2025-10-01 13:51:51,101 | INFO | Epoch 1011 Train Time 130.38265895843506s

2025-10-01 13:54:00,262 | INFO | Training epoch 1012, Batch 1000/1000: LR=2.48e-05, Loss=7.27e-03 BER=2.86e-03 FER=2.96e-02
2025-10-01 13:54:00,721 | INFO | Epoch 1012 Train Time 129.61792826652527s

2025-10-01 13:56:10,671 | INFO | Training epoch 1013, Batch 1000/1000: LR=2.47e-05, Loss=6.96e-03 BER=2.72e-03 FER=2.90e-02
2025-10-01 13:56:11,122 | INFO | Epoch 1013 Train Time 130.4002013206482s

2025-10-01 13:58:20,983 | INFO | Training epoch 1014, Batch 1000/1000: LR=2.46e-05, Loss=7.12e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 13:58:21,453 | INFO | Epoch 1014 Train Time 130.32894921302795s

2025-10-01 14:00:31,589 | INFO | Training epoch 1015, Batch 1000/1000: LR=2.45e-05, Loss=7.02e-03 BER=2.78e-03 FER=2.88e-02
2025-10-01 14:00:32,054 | INFO | Epoch 1015 Train Time 130.6002914905548s

2025-10-01 14:02:41,112 | INFO | Training epoch 1016, Batch 1000/1000: LR=2.44e-05, Loss=7.11e-03 BER=2.78e-03 FER=2.94e-02
2025-10-01 14:02:41,559 | INFO | Epoch 1016 Train Time 129.50427269935608s

2025-10-01 14:04:58,910 | INFO | Training epoch 1017, Batch 1000/1000: LR=2.43e-05, Loss=6.99e-03 BER=2.76e-03 FER=2.90e-02
2025-10-01 14:04:59,404 | INFO | Epoch 1017 Train Time 137.84344625473022s

2025-10-01 14:07:23,816 | INFO | Training epoch 1018, Batch 1000/1000: LR=2.42e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 14:07:24,333 | INFO | Epoch 1018 Train Time 144.92780709266663s

2025-10-01 14:09:46,976 | INFO | Training epoch 1019, Batch 1000/1000: LR=2.42e-05, Loss=7.12e-03 BER=2.80e-03 FER=2.96e-02
2025-10-01 14:09:47,411 | INFO | Epoch 1019 Train Time 143.0778329372406s

2025-10-01 14:12:12,490 | INFO | Training epoch 1020, Batch 1000/1000: LR=2.41e-05, Loss=7.31e-03 BER=2.91e-03 FER=2.96e-02
2025-10-01 14:12:13,014 | INFO | Epoch 1020 Train Time 145.6019492149353s

2025-10-01 14:14:38,723 | INFO | Training epoch 1021, Batch 1000/1000: LR=2.40e-05, Loss=7.07e-03 BER=2.79e-03 FER=2.96e-02
2025-10-01 14:14:39,179 | INFO | Epoch 1021 Train Time 146.1643340587616s

2025-10-01 14:17:01,195 | INFO | Training epoch 1022, Batch 1000/1000: LR=2.39e-05, Loss=6.88e-03 BER=2.72e-03 FER=2.86e-02
2025-10-01 14:17:01,757 | INFO | Epoch 1022 Train Time 142.5765962600708s

2025-10-01 14:19:14,550 | INFO | Training epoch 1023, Batch 1000/1000: LR=2.38e-05, Loss=7.17e-03 BER=2.82e-03 FER=2.91e-02
2025-10-01 14:19:15,014 | INFO | Epoch 1023 Train Time 133.25583696365356s

2025-10-01 14:21:26,770 | INFO | Training epoch 1024, Batch 1000/1000: LR=2.37e-05, Loss=7.09e-03 BER=2.79e-03 FER=2.90e-02
2025-10-01 14:21:27,247 | INFO | Epoch 1024 Train Time 132.23211550712585s

2025-10-01 14:23:38,050 | INFO | Training epoch 1025, Batch 1000/1000: LR=2.36e-05, Loss=7.14e-03 BER=2.80e-03 FER=2.88e-02
2025-10-01 14:23:38,528 | INFO | Epoch 1025 Train Time 131.2802951335907s

2025-10-01 14:25:49,282 | INFO | Training epoch 1026, Batch 1000/1000: LR=2.35e-05, Loss=7.15e-03 BER=2.78e-03 FER=2.92e-02
2025-10-01 14:25:49,749 | INFO | Epoch 1026 Train Time 131.2199010848999s

2025-10-01 14:28:00,080 | INFO | Training epoch 1027, Batch 1000/1000: LR=2.35e-05, Loss=7.10e-03 BER=2.78e-03 FER=2.92e-02
2025-10-01 14:28:00,552 | INFO | Epoch 1027 Train Time 130.80177068710327s

2025-10-01 14:30:08,099 | INFO | Training epoch 1028, Batch 1000/1000: LR=2.34e-05, Loss=6.85e-03 BER=2.70e-03 FER=2.81e-02
2025-10-01 14:30:08,558 | INFO | Epoch 1028 Train Time 128.00451970100403s

2025-10-01 14:32:18,867 | INFO | Training epoch 1029, Batch 1000/1000: LR=2.33e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 14:32:19,326 | INFO | Epoch 1029 Train Time 130.76749539375305s

2025-10-01 14:34:28,664 | INFO | Training epoch 1030, Batch 1000/1000: LR=2.32e-05, Loss=7.09e-03 BER=2.77e-03 FER=2.91e-02
2025-10-01 14:34:29,157 | INFO | Epoch 1030 Train Time 129.82997727394104s

2025-10-01 14:36:38,743 | INFO | Training epoch 1031, Batch 1000/1000: LR=2.31e-05, Loss=6.94e-03 BER=2.71e-03 FER=2.87e-02
2025-10-01 14:36:39,192 | INFO | Epoch 1031 Train Time 130.03434038162231s

2025-10-01 14:38:48,339 | INFO | Training epoch 1032, Batch 1000/1000: LR=2.30e-05, Loss=7.07e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 14:38:48,795 | INFO | Epoch 1032 Train Time 129.60143113136292s

2025-10-01 14:40:59,043 | INFO | Training epoch 1033, Batch 1000/1000: LR=2.29e-05, Loss=7.05e-03 BER=2.77e-03 FER=2.90e-02
2025-10-01 14:40:59,517 | INFO | Epoch 1033 Train Time 130.72154188156128s

2025-10-01 14:43:10,026 | INFO | Training epoch 1034, Batch 1000/1000: LR=2.28e-05, Loss=7.05e-03 BER=2.77e-03 FER=2.89e-02
2025-10-01 14:43:10,492 | INFO | Epoch 1034 Train Time 130.97373390197754s

2025-10-01 14:45:25,384 | INFO | Training epoch 1035, Batch 1000/1000: LR=2.28e-05, Loss=7.33e-03 BER=2.92e-03 FER=2.98e-02
2025-10-01 14:45:25,886 | INFO | Epoch 1035 Train Time 135.39170050621033s

2025-10-01 14:47:36,181 | INFO | Training epoch 1036, Batch 1000/1000: LR=2.27e-05, Loss=7.08e-03 BER=2.77e-03 FER=2.91e-02
2025-10-01 14:47:36,645 | INFO | Epoch 1036 Train Time 130.75843977928162s

2025-10-01 14:49:47,657 | INFO | Training epoch 1037, Batch 1000/1000: LR=2.26e-05, Loss=7.01e-03 BER=2.78e-03 FER=2.92e-02
2025-10-01 14:49:48,130 | INFO | Epoch 1037 Train Time 131.48419380187988s

2025-10-01 14:51:57,564 | INFO | Training epoch 1038, Batch 1000/1000: LR=2.25e-05, Loss=7.18e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 14:51:58,024 | INFO | Epoch 1038 Train Time 129.89292740821838s

2025-10-01 14:54:09,115 | INFO | Training epoch 1039, Batch 1000/1000: LR=2.24e-05, Loss=7.02e-03 BER=2.77e-03 FER=2.88e-02
2025-10-01 14:54:09,653 | INFO | Epoch 1039 Train Time 131.62791419029236s

2025-10-01 14:56:18,917 | INFO | Training epoch 1040, Batch 1000/1000: LR=2.23e-05, Loss=7.20e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 14:56:19,381 | INFO | Epoch 1040 Train Time 129.72664070129395s

2025-10-01 14:58:30,455 | INFO | Training epoch 1041, Batch 1000/1000: LR=2.22e-05, Loss=7.23e-03 BER=2.85e-03 FER=2.90e-02
2025-10-01 14:58:30,921 | INFO | Epoch 1041 Train Time 131.53925848007202s

2025-10-01 15:00:40,391 | INFO | Training epoch 1042, Batch 1000/1000: LR=2.22e-05, Loss=6.96e-03 BER=2.71e-03 FER=2.81e-02
2025-10-01 15:00:40,854 | INFO | Epoch 1042 Train Time 129.93120908737183s

2025-10-01 15:02:51,152 | INFO | Training epoch 1043, Batch 1000/1000: LR=2.21e-05, Loss=7.06e-03 BER=2.78e-03 FER=2.91e-02
2025-10-01 15:02:51,621 | INFO | Epoch 1043 Train Time 130.76571559906006s

2025-10-01 15:05:02,734 | INFO | Training epoch 1044, Batch 1000/1000: LR=2.20e-05, Loss=7.09e-03 BER=2.80e-03 FER=2.92e-02
2025-10-01 15:05:03,232 | INFO | Epoch 1044 Train Time 131.60985684394836s

2025-10-01 15:07:14,844 | INFO | Training epoch 1045, Batch 1000/1000: LR=2.19e-05, Loss=7.05e-03 BER=2.78e-03 FER=2.91e-02
2025-10-01 15:07:15,334 | INFO | Epoch 1045 Train Time 132.10132575035095s

2025-10-01 15:09:26,295 | INFO | Training epoch 1046, Batch 1000/1000: LR=2.18e-05, Loss=6.92e-03 BER=2.72e-03 FER=2.83e-02
2025-10-01 15:09:26,770 | INFO | Epoch 1046 Train Time 131.43436312675476s

2025-10-01 15:11:36,628 | INFO | Training epoch 1047, Batch 1000/1000: LR=2.17e-05, Loss=6.94e-03 BER=2.74e-03 FER=2.85e-02
2025-10-01 15:11:37,075 | INFO | Epoch 1047 Train Time 130.3031873703003s

2025-10-01 15:13:46,875 | INFO | Training epoch 1048, Batch 1000/1000: LR=2.17e-05, Loss=6.80e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 15:13:47,325 | INFO | Epoch 1048 Train Time 130.2496407032013s

2025-10-01 15:15:59,420 | INFO | Training epoch 1049, Batch 1000/1000: LR=2.16e-05, Loss=6.96e-03 BER=2.71e-03 FER=2.85e-02
2025-10-01 15:15:59,971 | INFO | Epoch 1049 Train Time 132.64406561851501s

2025-10-01 15:18:14,488 | INFO | Training epoch 1050, Batch 1000/1000: LR=2.15e-05, Loss=6.93e-03 BER=2.70e-03 FER=2.82e-02
2025-10-01 15:18:14,951 | INFO | Epoch 1050 Train Time 134.979966878891s

2025-10-01 15:20:28,158 | INFO | Training epoch 1051, Batch 1000/1000: LR=2.14e-05, Loss=6.74e-03 BER=2.66e-03 FER=2.80e-02
2025-10-01 15:20:28,639 | INFO | Epoch 1051 Train Time 133.68622040748596s

2025-10-01 15:20:28,639 | INFO | [P2] saving best_model (QAT) with loss 0.006742 at epoch 1051
2025-10-01 15:22:37,470 | INFO | Training epoch 1052, Batch 1000/1000: LR=2.13e-05, Loss=7.09e-03 BER=2.78e-03 FER=2.90e-02
2025-10-01 15:22:37,925 | INFO | Epoch 1052 Train Time 129.21646118164062s

2025-10-01 15:24:48,026 | INFO | Training epoch 1053, Batch 1000/1000: LR=2.12e-05, Loss=7.04e-03 BER=2.76e-03 FER=2.89e-02
2025-10-01 15:24:48,501 | INFO | Epoch 1053 Train Time 130.57532930374146s

2025-10-01 15:26:57,452 | INFO | Training epoch 1054, Batch 1000/1000: LR=2.12e-05, Loss=6.92e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 15:26:57,947 | INFO | Epoch 1054 Train Time 129.444105386734s

2025-10-01 15:29:09,181 | INFO | Training epoch 1055, Batch 1000/1000: LR=2.11e-05, Loss=7.08e-03 BER=2.79e-03 FER=2.93e-02
2025-10-01 15:29:09,638 | INFO | Epoch 1055 Train Time 131.69049978256226s

2025-10-01 15:31:19,013 | INFO | Training epoch 1056, Batch 1000/1000: LR=2.10e-05, Loss=7.09e-03 BER=2.82e-03 FER=2.94e-02
2025-10-01 15:31:19,482 | INFO | Epoch 1056 Train Time 129.84281206130981s

2025-10-01 15:33:29,958 | INFO | Training epoch 1057, Batch 1000/1000: LR=2.09e-05, Loss=7.35e-03 BER=2.88e-03 FER=2.96e-02
2025-10-01 15:33:30,435 | INFO | Epoch 1057 Train Time 130.9522521495819s

2025-10-01 15:35:39,627 | INFO | Training epoch 1058, Batch 1000/1000: LR=2.08e-05, Loss=7.03e-03 BER=2.79e-03 FER=2.88e-02
2025-10-01 15:35:40,104 | INFO | Epoch 1058 Train Time 129.6679437160492s

2025-10-01 15:37:50,513 | INFO | Training epoch 1059, Batch 1000/1000: LR=2.07e-05, Loss=7.04e-03 BER=2.76e-03 FER=2.89e-02
2025-10-01 15:37:50,972 | INFO | Epoch 1059 Train Time 130.86719799041748s

2025-10-01 15:39:59,863 | INFO | Training epoch 1060, Batch 1000/1000: LR=2.07e-05, Loss=7.01e-03 BER=2.75e-03 FER=2.85e-02
2025-10-01 15:40:00,344 | INFO | Epoch 1060 Train Time 129.3703076839447s

2025-10-01 15:42:10,225 | INFO | Training epoch 1061, Batch 1000/1000: LR=2.06e-05, Loss=7.13e-03 BER=2.81e-03 FER=2.88e-02
2025-10-01 15:42:10,697 | INFO | Epoch 1061 Train Time 130.3526794910431s

2025-10-01 15:44:20,989 | INFO | Training epoch 1062, Batch 1000/1000: LR=2.05e-05, Loss=7.28e-03 BER=2.88e-03 FER=2.96e-02
2025-10-01 15:44:21,445 | INFO | Epoch 1062 Train Time 130.74676299095154s

2025-10-01 15:46:32,057 | INFO | Training epoch 1063, Batch 1000/1000: LR=2.04e-05, Loss=7.10e-03 BER=2.79e-03 FER=2.85e-02
2025-10-01 15:46:32,524 | INFO | Epoch 1063 Train Time 131.07796931266785s

2025-10-01 15:48:48,414 | INFO | Training epoch 1064, Batch 1000/1000: LR=2.03e-05, Loss=7.13e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 15:48:48,865 | INFO | Epoch 1064 Train Time 136.34005689620972s

2025-10-01 15:50:58,959 | INFO | Training epoch 1065, Batch 1000/1000: LR=2.02e-05, Loss=7.19e-03 BER=2.83e-03 FER=2.95e-02
2025-10-01 15:50:59,444 | INFO | Epoch 1065 Train Time 130.5777678489685s

2025-10-01 15:53:08,986 | INFO | Training epoch 1066, Batch 1000/1000: LR=2.02e-05, Loss=6.99e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 15:53:09,430 | INFO | Epoch 1066 Train Time 129.98489141464233s

2025-10-01 15:55:19,550 | INFO | Training epoch 1067, Batch 1000/1000: LR=2.01e-05, Loss=6.83e-03 BER=2.66e-03 FER=2.84e-02
2025-10-01 15:55:20,031 | INFO | Epoch 1067 Train Time 130.60034132003784s

2025-10-01 15:57:28,637 | INFO | Training epoch 1068, Batch 1000/1000: LR=2.00e-05, Loss=6.93e-03 BER=2.72e-03 FER=2.90e-02
2025-10-01 15:57:29,106 | INFO | Epoch 1068 Train Time 129.07438826560974s

2025-10-01 15:59:40,296 | INFO | Training epoch 1069, Batch 1000/1000: LR=1.99e-05, Loss=7.10e-03 BER=2.78e-03 FER=2.88e-02
2025-10-01 15:59:40,771 | INFO | Epoch 1069 Train Time 131.66338968276978s

2025-10-01 16:01:50,241 | INFO | Training epoch 1070, Batch 1000/1000: LR=1.98e-05, Loss=6.99e-03 BER=2.73e-03 FER=2.84e-02
2025-10-01 16:01:50,702 | INFO | Epoch 1070 Train Time 129.9299201965332s

2025-10-01 16:04:01,316 | INFO | Training epoch 1071, Batch 1000/1000: LR=1.98e-05, Loss=7.03e-03 BER=2.76e-03 FER=2.91e-02
2025-10-01 16:04:01,775 | INFO | Epoch 1071 Train Time 131.07201147079468s

2025-10-01 16:06:12,367 | INFO | Training epoch 1072, Batch 1000/1000: LR=1.97e-05, Loss=6.83e-03 BER=2.70e-03 FER=2.82e-02
2025-10-01 16:06:12,825 | INFO | Epoch 1072 Train Time 131.04961729049683s

2025-10-01 16:08:24,295 | INFO | Training epoch 1073, Batch 1000/1000: LR=1.96e-05, Loss=7.03e-03 BER=2.77e-03 FER=2.87e-02
2025-10-01 16:08:24,777 | INFO | Epoch 1073 Train Time 131.95021891593933s

2025-10-01 16:10:33,409 | INFO | Training epoch 1074, Batch 1000/1000: LR=1.95e-05, Loss=7.14e-03 BER=2.81e-03 FER=2.92e-02
2025-10-01 16:10:33,874 | INFO | Epoch 1074 Train Time 129.09689140319824s

2025-10-01 16:12:44,808 | INFO | Training epoch 1075, Batch 1000/1000: LR=1.94e-05, Loss=7.20e-03 BER=2.85e-03 FER=2.96e-02
2025-10-01 16:12:45,312 | INFO | Epoch 1075 Train Time 131.43664383888245s

2025-10-01 16:14:55,614 | INFO | Training epoch 1076, Batch 1000/1000: LR=1.93e-05, Loss=6.83e-03 BER=2.68e-03 FER=2.82e-02
2025-10-01 16:14:56,077 | INFO | Epoch 1076 Train Time 130.76389336585999s

2025-10-01 16:17:06,384 | INFO | Training epoch 1077, Batch 1000/1000: LR=1.93e-05, Loss=7.16e-03 BER=2.81e-03 FER=2.91e-02
2025-10-01 16:17:06,850 | INFO | Epoch 1077 Train Time 130.77176785469055s

2025-10-01 16:19:23,094 | INFO | Training epoch 1078, Batch 1000/1000: LR=1.92e-05, Loss=6.96e-03 BER=2.75e-03 FER=2.86e-02
2025-10-01 16:19:23,580 | INFO | Epoch 1078 Train Time 136.7282259464264s

2025-10-01 16:21:38,554 | INFO | Training epoch 1079, Batch 1000/1000: LR=1.91e-05, Loss=7.11e-03 BER=2.80e-03 FER=2.94e-02
2025-10-01 16:21:38,994 | INFO | Epoch 1079 Train Time 135.4125304222107s

2025-10-01 16:23:48,970 | INFO | Training epoch 1080, Batch 1000/1000: LR=1.90e-05, Loss=7.00e-03 BER=2.76e-03 FER=2.88e-02
2025-10-01 16:23:49,432 | INFO | Epoch 1080 Train Time 130.43699836730957s

2025-10-01 16:25:59,178 | INFO | Training epoch 1081, Batch 1000/1000: LR=1.89e-05, Loss=7.23e-03 BER=2.87e-03 FER=3.00e-02
2025-10-01 16:25:59,647 | INFO | Epoch 1081 Train Time 130.21448731422424s

2025-10-01 16:28:09,521 | INFO | Training epoch 1082, Batch 1000/1000: LR=1.89e-05, Loss=6.93e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 16:28:10,000 | INFO | Epoch 1082 Train Time 130.35166478157043s

2025-10-01 16:30:20,328 | INFO | Training epoch 1083, Batch 1000/1000: LR=1.88e-05, Loss=7.12e-03 BER=2.77e-03 FER=2.87e-02
2025-10-01 16:30:20,810 | INFO | Epoch 1083 Train Time 130.80886006355286s

2025-10-01 16:32:29,023 | INFO | Training epoch 1084, Batch 1000/1000: LR=1.87e-05, Loss=7.28e-03 BER=2.86e-03 FER=2.96e-02
2025-10-01 16:32:29,497 | INFO | Epoch 1084 Train Time 128.686674118042s

2025-10-01 16:34:40,002 | INFO | Training epoch 1085, Batch 1000/1000: LR=1.86e-05, Loss=6.91e-03 BER=2.73e-03 FER=2.84e-02
2025-10-01 16:34:40,469 | INFO | Epoch 1085 Train Time 130.9705786705017s

2025-10-01 16:36:49,467 | INFO | Training epoch 1086, Batch 1000/1000: LR=1.85e-05, Loss=6.95e-03 BER=2.73e-03 FER=2.86e-02
2025-10-01 16:36:49,940 | INFO | Epoch 1086 Train Time 129.4692268371582s

2025-10-01 16:39:01,226 | INFO | Training epoch 1087, Batch 1000/1000: LR=1.85e-05, Loss=6.83e-03 BER=2.69e-03 FER=2.80e-02
2025-10-01 16:39:01,673 | INFO | Epoch 1087 Train Time 131.73284721374512s

2025-10-01 16:41:10,279 | INFO | Training epoch 1088, Batch 1000/1000: LR=1.84e-05, Loss=6.86e-03 BER=2.69e-03 FER=2.81e-02
2025-10-01 16:41:10,775 | INFO | Epoch 1088 Train Time 129.1010251045227s

2025-10-01 16:43:22,062 | INFO | Training epoch 1089, Batch 1000/1000: LR=1.83e-05, Loss=7.19e-03 BER=2.82e-03 FER=2.95e-02
2025-10-01 16:43:22,530 | INFO | Epoch 1089 Train Time 131.7537202835083s

2025-10-01 16:45:32,481 | INFO | Training epoch 1090, Batch 1000/1000: LR=1.82e-05, Loss=6.85e-03 BER=2.70e-03 FER=2.81e-02
2025-10-01 16:45:32,988 | INFO | Epoch 1090 Train Time 130.45759987831116s

2025-10-01 16:47:43,586 | INFO | Training epoch 1091, Batch 1000/1000: LR=1.82e-05, Loss=6.83e-03 BER=2.71e-03 FER=2.80e-02
2025-10-01 16:47:44,207 | INFO | Epoch 1091 Train Time 131.21699929237366s

2025-10-01 16:49:53,739 | INFO | Training epoch 1092, Batch 1000/1000: LR=1.81e-05, Loss=6.91e-03 BER=2.73e-03 FER=2.87e-02
2025-10-01 16:49:54,215 | INFO | Epoch 1092 Train Time 130.00796675682068s

2025-10-01 16:52:12,696 | INFO | Training epoch 1093, Batch 1000/1000: LR=1.80e-05, Loss=7.12e-03 BER=2.79e-03 FER=2.90e-02
2025-10-01 16:52:13,155 | INFO | Epoch 1093 Train Time 138.93841814994812s

2025-10-01 16:54:23,200 | INFO | Training epoch 1094, Batch 1000/1000: LR=1.79e-05, Loss=7.17e-03 BER=2.79e-03 FER=2.90e-02
2025-10-01 16:54:23,666 | INFO | Epoch 1094 Train Time 130.51039910316467s

2025-10-01 16:56:33,893 | INFO | Training epoch 1095, Batch 1000/1000: LR=1.78e-05, Loss=7.07e-03 BER=2.78e-03 FER=2.91e-02
2025-10-01 16:56:34,346 | INFO | Epoch 1095 Train Time 130.67861437797546s

2025-10-01 16:58:43,975 | INFO | Training epoch 1096, Batch 1000/1000: LR=1.78e-05, Loss=7.10e-03 BER=2.81e-03 FER=2.93e-02
2025-10-01 16:58:44,443 | INFO | Epoch 1096 Train Time 130.09627628326416s

2025-10-01 17:00:55,072 | INFO | Training epoch 1097, Batch 1000/1000: LR=1.77e-05, Loss=6.81e-03 BER=2.68e-03 FER=2.84e-02
2025-10-01 17:00:55,522 | INFO | Epoch 1097 Train Time 131.0784227848053s

2025-10-01 17:03:04,264 | INFO | Training epoch 1098, Batch 1000/1000: LR=1.76e-05, Loss=6.87e-03 BER=2.73e-03 FER=2.82e-02
2025-10-01 17:03:04,755 | INFO | Epoch 1098 Train Time 129.23120546340942s

2025-10-01 17:05:16,502 | INFO | Training epoch 1099, Batch 1000/1000: LR=1.75e-05, Loss=7.09e-03 BER=2.80e-03 FER=2.89e-02
2025-10-01 17:05:17,004 | INFO | Epoch 1099 Train Time 132.24823880195618s

2025-10-01 17:07:29,297 | INFO | Training epoch 1100, Batch 1000/1000: LR=1.75e-05, Loss=6.97e-03 BER=2.73e-03 FER=2.87e-02
2025-10-01 17:07:29,764 | INFO | Epoch 1100 Train Time 132.7583885192871s

2025-10-01 17:09:41,156 | INFO | Training epoch 1101, Batch 1000/1000: LR=1.74e-05, Loss=6.95e-03 BER=2.74e-03 FER=2.87e-02
2025-10-01 17:09:41,632 | INFO | Epoch 1101 Train Time 131.8662886619568s

2025-10-01 17:11:51,497 | INFO | Training epoch 1102, Batch 1000/1000: LR=1.73e-05, Loss=7.02e-03 BER=2.77e-03 FER=2.86e-02
2025-10-01 17:11:51,962 | INFO | Epoch 1102 Train Time 130.32978177070618s

2025-10-01 17:14:02,618 | INFO | Training epoch 1103, Batch 1000/1000: LR=1.72e-05, Loss=6.98e-03 BER=2.76e-03 FER=2.88e-02
2025-10-01 17:14:03,077 | INFO | Epoch 1103 Train Time 131.11376523971558s

2025-10-01 17:16:12,385 | INFO | Training epoch 1104, Batch 1000/1000: LR=1.71e-05, Loss=7.03e-03 BER=2.79e-03 FER=2.92e-02
2025-10-01 17:16:12,851 | INFO | Epoch 1104 Train Time 129.77315163612366s

2025-10-01 17:18:24,733 | INFO | Training epoch 1105, Batch 1000/1000: LR=1.71e-05, Loss=7.05e-03 BER=2.76e-03 FER=2.89e-02
2025-10-01 17:18:25,202 | INFO | Epoch 1105 Train Time 132.349356174469s

2025-10-01 17:20:36,977 | INFO | Training epoch 1106, Batch 1000/1000: LR=1.70e-05, Loss=6.82e-03 BER=2.68e-03 FER=2.84e-02
2025-10-01 17:20:37,460 | INFO | Epoch 1106 Train Time 132.25712943077087s

2025-10-01 17:22:56,756 | INFO | Training epoch 1107, Batch 1000/1000: LR=1.69e-05, Loss=6.94e-03 BER=2.72e-03 FER=2.89e-02
2025-10-01 17:22:57,212 | INFO | Epoch 1107 Train Time 139.751615524292s

2025-10-01 17:25:06,596 | INFO | Training epoch 1108, Batch 1000/1000: LR=1.68e-05, Loss=6.95e-03 BER=2.73e-03 FER=2.86e-02
2025-10-01 17:25:07,096 | INFO | Epoch 1108 Train Time 129.88293480873108s

2025-10-01 17:27:17,463 | INFO | Training epoch 1109, Batch 1000/1000: LR=1.68e-05, Loss=6.98e-03 BER=2.74e-03 FER=2.85e-02
2025-10-01 17:27:17,913 | INFO | Epoch 1109 Train Time 130.81580471992493s

2025-10-01 17:29:27,919 | INFO | Training epoch 1110, Batch 1000/1000: LR=1.67e-05, Loss=6.87e-03 BER=2.67e-03 FER=2.83e-02
2025-10-01 17:29:28,390 | INFO | Epoch 1110 Train Time 130.47543811798096s

2025-10-01 17:31:38,545 | INFO | Training epoch 1111, Batch 1000/1000: LR=1.66e-05, Loss=6.91e-03 BER=2.74e-03 FER=2.87e-02
2025-10-01 17:31:39,060 | INFO | Epoch 1111 Train Time 130.67001485824585s

2025-10-01 17:33:51,261 | INFO | Training epoch 1112, Batch 1000/1000: LR=1.65e-05, Loss=7.07e-03 BER=2.77e-03 FER=2.90e-02
2025-10-01 17:33:51,726 | INFO | Epoch 1112 Train Time 132.6646647453308s

2025-10-01 17:36:02,463 | INFO | Training epoch 1113, Batch 1000/1000: LR=1.65e-05, Loss=7.09e-03 BER=2.79e-03 FER=2.92e-02
2025-10-01 17:36:02,941 | INFO | Epoch 1113 Train Time 131.21340250968933s

2025-10-01 17:38:12,617 | INFO | Training epoch 1114, Batch 1000/1000: LR=1.64e-05, Loss=6.87e-03 BER=2.70e-03 FER=2.87e-02
2025-10-01 17:38:13,078 | INFO | Epoch 1114 Train Time 130.13645935058594s

2025-10-01 17:40:24,072 | INFO | Training epoch 1115, Batch 1000/1000: LR=1.63e-05, Loss=6.89e-03 BER=2.75e-03 FER=2.85e-02
2025-10-01 17:40:24,524 | INFO | Epoch 1115 Train Time 131.4453320503235s

2025-10-01 17:42:33,513 | INFO | Training epoch 1116, Batch 1000/1000: LR=1.62e-05, Loss=6.94e-03 BER=2.76e-03 FER=2.85e-02
2025-10-01 17:42:33,971 | INFO | Epoch 1116 Train Time 129.44527864456177s

2025-10-01 17:44:44,667 | INFO | Training epoch 1117, Batch 1000/1000: LR=1.62e-05, Loss=6.90e-03 BER=2.72e-03 FER=2.84e-02
2025-10-01 17:44:45,130 | INFO | Epoch 1117 Train Time 131.15782809257507s

2025-10-01 17:46:54,754 | INFO | Training epoch 1118, Batch 1000/1000: LR=1.61e-05, Loss=7.01e-03 BER=2.75e-03 FER=2.86e-02
2025-10-01 17:46:55,209 | INFO | Epoch 1118 Train Time 130.0787525177002s

2025-10-01 17:49:05,948 | INFO | Training epoch 1119, Batch 1000/1000: LR=1.60e-05, Loss=6.79e-03 BER=2.67e-03 FER=2.79e-02
2025-10-01 17:49:06,410 | INFO | Epoch 1119 Train Time 131.19901490211487s

2025-10-01 17:51:15,284 | INFO | Training epoch 1120, Batch 1000/1000: LR=1.59e-05, Loss=6.95e-03 BER=2.76e-03 FER=2.86e-02
2025-10-01 17:51:15,828 | INFO | Epoch 1120 Train Time 129.41794395446777s

2025-10-01 17:53:26,271 | INFO | Training epoch 1121, Batch 1000/1000: LR=1.59e-05, Loss=7.06e-03 BER=2.77e-03 FER=2.88e-02
2025-10-01 17:53:26,757 | INFO | Epoch 1121 Train Time 130.92759537696838s

2025-10-01 17:55:45,387 | INFO | Training epoch 1122, Batch 1000/1000: LR=1.58e-05, Loss=6.90e-03 BER=2.69e-03 FER=2.79e-02
2025-10-01 17:55:45,853 | INFO | Epoch 1122 Train Time 139.0954556465149s

2025-10-01 17:57:56,701 | INFO | Training epoch 1123, Batch 1000/1000: LR=1.57e-05, Loss=7.06e-03 BER=2.78e-03 FER=2.87e-02
2025-10-01 17:57:57,159 | INFO | Epoch 1123 Train Time 131.30437183380127s

2025-10-01 18:00:06,381 | INFO | Training epoch 1124, Batch 1000/1000: LR=1.56e-05, Loss=6.81e-03 BER=2.67e-03 FER=2.81e-02
2025-10-01 18:00:06,847 | INFO | Epoch 1124 Train Time 129.68718457221985s

2025-10-01 18:02:16,038 | INFO | Training epoch 1125, Batch 1000/1000: LR=1.56e-05, Loss=7.05e-03 BER=2.75e-03 FER=2.90e-02
2025-10-01 18:02:16,518 | INFO | Epoch 1125 Train Time 129.6699297428131s

2025-10-01 18:04:25,848 | INFO | Training epoch 1126, Batch 1000/1000: LR=1.55e-05, Loss=7.10e-03 BER=2.82e-03 FER=2.92e-02
2025-10-01 18:04:26,299 | INFO | Epoch 1126 Train Time 129.7793369293213s

2025-10-01 18:06:38,124 | INFO | Training epoch 1127, Batch 1000/1000: LR=1.54e-05, Loss=6.84e-03 BER=2.70e-03 FER=2.81e-02
2025-10-01 18:06:38,596 | INFO | Epoch 1127 Train Time 132.29588317871094s

2025-10-01 18:08:47,307 | INFO | Training epoch 1128, Batch 1000/1000: LR=1.54e-05, Loss=7.02e-03 BER=2.74e-03 FER=2.87e-02
2025-10-01 18:08:47,756 | INFO | Epoch 1128 Train Time 129.15903878211975s

2025-10-01 18:10:58,007 | INFO | Training epoch 1129, Batch 1000/1000: LR=1.53e-05, Loss=6.94e-03 BER=2.71e-03 FER=2.88e-02
2025-10-01 18:10:58,463 | INFO | Epoch 1129 Train Time 130.70553588867188s

2025-10-01 18:13:07,624 | INFO | Training epoch 1130, Batch 1000/1000: LR=1.52e-05, Loss=6.95e-03 BER=2.75e-03 FER=2.85e-02
2025-10-01 18:13:08,094 | INFO | Epoch 1130 Train Time 129.62992119789124s

2025-10-01 18:15:17,684 | INFO | Training epoch 1131, Batch 1000/1000: LR=1.51e-05, Loss=6.73e-03 BER=2.63e-03 FER=2.76e-02
2025-10-01 18:15:18,167 | INFO | Epoch 1131 Train Time 130.07238960266113s

2025-10-01 18:15:18,168 | INFO | [P2] saving best_model (QAT) with loss 0.006732 at epoch 1131
2025-10-01 18:17:25,832 | INFO | Training epoch 1132, Batch 1000/1000: LR=1.51e-05, Loss=6.94e-03 BER=2.74e-03 FER=2.86e-02
2025-10-01 18:17:26,297 | INFO | Epoch 1132 Train Time 128.0590443611145s

2025-10-01 18:19:35,738 | INFO | Training epoch 1133, Batch 1000/1000: LR=1.50e-05, Loss=7.22e-03 BER=2.84e-03 FER=2.95e-02
2025-10-01 18:19:36,203 | INFO | Epoch 1133 Train Time 129.903963804245s

2025-10-01 18:21:46,983 | INFO | Training epoch 1134, Batch 1000/1000: LR=1.49e-05, Loss=6.98e-03 BER=2.74e-03 FER=2.86e-02
2025-10-01 18:21:47,456 | INFO | Epoch 1134 Train Time 131.25265908241272s

2025-10-01 18:23:56,985 | INFO | Training epoch 1135, Batch 1000/1000: LR=1.48e-05, Loss=6.86e-03 BER=2.69e-03 FER=2.85e-02
2025-10-01 18:23:57,470 | INFO | Epoch 1135 Train Time 130.01270484924316s

2025-10-01 18:26:12,873 | INFO | Training epoch 1136, Batch 1000/1000: LR=1.48e-05, Loss=6.99e-03 BER=2.73e-03 FER=2.84e-02
2025-10-01 18:26:13,417 | INFO | Epoch 1136 Train Time 135.94554138183594s

2025-10-01 18:28:24,507 | INFO | Training epoch 1137, Batch 1000/1000: LR=1.47e-05, Loss=7.08e-03 BER=2.79e-03 FER=2.90e-02
2025-10-01 18:28:24,983 | INFO | Epoch 1137 Train Time 131.56532549858093s

2025-10-01 18:30:33,049 | INFO | Training epoch 1138, Batch 1000/1000: LR=1.46e-05, Loss=6.94e-03 BER=2.74e-03 FER=2.82e-02
2025-10-01 18:30:33,517 | INFO | Epoch 1138 Train Time 128.53331565856934s

2025-10-01 18:32:42,806 | INFO | Training epoch 1139, Batch 1000/1000: LR=1.46e-05, Loss=7.07e-03 BER=2.79e-03 FER=2.87e-02
2025-10-01 18:32:43,268 | INFO | Epoch 1139 Train Time 129.74992299079895s

2025-10-01 18:34:52,493 | INFO | Training epoch 1140, Batch 1000/1000: LR=1.45e-05, Loss=6.94e-03 BER=2.75e-03 FER=2.84e-02
2025-10-01 18:34:52,958 | INFO | Epoch 1140 Train Time 129.68830394744873s

2025-10-01 18:37:01,772 | INFO | Training epoch 1141, Batch 1000/1000: LR=1.44e-05, Loss=7.01e-03 BER=2.76e-03 FER=2.90e-02
2025-10-01 18:37:02,235 | INFO | Epoch 1141 Train Time 129.27645134925842s

2025-10-01 18:39:12,657 | INFO | Training epoch 1142, Batch 1000/1000: LR=1.43e-05, Loss=7.04e-03 BER=2.76e-03 FER=2.86e-02
2025-10-01 18:39:13,127 | INFO | Epoch 1142 Train Time 130.89048051834106s

2025-10-01 18:41:23,819 | INFO | Training epoch 1143, Batch 1000/1000: LR=1.43e-05, Loss=6.82e-03 BER=2.68e-03 FER=2.80e-02
2025-10-01 18:41:24,283 | INFO | Epoch 1143 Train Time 131.15567088127136s

2025-10-01 18:43:33,396 | INFO | Training epoch 1144, Batch 1000/1000: LR=1.42e-05, Loss=6.84e-03 BER=2.70e-03 FER=2.83e-02
2025-10-01 18:43:33,868 | INFO | Epoch 1144 Train Time 129.58421754837036s

2025-10-01 18:45:43,677 | INFO | Training epoch 1145, Batch 1000/1000: LR=1.41e-05, Loss=6.86e-03 BER=2.68e-03 FER=2.79e-02
2025-10-01 18:45:44,152 | INFO | Epoch 1145 Train Time 130.28271555900574s

2025-10-01 18:47:54,380 | INFO | Training epoch 1146, Batch 1000/1000: LR=1.41e-05, Loss=7.10e-03 BER=2.79e-03 FER=2.89e-02
2025-10-01 18:47:54,872 | INFO | Epoch 1146 Train Time 130.71857118606567s

2025-10-01 18:50:09,922 | INFO | Training epoch 1147, Batch 1000/1000: LR=1.40e-05, Loss=7.04e-03 BER=2.77e-03 FER=2.86e-02
2025-10-01 18:50:10,394 | INFO | Epoch 1147 Train Time 135.52142024040222s

2025-10-01 18:52:22,201 | INFO | Training epoch 1148, Batch 1000/1000: LR=1.39e-05, Loss=6.75e-03 BER=2.67e-03 FER=2.80e-02
2025-10-01 18:52:22,661 | INFO | Epoch 1148 Train Time 132.26528549194336s

2025-10-01 18:54:35,997 | INFO | Training epoch 1149, Batch 1000/1000: LR=1.39e-05, Loss=6.86e-03 BER=2.67e-03 FER=2.78e-02
2025-10-01 18:54:36,507 | INFO | Epoch 1149 Train Time 133.8437750339508s

2025-10-01 18:56:47,214 | INFO | Training epoch 1150, Batch 1000/1000: LR=1.38e-05, Loss=7.12e-03 BER=2.80e-03 FER=2.91e-02
2025-10-01 18:56:47,684 | INFO | Epoch 1150 Train Time 131.17556524276733s

2025-10-01 18:59:10,870 | INFO | Training epoch 1151, Batch 1000/1000: LR=1.37e-05, Loss=6.85e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 18:59:11,331 | INFO | Epoch 1151 Train Time 143.64566826820374s

2025-10-01 19:01:21,940 | INFO | Training epoch 1152, Batch 1000/1000: LR=1.36e-05, Loss=6.84e-03 BER=2.69e-03 FER=2.82e-02
2025-10-01 19:01:22,409 | INFO | Epoch 1152 Train Time 131.07750749588013s

2025-10-01 19:03:49,250 | INFO | Training epoch 1153, Batch 1000/1000: LR=1.36e-05, Loss=6.84e-03 BER=2.70e-03 FER=2.84e-02
2025-10-01 19:03:49,716 | INFO | Epoch 1153 Train Time 147.30580043792725s

2025-10-01 19:06:01,039 | INFO | Training epoch 1154, Batch 1000/1000: LR=1.35e-05, Loss=7.11e-03 BER=2.79e-03 FER=2.87e-02
2025-10-01 19:06:01,499 | INFO | Epoch 1154 Train Time 131.78192591667175s

2025-10-01 19:08:11,970 | INFO | Training epoch 1155, Batch 1000/1000: LR=1.34e-05, Loss=7.09e-03 BER=2.82e-03 FER=2.90e-02
2025-10-01 19:08:12,451 | INFO | Epoch 1155 Train Time 130.95122861862183s

2025-10-01 19:10:22,897 | INFO | Training epoch 1156, Batch 1000/1000: LR=1.34e-05, Loss=7.00e-03 BER=2.75e-03 FER=2.88e-02
2025-10-01 19:10:23,339 | INFO | Epoch 1156 Train Time 130.8880262374878s

2025-10-01 19:12:33,862 | INFO | Training epoch 1157, Batch 1000/1000: LR=1.33e-05, Loss=7.07e-03 BER=2.77e-03 FER=2.90e-02
2025-10-01 19:12:34,302 | INFO | Epoch 1157 Train Time 130.96211576461792s

2025-10-01 19:14:43,452 | INFO | Training epoch 1158, Batch 1000/1000: LR=1.32e-05, Loss=7.02e-03 BER=2.76e-03 FER=2.85e-02
2025-10-01 19:14:43,948 | INFO | Epoch 1158 Train Time 129.64473032951355s

2025-10-01 19:16:55,590 | INFO | Training epoch 1159, Batch 1000/1000: LR=1.32e-05, Loss=7.09e-03 BER=2.81e-03 FER=2.86e-02
2025-10-01 19:16:56,048 | INFO | Epoch 1159 Train Time 132.09856724739075s

2025-10-01 19:19:08,541 | INFO | Training epoch 1160, Batch 1000/1000: LR=1.31e-05, Loss=6.80e-03 BER=2.67e-03 FER=2.84e-02
2025-10-01 19:19:09,004 | INFO | Epoch 1160 Train Time 132.95564222335815s

2025-10-01 19:21:19,711 | INFO | Training epoch 1161, Batch 1000/1000: LR=1.30e-05, Loss=6.67e-03 BER=2.64e-03 FER=2.72e-02
2025-10-01 19:21:20,202 | INFO | Epoch 1161 Train Time 131.19682145118713s

2025-10-01 19:21:20,202 | INFO | [P2] saving best_model (QAT) with loss 0.006668 at epoch 1161
2025-10-01 19:23:29,314 | INFO | Training epoch 1162, Batch 1000/1000: LR=1.30e-05, Loss=6.98e-03 BER=2.74e-03 FER=2.90e-02
2025-10-01 19:23:29,773 | INFO | Epoch 1162 Train Time 129.49448919296265s

2025-10-01 19:25:39,618 | INFO | Training epoch 1163, Batch 1000/1000: LR=1.29e-05, Loss=6.92e-03 BER=2.73e-03 FER=2.84e-02
2025-10-01 19:25:40,153 | INFO | Epoch 1163 Train Time 130.3790409564972s

2025-10-01 19:27:48,500 | INFO | Training epoch 1164, Batch 1000/1000: LR=1.28e-05, Loss=6.97e-03 BER=2.76e-03 FER=2.88e-02
2025-10-01 19:27:48,966 | INFO | Epoch 1164 Train Time 128.8115427494049s

2025-10-01 19:30:07,739 | INFO | Training epoch 1165, Batch 1000/1000: LR=1.28e-05, Loss=6.73e-03 BER=2.66e-03 FER=2.74e-02
2025-10-01 19:30:08,204 | INFO | Epoch 1165 Train Time 139.23694157600403s

2025-10-01 19:32:16,822 | INFO | Training epoch 1166, Batch 1000/1000: LR=1.27e-05, Loss=7.02e-03 BER=2.72e-03 FER=2.85e-02
2025-10-01 19:32:17,302 | INFO | Epoch 1166 Train Time 129.09760212898254s

2025-10-01 19:34:26,791 | INFO | Training epoch 1167, Batch 1000/1000: LR=1.26e-05, Loss=6.92e-03 BER=2.73e-03 FER=2.85e-02
2025-10-01 19:34:27,263 | INFO | Epoch 1167 Train Time 129.95997619628906s

2025-10-01 19:36:36,201 | INFO | Training epoch 1168, Batch 1000/1000: LR=1.26e-05, Loss=6.88e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 19:36:36,683 | INFO | Epoch 1168 Train Time 129.41921043395996s

2025-10-01 19:38:46,562 | INFO | Training epoch 1169, Batch 1000/1000: LR=1.25e-05, Loss=6.82e-03 BER=2.70e-03 FER=2.82e-02
2025-10-01 19:38:47,020 | INFO | Epoch 1169 Train Time 130.3352348804474s

2025-10-01 19:40:55,872 | INFO | Training epoch 1170, Batch 1000/1000: LR=1.24e-05, Loss=6.95e-03 BER=2.70e-03 FER=2.82e-02
2025-10-01 19:40:56,330 | INFO | Epoch 1170 Train Time 129.30908966064453s

2025-10-01 19:43:06,842 | INFO | Training epoch 1171, Batch 1000/1000: LR=1.24e-05, Loss=6.80e-03 BER=2.69e-03 FER=2.82e-02
2025-10-01 19:43:07,290 | INFO | Epoch 1171 Train Time 130.9591360092163s

2025-10-01 19:45:16,535 | INFO | Training epoch 1172, Batch 1000/1000: LR=1.23e-05, Loss=6.93e-03 BER=2.72e-03 FER=2.81e-02
2025-10-01 19:45:16,988 | INFO | Epoch 1172 Train Time 129.69704246520996s

2025-10-01 19:47:26,634 | INFO | Training epoch 1173, Batch 1000/1000: LR=1.22e-05, Loss=7.14e-03 BER=2.81e-03 FER=2.86e-02
2025-10-01 19:47:27,074 | INFO | Epoch 1173 Train Time 130.08489441871643s

2025-10-01 19:49:36,458 | INFO | Training epoch 1174, Batch 1000/1000: LR=1.22e-05, Loss=6.86e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 19:49:36,905 | INFO | Epoch 1174 Train Time 129.83031272888184s

2025-10-01 19:51:46,263 | INFO | Training epoch 1175, Batch 1000/1000: LR=1.21e-05, Loss=7.05e-03 BER=2.79e-03 FER=2.91e-02
2025-10-01 19:51:46,723 | INFO | Epoch 1175 Train Time 129.81712889671326s

2025-10-01 19:53:55,924 | INFO | Training epoch 1176, Batch 1000/1000: LR=1.20e-05, Loss=6.93e-03 BER=2.73e-03 FER=2.83e-02
2025-10-01 19:53:56,373 | INFO | Epoch 1176 Train Time 129.6482493877411s

2025-10-01 19:56:06,165 | INFO | Training epoch 1177, Batch 1000/1000: LR=1.20e-05, Loss=7.11e-03 BER=2.82e-03 FER=2.90e-02
2025-10-01 19:56:06,654 | INFO | Epoch 1177 Train Time 130.27998328208923s

2025-10-01 19:58:14,983 | INFO | Training epoch 1178, Batch 1000/1000: LR=1.19e-05, Loss=6.89e-03 BER=2.71e-03 FER=2.85e-02
2025-10-01 19:58:15,463 | INFO | Epoch 1178 Train Time 128.80795526504517s

2025-10-01 20:00:24,364 | INFO | Training epoch 1179, Batch 1000/1000: LR=1.18e-05, Loss=6.97e-03 BER=2.75e-03 FER=2.84e-02
2025-10-01 20:00:24,812 | INFO | Epoch 1179 Train Time 129.34802317619324s

2025-10-01 20:02:42,697 | INFO | Training epoch 1180, Batch 1000/1000: LR=1.18e-05, Loss=6.85e-03 BER=2.67e-03 FER=2.77e-02
2025-10-01 20:02:43,157 | INFO | Epoch 1180 Train Time 138.34294199943542s

2025-10-01 20:04:53,354 | INFO | Training epoch 1181, Batch 1000/1000: LR=1.17e-05, Loss=7.00e-03 BER=2.77e-03 FER=2.87e-02
2025-10-01 20:04:53,829 | INFO | Epoch 1181 Train Time 130.67106246948242s

2025-10-01 20:07:04,944 | INFO | Training epoch 1182, Batch 1000/1000: LR=1.16e-05, Loss=6.86e-03 BER=2.70e-03 FER=2.77e-02
2025-10-01 20:07:05,407 | INFO | Epoch 1182 Train Time 131.57716917991638s

2025-10-01 20:09:13,642 | INFO | Training epoch 1183, Batch 1000/1000: LR=1.16e-05, Loss=6.91e-03 BER=2.72e-03 FER=2.85e-02
2025-10-01 20:09:14,122 | INFO | Epoch 1183 Train Time 128.71371865272522s

2025-10-01 20:11:25,209 | INFO | Training epoch 1184, Batch 1000/1000: LR=1.15e-05, Loss=6.95e-03 BER=2.73e-03 FER=2.79e-02
2025-10-01 20:11:25,669 | INFO | Epoch 1184 Train Time 131.54661536216736s

2025-10-01 20:13:54,278 | INFO | Training epoch 1185, Batch 1000/1000: LR=1.15e-05, Loss=7.02e-03 BER=2.76e-03 FER=2.86e-02
2025-10-01 20:13:54,769 | INFO | Epoch 1185 Train Time 149.0987458229065s

2025-10-01 20:16:28,426 | INFO | Training epoch 1186, Batch 1000/1000: LR=1.14e-05, Loss=6.62e-03 BER=2.62e-03 FER=2.73e-02
2025-10-01 20:16:28,953 | INFO | Epoch 1186 Train Time 154.18203592300415s

2025-10-01 20:16:28,953 | INFO | [P2] saving best_model (QAT) with loss 0.006621 at epoch 1186
2025-10-01 20:18:42,457 | INFO | Training epoch 1187, Batch 1000/1000: LR=1.13e-05, Loss=7.10e-03 BER=2.81e-03 FER=2.93e-02
2025-10-01 20:18:42,927 | INFO | Epoch 1187 Train Time 133.88564538955688s

2025-10-01 20:20:55,143 | INFO | Training epoch 1188, Batch 1000/1000: LR=1.13e-05, Loss=6.93e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 20:20:55,617 | INFO | Epoch 1188 Train Time 132.68786811828613s

2025-10-01 20:23:07,388 | INFO | Training epoch 1189, Batch 1000/1000: LR=1.12e-05, Loss=6.85e-03 BER=2.69e-03 FER=2.85e-02
2025-10-01 20:23:07,844 | INFO | Epoch 1189 Train Time 132.22685766220093s

2025-10-01 20:25:18,750 | INFO | Training epoch 1190, Batch 1000/1000: LR=1.11e-05, Loss=6.83e-03 BER=2.66e-03 FER=2.77e-02
2025-10-01 20:25:19,216 | INFO | Epoch 1190 Train Time 131.37126874923706s

2025-10-01 20:27:30,479 | INFO | Training epoch 1191, Batch 1000/1000: LR=1.11e-05, Loss=6.89e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 20:27:30,919 | INFO | Epoch 1191 Train Time 131.70127773284912s

2025-10-01 20:29:38,870 | INFO | Training epoch 1192, Batch 1000/1000: LR=1.10e-05, Loss=6.88e-03 BER=2.71e-03 FER=2.79e-02
2025-10-01 20:29:39,342 | INFO | Epoch 1192 Train Time 128.4213626384735s

2025-10-01 20:31:46,842 | INFO | Training epoch 1193, Batch 1000/1000: LR=1.09e-05, Loss=6.65e-03 BER=2.63e-03 FER=2.74e-02
2025-10-01 20:31:47,305 | INFO | Epoch 1193 Train Time 127.96201467514038s

2025-10-01 20:34:05,914 | INFO | Training epoch 1194, Batch 1000/1000: LR=1.09e-05, Loss=6.85e-03 BER=2.73e-03 FER=2.83e-02
2025-10-01 20:34:06,362 | INFO | Epoch 1194 Train Time 139.0559344291687s

2025-10-01 20:36:14,947 | INFO | Training epoch 1195, Batch 1000/1000: LR=1.08e-05, Loss=6.96e-03 BER=2.73e-03 FER=2.80e-02
2025-10-01 20:36:15,407 | INFO | Epoch 1195 Train Time 129.04352498054504s

2025-10-01 20:38:23,330 | INFO | Training epoch 1196, Batch 1000/1000: LR=1.08e-05, Loss=7.13e-03 BER=2.78e-03 FER=2.88e-02
2025-10-01 20:38:23,791 | INFO | Epoch 1196 Train Time 128.38299012184143s

2025-10-01 20:40:32,162 | INFO | Training epoch 1197, Batch 1000/1000: LR=1.07e-05, Loss=6.76e-03 BER=2.65e-03 FER=2.78e-02
2025-10-01 20:40:32,652 | INFO | Epoch 1197 Train Time 128.86008405685425s

2025-10-01 20:42:40,844 | INFO | Training epoch 1198, Batch 1000/1000: LR=1.06e-05, Loss=6.89e-03 BER=2.72e-03 FER=2.82e-02
2025-10-01 20:42:41,294 | INFO | Epoch 1198 Train Time 128.64117550849915s

2025-10-01 20:44:52,343 | INFO | Training epoch 1199, Batch 1000/1000: LR=1.06e-05, Loss=6.82e-03 BER=2.67e-03 FER=2.80e-02
2025-10-01 20:44:52,812 | INFO | Epoch 1199 Train Time 131.51669788360596s

2025-10-01 20:47:01,604 | INFO | Training epoch 1200, Batch 1000/1000: LR=1.05e-05, Loss=6.92e-03 BER=2.72e-03 FER=2.80e-02
2025-10-01 20:47:02,070 | INFO | Epoch 1200 Train Time 129.25769805908203s

2025-10-01 20:49:13,216 | INFO | Training epoch 1201, Batch 1000/1000: LR=1.05e-05, Loss=7.01e-03 BER=2.78e-03 FER=2.85e-02
2025-10-01 20:49:13,675 | INFO | Epoch 1201 Train Time 131.60341572761536s

2025-10-01 20:51:23,627 | INFO | Training epoch 1202, Batch 1000/1000: LR=1.04e-05, Loss=6.94e-03 BER=2.70e-03 FER=2.82e-02
2025-10-01 20:51:24,101 | INFO | Epoch 1202 Train Time 130.42526292800903s

2025-10-01 20:53:35,500 | INFO | Training epoch 1203, Batch 1000/1000: LR=1.03e-05, Loss=6.97e-03 BER=2.74e-03 FER=2.86e-02
2025-10-01 20:53:35,963 | INFO | Epoch 1203 Train Time 131.8613407611847s

2025-10-01 20:55:46,153 | INFO | Training epoch 1204, Batch 1000/1000: LR=1.03e-05, Loss=6.71e-03 BER=2.62e-03 FER=2.70e-02
2025-10-01 20:55:46,628 | INFO | Epoch 1204 Train Time 130.66348934173584s

2025-10-01 20:57:57,593 | INFO | Training epoch 1205, Batch 1000/1000: LR=1.02e-05, Loss=6.82e-03 BER=2.69e-03 FER=2.83e-02
2025-10-01 20:57:58,088 | INFO | Epoch 1205 Train Time 131.45916938781738s

2025-10-01 21:00:11,377 | INFO | Training epoch 1206, Batch 1000/1000: LR=1.02e-05, Loss=6.88e-03 BER=2.71e-03 FER=2.86e-02
2025-10-01 21:00:11,915 | INFO | Epoch 1206 Train Time 133.82577443122864s

2025-10-01 21:02:26,896 | INFO | Training epoch 1207, Batch 1000/1000: LR=1.01e-05, Loss=6.85e-03 BER=2.69e-03 FER=2.77e-02
2025-10-01 21:02:27,360 | INFO | Epoch 1207 Train Time 135.44406819343567s

2025-10-01 21:04:41,536 | INFO | Training epoch 1208, Batch 1000/1000: LR=1.00e-05, Loss=7.09e-03 BER=2.79e-03 FER=2.90e-02
2025-10-01 21:04:42,056 | INFO | Epoch 1208 Train Time 134.69478821754456s

2025-10-01 21:07:07,034 | INFO | Training epoch 1209, Batch 1000/1000: LR=9.97e-06, Loss=7.11e-03 BER=2.81e-03 FER=2.91e-02
2025-10-01 21:07:07,511 | INFO | Epoch 1209 Train Time 145.45466089248657s

2025-10-01 21:09:20,797 | INFO | Training epoch 1210, Batch 1000/1000: LR=9.91e-06, Loss=6.89e-03 BER=2.72e-03 FER=2.79e-02
2025-10-01 21:09:21,274 | INFO | Epoch 1210 Train Time 133.76143145561218s

2025-10-01 21:11:37,530 | INFO | Training epoch 1211, Batch 1000/1000: LR=9.85e-06, Loss=6.88e-03 BER=2.71e-03 FER=2.86e-02
2025-10-01 21:11:37,999 | INFO | Epoch 1211 Train Time 136.72409987449646s

2025-10-01 21:13:58,332 | INFO | Training epoch 1212, Batch 1000/1000: LR=9.79e-06, Loss=6.97e-03 BER=2.74e-03 FER=2.85e-02
2025-10-01 21:13:58,843 | INFO | Epoch 1212 Train Time 140.84350419044495s

2025-10-01 21:16:14,907 | INFO | Training epoch 1213, Batch 1000/1000: LR=9.74e-06, Loss=7.00e-03 BER=2.76e-03 FER=2.88e-02
2025-10-01 21:16:15,377 | INFO | Epoch 1213 Train Time 136.53282022476196s

2025-10-01 21:18:36,381 | INFO | Training epoch 1214, Batch 1000/1000: LR=9.68e-06, Loss=7.01e-03 BER=2.76e-03 FER=2.85e-02
2025-10-01 21:18:36,881 | INFO | Epoch 1214 Train Time 141.5030882358551s

2025-10-01 21:20:57,614 | INFO | Training epoch 1215, Batch 1000/1000: LR=9.62e-06, Loss=7.11e-03 BER=2.77e-03 FER=2.86e-02
2025-10-01 21:20:58,110 | INFO | Epoch 1215 Train Time 141.22776436805725s

2025-10-01 21:23:13,418 | INFO | Training epoch 1216, Batch 1000/1000: LR=9.56e-06, Loss=7.09e-03 BER=2.80e-03 FER=2.87e-02
2025-10-01 21:23:13,922 | INFO | Epoch 1216 Train Time 135.81081891059875s

2025-10-01 21:25:31,119 | INFO | Training epoch 1217, Batch 1000/1000: LR=9.50e-06, Loss=7.03e-03 BER=2.77e-03 FER=2.88e-02
2025-10-01 21:25:31,626 | INFO | Epoch 1217 Train Time 137.70294451713562s

2025-10-01 21:27:51,694 | INFO | Training epoch 1218, Batch 1000/1000: LR=9.44e-06, Loss=6.73e-03 BER=2.65e-03 FER=2.79e-02
2025-10-01 21:27:52,192 | INFO | Epoch 1218 Train Time 140.56399869918823s

2025-10-01 21:30:09,355 | INFO | Training epoch 1219, Batch 1000/1000: LR=9.39e-06, Loss=6.87e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 21:30:09,860 | INFO | Epoch 1219 Train Time 137.66577339172363s

2025-10-01 21:32:28,441 | INFO | Training epoch 1220, Batch 1000/1000: LR=9.33e-06, Loss=6.72e-03 BER=2.63e-03 FER=2.71e-02
2025-10-01 21:32:28,963 | INFO | Epoch 1220 Train Time 139.10202479362488s

2025-10-01 21:34:50,588 | INFO | Training epoch 1221, Batch 1000/1000: LR=9.27e-06, Loss=6.99e-03 BER=2.74e-03 FER=2.85e-02
2025-10-01 21:34:51,097 | INFO | Epoch 1221 Train Time 142.13348269462585s

2025-10-01 21:37:10,571 | INFO | Training epoch 1222, Batch 1000/1000: LR=9.21e-06, Loss=6.84e-03 BER=2.71e-03 FER=2.83e-02
2025-10-01 21:37:11,105 | INFO | Epoch 1222 Train Time 140.00652861595154s

2025-10-01 21:39:40,446 | INFO | Training epoch 1223, Batch 1000/1000: LR=9.16e-06, Loss=6.88e-03 BER=2.71e-03 FER=2.85e-02
2025-10-01 21:39:40,965 | INFO | Epoch 1223 Train Time 149.85889148712158s

2025-10-01 21:41:54,516 | INFO | Training epoch 1224, Batch 1000/1000: LR=9.10e-06, Loss=6.77e-03 BER=2.65e-03 FER=2.77e-02
2025-10-01 21:41:54,996 | INFO | Epoch 1224 Train Time 134.03059601783752s

2025-10-01 21:44:16,006 | INFO | Training epoch 1225, Batch 1000/1000: LR=9.04e-06, Loss=6.99e-03 BER=2.78e-03 FER=2.86e-02
2025-10-01 21:44:16,528 | INFO | Epoch 1225 Train Time 141.53026270866394s

2025-10-01 21:46:29,196 | INFO | Training epoch 1226, Batch 1000/1000: LR=8.99e-06, Loss=7.03e-03 BER=2.78e-03 FER=2.91e-02
2025-10-01 21:46:29,665 | INFO | Epoch 1226 Train Time 133.13619589805603s

2025-10-01 21:48:45,605 | INFO | Training epoch 1227, Batch 1000/1000: LR=8.93e-06, Loss=6.72e-03 BER=2.67e-03 FER=2.78e-02
2025-10-01 21:48:46,120 | INFO | Epoch 1227 Train Time 136.45460057258606s

2025-10-01 21:51:04,599 | INFO | Training epoch 1228, Batch 1000/1000: LR=8.87e-06, Loss=6.99e-03 BER=2.74e-03 FER=2.89e-02
2025-10-01 21:51:05,095 | INFO | Epoch 1228 Train Time 138.97377371788025s

2025-10-01 21:53:23,769 | INFO | Training epoch 1229, Batch 1000/1000: LR=8.82e-06, Loss=6.91e-03 BER=2.73e-03 FER=2.81e-02
2025-10-01 21:53:24,255 | INFO | Epoch 1229 Train Time 139.15918684005737s

2025-10-01 21:55:40,872 | INFO | Training epoch 1230, Batch 1000/1000: LR=8.76e-06, Loss=7.11e-03 BER=2.80e-03 FER=2.90e-02
2025-10-01 21:55:41,354 | INFO | Epoch 1230 Train Time 137.09755039215088s

2025-10-01 21:57:55,211 | INFO | Training epoch 1231, Batch 1000/1000: LR=8.71e-06, Loss=6.86e-03 BER=2.71e-03 FER=2.83e-02
2025-10-01 21:57:55,677 | INFO | Epoch 1231 Train Time 134.32198667526245s

2025-10-01 22:00:07,690 | INFO | Training epoch 1232, Batch 1000/1000: LR=8.65e-06, Loss=6.91e-03 BER=2.72e-03 FER=2.85e-02
2025-10-01 22:00:08,172 | INFO | Epoch 1232 Train Time 132.49374055862427s

2025-10-01 22:02:19,870 | INFO | Training epoch 1233, Batch 1000/1000: LR=8.60e-06, Loss=6.76e-03 BER=2.65e-03 FER=2.77e-02
2025-10-01 22:02:20,334 | INFO | Epoch 1233 Train Time 132.1611979007721s

2025-10-01 22:04:31,181 | INFO | Training epoch 1234, Batch 1000/1000: LR=8.54e-06, Loss=6.88e-03 BER=2.69e-03 FER=2.81e-02
2025-10-01 22:04:31,649 | INFO | Epoch 1234 Train Time 131.3139910697937s

2025-10-01 22:06:43,393 | INFO | Training epoch 1235, Batch 1000/1000: LR=8.49e-06, Loss=6.80e-03 BER=2.67e-03 FER=2.73e-02
2025-10-01 22:06:43,928 | INFO | Epoch 1235 Train Time 132.27802801132202s

2025-10-01 22:08:58,005 | INFO | Training epoch 1236, Batch 1000/1000: LR=8.43e-06, Loss=7.00e-03 BER=2.75e-03 FER=2.83e-02
2025-10-01 22:08:58,521 | INFO | Epoch 1236 Train Time 134.59217715263367s

2025-10-01 22:11:24,194 | INFO | Training epoch 1237, Batch 1000/1000: LR=8.38e-06, Loss=6.97e-03 BER=2.73e-03 FER=2.80e-02
2025-10-01 22:11:24,675 | INFO | Epoch 1237 Train Time 146.1539876461029s

2025-10-01 22:13:43,074 | INFO | Training epoch 1238, Batch 1000/1000: LR=8.32e-06, Loss=7.03e-03 BER=2.71e-03 FER=2.83e-02
2025-10-01 22:13:43,554 | INFO | Epoch 1238 Train Time 138.87824249267578s

2025-10-01 22:15:55,602 | INFO | Training epoch 1239, Batch 1000/1000: LR=8.27e-06, Loss=7.04e-03 BER=2.81e-03 FER=2.84e-02
2025-10-01 22:15:56,092 | INFO | Epoch 1239 Train Time 132.5370740890503s

2025-10-01 22:18:08,856 | INFO | Training epoch 1240, Batch 1000/1000: LR=8.21e-06, Loss=6.76e-03 BER=2.69e-03 FER=2.80e-02
2025-10-01 22:18:09,323 | INFO | Epoch 1240 Train Time 133.22931027412415s

2025-10-01 22:20:26,824 | INFO | Training epoch 1241, Batch 1000/1000: LR=8.16e-06, Loss=6.98e-03 BER=2.74e-03 FER=2.80e-02
2025-10-01 22:20:27,305 | INFO | Epoch 1241 Train Time 137.98091006278992s

2025-10-01 22:22:43,545 | INFO | Training epoch 1242, Batch 1000/1000: LR=8.11e-06, Loss=6.93e-03 BER=2.72e-03 FER=2.80e-02
2025-10-01 22:22:44,263 | INFO | Epoch 1242 Train Time 136.95770072937012s

2025-10-01 22:24:58,871 | INFO | Training epoch 1243, Batch 1000/1000: LR=8.05e-06, Loss=6.80e-03 BER=2.70e-03 FER=2.79e-02
2025-10-01 22:24:59,349 | INFO | Epoch 1243 Train Time 135.08405303955078s

2025-10-01 22:27:12,725 | INFO | Training epoch 1244, Batch 1000/1000: LR=8.00e-06, Loss=6.89e-03 BER=2.70e-03 FER=2.80e-02
2025-10-01 22:27:13,226 | INFO | Epoch 1244 Train Time 133.87598633766174s

2025-10-01 22:29:29,136 | INFO | Training epoch 1245, Batch 1000/1000: LR=7.95e-06, Loss=6.95e-03 BER=2.74e-03 FER=2.84e-02
2025-10-01 22:29:29,596 | INFO | Epoch 1245 Train Time 136.3690357208252s

2025-10-01 22:31:46,233 | INFO | Training epoch 1246, Batch 1000/1000: LR=7.89e-06, Loss=6.90e-03 BER=2.72e-03 FER=2.82e-02
2025-10-01 22:31:46,727 | INFO | Epoch 1246 Train Time 137.1296944618225s

2025-10-01 22:34:03,401 | INFO | Training epoch 1247, Batch 1000/1000: LR=7.84e-06, Loss=6.82e-03 BER=2.69e-03 FER=2.78e-02
2025-10-01 22:34:03,880 | INFO | Epoch 1247 Train Time 137.15152382850647s

2025-10-01 22:36:19,969 | INFO | Training epoch 1248, Batch 1000/1000: LR=7.79e-06, Loss=6.60e-03 BER=2.62e-03 FER=2.73e-02
2025-10-01 22:36:20,498 | INFO | Epoch 1248 Train Time 136.6177477836609s

2025-10-01 22:36:20,499 | INFO | [P2] saving best_model (QAT) with loss 0.006602 at epoch 1248
2025-10-01 22:38:36,851 | INFO | Training epoch 1249, Batch 1000/1000: LR=7.74e-06, Loss=6.97e-03 BER=2.73e-03 FER=2.83e-02
2025-10-01 22:38:37,351 | INFO | Epoch 1249 Train Time 136.77019119262695s

2025-10-01 22:41:04,374 | INFO | Training epoch 1250, Batch 1000/1000: LR=7.68e-06, Loss=6.72e-03 BER=2.66e-03 FER=2.79e-02
2025-10-01 22:41:04,855 | INFO | Epoch 1250 Train Time 147.50291848182678s

2025-10-01 22:43:40,132 | INFO | Training epoch 1251, Batch 1000/1000: LR=7.63e-06, Loss=6.94e-03 BER=2.70e-03 FER=2.74e-02
2025-10-01 22:43:40,623 | INFO | Epoch 1251 Train Time 155.76658368110657s

2025-10-01 22:45:59,049 | INFO | Training epoch 1252, Batch 1000/1000: LR=7.58e-06, Loss=6.75e-03 BER=2.64e-03 FER=2.79e-02
2025-10-01 22:45:59,599 | INFO | Epoch 1252 Train Time 138.97509002685547s

2025-10-01 22:48:27,530 | INFO | Training epoch 1253, Batch 1000/1000: LR=7.53e-06, Loss=6.86e-03 BER=2.75e-03 FER=2.84e-02
2025-10-01 22:48:28,028 | INFO | Epoch 1253 Train Time 148.42745065689087s

2025-10-01 22:50:42,643 | INFO | Training epoch 1254, Batch 1000/1000: LR=7.48e-06, Loss=6.84e-03 BER=2.68e-03 FER=2.77e-02
2025-10-01 22:50:43,133 | INFO | Epoch 1254 Train Time 135.10520148277283s

2025-10-01 22:53:01,342 | INFO | Training epoch 1255, Batch 1000/1000: LR=7.43e-06, Loss=6.79e-03 BER=2.68e-03 FER=2.79e-02
2025-10-01 22:53:01,869 | INFO | Epoch 1255 Train Time 138.73475170135498s

2025-10-01 22:55:15,913 | INFO | Training epoch 1256, Batch 1000/1000: LR=7.37e-06, Loss=6.87e-03 BER=2.71e-03 FER=2.86e-02
2025-10-01 22:55:16,385 | INFO | Epoch 1256 Train Time 134.51507449150085s

2025-10-01 22:57:35,222 | INFO | Training epoch 1257, Batch 1000/1000: LR=7.32e-06, Loss=6.78e-03 BER=2.70e-03 FER=2.79e-02
2025-10-01 22:57:35,707 | INFO | Epoch 1257 Train Time 139.32126426696777s

2025-10-01 22:59:48,912 | INFO | Training epoch 1258, Batch 1000/1000: LR=7.27e-06, Loss=7.02e-03 BER=2.77e-03 FER=2.86e-02
2025-10-01 22:59:49,397 | INFO | Epoch 1258 Train Time 133.6890127658844s

2025-10-01 23:01:59,486 | INFO | Training epoch 1259, Batch 1000/1000: LR=7.22e-06, Loss=6.65e-03 BER=2.62e-03 FER=2.75e-02
2025-10-01 23:01:59,975 | INFO | Epoch 1259 Train Time 130.57633423805237s

2025-10-01 23:04:10,046 | INFO | Training epoch 1260, Batch 1000/1000: LR=7.17e-06, Loss=6.92e-03 BER=2.75e-03 FER=2.83e-02
2025-10-01 23:04:10,538 | INFO | Epoch 1260 Train Time 130.56185483932495s

2025-10-01 23:06:20,876 | INFO | Training epoch 1261, Batch 1000/1000: LR=7.12e-06, Loss=7.08e-03 BER=2.78e-03 FER=2.89e-02
2025-10-01 23:06:21,339 | INFO | Epoch 1261 Train Time 130.80030345916748s

2025-10-01 23:08:32,782 | INFO | Training epoch 1262, Batch 1000/1000: LR=7.07e-06, Loss=7.00e-03 BER=2.76e-03 FER=2.82e-02
2025-10-01 23:08:33,304 | INFO | Epoch 1262 Train Time 131.96358180046082s

2025-10-01 23:10:47,793 | INFO | Training epoch 1263, Batch 1000/1000: LR=7.02e-06, Loss=6.79e-03 BER=2.70e-03 FER=2.80e-02
2025-10-01 23:10:48,291 | INFO | Epoch 1263 Train Time 134.98656463623047s

2025-10-01 23:13:02,128 | INFO | Training epoch 1264, Batch 1000/1000: LR=6.97e-06, Loss=6.85e-03 BER=2.68e-03 FER=2.76e-02
2025-10-01 23:13:02,616 | INFO | Epoch 1264 Train Time 134.32367658615112s

2025-10-01 23:15:32,451 | INFO | Training epoch 1265, Batch 1000/1000: LR=6.92e-06, Loss=6.95e-03 BER=2.74e-03 FER=2.82e-02
2025-10-01 23:15:32,972 | INFO | Epoch 1265 Train Time 150.35565185546875s

2025-10-01 23:17:49,848 | INFO | Training epoch 1266, Batch 1000/1000: LR=6.88e-06, Loss=6.93e-03 BER=2.72e-03 FER=2.82e-02
2025-10-01 23:17:50,462 | INFO | Epoch 1266 Train Time 137.48834919929504s

2025-10-01 23:20:10,566 | INFO | Training epoch 1267, Batch 1000/1000: LR=6.83e-06, Loss=6.89e-03 BER=2.72e-03 FER=2.84e-02
2025-10-01 23:20:11,048 | INFO | Epoch 1267 Train Time 140.58545660972595s

2025-10-01 23:22:25,982 | INFO | Training epoch 1268, Batch 1000/1000: LR=6.78e-06, Loss=6.77e-03 BER=2.68e-03 FER=2.78e-02
2025-10-01 23:22:26,438 | INFO | Epoch 1268 Train Time 135.38820481300354s

2025-10-01 23:24:42,875 | INFO | Training epoch 1269, Batch 1000/1000: LR=6.73e-06, Loss=6.79e-03 BER=2.68e-03 FER=2.81e-02
2025-10-01 23:24:43,369 | INFO | Epoch 1269 Train Time 136.92997646331787s

2025-10-01 23:26:58,539 | INFO | Training epoch 1270, Batch 1000/1000: LR=6.68e-06, Loss=6.72e-03 BER=2.64e-03 FER=2.78e-02
2025-10-01 23:26:59,033 | INFO | Epoch 1270 Train Time 135.66247701644897s

2025-10-01 23:29:16,805 | INFO | Training epoch 1271, Batch 1000/1000: LR=6.63e-06, Loss=6.83e-03 BER=2.68e-03 FER=2.74e-02
2025-10-01 23:29:17,285 | INFO | Epoch 1271 Train Time 138.25163793563843s

2025-10-01 23:31:33,994 | INFO | Training epoch 1272, Batch 1000/1000: LR=6.58e-06, Loss=6.94e-03 BER=2.71e-03 FER=2.82e-02
2025-10-01 23:31:34,457 | INFO | Epoch 1272 Train Time 137.1704580783844s

2025-10-01 23:33:53,817 | INFO | Training epoch 1273, Batch 1000/1000: LR=6.54e-06, Loss=6.88e-03 BER=2.71e-03 FER=2.83e-02
2025-10-01 23:33:54,354 | INFO | Epoch 1273 Train Time 139.89617896080017s

2025-10-01 23:36:11,481 | INFO | Training epoch 1274, Batch 1000/1000: LR=6.49e-06, Loss=6.83e-03 BER=2.70e-03 FER=2.78e-02
2025-10-01 23:36:11,967 | INFO | Epoch 1274 Train Time 137.61195373535156s

2025-10-01 23:38:30,233 | INFO | Training epoch 1275, Batch 1000/1000: LR=6.44e-06, Loss=6.81e-03 BER=2.67e-03 FER=2.79e-02
2025-10-01 23:38:30,741 | INFO | Epoch 1275 Train Time 138.77323055267334s

2025-10-01 23:40:58,266 | INFO | Training epoch 1276, Batch 1000/1000: LR=6.40e-06, Loss=6.71e-03 BER=2.66e-03 FER=2.78e-02
2025-10-01 23:40:58,763 | INFO | Epoch 1276 Train Time 148.0207495689392s

2025-10-01 23:43:13,106 | INFO | Training epoch 1277, Batch 1000/1000: LR=6.35e-06, Loss=6.87e-03 BER=2.67e-03 FER=2.81e-02
2025-10-01 23:43:13,605 | INFO | Epoch 1277 Train Time 134.84048867225647s

2025-10-01 23:45:26,601 | INFO | Training epoch 1278, Batch 1000/1000: LR=6.30e-06, Loss=6.83e-03 BER=2.69e-03 FER=2.78e-02
2025-10-01 23:45:27,081 | INFO | Epoch 1278 Train Time 133.47484397888184s

2025-10-01 23:48:10,692 | INFO | Training epoch 1279, Batch 1000/1000: LR=6.25e-06, Loss=6.78e-03 BER=2.69e-03 FER=2.79e-02
2025-10-01 23:48:11,213 | INFO | Epoch 1279 Train Time 164.1309027671814s

2025-10-01 23:50:31,834 | INFO | Training epoch 1280, Batch 1000/1000: LR=6.21e-06, Loss=6.83e-03 BER=2.66e-03 FER=2.80e-02
2025-10-01 23:50:32,395 | INFO | Epoch 1280 Train Time 141.17964959144592s

2025-10-01 23:52:48,252 | INFO | Training epoch 1281, Batch 1000/1000: LR=6.16e-06, Loss=6.92e-03 BER=2.74e-03 FER=2.81e-02
2025-10-01 23:52:48,742 | INFO | Epoch 1281 Train Time 136.34512400627136s

2025-10-01 23:55:04,486 | INFO | Training epoch 1282, Batch 1000/1000: LR=6.12e-06, Loss=6.90e-03 BER=2.71e-03 FER=2.84e-02
2025-10-01 23:55:05,003 | INFO | Epoch 1282 Train Time 136.26131129264832s

2025-10-01 23:57:22,666 | INFO | Training epoch 1283, Batch 1000/1000: LR=6.07e-06, Loss=6.88e-03 BER=2.70e-03 FER=2.85e-02
2025-10-01 23:57:23,190 | INFO | Epoch 1283 Train Time 138.1850461959839s

2025-10-01 23:59:40,593 | INFO | Training epoch 1284, Batch 1000/1000: LR=6.02e-06, Loss=6.81e-03 BER=2.68e-03 FER=2.75e-02
2025-10-01 23:59:41,105 | INFO | Epoch 1284 Train Time 137.9138264656067s

2025-10-02 00:01:59,746 | INFO | Training epoch 1285, Batch 1000/1000: LR=5.98e-06, Loss=6.86e-03 BER=2.70e-03 FER=2.77e-02
2025-10-02 00:02:00,362 | INFO | Epoch 1285 Train Time 139.25610327720642s

2025-10-02 00:04:12,871 | INFO | Training epoch 1286, Batch 1000/1000: LR=5.93e-06, Loss=6.92e-03 BER=2.73e-03 FER=2.83e-02
2025-10-02 00:04:13,351 | INFO | Epoch 1286 Train Time 132.98773217201233s

2025-10-02 00:06:30,800 | INFO | Training epoch 1287, Batch 1000/1000: LR=5.89e-06, Loss=6.77e-03 BER=2.68e-03 FER=2.78e-02
2025-10-02 00:06:31,262 | INFO | Epoch 1287 Train Time 137.91038060188293s

2025-10-02 00:08:46,031 | INFO | Training epoch 1288, Batch 1000/1000: LR=5.84e-06, Loss=6.69e-03 BER=2.59e-03 FER=2.73e-02
2025-10-02 00:08:46,508 | INFO | Epoch 1288 Train Time 135.24452018737793s

2025-10-02 00:11:04,145 | INFO | Training epoch 1289, Batch 1000/1000: LR=5.80e-06, Loss=6.81e-03 BER=2.68e-03 FER=2.74e-02
2025-10-02 00:11:04,652 | INFO | Epoch 1289 Train Time 138.14397144317627s

2025-10-02 00:13:33,848 | INFO | Training epoch 1290, Batch 1000/1000: LR=5.76e-06, Loss=7.16e-03 BER=2.81e-03 FER=2.88e-02
2025-10-02 00:13:34,315 | INFO | Epoch 1290 Train Time 149.66243076324463s

2025-10-02 00:15:48,995 | INFO | Training epoch 1291, Batch 1000/1000: LR=5.71e-06, Loss=6.95e-03 BER=2.75e-03 FER=2.82e-02
2025-10-02 00:15:49,462 | INFO | Epoch 1291 Train Time 135.14569234848022s

2025-10-02 00:18:06,915 | INFO | Training epoch 1292, Batch 1000/1000: LR=5.67e-06, Loss=6.82e-03 BER=2.69e-03 FER=2.78e-02
2025-10-02 00:18:07,471 | INFO | Epoch 1292 Train Time 138.00787615776062s

2025-10-02 00:20:46,302 | INFO | Training epoch 1293, Batch 1000/1000: LR=5.62e-06, Loss=6.77e-03 BER=2.66e-03 FER=2.76e-02
2025-10-02 00:20:46,909 | INFO | Epoch 1293 Train Time 159.43717122077942s

2025-10-02 00:23:06,043 | INFO | Training epoch 1294, Batch 1000/1000: LR=5.58e-06, Loss=7.08e-03 BER=2.77e-03 FER=2.86e-02
2025-10-02 00:23:06,537 | INFO | Epoch 1294 Train Time 139.6264660358429s

2025-10-02 00:25:32,503 | INFO | Training epoch 1295, Batch 1000/1000: LR=5.54e-06, Loss=6.70e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 00:25:33,026 | INFO | Epoch 1295 Train Time 146.487651348114s

2025-10-02 00:27:52,374 | INFO | Training epoch 1296, Batch 1000/1000: LR=5.49e-06, Loss=6.86e-03 BER=2.70e-03 FER=2.79e-02
2025-10-02 00:27:52,867 | INFO | Epoch 1296 Train Time 139.8403787612915s

2025-10-02 00:30:11,680 | INFO | Training epoch 1297, Batch 1000/1000: LR=5.45e-06, Loss=6.70e-03 BER=2.65e-03 FER=2.74e-02
2025-10-02 00:30:12,193 | INFO | Epoch 1297 Train Time 139.32498693466187s

2025-10-02 00:32:34,014 | INFO | Training epoch 1298, Batch 1000/1000: LR=5.41e-06, Loss=6.77e-03 BER=2.67e-03 FER=2.76e-02
2025-10-02 00:32:34,520 | INFO | Epoch 1298 Train Time 142.32537531852722s

2025-10-02 00:34:50,449 | INFO | Training epoch 1299, Batch 1000/1000: LR=5.36e-06, Loss=6.84e-03 BER=2.70e-03 FER=2.80e-02
2025-10-02 00:34:50,948 | INFO | Epoch 1299 Train Time 136.42718029022217s

2025-10-02 00:37:11,138 | INFO | Training epoch 1300, Batch 1000/1000: LR=5.32e-06, Loss=6.82e-03 BER=2.70e-03 FER=2.79e-02
2025-10-02 00:37:11,765 | INFO | Epoch 1300 Train Time 140.81641459465027s

2025-10-02 00:39:27,166 | INFO | Training epoch 1301, Batch 1000/1000: LR=5.28e-06, Loss=6.81e-03 BER=2.69e-03 FER=2.76e-02
2025-10-02 00:39:27,669 | INFO | Epoch 1301 Train Time 135.9004135131836s

2025-10-02 00:41:45,869 | INFO | Training epoch 1302, Batch 1000/1000: LR=5.24e-06, Loss=6.93e-03 BER=2.71e-03 FER=2.83e-02
2025-10-02 00:41:46,375 | INFO | Epoch 1302 Train Time 138.70500826835632s

2025-10-02 00:44:02,174 | INFO | Training epoch 1303, Batch 1000/1000: LR=5.20e-06, Loss=7.08e-03 BER=2.82e-03 FER=2.91e-02
2025-10-02 00:44:02,677 | INFO | Epoch 1303 Train Time 136.30010890960693s

2025-10-02 00:46:19,783 | INFO | Training epoch 1304, Batch 1000/1000: LR=5.15e-06, Loss=7.01e-03 BER=2.73e-03 FER=2.80e-02
2025-10-02 00:46:20,324 | INFO | Epoch 1304 Train Time 137.6457862854004s

2025-10-02 00:48:35,537 | INFO | Training epoch 1305, Batch 1000/1000: LR=5.11e-06, Loss=6.69e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 00:48:36,051 | INFO | Epoch 1305 Train Time 135.72633409500122s

2025-10-02 00:50:51,669 | INFO | Training epoch 1306, Batch 1000/1000: LR=5.07e-06, Loss=6.85e-03 BER=2.70e-03 FER=2.78e-02
2025-10-02 00:50:52,175 | INFO | Epoch 1306 Train Time 136.1233959197998s

2025-10-02 00:53:22,094 | INFO | Training epoch 1307, Batch 1000/1000: LR=5.03e-06, Loss=6.90e-03 BER=2.70e-03 FER=2.81e-02
2025-10-02 00:53:22,629 | INFO | Epoch 1307 Train Time 150.45258808135986s

2025-10-02 00:55:41,125 | INFO | Training epoch 1308, Batch 1000/1000: LR=4.99e-06, Loss=6.89e-03 BER=2.73e-03 FER=2.82e-02
2025-10-02 00:55:41,632 | INFO | Epoch 1308 Train Time 139.00163626670837s

2025-10-02 00:57:59,371 | INFO | Training epoch 1309, Batch 1000/1000: LR=4.95e-06, Loss=7.10e-03 BER=2.81e-03 FER=2.90e-02
2025-10-02 00:57:59,883 | INFO | Epoch 1309 Train Time 138.25016617774963s

2025-10-02 01:00:13,305 | INFO | Training epoch 1310, Batch 1000/1000: LR=4.91e-06, Loss=6.65e-03 BER=2.61e-03 FER=2.72e-02
2025-10-02 01:00:13,769 | INFO | Epoch 1310 Train Time 133.8851306438446s

2025-10-02 01:02:28,156 | INFO | Training epoch 1311, Batch 1000/1000: LR=4.87e-06, Loss=6.95e-03 BER=2.73e-03 FER=2.79e-02
2025-10-02 01:02:28,684 | INFO | Epoch 1311 Train Time 134.91291570663452s

2025-10-02 01:04:43,928 | INFO | Training epoch 1312, Batch 1000/1000: LR=4.83e-06, Loss=6.86e-03 BER=2.70e-03 FER=2.79e-02
2025-10-02 01:04:44,427 | INFO | Epoch 1312 Train Time 135.7422752380371s

2025-10-02 01:06:57,785 | INFO | Training epoch 1313, Batch 1000/1000: LR=4.79e-06, Loss=6.69e-03 BER=2.62e-03 FER=2.72e-02
2025-10-02 01:06:58,263 | INFO | Epoch 1313 Train Time 133.83425545692444s

2025-10-02 01:09:13,840 | INFO | Training epoch 1314, Batch 1000/1000: LR=4.75e-06, Loss=6.83e-03 BER=2.70e-03 FER=2.77e-02
2025-10-02 01:09:14,354 | INFO | Epoch 1314 Train Time 136.089262008667s

2025-10-02 01:11:27,561 | INFO | Training epoch 1315, Batch 1000/1000: LR=4.71e-06, Loss=6.62e-03 BER=2.61e-03 FER=2.75e-02
2025-10-02 01:11:28,058 | INFO | Epoch 1315 Train Time 133.7032811641693s

2025-10-02 01:13:46,649 | INFO | Training epoch 1316, Batch 1000/1000: LR=4.67e-06, Loss=6.77e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 01:13:47,169 | INFO | Epoch 1316 Train Time 139.10964179039001s

2025-10-02 01:16:02,743 | INFO | Training epoch 1317, Batch 1000/1000: LR=4.63e-06, Loss=6.73e-03 BER=2.62e-03 FER=2.71e-02
2025-10-02 01:16:03,235 | INFO | Epoch 1317 Train Time 136.06556296348572s

2025-10-02 01:18:23,549 | INFO | Training epoch 1318, Batch 1000/1000: LR=4.59e-06, Loss=6.55e-03 BER=2.59e-03 FER=2.68e-02
2025-10-02 01:18:24,065 | INFO | Epoch 1318 Train Time 140.82855582237244s

2025-10-02 01:18:24,065 | INFO | [P2] saving best_model (QAT) with loss 0.006555 at epoch 1318
2025-10-02 01:20:43,521 | INFO | Training epoch 1319, Batch 1000/1000: LR=4.55e-06, Loss=6.98e-03 BER=2.76e-03 FER=2.81e-02
2025-10-02 01:20:44,039 | INFO | Epoch 1319 Train Time 139.89686584472656s

2025-10-02 01:22:56,012 | INFO | Training epoch 1320, Batch 1000/1000: LR=4.51e-06, Loss=6.55e-03 BER=2.58e-03 FER=2.72e-02
2025-10-02 01:22:56,523 | INFO | Epoch 1320 Train Time 132.4833414554596s

2025-10-02 01:22:56,524 | INFO | [P2] saving best_model (QAT) with loss 0.006549 at epoch 1320
2025-10-02 01:25:17,847 | INFO | Training epoch 1321, Batch 1000/1000: LR=4.48e-06, Loss=6.89e-03 BER=2.75e-03 FER=2.81e-02
2025-10-02 01:25:18,378 | INFO | Epoch 1321 Train Time 141.78263807296753s

2025-10-02 01:27:45,055 | INFO | Training epoch 1322, Batch 1000/1000: LR=4.44e-06, Loss=6.72e-03 BER=2.62e-03 FER=2.71e-02
2025-10-02 01:27:45,598 | INFO | Epoch 1322 Train Time 147.21921586990356s

2025-10-02 01:30:03,085 | INFO | Training epoch 1323, Batch 1000/1000: LR=4.40e-06, Loss=6.76e-03 BER=2.67e-03 FER=2.74e-02
2025-10-02 01:30:03,561 | INFO | Epoch 1323 Train Time 137.9621217250824s

2025-10-02 01:32:18,709 | INFO | Training epoch 1324, Batch 1000/1000: LR=4.36e-06, Loss=6.69e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 01:32:19,197 | INFO | Epoch 1324 Train Time 135.63522815704346s

2025-10-02 01:34:39,940 | INFO | Training epoch 1325, Batch 1000/1000: LR=4.33e-06, Loss=6.91e-03 BER=2.69e-03 FER=2.78e-02
2025-10-02 01:34:40,422 | INFO | Epoch 1325 Train Time 141.22372961044312s

2025-10-02 01:36:58,781 | INFO | Training epoch 1326, Batch 1000/1000: LR=4.29e-06, Loss=6.63e-03 BER=2.60e-03 FER=2.74e-02
2025-10-02 01:36:59,278 | INFO | Epoch 1326 Train Time 138.8547625541687s

2025-10-02 01:39:14,278 | INFO | Training epoch 1327, Batch 1000/1000: LR=4.25e-06, Loss=6.88e-03 BER=2.71e-03 FER=2.83e-02
2025-10-02 01:39:14,794 | INFO | Epoch 1327 Train Time 135.5156590938568s

2025-10-02 01:41:27,610 | INFO | Training epoch 1328, Batch 1000/1000: LR=4.21e-06, Loss=6.81e-03 BER=2.70e-03 FER=2.77e-02
2025-10-02 01:41:28,268 | INFO | Epoch 1328 Train Time 133.47133564949036s

2025-10-02 01:43:42,512 | INFO | Training epoch 1329, Batch 1000/1000: LR=4.18e-06, Loss=6.76e-03 BER=2.68e-03 FER=2.74e-02
2025-10-02 01:43:43,007 | INFO | Epoch 1329 Train Time 134.73782539367676s

2025-10-02 01:45:55,467 | INFO | Training epoch 1330, Batch 1000/1000: LR=4.14e-06, Loss=6.93e-03 BER=2.73e-03 FER=2.83e-02
2025-10-02 01:45:55,948 | INFO | Epoch 1330 Train Time 132.93972253799438s

2025-10-02 01:48:10,328 | INFO | Training epoch 1331, Batch 1000/1000: LR=4.10e-06, Loss=6.83e-03 BER=2.68e-03 FER=2.78e-02
2025-10-02 01:48:10,816 | INFO | Epoch 1331 Train Time 134.86556673049927s

2025-10-02 01:50:29,176 | INFO | Training epoch 1332, Batch 1000/1000: LR=4.07e-06, Loss=6.89e-03 BER=2.69e-03 FER=2.78e-02
2025-10-02 01:50:29,653 | INFO | Epoch 1332 Train Time 138.83699131011963s

2025-10-02 01:52:49,666 | INFO | Training epoch 1333, Batch 1000/1000: LR=4.03e-06, Loss=6.75e-03 BER=2.66e-03 FER=2.77e-02
2025-10-02 01:52:50,230 | INFO | Epoch 1333 Train Time 140.57659935951233s

2025-10-02 01:55:02,521 | INFO | Training epoch 1334, Batch 1000/1000: LR=4.00e-06, Loss=6.88e-03 BER=2.72e-03 FER=2.79e-02
2025-10-02 01:55:03,016 | INFO | Epoch 1334 Train Time 132.78517842292786s

2025-10-02 01:57:19,376 | INFO | Training epoch 1335, Batch 1000/1000: LR=3.96e-06, Loss=6.84e-03 BER=2.71e-03 FER=2.81e-02
2025-10-02 01:57:19,901 | INFO | Epoch 1335 Train Time 136.88293862342834s

2025-10-02 01:59:53,186 | INFO | Training epoch 1336, Batch 1000/1000: LR=3.93e-06, Loss=6.73e-03 BER=2.63e-03 FER=2.76e-02
2025-10-02 01:59:53,666 | INFO | Epoch 1336 Train Time 153.76246094703674s

2025-10-02 02:02:06,342 | INFO | Training epoch 1337, Batch 1000/1000: LR=3.89e-06, Loss=6.60e-03 BER=2.61e-03 FER=2.74e-02
2025-10-02 02:02:06,833 | INFO | Epoch 1337 Train Time 133.16621708869934s

2025-10-02 02:04:20,986 | INFO | Training epoch 1338, Batch 1000/1000: LR=3.86e-06, Loss=6.90e-03 BER=2.71e-03 FER=2.76e-02
2025-10-02 02:04:21,483 | INFO | Epoch 1338 Train Time 134.64802050590515s

2025-10-02 02:06:32,881 | INFO | Training epoch 1339, Batch 1000/1000: LR=3.82e-06, Loss=6.85e-03 BER=2.73e-03 FER=2.80e-02
2025-10-02 02:06:33,341 | INFO | Epoch 1339 Train Time 131.85726284980774s

2025-10-02 02:08:43,175 | INFO | Training epoch 1340, Batch 1000/1000: LR=3.79e-06, Loss=6.63e-03 BER=2.62e-03 FER=2.69e-02
2025-10-02 02:08:43,665 | INFO | Epoch 1340 Train Time 130.32271218299866s

2025-10-02 02:10:58,443 | INFO | Training epoch 1341, Batch 1000/1000: LR=3.75e-06, Loss=6.81e-03 BER=2.68e-03 FER=2.80e-02
2025-10-02 02:10:58,935 | INFO | Epoch 1341 Train Time 135.26938152313232s

2025-10-02 02:13:14,833 | INFO | Training epoch 1342, Batch 1000/1000: LR=3.72e-06, Loss=6.64e-03 BER=2.60e-03 FER=2.77e-02
2025-10-02 02:13:15,308 | INFO | Epoch 1342 Train Time 136.371102809906s

2025-10-02 02:15:33,219 | INFO | Training epoch 1343, Batch 1000/1000: LR=3.69e-06, Loss=6.77e-03 BER=2.65e-03 FER=2.74e-02
2025-10-02 02:15:33,702 | INFO | Epoch 1343 Train Time 138.39364075660706s

2025-10-02 02:17:48,588 | INFO | Training epoch 1344, Batch 1000/1000: LR=3.65e-06, Loss=6.71e-03 BER=2.66e-03 FER=2.71e-02
2025-10-02 02:17:49,167 | INFO | Epoch 1344 Train Time 135.46389245986938s

2025-10-02 02:20:12,480 | INFO | Training epoch 1345, Batch 1000/1000: LR=3.62e-06, Loss=6.63e-03 BER=2.62e-03 FER=2.70e-02
2025-10-02 02:20:13,039 | INFO | Epoch 1345 Train Time 143.87044978141785s

2025-10-02 02:22:29,024 | INFO | Training epoch 1346, Batch 1000/1000: LR=3.59e-06, Loss=6.48e-03 BER=2.58e-03 FER=2.74e-02
2025-10-02 02:22:29,519 | INFO | Epoch 1346 Train Time 136.47960019111633s

2025-10-02 02:22:29,520 | INFO | [P2] saving best_model (QAT) with loss 0.006481 at epoch 1346
2025-10-02 02:24:42,922 | INFO | Training epoch 1347, Batch 1000/1000: LR=3.55e-06, Loss=6.66e-03 BER=2.61e-03 FER=2.71e-02
2025-10-02 02:24:43,427 | INFO | Epoch 1347 Train Time 133.8344542980194s

2025-10-02 02:26:57,302 | INFO | Training epoch 1348, Batch 1000/1000: LR=3.52e-06, Loss=6.79e-03 BER=2.70e-03 FER=2.80e-02
2025-10-02 02:26:57,786 | INFO | Epoch 1348 Train Time 134.35802817344666s

2025-10-02 02:29:09,223 | INFO | Training epoch 1349, Batch 1000/1000: LR=3.49e-06, Loss=6.78e-03 BER=2.72e-03 FER=2.76e-02
2025-10-02 02:29:09,695 | INFO | Epoch 1349 Train Time 131.90879559516907s

2025-10-02 02:31:36,251 | INFO | Training epoch 1350, Batch 1000/1000: LR=3.45e-06, Loss=6.83e-03 BER=2.73e-03 FER=2.84e-02
2025-10-02 02:31:36,786 | INFO | Epoch 1350 Train Time 147.08907008171082s

2025-10-02 02:33:52,255 | INFO | Training epoch 1351, Batch 1000/1000: LR=3.42e-06, Loss=7.04e-03 BER=2.77e-03 FER=2.86e-02
2025-10-02 02:33:52,738 | INFO | Epoch 1351 Train Time 135.95166611671448s

2025-10-02 02:36:03,897 | INFO | Training epoch 1352, Batch 1000/1000: LR=3.39e-06, Loss=6.85e-03 BER=2.73e-03 FER=2.80e-02
2025-10-02 02:36:04,373 | INFO | Epoch 1352 Train Time 131.63336896896362s

2025-10-02 02:38:16,698 | INFO | Training epoch 1353, Batch 1000/1000: LR=3.36e-06, Loss=6.79e-03 BER=2.70e-03 FER=2.81e-02
2025-10-02 02:38:17,168 | INFO | Epoch 1353 Train Time 132.79472827911377s

2025-10-02 02:40:28,965 | INFO | Training epoch 1354, Batch 1000/1000: LR=3.33e-06, Loss=6.88e-03 BER=2.74e-03 FER=2.82e-02
2025-10-02 02:40:29,441 | INFO | Epoch 1354 Train Time 132.27137804031372s

2025-10-02 02:42:43,547 | INFO | Training epoch 1355, Batch 1000/1000: LR=3.30e-06, Loss=6.67e-03 BER=2.65e-03 FER=2.77e-02
2025-10-02 02:42:44,101 | INFO | Epoch 1355 Train Time 134.6595287322998s

2025-10-02 02:44:55,852 | INFO | Training epoch 1356, Batch 1000/1000: LR=3.27e-06, Loss=6.63e-03 BER=2.61e-03 FER=2.67e-02
2025-10-02 02:44:56,326 | INFO | Epoch 1356 Train Time 132.22340655326843s

2025-10-02 02:47:09,410 | INFO | Training epoch 1357, Batch 1000/1000: LR=3.23e-06, Loss=6.76e-03 BER=2.66e-03 FER=2.76e-02
2025-10-02 02:47:09,885 | INFO | Epoch 1357 Train Time 133.55834221839905s

2025-10-02 02:49:22,112 | INFO | Training epoch 1358, Batch 1000/1000: LR=3.20e-06, Loss=6.71e-03 BER=2.64e-03 FER=2.72e-02
2025-10-02 02:49:22,576 | INFO | Epoch 1358 Train Time 132.68957042694092s

2025-10-02 02:51:35,771 | INFO | Training epoch 1359, Batch 1000/1000: LR=3.17e-06, Loss=6.73e-03 BER=2.63e-03 FER=2.73e-02
2025-10-02 02:51:36,240 | INFO | Epoch 1359 Train Time 133.66196990013123s

2025-10-02 02:53:48,972 | INFO | Training epoch 1360, Batch 1000/1000: LR=3.14e-06, Loss=6.99e-03 BER=2.72e-03 FER=2.82e-02
2025-10-02 02:53:49,476 | INFO | Epoch 1360 Train Time 133.23548102378845s

2025-10-02 02:56:02,852 | INFO | Training epoch 1361, Batch 1000/1000: LR=3.11e-06, Loss=6.69e-03 BER=2.63e-03 FER=2.69e-02
2025-10-02 02:56:03,319 | INFO | Epoch 1361 Train Time 133.84244084358215s

2025-10-02 02:58:15,954 | INFO | Training epoch 1362, Batch 1000/1000: LR=3.08e-06, Loss=6.65e-03 BER=2.62e-03 FER=2.73e-02
2025-10-02 02:58:16,432 | INFO | Epoch 1362 Train Time 133.11114048957825s

2025-10-02 03:00:30,276 | INFO | Training epoch 1363, Batch 1000/1000: LR=3.05e-06, Loss=6.68e-03 BER=2.61e-03 FER=2.74e-02
2025-10-02 03:00:30,751 | INFO | Epoch 1363 Train Time 134.31792855262756s

2025-10-02 03:02:45,936 | INFO | Training epoch 1364, Batch 1000/1000: LR=3.02e-06, Loss=6.76e-03 BER=2.67e-03 FER=2.75e-02
2025-10-02 03:02:46,450 | INFO | Epoch 1364 Train Time 135.69894862174988s

2025-10-02 03:05:17,583 | INFO | Training epoch 1365, Batch 1000/1000: LR=2.99e-06, Loss=6.78e-03 BER=2.62e-03 FER=2.72e-02
2025-10-02 03:05:18,057 | INFO | Epoch 1365 Train Time 151.60448455810547s

2025-10-02 03:07:29,435 | INFO | Training epoch 1366, Batch 1000/1000: LR=2.97e-06, Loss=6.83e-03 BER=2.70e-03 FER=2.77e-02
2025-10-02 03:07:29,910 | INFO | Epoch 1366 Train Time 131.85240983963013s

2025-10-02 03:09:44,154 | INFO | Training epoch 1367, Batch 1000/1000: LR=2.94e-06, Loss=6.64e-03 BER=2.61e-03 FER=2.74e-02
2025-10-02 03:09:44,648 | INFO | Epoch 1367 Train Time 134.73616218566895s

2025-10-02 03:11:57,219 | INFO | Training epoch 1368, Batch 1000/1000: LR=2.91e-06, Loss=6.73e-03 BER=2.66e-03 FER=2.77e-02
2025-10-02 03:11:57,691 | INFO | Epoch 1368 Train Time 133.04097032546997s

2025-10-02 03:14:14,494 | INFO | Training epoch 1369, Batch 1000/1000: LR=2.88e-06, Loss=6.70e-03 BER=2.66e-03 FER=2.74e-02
2025-10-02 03:14:14,975 | INFO | Epoch 1369 Train Time 137.28386783599854s

2025-10-02 03:16:40,758 | INFO | Training epoch 1370, Batch 1000/1000: LR=2.85e-06, Loss=6.67e-03 BER=2.62e-03 FER=2.74e-02
2025-10-02 03:16:41,260 | INFO | Epoch 1370 Train Time 146.28220200538635s

2025-10-02 03:19:22,675 | INFO | Training epoch 1371, Batch 1000/1000: LR=2.82e-06, Loss=6.72e-03 BER=2.66e-03 FER=2.73e-02
2025-10-02 03:19:23,179 | INFO | Epoch 1371 Train Time 161.91784024238586s

2025-10-02 03:22:13,969 | INFO | Training epoch 1372, Batch 1000/1000: LR=2.80e-06, Loss=6.64e-03 BER=2.59e-03 FER=2.71e-02
2025-10-02 03:22:14,446 | INFO | Epoch 1372 Train Time 171.2651150226593s

2025-10-02 03:24:28,187 | INFO | Training epoch 1373, Batch 1000/1000: LR=2.77e-06, Loss=6.90e-03 BER=2.73e-03 FER=2.80e-02
2025-10-02 03:24:28,667 | INFO | Epoch 1373 Train Time 134.22061276435852s

2025-10-02 03:26:40,324 | INFO | Training epoch 1374, Batch 1000/1000: LR=2.74e-06, Loss=6.70e-03 BER=2.64e-03 FER=2.76e-02
2025-10-02 03:26:40,808 | INFO | Epoch 1374 Train Time 132.1396484375s

2025-10-02 03:28:54,944 | INFO | Training epoch 1375, Batch 1000/1000: LR=2.71e-06, Loss=6.75e-03 BER=2.66e-03 FER=2.77e-02
2025-10-02 03:28:55,417 | INFO | Epoch 1375 Train Time 134.60783314704895s

2025-10-02 03:31:07,715 | INFO | Training epoch 1376, Batch 1000/1000: LR=2.69e-06, Loss=6.68e-03 BER=2.65e-03 FER=2.75e-02
2025-10-02 03:31:08,193 | INFO | Epoch 1376 Train Time 132.77586030960083s

2025-10-02 03:33:22,405 | INFO | Training epoch 1377, Batch 1000/1000: LR=2.66e-06, Loss=6.83e-03 BER=2.71e-03 FER=2.82e-02
2025-10-02 03:33:22,871 | INFO | Epoch 1377 Train Time 134.67609882354736s

2025-10-02 03:35:38,489 | INFO | Training epoch 1378, Batch 1000/1000: LR=2.63e-06, Loss=6.79e-03 BER=2.72e-03 FER=2.80e-02
2025-10-02 03:35:38,990 | INFO | Epoch 1378 Train Time 136.11815118789673s

2025-10-02 03:38:11,537 | INFO | Training epoch 1379, Batch 1000/1000: LR=2.61e-06, Loss=6.76e-03 BER=2.68e-03 FER=2.77e-02
2025-10-02 03:38:12,008 | INFO | Epoch 1379 Train Time 153.01692867279053s

2025-10-02 03:40:24,693 | INFO | Training epoch 1380, Batch 1000/1000: LR=2.58e-06, Loss=6.70e-03 BER=2.65e-03 FER=2.67e-02
2025-10-02 03:40:25,153 | INFO | Epoch 1380 Train Time 133.14421677589417s

2025-10-02 03:42:38,837 | INFO | Training epoch 1381, Batch 1000/1000: LR=2.56e-06, Loss=6.72e-03 BER=2.64e-03 FER=2.73e-02
2025-10-02 03:42:39,309 | INFO | Epoch 1381 Train Time 134.1555655002594s

2025-10-02 03:44:52,335 | INFO | Training epoch 1382, Batch 1000/1000: LR=2.53e-06, Loss=6.54e-03 BER=2.58e-03 FER=2.67e-02
2025-10-02 03:44:52,843 | INFO | Epoch 1382 Train Time 133.5322015285492s

2025-10-02 03:47:06,463 | INFO | Training epoch 1383, Batch 1000/1000: LR=2.50e-06, Loss=6.75e-03 BER=2.67e-03 FER=2.74e-02
2025-10-02 03:47:06,948 | INFO | Epoch 1383 Train Time 134.10415387153625s

2025-10-02 03:49:19,023 | INFO | Training epoch 1384, Batch 1000/1000: LR=2.48e-06, Loss=6.64e-03 BER=2.62e-03 FER=2.73e-02
2025-10-02 03:49:19,495 | INFO | Epoch 1384 Train Time 132.54607844352722s

2025-10-02 03:51:32,990 | INFO | Training epoch 1385, Batch 1000/1000: LR=2.45e-06, Loss=6.50e-03 BER=2.56e-03 FER=2.66e-02
2025-10-02 03:51:33,463 | INFO | Epoch 1385 Train Time 133.96732831001282s

2025-10-02 03:53:45,296 | INFO | Training epoch 1386, Batch 1000/1000: LR=2.43e-06, Loss=6.74e-03 BER=2.65e-03 FER=2.74e-02
2025-10-02 03:53:45,821 | INFO | Epoch 1386 Train Time 132.3566963672638s

2025-10-02 03:55:59,312 | INFO | Training epoch 1387, Batch 1000/1000: LR=2.40e-06, Loss=6.61e-03 BER=2.61e-03 FER=2.69e-02
2025-10-02 03:55:59,787 | INFO | Epoch 1387 Train Time 133.9638352394104s

2025-10-02 03:58:11,222 | INFO | Training epoch 1388, Batch 1000/1000: LR=2.38e-06, Loss=6.61e-03 BER=2.62e-03 FER=2.73e-02
2025-10-02 03:58:11,695 | INFO | Epoch 1388 Train Time 131.90696907043457s

2025-10-02 04:00:25,697 | INFO | Training epoch 1389, Batch 1000/1000: LR=2.36e-06, Loss=6.67e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 04:00:26,181 | INFO | Epoch 1389 Train Time 134.48515510559082s

2025-10-02 04:02:37,185 | INFO | Training epoch 1390, Batch 1000/1000: LR=2.33e-06, Loss=6.63e-03 BER=2.63e-03 FER=2.69e-02
2025-10-02 04:02:37,652 | INFO | Epoch 1390 Train Time 131.47000360488892s

2025-10-02 04:04:51,404 | INFO | Training epoch 1391, Batch 1000/1000: LR=2.31e-06, Loss=6.87e-03 BER=2.69e-03 FER=2.79e-02
2025-10-02 04:04:51,878 | INFO | Epoch 1391 Train Time 134.2246651649475s

2025-10-02 04:07:03,302 | INFO | Training epoch 1392, Batch 1000/1000: LR=2.28e-06, Loss=6.70e-03 BER=2.62e-03 FER=2.75e-02
2025-10-02 04:07:03,779 | INFO | Epoch 1392 Train Time 131.90044379234314s

2025-10-02 04:09:27,655 | INFO | Training epoch 1393, Batch 1000/1000: LR=2.26e-06, Loss=6.82e-03 BER=2.69e-03 FER=2.77e-02
2025-10-02 04:09:28,166 | INFO | Epoch 1393 Train Time 144.385915517807s

2025-10-02 04:11:52,614 | INFO | Training epoch 1394, Batch 1000/1000: LR=2.24e-06, Loss=6.76e-03 BER=2.66e-03 FER=2.70e-02
2025-10-02 04:11:53,083 | INFO | Epoch 1394 Train Time 144.91625022888184s

2025-10-02 04:14:12,458 | INFO | Training epoch 1395, Batch 1000/1000: LR=2.21e-06, Loss=6.86e-03 BER=2.75e-03 FER=2.81e-02
2025-10-02 04:14:12,973 | INFO | Epoch 1395 Train Time 139.88840079307556s

2025-10-02 04:16:23,831 | INFO | Training epoch 1396, Batch 1000/1000: LR=2.19e-06, Loss=6.79e-03 BER=2.68e-03 FER=2.78e-02
2025-10-02 04:16:24,304 | INFO | Epoch 1396 Train Time 131.32940864562988s

2025-10-02 04:18:38,841 | INFO | Training epoch 1397, Batch 1000/1000: LR=2.17e-06, Loss=6.46e-03 BER=2.52e-03 FER=2.66e-02
2025-10-02 04:18:39,318 | INFO | Epoch 1397 Train Time 135.01277232170105s

2025-10-02 04:18:39,319 | INFO | [P2] saving best_model (QAT) with loss 0.006459 at epoch 1397
2025-10-02 04:20:54,237 | INFO | Training epoch 1398, Batch 1000/1000: LR=2.15e-06, Loss=6.61e-03 BER=2.61e-03 FER=2.70e-02
2025-10-02 04:20:54,724 | INFO | Epoch 1398 Train Time 135.3269441127777s

2025-10-02 04:23:07,391 | INFO | Training epoch 1399, Batch 1000/1000: LR=2.13e-06, Loss=6.70e-03 BER=2.64e-03 FER=2.73e-02
2025-10-02 04:23:07,866 | INFO | Epoch 1399 Train Time 133.1415183544159s

2025-10-02 04:25:19,483 | INFO | Training epoch 1400, Batch 1000/1000: LR=2.10e-06, Loss=6.77e-03 BER=2.65e-03 FER=2.74e-02
2025-10-02 04:25:19,949 | INFO | Epoch 1400 Train Time 132.082097530365s

2025-10-02 04:27:33,283 | INFO | Training epoch 1401, Batch 1000/1000: LR=2.08e-06, Loss=6.60e-03 BER=2.62e-03 FER=2.71e-02
2025-10-02 04:27:33,771 | INFO | Epoch 1401 Train Time 133.82043194770813s

2025-10-02 04:29:45,733 | INFO | Training epoch 1402, Batch 1000/1000: LR=2.06e-06, Loss=6.69e-03 BER=2.65e-03 FER=2.75e-02
2025-10-02 04:29:46,249 | INFO | Epoch 1402 Train Time 132.47722506523132s

2025-10-02 04:31:58,977 | INFO | Training epoch 1403, Batch 1000/1000: LR=2.04e-06, Loss=6.59e-03 BER=2.60e-03 FER=2.68e-02
2025-10-02 04:31:59,444 | INFO | Epoch 1403 Train Time 133.19414854049683s

2025-10-02 04:34:11,269 | INFO | Training epoch 1404, Batch 1000/1000: LR=2.02e-06, Loss=6.77e-03 BER=2.67e-03 FER=2.75e-02
2025-10-02 04:34:11,744 | INFO | Epoch 1404 Train Time 132.29896712303162s

2025-10-02 04:36:24,819 | INFO | Training epoch 1405, Batch 1000/1000: LR=2.00e-06, Loss=6.71e-03 BER=2.64e-03 FER=2.72e-02
2025-10-02 04:36:25,273 | INFO | Epoch 1405 Train Time 133.52791571617126s

2025-10-02 04:38:37,306 | INFO | Training epoch 1406, Batch 1000/1000: LR=1.98e-06, Loss=6.90e-03 BER=2.73e-03 FER=2.81e-02
2025-10-02 04:38:37,794 | INFO | Epoch 1406 Train Time 132.5203573703766s

2025-10-02 04:40:52,074 | INFO | Training epoch 1407, Batch 1000/1000: LR=1.96e-06, Loss=6.65e-03 BER=2.63e-03 FER=2.71e-02
2025-10-02 04:40:52,541 | INFO | Epoch 1407 Train Time 134.74552989006042s

2025-10-02 04:43:20,993 | INFO | Training epoch 1408, Batch 1000/1000: LR=1.94e-06, Loss=6.66e-03 BER=2.65e-03 FER=2.73e-02
2025-10-02 04:43:21,509 | INFO | Epoch 1408 Train Time 148.96840572357178s

2025-10-02 04:45:40,500 | INFO | Training epoch 1409, Batch 1000/1000: LR=1.92e-06, Loss=6.84e-03 BER=2.69e-03 FER=2.81e-02
2025-10-02 04:45:40,960 | INFO | Epoch 1409 Train Time 139.44949316978455s

2025-10-02 04:47:52,362 | INFO | Training epoch 1410, Batch 1000/1000: LR=1.90e-06, Loss=6.63e-03 BER=2.63e-03 FER=2.73e-02
2025-10-02 04:47:52,835 | INFO | Epoch 1410 Train Time 131.87382864952087s

2025-10-02 04:50:05,599 | INFO | Training epoch 1411, Batch 1000/1000: LR=1.88e-06, Loss=6.63e-03 BER=2.63e-03 FER=2.70e-02
2025-10-02 04:50:06,072 | INFO | Epoch 1411 Train Time 133.236661195755s

2025-10-02 04:52:17,801 | INFO | Training epoch 1412, Batch 1000/1000: LR=1.86e-06, Loss=6.79e-03 BER=2.70e-03 FER=2.81e-02
2025-10-02 04:52:18,268 | INFO | Epoch 1412 Train Time 132.194354057312s

2025-10-02 04:54:30,811 | INFO | Training epoch 1413, Batch 1000/1000: LR=1.84e-06, Loss=6.68e-03 BER=2.65e-03 FER=2.75e-02
2025-10-02 04:54:31,274 | INFO | Epoch 1413 Train Time 133.00524950027466s

2025-10-02 04:56:43,243 | INFO | Training epoch 1414, Batch 1000/1000: LR=1.82e-06, Loss=6.58e-03 BER=2.57e-03 FER=2.68e-02
2025-10-02 04:56:43,734 | INFO | Epoch 1414 Train Time 132.45842504501343s

2025-10-02 04:58:55,989 | INFO | Training epoch 1415, Batch 1000/1000: LR=1.80e-06, Loss=6.85e-03 BER=2.69e-03 FER=2.79e-02
2025-10-02 04:58:56,506 | INFO | Epoch 1415 Train Time 132.77150440216064s

2025-10-02 05:01:08,216 | INFO | Training epoch 1416, Batch 1000/1000: LR=1.78e-06, Loss=6.67e-03 BER=2.62e-03 FER=2.75e-02
2025-10-02 05:01:08,696 | INFO | Epoch 1416 Train Time 132.19022059440613s

2025-10-02 05:03:21,493 | INFO | Training epoch 1417, Batch 1000/1000: LR=1.76e-06, Loss=6.82e-03 BER=2.68e-03 FER=2.76e-02
2025-10-02 05:03:21,951 | INFO | Epoch 1417 Train Time 133.25381302833557s

2025-10-02 05:05:32,996 | INFO | Training epoch 1418, Batch 1000/1000: LR=1.75e-06, Loss=6.65e-03 BER=2.61e-03 FER=2.68e-02
2025-10-02 05:05:33,453 | INFO | Epoch 1418 Train Time 131.50097227096558s

2025-10-02 05:07:45,989 | INFO | Training epoch 1419, Batch 1000/1000: LR=1.73e-06, Loss=6.65e-03 BER=2.65e-03 FER=2.70e-02
2025-10-02 05:07:46,450 | INFO | Epoch 1419 Train Time 132.99729871749878s

2025-10-02 05:09:57,837 | INFO | Training epoch 1420, Batch 1000/1000: LR=1.71e-06, Loss=7.03e-03 BER=2.78e-03 FER=2.84e-02
2025-10-02 05:09:58,339 | INFO | Epoch 1420 Train Time 131.8879964351654s

2025-10-02 05:12:11,123 | INFO | Training epoch 1421, Batch 1000/1000: LR=1.69e-06, Loss=6.69e-03 BER=2.63e-03 FER=2.75e-02
2025-10-02 05:12:11,635 | INFO | Epoch 1421 Train Time 133.29503321647644s

2025-10-02 05:14:27,824 | INFO | Training epoch 1422, Batch 1000/1000: LR=1.68e-06, Loss=6.51e-03 BER=2.59e-03 FER=2.68e-02
2025-10-02 05:14:28,342 | INFO | Epoch 1422 Train Time 136.7055823802948s

2025-10-02 05:17:05,833 | INFO | Training epoch 1423, Batch 1000/1000: LR=1.66e-06, Loss=6.57e-03 BER=2.58e-03 FER=2.66e-02
2025-10-02 05:17:06,344 | INFO | Epoch 1423 Train Time 157.99982905387878s

2025-10-02 05:19:18,733 | INFO | Training epoch 1424, Batch 1000/1000: LR=1.64e-06, Loss=6.71e-03 BER=2.64e-03 FER=2.72e-02
2025-10-02 05:19:19,206 | INFO | Epoch 1424 Train Time 132.86173605918884s

2025-10-02 05:21:32,009 | INFO | Training epoch 1425, Batch 1000/1000: LR=1.63e-06, Loss=6.82e-03 BER=2.68e-03 FER=2.79e-02
2025-10-02 05:21:32,504 | INFO | Epoch 1425 Train Time 133.29718232154846s

2025-10-02 05:23:44,186 | INFO | Training epoch 1426, Batch 1000/1000: LR=1.61e-06, Loss=6.65e-03 BER=2.62e-03 FER=2.70e-02
2025-10-02 05:23:44,683 | INFO | Epoch 1426 Train Time 132.17740106582642s

2025-10-02 05:25:57,649 | INFO | Training epoch 1427, Batch 1000/1000: LR=1.59e-06, Loss=6.76e-03 BER=2.67e-03 FER=2.72e-02
2025-10-02 05:25:58,124 | INFO | Epoch 1427 Train Time 133.43990993499756s

2025-10-02 05:28:09,481 | INFO | Training epoch 1428, Batch 1000/1000: LR=1.58e-06, Loss=6.70e-03 BER=2.62e-03 FER=2.71e-02
2025-10-02 05:28:09,957 | INFO | Epoch 1428 Train Time 131.83262276649475s

2025-10-02 05:30:22,792 | INFO | Training epoch 1429, Batch 1000/1000: LR=1.56e-06, Loss=6.91e-03 BER=2.75e-03 FER=2.82e-02
2025-10-02 05:30:23,262 | INFO | Epoch 1429 Train Time 133.30387616157532s

2025-10-02 05:32:34,736 | INFO | Training epoch 1430, Batch 1000/1000: LR=1.55e-06, Loss=6.68e-03 BER=2.64e-03 FER=2.75e-02
2025-10-02 05:32:35,216 | INFO | Epoch 1430 Train Time 131.95316338539124s

2025-10-02 05:34:48,614 | INFO | Training epoch 1431, Batch 1000/1000: LR=1.53e-06, Loss=6.51e-03 BER=2.58e-03 FER=2.67e-02
2025-10-02 05:34:49,121 | INFO | Epoch 1431 Train Time 133.90315413475037s

2025-10-02 05:37:00,137 | INFO | Training epoch 1432, Batch 1000/1000: LR=1.52e-06, Loss=6.59e-03 BER=2.57e-03 FER=2.69e-02
2025-10-02 05:37:00,618 | INFO | Epoch 1432 Train Time 131.49640345573425s

2025-10-02 05:39:13,814 | INFO | Training epoch 1433, Batch 1000/1000: LR=1.50e-06, Loss=6.79e-03 BER=2.70e-03 FER=2.80e-02
2025-10-02 05:39:14,282 | INFO | Epoch 1433 Train Time 133.66338348388672s

2025-10-02 05:41:25,034 | INFO | Training epoch 1434, Batch 1000/1000: LR=1.49e-06, Loss=6.73e-03 BER=2.66e-03 FER=2.77e-02
2025-10-02 05:41:25,494 | INFO | Epoch 1434 Train Time 131.2106430530548s

2025-10-02 05:43:38,231 | INFO | Training epoch 1435, Batch 1000/1000: LR=1.47e-06, Loss=6.58e-03 BER=2.60e-03 FER=2.66e-02
2025-10-02 05:43:38,721 | INFO | Epoch 1435 Train Time 133.22576594352722s

2025-10-02 05:45:50,358 | INFO | Training epoch 1436, Batch 1000/1000: LR=1.46e-06, Loss=6.48e-03 BER=2.58e-03 FER=2.68e-02
2025-10-02 05:45:50,825 | INFO | Epoch 1436 Train Time 132.10290932655334s

2025-10-02 05:48:08,355 | INFO | Training epoch 1437, Batch 1000/1000: LR=1.44e-06, Loss=6.62e-03 BER=2.59e-03 FER=2.67e-02
2025-10-02 05:48:08,877 | INFO | Epoch 1437 Train Time 138.05044841766357s

2025-10-02 05:50:40,890 | INFO | Training epoch 1438, Batch 1000/1000: LR=1.43e-06, Loss=6.73e-03 BER=2.68e-03 FER=2.74e-02
2025-10-02 05:50:41,402 | INFO | Epoch 1438 Train Time 152.52367043495178s

2025-10-02 05:53:00,579 | INFO | Training epoch 1439, Batch 1000/1000: LR=1.42e-06, Loss=6.65e-03 BER=2.61e-03 FER=2.72e-02
2025-10-02 05:53:01,065 | INFO | Epoch 1439 Train Time 139.66235041618347s

2025-10-02 05:55:12,717 | INFO | Training epoch 1440, Batch 1000/1000: LR=1.40e-06, Loss=6.69e-03 BER=2.63e-03 FER=2.71e-02
2025-10-02 05:55:13,219 | INFO | Epoch 1440 Train Time 132.15255331993103s

2025-10-02 05:57:25,781 | INFO | Training epoch 1441, Batch 1000/1000: LR=1.39e-06, Loss=7.03e-03 BER=2.78e-03 FER=2.85e-02
2025-10-02 05:57:26,242 | INFO | Epoch 1441 Train Time 133.02243661880493s

2025-10-02 05:59:37,976 | INFO | Training epoch 1442, Batch 1000/1000: LR=1.38e-06, Loss=6.61e-03 BER=2.62e-03 FER=2.71e-02
2025-10-02 05:59:38,465 | INFO | Epoch 1442 Train Time 132.22229766845703s

2025-10-02 06:01:52,186 | INFO | Training epoch 1443, Batch 1000/1000: LR=1.36e-06, Loss=6.66e-03 BER=2.63e-03 FER=2.70e-02
2025-10-02 06:01:52,660 | INFO | Epoch 1443 Train Time 134.19398474693298s

2025-10-02 06:04:03,785 | INFO | Training epoch 1444, Batch 1000/1000: LR=1.35e-06, Loss=6.75e-03 BER=2.66e-03 FER=2.72e-02
2025-10-02 06:04:04,258 | INFO | Epoch 1444 Train Time 131.5962109565735s

2025-10-02 06:06:17,370 | INFO | Training epoch 1445, Batch 1000/1000: LR=1.34e-06, Loss=6.90e-03 BER=2.71e-03 FER=2.78e-02
2025-10-02 06:06:17,854 | INFO | Epoch 1445 Train Time 133.59561467170715s

2025-10-02 06:08:28,943 | INFO | Training epoch 1446, Batch 1000/1000: LR=1.33e-06, Loss=6.66e-03 BER=2.62e-03 FER=2.68e-02
2025-10-02 06:08:29,409 | INFO | Epoch 1446 Train Time 131.55352544784546s

2025-10-02 06:10:42,625 | INFO | Training epoch 1447, Batch 1000/1000: LR=1.32e-06, Loss=6.51e-03 BER=2.56e-03 FER=2.65e-02
2025-10-02 06:10:43,106 | INFO | Epoch 1447 Train Time 133.69607090950012s

2025-10-02 06:12:54,145 | INFO | Training epoch 1448, Batch 1000/1000: LR=1.30e-06, Loss=6.72e-03 BER=2.62e-03 FER=2.73e-02
2025-10-02 06:12:54,608 | INFO | Epoch 1448 Train Time 131.50095438957214s

2025-10-02 06:15:30,183 | INFO | Training epoch 1449, Batch 1000/1000: LR=1.29e-06, Loss=6.50e-03 BER=2.57e-03 FER=2.64e-02
2025-10-02 06:15:30,698 | INFO | Epoch 1449 Train Time 156.0888488292694s

2025-10-02 06:18:11,863 | INFO | Training epoch 1450, Batch 1000/1000: LR=1.28e-06, Loss=6.56e-03 BER=2.58e-03 FER=2.67e-02
2025-10-02 06:18:12,403 | INFO | Epoch 1450 Train Time 161.70444560050964s

2025-10-02 06:20:25,464 | INFO | Training epoch 1451, Batch 1000/1000: LR=1.27e-06, Loss=6.65e-03 BER=2.64e-03 FER=2.75e-02
2025-10-02 06:20:25,983 | INFO | Epoch 1451 Train Time 133.5781831741333s

2025-10-02 06:22:42,648 | INFO | Training epoch 1452, Batch 1000/1000: LR=1.26e-06, Loss=6.59e-03 BER=2.58e-03 FER=2.68e-02
2025-10-02 06:22:43,149 | INFO | Epoch 1452 Train Time 137.16430640220642s

2025-10-02 06:25:15,655 | INFO | Training epoch 1453, Batch 1000/1000: LR=1.25e-06, Loss=6.91e-03 BER=2.72e-03 FER=2.78e-02
2025-10-02 06:25:16,153 | INFO | Epoch 1453 Train Time 153.00330066680908s

2025-10-02 06:27:47,383 | INFO | Training epoch 1454, Batch 1000/1000: LR=1.24e-06, Loss=6.75e-03 BER=2.67e-03 FER=2.78e-02
2025-10-02 06:27:47,886 | INFO | Epoch 1454 Train Time 151.73167324066162s

2025-10-02 06:30:07,678 | INFO | Training epoch 1455, Batch 1000/1000: LR=1.23e-06, Loss=6.73e-03 BER=2.66e-03 FER=2.76e-02
2025-10-02 06:30:08,171 | INFO | Epoch 1455 Train Time 140.28410506248474s

2025-10-02 06:32:19,261 | INFO | Training epoch 1456, Batch 1000/1000: LR=1.22e-06, Loss=6.72e-03 BER=2.67e-03 FER=2.73e-02
2025-10-02 06:32:19,752 | INFO | Epoch 1456 Train Time 131.58031392097473s

2025-10-02 06:34:32,063 | INFO | Training epoch 1457, Batch 1000/1000: LR=1.21e-06, Loss=6.67e-03 BER=2.63e-03 FER=2.73e-02
2025-10-02 06:34:32,540 | INFO | Epoch 1457 Train Time 132.78721594810486s

2025-10-02 06:36:42,953 | INFO | Training epoch 1458, Batch 1000/1000: LR=1.20e-06, Loss=6.60e-03 BER=2.63e-03 FER=2.69e-02
2025-10-02 06:36:43,411 | INFO | Epoch 1458 Train Time 130.86949467658997s

2025-10-02 06:38:56,313 | INFO | Training epoch 1459, Batch 1000/1000: LR=1.19e-06, Loss=6.64e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 06:38:56,789 | INFO | Epoch 1459 Train Time 133.3774025440216s

2025-10-02 06:41:07,627 | INFO | Training epoch 1460, Batch 1000/1000: LR=1.18e-06, Loss=6.83e-03 BER=2.71e-03 FER=2.75e-02
2025-10-02 06:41:08,098 | INFO | Epoch 1460 Train Time 131.30778217315674s

2025-10-02 06:43:20,839 | INFO | Training epoch 1461, Batch 1000/1000: LR=1.17e-06, Loss=6.94e-03 BER=2.72e-03 FER=2.80e-02
2025-10-02 06:43:21,314 | INFO | Epoch 1461 Train Time 133.21530985832214s

2025-10-02 06:45:32,667 | INFO | Training epoch 1462, Batch 1000/1000: LR=1.17e-06, Loss=6.68e-03 BER=2.62e-03 FER=2.69e-02
2025-10-02 06:45:33,141 | INFO | Epoch 1462 Train Time 131.82532048225403s

2025-10-02 06:47:45,124 | INFO | Training epoch 1463, Batch 1000/1000: LR=1.16e-06, Loss=6.85e-03 BER=2.72e-03 FER=2.74e-02
2025-10-02 06:47:45,591 | INFO | Epoch 1463 Train Time 132.4481213092804s

2025-10-02 06:49:56,497 | INFO | Training epoch 1464, Batch 1000/1000: LR=1.15e-06, Loss=6.84e-03 BER=2.70e-03 FER=2.75e-02
2025-10-02 06:49:56,969 | INFO | Epoch 1464 Train Time 131.37707042694092s

2025-10-02 06:52:09,661 | INFO | Training epoch 1465, Batch 1000/1000: LR=1.14e-06, Loss=6.42e-03 BER=2.57e-03 FER=2.62e-02
2025-10-02 06:52:10,128 | INFO | Epoch 1465 Train Time 133.15855717658997s

2025-10-02 06:52:10,128 | INFO | [P2] saving best_model (QAT) with loss 0.006421 at epoch 1465
2025-10-02 06:54:21,502 | INFO | Training epoch 1466, Batch 1000/1000: LR=1.13e-06, Loss=6.62e-03 BER=2.63e-03 FER=2.67e-02
2025-10-02 06:54:21,979 | INFO | Epoch 1466 Train Time 131.7738299369812s

2025-10-02 06:56:34,943 | INFO | Training epoch 1467, Batch 1000/1000: LR=1.13e-06, Loss=6.89e-03 BER=2.73e-03 FER=2.79e-02
2025-10-02 06:56:35,425 | INFO | Epoch 1467 Train Time 133.4447774887085s

2025-10-02 06:58:46,613 | INFO | Training epoch 1468, Batch 1000/1000: LR=1.12e-06, Loss=6.61e-03 BER=2.59e-03 FER=2.69e-02
2025-10-02 06:58:47,099 | INFO | Epoch 1468 Train Time 131.67358350753784s

2025-10-02 07:01:17,246 | INFO | Training epoch 1469, Batch 1000/1000: LR=1.11e-06, Loss=6.80e-03 BER=2.68e-03 FER=2.76e-02
2025-10-02 07:01:17,764 | INFO | Epoch 1469 Train Time 150.6630871295929s

2025-10-02 07:03:49,917 | INFO | Training epoch 1470, Batch 1000/1000: LR=1.10e-06, Loss=6.52e-03 BER=2.56e-03 FER=2.64e-02
2025-10-02 07:03:50,469 | INFO | Epoch 1470 Train Time 152.70464372634888s

2025-10-02 07:06:23,899 | INFO | Training epoch 1471, Batch 1000/1000: LR=1.10e-06, Loss=6.62e-03 BER=2.58e-03 FER=2.66e-02
2025-10-02 07:06:24,416 | INFO | Epoch 1471 Train Time 153.94530582427979s

2025-10-02 07:08:39,969 | INFO | Training epoch 1472, Batch 1000/1000: LR=1.09e-06, Loss=6.63e-03 BER=2.62e-03 FER=2.70e-02
2025-10-02 07:08:40,469 | INFO | Epoch 1472 Train Time 136.05240488052368s

2025-10-02 07:10:53,708 | INFO | Training epoch 1473, Batch 1000/1000: LR=1.09e-06, Loss=6.62e-03 BER=2.63e-03 FER=2.73e-02
2025-10-02 07:10:54,172 | INFO | Epoch 1473 Train Time 133.70285630226135s

2025-10-02 07:13:05,973 | INFO | Training epoch 1474, Batch 1000/1000: LR=1.08e-06, Loss=6.64e-03 BER=2.59e-03 FER=2.66e-02
2025-10-02 07:13:06,451 | INFO | Epoch 1474 Train Time 132.27866053581238s

2025-10-02 07:15:24,318 | INFO | Training epoch 1475, Batch 1000/1000: LR=1.07e-06, Loss=6.69e-03 BER=2.63e-03 FER=2.74e-02
2025-10-02 07:15:24,803 | INFO | Epoch 1475 Train Time 138.35080409049988s

2025-10-02 07:17:36,166 | INFO | Training epoch 1476, Batch 1000/1000: LR=1.07e-06, Loss=6.74e-03 BER=2.65e-03 FER=2.72e-02
2025-10-02 07:17:36,652 | INFO | Epoch 1476 Train Time 131.84751796722412s

2025-10-02 07:19:51,009 | INFO | Training epoch 1477, Batch 1000/1000: LR=1.06e-06, Loss=6.76e-03 BER=2.68e-03 FER=2.78e-02
2025-10-02 07:19:51,520 | INFO | Epoch 1477 Train Time 134.86686992645264s

2025-10-02 07:22:05,665 | INFO | Training epoch 1478, Batch 1000/1000: LR=1.06e-06, Loss=6.69e-03 BER=2.61e-03 FER=2.72e-02
2025-10-02 07:22:06,138 | INFO | Epoch 1478 Train Time 134.6177020072937s

2025-10-02 07:24:19,468 | INFO | Training epoch 1479, Batch 1000/1000: LR=1.05e-06, Loss=6.85e-03 BER=2.68e-03 FER=2.76e-02
2025-10-02 07:24:19,961 | INFO | Epoch 1479 Train Time 133.82151579856873s

2025-10-02 07:26:31,567 | INFO | Training epoch 1480, Batch 1000/1000: LR=1.05e-06, Loss=6.60e-03 BER=2.61e-03 FER=2.67e-02
2025-10-02 07:26:32,058 | INFO | Epoch 1480 Train Time 132.0965678691864s

2025-10-02 07:28:44,711 | INFO | Training epoch 1481, Batch 1000/1000: LR=1.04e-06, Loss=6.66e-03 BER=2.63e-03 FER=2.71e-02
2025-10-02 07:28:45,238 | INFO | Epoch 1481 Train Time 133.17816305160522s

2025-10-02 07:30:56,625 | INFO | Training epoch 1482, Batch 1000/1000: LR=1.04e-06, Loss=6.83e-03 BER=2.70e-03 FER=2.74e-02
2025-10-02 07:30:57,103 | INFO | Epoch 1482 Train Time 131.86353278160095s

2025-10-02 07:33:10,739 | INFO | Training epoch 1483, Batch 1000/1000: LR=1.04e-06, Loss=6.68e-03 BER=2.62e-03 FER=2.71e-02
2025-10-02 07:33:11,204 | INFO | Epoch 1483 Train Time 134.10006403923035s

2025-10-02 07:35:23,074 | INFO | Training epoch 1484, Batch 1000/1000: LR=1.03e-06, Loss=6.83e-03 BER=2.72e-03 FER=2.76e-02
2025-10-02 07:35:23,551 | INFO | Epoch 1484 Train Time 132.34481859207153s

2025-10-02 07:37:39,599 | INFO | Training epoch 1485, Batch 1000/1000: LR=1.03e-06, Loss=6.68e-03 BER=2.64e-03 FER=2.75e-02
2025-10-02 07:37:40,108 | INFO | Epoch 1485 Train Time 136.55577087402344s

2025-10-02 07:40:14,010 | INFO | Training epoch 1486, Batch 1000/1000: LR=1.02e-06, Loss=6.56e-03 BER=2.58e-03 FER=2.65e-02
2025-10-02 07:40:14,530 | INFO | Epoch 1486 Train Time 154.41996574401855s

2025-10-02 07:42:47,557 | INFO | Training epoch 1487, Batch 1000/1000: LR=1.02e-06, Loss=6.66e-03 BER=2.64e-03 FER=2.72e-02
2025-10-02 07:42:48,055 | INFO | Epoch 1487 Train Time 153.5239155292511s

2025-10-02 07:45:20,806 | INFO | Training epoch 1488, Batch 1000/1000: LR=1.02e-06, Loss=6.58e-03 BER=2.61e-03 FER=2.68e-02
2025-10-02 07:45:21,279 | INFO | Epoch 1488 Train Time 153.22351264953613s

2025-10-02 07:47:33,701 | INFO | Training epoch 1489, Batch 1000/1000: LR=1.02e-06, Loss=6.65e-03 BER=2.65e-03 FER=2.75e-02
2025-10-02 07:47:34,181 | INFO | Epoch 1489 Train Time 132.90053749084473s

2025-10-02 07:49:45,979 | INFO | Training epoch 1490, Batch 1000/1000: LR=1.01e-06, Loss=6.53e-03 BER=2.56e-03 FER=2.63e-02
2025-10-02 07:49:46,478 | INFO | Epoch 1490 Train Time 132.29590702056885s

2025-10-02 07:51:59,668 | INFO | Training epoch 1491, Batch 1000/1000: LR=1.01e-06, Loss=6.88e-03 BER=2.71e-03 FER=2.78e-02
2025-10-02 07:52:00,152 | INFO | Epoch 1491 Train Time 133.67324900627136s

2025-10-02 07:54:12,472 | INFO | Training epoch 1492, Batch 1000/1000: LR=1.01e-06, Loss=6.76e-03 BER=2.68e-03 FER=2.73e-02
2025-10-02 07:54:12,955 | INFO | Epoch 1492 Train Time 132.80188846588135s

2025-10-02 07:56:26,912 | INFO | Training epoch 1493, Batch 1000/1000: LR=1.01e-06, Loss=6.59e-03 BER=2.60e-03 FER=2.68e-02
2025-10-02 07:56:27,379 | INFO | Epoch 1493 Train Time 134.42349243164062s

2025-10-02 07:58:41,093 | INFO | Training epoch 1494, Batch 1000/1000: LR=1.01e-06, Loss=6.61e-03 BER=2.62e-03 FER=2.73e-02
2025-10-02 07:58:41,562 | INFO | Epoch 1494 Train Time 134.18172955513s

2025-10-02 08:00:55,589 | INFO | Training epoch 1495, Batch 1000/1000: LR=1.00e-06, Loss=6.70e-03 BER=2.65e-03 FER=2.75e-02
2025-10-02 08:00:56,063 | INFO | Epoch 1495 Train Time 134.5000307559967s

2025-10-02 08:03:08,614 | INFO | Training epoch 1496, Batch 1000/1000: LR=1.00e-06, Loss=6.68e-03 BER=2.64e-03 FER=2.71e-02
2025-10-02 08:03:09,097 | INFO | Epoch 1496 Train Time 133.0333149433136s

2025-10-02 08:05:23,057 | INFO | Training epoch 1497, Batch 1000/1000: LR=1.00e-06, Loss=6.88e-03 BER=2.73e-03 FER=2.84e-02
2025-10-02 08:05:23,543 | INFO | Epoch 1497 Train Time 134.4432816505432s

2025-10-02 08:07:34,335 | INFO | Training epoch 1498, Batch 1000/1000: LR=1.00e-06, Loss=6.73e-03 BER=2.64e-03 FER=2.76e-02
2025-10-02 08:07:34,828 | INFO | Epoch 1498 Train Time 131.28431749343872s

2025-10-02 08:09:50,548 | INFO | Training epoch 1499, Batch 1000/1000: LR=1.00e-06, Loss=6.67e-03 BER=2.61e-03 FER=2.71e-02
2025-10-02 08:09:51,041 | INFO | Epoch 1499 Train Time 136.21214747428894s

2025-10-02 08:12:02,426 | INFO | Training epoch 1500, Batch 1000/1000: LR=1.00e-06, Loss=6.77e-03 BER=2.65e-03 FER=2.72e-02
2025-10-02 08:12:02,948 | INFO | Epoch 1500 Train Time 131.90543055534363s

2025-10-02 08:12:02,986 | INFO | Checkpoint saved: runs\20250929_092917\stage2_qat__LDPC_n49_k24__Ndec6_d64_h8.pth
2025-10-02 08:12:03,014 | INFO | Checkpoint saved: runs\20250929_092917\stage2_qat__LDPC_n49_k24__Ndec6_d64_h8__e1500_loss0.006768.pth
2025-10-02 08:12:03,219 | INFO | Loaded checkpoint: runs\20250929_092917\stage2_qat__LDPC_n49_k24__Ndec6_d64_h8.pth (strict=False)
2025-10-02 08:12:03,268 | INFO | Checkpoint saved: runs\20250929_092917\stage2_infer_frozen__LDPC_n49_k24__Ndec6_d64_h8__e1465_loss0.006421.pth
2025-10-02 08:13:12,685 | INFO | FER count threshold reached for EbN0:4
2025-10-02 08:13:13,156 | INFO | Test EbN0=4, BER=2.71e-03
2025-10-02 08:14:24,023 | INFO | FER count threshold reached for EbN0:5
2025-10-02 08:14:24,504 | INFO | Test EbN0=5, BER=2.07e-04
2025-10-02 08:24:51,286 | INFO | FER count threshold reached for EbN0:6
2025-10-02 08:24:51,792 | INFO | Test EbN0=6, BER=7.77e-06
2025-10-02 08:24:51,792 | INFO | 
Test Loss 4: 6.8928e-03 5: 5.7727e-04 6: 2.3345e-05
2025-10-02 08:24:51,792 | INFO | Test FER 4: 2.9187e-02 5: 2.6905e-03 6: 1.1376e-04
2025-10-02 08:24:51,793 | INFO | Test BER 4: 2.7088e-03 5: 2.0703e-04 6: 7.7697e-06
2025-10-02 08:24:51,793 | INFO | Test -ln(BER) 4: 5.9112e+00 5: 8.4827e+00 6: 1.1765e+01
2025-10-02 08:24:51,793 | INFO | # of testing samples: [100352.0, 100352.0, 887808.0]
 Test Time 768.524313211441 s

2025-10-02 08:24:51,797 | INFO | Done.
