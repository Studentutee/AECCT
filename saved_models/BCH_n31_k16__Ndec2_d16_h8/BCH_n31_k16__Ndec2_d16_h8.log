2025-10-18 15:57:27,413 | INFO | Device: cuda
2025-10-18 15:57:52,573 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=2.47e-01 BER=8.78e-02 FER=7.75e-01
2025-10-18 15:57:52,631 | INFO | Epoch 1 Train Time 23.782535552978516s

2025-10-18 15:57:52,631 | INFO | [P1] saving best_model with loss 0.246916 at epoch 1
2025-10-18 15:58:15,506 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=1.55e-01 BER=3.84e-02 FER=6.47e-01
2025-10-18 15:58:15,588 | INFO | Epoch 2 Train Time 22.93675470352173s

2025-10-18 15:58:15,588 | INFO | [P1] saving best_model with loss 0.155196 at epoch 2
2025-10-18 15:58:36,427 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=1.43e-01 BER=3.85e-02 FER=6.48e-01
2025-10-18 15:58:36,500 | INFO | Epoch 3 Train Time 20.89044427871704s

2025-10-18 15:58:36,502 | INFO | [P1] saving best_model with loss 0.143486 at epoch 3
2025-10-18 15:58:59,343 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=1.30e-01 BER=3.83e-02 FER=6.43e-01
2025-10-18 15:58:59,428 | INFO | Epoch 4 Train Time 22.902063131332397s

2025-10-18 15:58:59,428 | INFO | [P1] saving best_model with loss 0.129861 at epoch 4
2025-10-18 15:59:22,420 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=1.17e-01 BER=3.56e-02 FER=5.64e-01
2025-10-18 15:59:22,511 | INFO | Epoch 5 Train Time 23.066470623016357s

2025-10-18 15:59:22,511 | INFO | [P1] saving best_model with loss 0.116984 at epoch 5
2025-10-18 15:59:42,826 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=1.07e-01 BER=3.16e-02 FER=4.52e-01
2025-10-18 15:59:42,913 | INFO | Epoch 6 Train Time 20.379897117614746s

2025-10-18 15:59:42,914 | INFO | [P1] saving best_model with loss 0.106611 at epoch 6
2025-10-18 16:00:03,499 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=9.78e-02 BER=2.82e-02 FER=3.67e-01
2025-10-18 16:00:03,577 | INFO | Epoch 7 Train Time 20.650527954101562s

2025-10-18 16:00:03,578 | INFO | [P1] saving best_model with loss 0.097765 at epoch 7
2025-10-18 16:00:26,145 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=9.19e-02 BER=2.65e-02 FER=3.33e-01
2025-10-18 16:00:26,218 | INFO | Epoch 8 Train Time 22.627792358398438s

2025-10-18 16:00:26,219 | INFO | [P1] saving best_model with loss 0.091923 at epoch 8
2025-10-18 16:00:50,052 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=8.58e-02 BER=2.54e-02 FER=3.23e-01
2025-10-18 16:00:50,147 | INFO | Epoch 9 Train Time 23.90661382675171s

2025-10-18 16:00:50,148 | INFO | [P1] saving best_model with loss 0.085790 at epoch 9
2025-10-18 16:01:14,457 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=7.99e-02 BER=2.47e-02 FER=3.18e-01
2025-10-18 16:01:14,532 | INFO | Epoch 10 Train Time 24.371277570724487s

2025-10-18 16:01:14,533 | INFO | [P1] saving best_model with loss 0.079906 at epoch 10
2025-10-18 16:01:36,381 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=7.43e-02 BER=2.37e-02 FER=3.12e-01
2025-10-18 16:01:36,455 | INFO | Epoch 11 Train Time 21.905935287475586s

2025-10-18 16:01:36,456 | INFO | [P1] saving best_model with loss 0.074333 at epoch 11
2025-10-18 16:02:00,049 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=6.95e-02 BER=2.27e-02 FER=3.03e-01
2025-10-18 16:02:00,145 | INFO | Epoch 12 Train Time 23.66537094116211s

2025-10-18 16:02:00,146 | INFO | [P1] saving best_model with loss 0.069534 at epoch 12
2025-10-18 16:02:22,551 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=6.39e-02 BER=2.11e-02 FER=2.89e-01
2025-10-18 16:02:22,628 | INFO | Epoch 13 Train Time 22.45507836341858s

2025-10-18 16:02:22,628 | INFO | [P1] saving best_model with loss 0.063863 at epoch 13
2025-10-18 16:02:45,894 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=6.01e-02 BER=2.04e-02 FER=2.80e-01
2025-10-18 16:02:45,985 | INFO | Epoch 14 Train Time 23.338972568511963s

2025-10-18 16:02:45,985 | INFO | [P1] saving best_model with loss 0.060076 at epoch 14
2025-10-18 16:03:09,538 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=5.63e-02 BER=1.94e-02 FER=2.69e-01
2025-10-18 16:03:09,603 | INFO | Epoch 15 Train Time 23.59144926071167s

2025-10-18 16:03:09,605 | INFO | [P1] saving best_model with loss 0.056256 at epoch 15
2025-10-18 16:03:33,641 | INFO | Training epoch 16, Batch 1000/1000: LR=1.00e-04, Loss=5.28e-02 BER=1.85e-02 FER=2.59e-01
2025-10-18 16:03:33,730 | INFO | Epoch 16 Train Time 24.1124324798584s

2025-10-18 16:03:33,731 | INFO | [P1] saving best_model with loss 0.052770 at epoch 16
2025-10-18 16:03:58,569 | INFO | Training epoch 17, Batch 1000/1000: LR=1.00e-04, Loss=5.03e-02 BER=1.78e-02 FER=2.49e-01
2025-10-18 16:03:58,666 | INFO | Epoch 17 Train Time 24.910229444503784s

2025-10-18 16:03:58,667 | INFO | [P1] saving best_model with loss 0.050254 at epoch 17
2025-10-18 16:04:19,112 | INFO | Training epoch 18, Batch 1000/1000: LR=1.00e-04, Loss=4.84e-02 BER=1.73e-02 FER=2.41e-01
2025-10-18 16:04:19,184 | INFO | Epoch 18 Train Time 20.496976375579834s

2025-10-18 16:04:19,185 | INFO | [P1] saving best_model with loss 0.048414 at epoch 18
2025-10-18 16:04:42,855 | INFO | Training epoch 19, Batch 1000/1000: LR=1.00e-04, Loss=4.64e-02 BER=1.68e-02 FER=2.33e-01
2025-10-18 16:04:42,947 | INFO | Epoch 19 Train Time 23.74985647201538s

2025-10-18 16:04:42,948 | INFO | [P1] saving best_model with loss 0.046357 at epoch 19
2025-10-18 16:05:06,666 | INFO | Training epoch 20, Batch 1000/1000: LR=1.00e-04, Loss=4.51e-02 BER=1.64e-02 FER=2.27e-01
2025-10-18 16:05:06,752 | INFO | Epoch 20 Train Time 23.778270483016968s

2025-10-18 16:05:06,753 | INFO | [P1] saving best_model with loss 0.045066 at epoch 20
2025-10-18 16:05:30,325 | INFO | Training epoch 21, Batch 1000/1000: LR=1.00e-04, Loss=4.36e-02 BER=1.59e-02 FER=2.20e-01
2025-10-18 16:05:30,408 | INFO | Epoch 21 Train Time 23.64271593093872s

2025-10-18 16:05:30,409 | INFO | [P1] saving best_model with loss 0.043582 at epoch 21
2025-10-18 16:05:53,709 | INFO | Training epoch 22, Batch 1000/1000: LR=1.00e-04, Loss=4.26e-02 BER=1.56e-02 FER=2.16e-01
2025-10-18 16:05:53,800 | INFO | Epoch 22 Train Time 23.373460292816162s

2025-10-18 16:05:53,801 | INFO | [P1] saving best_model with loss 0.042586 at epoch 22
2025-10-18 16:06:17,543 | INFO | Training epoch 23, Batch 1000/1000: LR=1.00e-04, Loss=4.19e-02 BER=1.54e-02 FER=2.12e-01
2025-10-18 16:06:17,629 | INFO | Epoch 23 Train Time 23.807270765304565s

2025-10-18 16:06:17,630 | INFO | [P1] saving best_model with loss 0.041904 at epoch 23
2025-10-18 16:06:41,039 | INFO | Training epoch 24, Batch 1000/1000: LR=1.00e-04, Loss=4.10e-02 BER=1.51e-02 FER=2.08e-01
2025-10-18 16:06:41,109 | INFO | Epoch 24 Train Time 23.454519987106323s

2025-10-18 16:06:41,109 | INFO | [P1] saving best_model with loss 0.040980 at epoch 24
2025-10-18 16:07:05,120 | INFO | Training epoch 25, Batch 1000/1000: LR=1.00e-04, Loss=4.07e-02 BER=1.51e-02 FER=2.06e-01
2025-10-18 16:07:05,201 | INFO | Epoch 25 Train Time 24.077743768692017s

2025-10-18 16:07:05,201 | INFO | [P1] saving best_model with loss 0.040686 at epoch 25
2025-10-18 16:07:29,627 | INFO | Training epoch 26, Batch 1000/1000: LR=1.00e-04, Loss=4.02e-02 BER=1.48e-02 FER=2.02e-01
2025-10-18 16:07:29,710 | INFO | Epoch 26 Train Time 24.496514797210693s

2025-10-18 16:07:29,710 | INFO | [P1] saving best_model with loss 0.040155 at epoch 26
2025-10-18 16:07:53,352 | INFO | Training epoch 27, Batch 1000/1000: LR=1.00e-04, Loss=3.92e-02 BER=1.44e-02 FER=1.98e-01
2025-10-18 16:07:53,437 | INFO | Epoch 27 Train Time 23.714221715927124s

2025-10-18 16:07:53,438 | INFO | [P1] saving best_model with loss 0.039239 at epoch 27
2025-10-18 16:08:16,739 | INFO | Training epoch 28, Batch 1000/1000: LR=1.00e-04, Loss=3.92e-02 BER=1.46e-02 FER=1.99e-01
2025-10-18 16:08:16,838 | INFO | Epoch 28 Train Time 23.383646965026855s

2025-10-18 16:08:16,838 | INFO | [P1] saving best_model with loss 0.039223 at epoch 28
2025-10-18 16:08:40,030 | INFO | Training epoch 29, Batch 1000/1000: LR=1.00e-04, Loss=3.88e-02 BER=1.45e-02 FER=1.97e-01
2025-10-18 16:08:40,106 | INFO | Epoch 29 Train Time 23.25121283531189s

2025-10-18 16:08:40,107 | INFO | [P1] saving best_model with loss 0.038831 at epoch 29
2025-10-18 16:09:03,761 | INFO | Training epoch 30, Batch 1000/1000: LR=9.99e-05, Loss=3.81e-02 BER=1.41e-02 FER=1.92e-01
2025-10-18 16:09:03,844 | INFO | Epoch 30 Train Time 23.723108530044556s

2025-10-18 16:09:03,844 | INFO | [P1] saving best_model with loss 0.038130 at epoch 30
2025-10-18 16:09:27,555 | INFO | Training epoch 31, Batch 1000/1000: LR=9.99e-05, Loss=3.82e-02 BER=1.42e-02 FER=1.93e-01
2025-10-18 16:09:27,632 | INFO | Epoch 31 Train Time 23.769641160964966s

2025-10-18 16:09:51,539 | INFO | Training epoch 32, Batch 1000/1000: LR=9.99e-05, Loss=3.77e-02 BER=1.39e-02 FER=1.90e-01
2025-10-18 16:09:51,622 | INFO | Epoch 32 Train Time 23.98844289779663s

2025-10-18 16:09:51,623 | INFO | [P1] saving best_model with loss 0.037697 at epoch 32
2025-10-18 16:10:15,634 | INFO | Training epoch 33, Batch 1000/1000: LR=9.99e-05, Loss=3.72e-02 BER=1.39e-02 FER=1.89e-01
2025-10-18 16:10:15,710 | INFO | Epoch 33 Train Time 24.061066150665283s

2025-10-18 16:10:15,712 | INFO | [P1] saving best_model with loss 0.037166 at epoch 33
2025-10-18 16:10:39,038 | INFO | Training epoch 34, Batch 1000/1000: LR=9.99e-05, Loss=3.73e-02 BER=1.38e-02 FER=1.89e-01
2025-10-18 16:10:39,126 | INFO | Epoch 34 Train Time 23.39632797241211s

2025-10-18 16:11:02,725 | INFO | Training epoch 35, Batch 1000/1000: LR=9.99e-05, Loss=3.71e-02 BER=1.39e-02 FER=1.88e-01
2025-10-18 16:11:02,804 | INFO | Epoch 35 Train Time 23.677740573883057s

2025-10-18 16:11:02,807 | INFO | [P1] saving best_model with loss 0.037052 at epoch 35
2025-10-18 16:11:25,943 | INFO | Training epoch 36, Batch 1000/1000: LR=9.99e-05, Loss=3.68e-02 BER=1.38e-02 FER=1.87e-01
2025-10-18 16:11:26,018 | INFO | Epoch 36 Train Time 23.19527530670166s

2025-10-18 16:11:26,018 | INFO | [P1] saving best_model with loss 0.036809 at epoch 36
2025-10-18 16:11:49,044 | INFO | Training epoch 37, Batch 1000/1000: LR=9.99e-05, Loss=3.66e-02 BER=1.37e-02 FER=1.87e-01
2025-10-18 16:11:49,127 | INFO | Epoch 37 Train Time 23.09250545501709s

2025-10-18 16:11:49,129 | INFO | [P1] saving best_model with loss 0.036638 at epoch 37
2025-10-18 16:12:12,556 | INFO | Training epoch 38, Batch 1000/1000: LR=9.99e-05, Loss=3.70e-02 BER=1.38e-02 FER=1.87e-01
2025-10-18 16:12:12,612 | INFO | Epoch 38 Train Time 23.458165884017944s

2025-10-18 16:12:33,095 | INFO | Training epoch 39, Batch 1000/1000: LR=9.99e-05, Loss=3.70e-02 BER=1.38e-02 FER=1.87e-01
2025-10-18 16:12:33,163 | INFO | Epoch 39 Train Time 20.550976514816284s

2025-10-18 16:12:56,546 | INFO | Training epoch 40, Batch 1000/1000: LR=9.99e-05, Loss=3.64e-02 BER=1.35e-02 FER=1.84e-01
2025-10-18 16:12:56,618 | INFO | Epoch 40 Train Time 23.453736305236816s

2025-10-18 16:12:56,619 | INFO | [P1] saving best_model with loss 0.036372 at epoch 40
2025-10-18 16:13:20,255 | INFO | Training epoch 41, Batch 1000/1000: LR=9.99e-05, Loss=3.58e-02 BER=1.33e-02 FER=1.81e-01
2025-10-18 16:13:20,358 | INFO | Epoch 41 Train Time 23.714489936828613s

2025-10-18 16:13:20,358 | INFO | [P1] saving best_model with loss 0.035774 at epoch 41
2025-10-18 16:13:44,097 | INFO | Training epoch 42, Batch 1000/1000: LR=9.99e-05, Loss=3.56e-02 BER=1.33e-02 FER=1.81e-01
2025-10-18 16:13:44,186 | INFO | Epoch 42 Train Time 23.809438467025757s

2025-10-18 16:13:44,187 | INFO | [P1] saving best_model with loss 0.035606 at epoch 42
2025-10-18 16:14:07,834 | INFO | Training epoch 43, Batch 1000/1000: LR=9.99e-05, Loss=3.58e-02 BER=1.34e-02 FER=1.80e-01
2025-10-18 16:14:07,917 | INFO | Epoch 43 Train Time 23.710659980773926s

2025-10-18 16:14:31,423 | INFO | Training epoch 44, Batch 1000/1000: LR=9.99e-05, Loss=3.57e-02 BER=1.33e-02 FER=1.80e-01
2025-10-18 16:14:31,495 | INFO | Epoch 44 Train Time 23.57763671875s

2025-10-18 16:14:55,138 | INFO | Training epoch 45, Batch 1000/1000: LR=9.99e-05, Loss=3.55e-02 BER=1.33e-02 FER=1.79e-01
2025-10-18 16:14:55,232 | INFO | Epoch 45 Train Time 23.73620080947876s

2025-10-18 16:14:55,233 | INFO | [P1] saving best_model with loss 0.035494 at epoch 45
2025-10-18 16:15:16,845 | INFO | Training epoch 46, Batch 1000/1000: LR=9.99e-05, Loss=3.51e-02 BER=1.31e-02 FER=1.79e-01
2025-10-18 16:15:16,930 | INFO | Epoch 46 Train Time 21.683938026428223s

2025-10-18 16:15:16,931 | INFO | [P1] saving best_model with loss 0.035058 at epoch 46
2025-10-18 16:15:40,658 | INFO | Training epoch 47, Batch 1000/1000: LR=9.99e-05, Loss=3.50e-02 BER=1.31e-02 FER=1.79e-01
2025-10-18 16:15:40,750 | INFO | Epoch 47 Train Time 23.80256152153015s

2025-10-18 16:15:40,751 | INFO | [P1] saving best_model with loss 0.034996 at epoch 47
2025-10-18 16:16:04,145 | INFO | Training epoch 48, Batch 1000/1000: LR=9.99e-05, Loss=3.53e-02 BER=1.32e-02 FER=1.79e-01
2025-10-18 16:16:04,227 | INFO | Epoch 48 Train Time 23.444340229034424s

2025-10-18 16:16:28,565 | INFO | Training epoch 49, Batch 1000/1000: LR=9.99e-05, Loss=3.50e-02 BER=1.31e-02 FER=1.77e-01
2025-10-18 16:16:28,633 | INFO | Epoch 49 Train Time 24.40422248840332s

2025-10-18 16:16:28,633 | INFO | [P1] saving best_model with loss 0.034978 at epoch 49
2025-10-18 16:16:52,046 | INFO | Training epoch 50, Batch 1000/1000: LR=9.99e-05, Loss=3.52e-02 BER=1.32e-02 FER=1.79e-01
2025-10-18 16:16:52,119 | INFO | Epoch 50 Train Time 23.46969985961914s

2025-10-18 16:17:15,257 | INFO | Training epoch 51, Batch 1000/1000: LR=9.98e-05, Loss=3.49e-02 BER=1.31e-02 FER=1.79e-01
2025-10-18 16:17:15,347 | INFO | Epoch 51 Train Time 23.226914405822754s

2025-10-18 16:17:15,347 | INFO | [P1] saving best_model with loss 0.034901 at epoch 51
2025-10-18 16:17:38,652 | INFO | Training epoch 52, Batch 1000/1000: LR=9.98e-05, Loss=3.46e-02 BER=1.30e-02 FER=1.77e-01
2025-10-18 16:17:38,737 | INFO | Epoch 52 Train Time 23.371963500976562s

2025-10-18 16:17:38,737 | INFO | [P1] saving best_model with loss 0.034604 at epoch 52
2025-10-18 16:18:01,919 | INFO | Training epoch 53, Batch 1000/1000: LR=9.98e-05, Loss=3.50e-02 BER=1.31e-02 FER=1.77e-01
2025-10-18 16:18:02,009 | INFO | Epoch 53 Train Time 23.248902082443237s

2025-10-18 16:18:25,549 | INFO | Training epoch 54, Batch 1000/1000: LR=9.98e-05, Loss=3.46e-02 BER=1.30e-02 FER=1.76e-01
2025-10-18 16:18:25,627 | INFO | Epoch 54 Train Time 23.616400480270386s

2025-10-18 16:18:50,004 | INFO | Training epoch 55, Batch 1000/1000: LR=9.98e-05, Loss=3.43e-02 BER=1.29e-02 FER=1.74e-01
2025-10-18 16:18:50,084 | INFO | Epoch 55 Train Time 24.455674409866333s

2025-10-18 16:18:50,084 | INFO | [P1] saving best_model with loss 0.034346 at epoch 55
2025-10-18 16:19:13,240 | INFO | Training epoch 56, Batch 1000/1000: LR=9.98e-05, Loss=3.44e-02 BER=1.29e-02 FER=1.75e-01
2025-10-18 16:19:13,316 | INFO | Epoch 56 Train Time 23.21966814994812s

2025-10-18 16:19:36,040 | INFO | Training epoch 57, Batch 1000/1000: LR=9.98e-05, Loss=3.46e-02 BER=1.29e-02 FER=1.74e-01
2025-10-18 16:19:36,132 | INFO | Epoch 57 Train Time 22.813525676727295s

2025-10-18 16:19:59,538 | INFO | Training epoch 58, Batch 1000/1000: LR=9.98e-05, Loss=3.44e-02 BER=1.28e-02 FER=1.74e-01
2025-10-18 16:19:59,621 | INFO | Epoch 58 Train Time 23.487483024597168s

2025-10-18 16:20:23,548 | INFO | Training epoch 59, Batch 1000/1000: LR=9.98e-05, Loss=3.42e-02 BER=1.28e-02 FER=1.73e-01
2025-10-18 16:20:23,646 | INFO | Epoch 59 Train Time 24.02366304397583s

2025-10-18 16:20:23,647 | INFO | [P1] saving best_model with loss 0.034168 at epoch 59
2025-10-18 16:20:46,030 | INFO | Training epoch 60, Batch 1000/1000: LR=9.98e-05, Loss=3.38e-02 BER=1.27e-02 FER=1.73e-01
2025-10-18 16:20:46,107 | INFO | Epoch 60 Train Time 22.436227321624756s

2025-10-18 16:20:46,108 | INFO | [P1] saving best_model with loss 0.033809 at epoch 60
2025-10-18 16:21:08,931 | INFO | Training epoch 61, Batch 1000/1000: LR=9.98e-05, Loss=3.39e-02 BER=1.28e-02 FER=1.73e-01
2025-10-18 16:21:09,008 | INFO | Epoch 61 Train Time 22.878201007843018s

2025-10-18 16:21:32,158 | INFO | Training epoch 62, Batch 1000/1000: LR=9.98e-05, Loss=3.40e-02 BER=1.28e-02 FER=1.73e-01
2025-10-18 16:21:32,257 | INFO | Epoch 62 Train Time 23.248642206192017s

2025-10-18 16:21:55,013 | INFO | Training epoch 63, Batch 1000/1000: LR=9.98e-05, Loss=3.41e-02 BER=1.27e-02 FER=1.72e-01
2025-10-18 16:21:55,105 | INFO | Epoch 63 Train Time 22.846676349639893s

2025-10-18 16:22:17,147 | INFO | Training epoch 64, Batch 1000/1000: LR=9.98e-05, Loss=3.38e-02 BER=1.27e-02 FER=1.72e-01
2025-10-18 16:22:17,220 | INFO | Epoch 64 Train Time 22.112849950790405s

2025-10-18 16:22:40,424 | INFO | Training epoch 65, Batch 1000/1000: LR=9.98e-05, Loss=3.38e-02 BER=1.27e-02 FER=1.71e-01
2025-10-18 16:22:40,522 | INFO | Epoch 65 Train Time 23.299352407455444s

2025-10-18 16:23:03,431 | INFO | Training epoch 66, Batch 1000/1000: LR=9.97e-05, Loss=3.38e-02 BER=1.27e-02 FER=1.71e-01
2025-10-18 16:23:03,511 | INFO | Epoch 66 Train Time 22.988505363464355s

2025-10-18 16:23:03,512 | INFO | [P1] saving best_model with loss 0.033772 at epoch 66
2025-10-18 16:23:26,661 | INFO | Training epoch 67, Batch 1000/1000: LR=9.97e-05, Loss=3.38e-02 BER=1.27e-02 FER=1.71e-01
2025-10-18 16:23:26,739 | INFO | Epoch 67 Train Time 23.20740246772766s

2025-10-18 16:23:50,144 | INFO | Training epoch 68, Batch 1000/1000: LR=9.97e-05, Loss=3.36e-02 BER=1.26e-02 FER=1.70e-01
2025-10-18 16:23:50,220 | INFO | Epoch 68 Train Time 23.479828119277954s

2025-10-18 16:23:50,221 | INFO | [P1] saving best_model with loss 0.033602 at epoch 68
2025-10-18 16:24:13,512 | INFO | Training epoch 69, Batch 1000/1000: LR=9.97e-05, Loss=3.32e-02 BER=1.24e-02 FER=1.69e-01
2025-10-18 16:24:13,599 | INFO | Epoch 69 Train Time 23.3634033203125s

2025-10-18 16:24:13,600 | INFO | [P1] saving best_model with loss 0.033153 at epoch 69
2025-10-18 16:24:36,658 | INFO | Training epoch 70, Batch 1000/1000: LR=9.97e-05, Loss=3.34e-02 BER=1.26e-02 FER=1.70e-01
2025-10-18 16:24:36,743 | INFO | Epoch 70 Train Time 23.123995542526245s

2025-10-18 16:25:00,834 | INFO | Training epoch 71, Batch 1000/1000: LR=9.97e-05, Loss=3.31e-02 BER=1.24e-02 FER=1.69e-01
2025-10-18 16:25:00,915 | INFO | Epoch 71 Train Time 24.17127799987793s

2025-10-18 16:25:00,916 | INFO | [P1] saving best_model with loss 0.033124 at epoch 71
2025-10-18 16:25:24,655 | INFO | Training epoch 72, Batch 1000/1000: LR=9.97e-05, Loss=3.30e-02 BER=1.24e-02 FER=1.67e-01
2025-10-18 16:25:24,749 | INFO | Epoch 72 Train Time 23.819871425628662s

2025-10-18 16:25:24,749 | INFO | [P1] saving best_model with loss 0.032950 at epoch 72
2025-10-18 16:25:47,941 | INFO | Training epoch 73, Batch 1000/1000: LR=9.97e-05, Loss=3.34e-02 BER=1.25e-02 FER=1.69e-01
2025-10-18 16:25:48,029 | INFO | Epoch 73 Train Time 23.242379426956177s

2025-10-18 16:26:11,533 | INFO | Training epoch 74, Batch 1000/1000: LR=9.97e-05, Loss=3.28e-02 BER=1.24e-02 FER=1.69e-01
2025-10-18 16:26:11,603 | INFO | Epoch 74 Train Time 23.5722234249115s

2025-10-18 16:26:11,603 | INFO | [P1] saving best_model with loss 0.032802 at epoch 74
2025-10-18 16:26:31,633 | INFO | Training epoch 75, Batch 1000/1000: LR=9.97e-05, Loss=3.35e-02 BER=1.26e-02 FER=1.69e-01
2025-10-18 16:26:31,706 | INFO | Epoch 75 Train Time 20.08731961250305s

2025-10-18 16:26:55,220 | INFO | Training epoch 76, Batch 1000/1000: LR=9.97e-05, Loss=3.31e-02 BER=1.24e-02 FER=1.68e-01
2025-10-18 16:26:55,293 | INFO | Epoch 76 Train Time 23.58618712425232s

2025-10-18 16:27:18,952 | INFO | Training epoch 77, Batch 1000/1000: LR=9.96e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.66e-01
2025-10-18 16:27:19,024 | INFO | Epoch 77 Train Time 23.73039174079895s

2025-10-18 16:27:19,026 | INFO | [P1] saving best_model with loss 0.032715 at epoch 77
2025-10-18 16:27:42,951 | INFO | Training epoch 78, Batch 1000/1000: LR=9.96e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.66e-01
2025-10-18 16:27:43,035 | INFO | Epoch 78 Train Time 23.982585430145264s

2025-10-18 16:28:06,728 | INFO | Training epoch 79, Batch 1000/1000: LR=9.96e-05, Loss=3.34e-02 BER=1.26e-02 FER=1.68e-01
2025-10-18 16:28:06,810 | INFO | Epoch 79 Train Time 23.773984670639038s

2025-10-18 16:28:29,836 | INFO | Training epoch 80, Batch 1000/1000: LR=9.96e-05, Loss=3.30e-02 BER=1.23e-02 FER=1.67e-01
2025-10-18 16:28:29,908 | INFO | Epoch 80 Train Time 23.09722590446472s

2025-10-18 16:28:53,223 | INFO | Training epoch 81, Batch 1000/1000: LR=9.96e-05, Loss=3.31e-02 BER=1.25e-02 FER=1.68e-01
2025-10-18 16:28:53,314 | INFO | Epoch 81 Train Time 23.403249979019165s

2025-10-18 16:29:16,916 | INFO | Training epoch 82, Batch 1000/1000: LR=9.96e-05, Loss=3.30e-02 BER=1.23e-02 FER=1.65e-01
2025-10-18 16:29:17,007 | INFO | Epoch 82 Train Time 23.691755056381226s

2025-10-18 16:29:40,230 | INFO | Training epoch 83, Batch 1000/1000: LR=9.96e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.66e-01
2025-10-18 16:29:40,304 | INFO | Epoch 83 Train Time 23.29552412033081s

2025-10-18 16:29:40,306 | INFO | [P1] saving best_model with loss 0.032671 at epoch 83
2025-10-18 16:30:04,268 | INFO | Training epoch 84, Batch 1000/1000: LR=9.96e-05, Loss=3.29e-02 BER=1.23e-02 FER=1.66e-01
2025-10-18 16:30:04,359 | INFO | Epoch 84 Train Time 24.036399841308594s

2025-10-18 16:30:27,458 | INFO | Training epoch 85, Batch 1000/1000: LR=9.96e-05, Loss=3.28e-02 BER=1.24e-02 FER=1.66e-01
2025-10-18 16:30:27,544 | INFO | Epoch 85 Train Time 23.18231201171875s

2025-10-18 16:30:49,581 | INFO | Training epoch 86, Batch 1000/1000: LR=9.96e-05, Loss=3.30e-02 BER=1.24e-02 FER=1.67e-01
2025-10-18 16:30:49,669 | INFO | Epoch 86 Train Time 22.123453855514526s

2025-10-18 16:31:13,197 | INFO | Training epoch 87, Batch 1000/1000: LR=9.95e-05, Loss=3.32e-02 BER=1.25e-02 FER=1.67e-01
2025-10-18 16:31:13,286 | INFO | Epoch 87 Train Time 23.614357471466064s

2025-10-18 16:31:36,957 | INFO | Training epoch 88, Batch 1000/1000: LR=9.95e-05, Loss=3.29e-02 BER=1.24e-02 FER=1.67e-01
2025-10-18 16:31:37,044 | INFO | Epoch 88 Train Time 23.755886554718018s

2025-10-18 16:32:00,558 | INFO | Training epoch 89, Batch 1000/1000: LR=9.95e-05, Loss=3.21e-02 BER=1.21e-02 FER=1.64e-01
2025-10-18 16:32:00,653 | INFO | Epoch 89 Train Time 23.608514070510864s

2025-10-18 16:32:00,653 | INFO | [P1] saving best_model with loss 0.032114 at epoch 89
2025-10-18 16:32:23,227 | INFO | Training epoch 90, Batch 1000/1000: LR=9.95e-05, Loss=3.24e-02 BER=1.22e-02 FER=1.64e-01
2025-10-18 16:32:23,303 | INFO | Epoch 90 Train Time 22.627272367477417s

2025-10-18 16:32:46,537 | INFO | Training epoch 91, Batch 1000/1000: LR=9.95e-05, Loss=3.25e-02 BER=1.22e-02 FER=1.65e-01
2025-10-18 16:32:46,611 | INFO | Epoch 91 Train Time 23.30699062347412s

2025-10-18 16:33:10,512 | INFO | Training epoch 92, Batch 1000/1000: LR=9.95e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.66e-01
2025-10-18 16:33:10,596 | INFO | Epoch 92 Train Time 23.984407663345337s

2025-10-18 16:33:33,917 | INFO | Training epoch 93, Batch 1000/1000: LR=9.95e-05, Loss=3.30e-02 BER=1.24e-02 FER=1.66e-01
2025-10-18 16:33:34,006 | INFO | Epoch 93 Train Time 23.40787100791931s

2025-10-18 16:33:58,357 | INFO | Training epoch 94, Batch 1000/1000: LR=9.95e-05, Loss=3.26e-02 BER=1.22e-02 FER=1.64e-01
2025-10-18 16:33:58,442 | INFO | Epoch 94 Train Time 24.434632301330566s

2025-10-18 16:34:21,747 | INFO | Training epoch 95, Batch 1000/1000: LR=9.95e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.64e-01
2025-10-18 16:34:21,838 | INFO | Epoch 95 Train Time 23.39448094367981s

2025-10-18 16:34:45,021 | INFO | Training epoch 96, Batch 1000/1000: LR=9.94e-05, Loss=3.26e-02 BER=1.22e-02 FER=1.64e-01
2025-10-18 16:34:45,088 | INFO | Epoch 96 Train Time 23.249488353729248s

2025-10-18 16:35:08,057 | INFO | Training epoch 97, Batch 1000/1000: LR=9.94e-05, Loss=3.23e-02 BER=1.22e-02 FER=1.64e-01
2025-10-18 16:35:08,144 | INFO | Epoch 97 Train Time 23.055601358413696s

2025-10-18 16:35:31,154 | INFO | Training epoch 98, Batch 1000/1000: LR=9.94e-05, Loss=3.21e-02 BER=1.21e-02 FER=1.62e-01
2025-10-18 16:35:31,234 | INFO | Epoch 98 Train Time 23.088814735412598s

2025-10-18 16:35:31,235 | INFO | [P1] saving best_model with loss 0.032083 at epoch 98
2025-10-18 16:35:54,817 | INFO | Training epoch 99, Batch 1000/1000: LR=9.94e-05, Loss=3.23e-02 BER=1.22e-02 FER=1.63e-01
2025-10-18 16:35:54,909 | INFO | Epoch 99 Train Time 23.66167116165161s

2025-10-18 16:36:19,330 | INFO | Training epoch 100, Batch 1000/1000: LR=9.94e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.62e-01
2025-10-18 16:36:19,415 | INFO | Epoch 100 Train Time 24.5045223236084s

2025-10-18 16:36:43,099 | INFO | Training epoch 101, Batch 1000/1000: LR=9.94e-05, Loss=3.24e-02 BER=1.22e-02 FER=1.64e-01
2025-10-18 16:36:43,187 | INFO | Epoch 101 Train Time 23.77008605003357s

2025-10-18 16:37:04,058 | INFO | Training epoch 102, Batch 1000/1000: LR=9.94e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.61e-01
2025-10-18 16:37:04,142 | INFO | Epoch 102 Train Time 20.95389223098755s

2025-10-18 16:37:27,627 | INFO | Training epoch 103, Batch 1000/1000: LR=9.94e-05, Loss=3.22e-02 BER=1.22e-02 FER=1.63e-01
2025-10-18 16:37:27,728 | INFO | Epoch 103 Train Time 23.585683345794678s

2025-10-18 16:37:50,807 | INFO | Training epoch 104, Batch 1000/1000: LR=9.94e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.63e-01
2025-10-18 16:37:50,907 | INFO | Epoch 104 Train Time 23.175660610198975s

2025-10-18 16:38:15,075 | INFO | Training epoch 105, Batch 1000/1000: LR=9.93e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.62e-01
2025-10-18 16:38:15,161 | INFO | Epoch 105 Train Time 24.25168204307556s

2025-10-18 16:38:37,635 | INFO | Training epoch 106, Batch 1000/1000: LR=9.93e-05, Loss=3.22e-02 BER=1.22e-02 FER=1.62e-01
2025-10-18 16:38:37,717 | INFO | Epoch 106 Train Time 22.5547993183136s

2025-10-18 16:39:01,047 | INFO | Training epoch 107, Batch 1000/1000: LR=9.93e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.60e-01
2025-10-18 16:39:01,150 | INFO | Epoch 107 Train Time 23.43000030517578s

2025-10-18 16:39:01,152 | INFO | [P1] saving best_model with loss 0.031652 at epoch 107
2025-10-18 16:39:24,758 | INFO | Training epoch 108, Batch 1000/1000: LR=9.93e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.60e-01
2025-10-18 16:39:24,836 | INFO | Epoch 108 Train Time 23.671319723129272s

2025-10-18 16:39:46,844 | INFO | Training epoch 109, Batch 1000/1000: LR=9.93e-05, Loss=3.18e-02 BER=1.20e-02 FER=1.61e-01
2025-10-18 16:39:46,932 | INFO | Epoch 109 Train Time 22.093284845352173s

2025-10-18 16:40:10,138 | INFO | Training epoch 110, Batch 1000/1000: LR=9.93e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.61e-01
2025-10-18 16:40:10,225 | INFO | Epoch 110 Train Time 23.292635440826416s

2025-10-18 16:40:33,351 | INFO | Training epoch 111, Batch 1000/1000: LR=9.93e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.61e-01
2025-10-18 16:40:33,430 | INFO | Epoch 111 Train Time 23.203284978866577s

2025-10-18 16:40:56,920 | INFO | Training epoch 112, Batch 1000/1000: LR=9.92e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.60e-01
2025-10-18 16:40:57,013 | INFO | Epoch 112 Train Time 23.58248257637024s

2025-10-18 16:41:21,246 | INFO | Training epoch 113, Batch 1000/1000: LR=9.92e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.61e-01
2025-10-18 16:41:21,329 | INFO | Epoch 113 Train Time 24.314165353775024s

2025-10-18 16:41:44,836 | INFO | Training epoch 114, Batch 1000/1000: LR=9.92e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.61e-01
2025-10-18 16:41:44,917 | INFO | Epoch 114 Train Time 23.58723545074463s

2025-10-18 16:42:05,230 | INFO | Training epoch 115, Batch 1000/1000: LR=9.92e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.61e-01
2025-10-18 16:42:05,302 | INFO | Epoch 115 Train Time 20.384300231933594s

2025-10-18 16:42:29,005 | INFO | Training epoch 116, Batch 1000/1000: LR=9.92e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.60e-01
2025-10-18 16:42:29,072 | INFO | Epoch 116 Train Time 23.767889499664307s

2025-10-18 16:42:52,445 | INFO | Training epoch 117, Batch 1000/1000: LR=9.92e-05, Loss=3.19e-02 BER=1.21e-02 FER=1.60e-01
2025-10-18 16:42:52,537 | INFO | Epoch 117 Train Time 23.464457035064697s

2025-10-18 16:43:16,140 | INFO | Training epoch 118, Batch 1000/1000: LR=9.92e-05, Loss=3.22e-02 BER=1.20e-02 FER=1.60e-01
2025-10-18 16:43:16,230 | INFO | Epoch 118 Train Time 23.691889762878418s

2025-10-18 16:43:39,744 | INFO | Training epoch 119, Batch 1000/1000: LR=9.92e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.59e-01
2025-10-18 16:43:39,842 | INFO | Epoch 119 Train Time 23.610578060150146s

2025-10-18 16:44:03,120 | INFO | Training epoch 120, Batch 1000/1000: LR=9.91e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.59e-01
2025-10-18 16:44:03,194 | INFO | Epoch 120 Train Time 23.35071611404419s

2025-10-18 16:44:25,846 | INFO | Training epoch 121, Batch 1000/1000: LR=9.91e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.59e-01
2025-10-18 16:44:25,921 | INFO | Epoch 121 Train Time 22.725255966186523s

2025-10-18 16:44:25,921 | INFO | [P1] saving best_model with loss 0.031501 at epoch 121
2025-10-18 16:44:49,228 | INFO | Training epoch 122, Batch 1000/1000: LR=9.91e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.60e-01
2025-10-18 16:44:49,313 | INFO | Epoch 122 Train Time 23.37794303894043s

2025-10-18 16:45:12,544 | INFO | Training epoch 123, Batch 1000/1000: LR=9.91e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.58e-01
2025-10-18 16:45:12,625 | INFO | Epoch 123 Train Time 23.31016254425049s

2025-10-18 16:45:12,626 | INFO | [P1] saving best_model with loss 0.031343 at epoch 123
2025-10-18 16:45:36,807 | INFO | Training epoch 124, Batch 1000/1000: LR=9.91e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.59e-01
2025-10-18 16:45:36,877 | INFO | Epoch 124 Train Time 24.23074197769165s

2025-10-18 16:46:00,123 | INFO | Training epoch 125, Batch 1000/1000: LR=9.91e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:46:00,194 | INFO | Epoch 125 Train Time 23.31610631942749s

2025-10-18 16:46:23,807 | INFO | Training epoch 126, Batch 1000/1000: LR=9.90e-05, Loss=3.15e-02 BER=1.19e-02 FER=1.59e-01
2025-10-18 16:46:23,893 | INFO | Epoch 126 Train Time 23.6971378326416s

2025-10-18 16:46:47,845 | INFO | Training epoch 127, Batch 1000/1000: LR=9.90e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:46:47,934 | INFO | Epoch 127 Train Time 24.03990387916565s

2025-10-18 16:47:10,636 | INFO | Training epoch 128, Batch 1000/1000: LR=9.90e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:47:10,717 | INFO | Epoch 128 Train Time 22.782459020614624s

2025-10-18 16:47:10,718 | INFO | [P1] saving best_model with loss 0.031296 at epoch 128
2025-10-18 16:47:33,437 | INFO | Training epoch 129, Batch 1000/1000: LR=9.90e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.58e-01
2025-10-18 16:47:33,511 | INFO | Epoch 129 Train Time 22.77261447906494s

2025-10-18 16:47:57,429 | INFO | Training epoch 130, Batch 1000/1000: LR=9.90e-05, Loss=3.15e-02 BER=1.19e-02 FER=1.60e-01
2025-10-18 16:47:57,515 | INFO | Epoch 130 Train Time 24.002869606018066s

2025-10-18 16:48:21,465 | INFO | Training epoch 131, Batch 1000/1000: LR=9.90e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:48:21,548 | INFO | Epoch 131 Train Time 24.032036304473877s

2025-10-18 16:48:43,621 | INFO | Training epoch 132, Batch 1000/1000: LR=9.90e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-18 16:48:43,692 | INFO | Epoch 132 Train Time 22.140775442123413s

2025-10-18 16:49:05,504 | INFO | Training epoch 133, Batch 1000/1000: LR=9.89e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:49:05,592 | INFO | Epoch 133 Train Time 21.89979338645935s

2025-10-18 16:49:29,560 | INFO | Training epoch 134, Batch 1000/1000: LR=9.89e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.55e-01
2025-10-18 16:49:29,637 | INFO | Epoch 134 Train Time 24.044209957122803s

2025-10-18 16:49:29,638 | INFO | [P1] saving best_model with loss 0.030949 at epoch 134
2025-10-18 16:49:52,266 | INFO | Training epoch 135, Batch 1000/1000: LR=9.89e-05, Loss=3.11e-02 BER=1.18e-02 FER=1.56e-01
2025-10-18 16:49:52,340 | INFO | Epoch 135 Train Time 22.686039209365845s

2025-10-18 16:50:15,433 | INFO | Training epoch 136, Batch 1000/1000: LR=9.89e-05, Loss=3.14e-02 BER=1.19e-02 FER=1.57e-01
2025-10-18 16:50:15,512 | INFO | Epoch 136 Train Time 23.170662879943848s

2025-10-18 16:50:38,132 | INFO | Training epoch 137, Batch 1000/1000: LR=9.89e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.58e-01
2025-10-18 16:50:38,214 | INFO | Epoch 137 Train Time 22.701709747314453s

2025-10-18 16:51:02,335 | INFO | Training epoch 138, Batch 1000/1000: LR=9.89e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:51:02,409 | INFO | Epoch 138 Train Time 24.193780183792114s

2025-10-18 16:51:25,856 | INFO | Training epoch 139, Batch 1000/1000: LR=9.88e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.56e-01
2025-10-18 16:51:25,953 | INFO | Epoch 139 Train Time 23.54205584526062s

2025-10-18 16:51:49,439 | INFO | Training epoch 140, Batch 1000/1000: LR=9.88e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:51:49,518 | INFO | Epoch 140 Train Time 23.564652919769287s

2025-10-18 16:52:11,964 | INFO | Training epoch 141, Batch 1000/1000: LR=9.88e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.57e-01
2025-10-18 16:52:12,035 | INFO | Epoch 141 Train Time 22.51593852043152s

2025-10-18 16:52:35,425 | INFO | Training epoch 142, Batch 1000/1000: LR=9.88e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.56e-01
2025-10-18 16:52:35,509 | INFO | Epoch 142 Train Time 23.47253656387329s

2025-10-18 16:52:58,416 | INFO | Training epoch 143, Batch 1000/1000: LR=9.88e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-18 16:52:58,507 | INFO | Epoch 143 Train Time 22.996917963027954s

2025-10-18 16:53:21,026 | INFO | Training epoch 144, Batch 1000/1000: LR=9.88e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-18 16:53:21,109 | INFO | Epoch 144 Train Time 22.600627660751343s

2025-10-18 16:53:45,205 | INFO | Training epoch 145, Batch 1000/1000: LR=9.87e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-18 16:53:45,293 | INFO | Epoch 145 Train Time 24.18230128288269s

2025-10-18 16:54:08,256 | INFO | Training epoch 146, Batch 1000/1000: LR=9.87e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.56e-01
2025-10-18 16:54:08,337 | INFO | Epoch 146 Train Time 23.04332208633423s

2025-10-18 16:54:31,504 | INFO | Training epoch 147, Batch 1000/1000: LR=9.87e-05, Loss=3.12e-02 BER=1.18e-02 FER=1.57e-01
2025-10-18 16:54:31,594 | INFO | Epoch 147 Train Time 23.25545334815979s

2025-10-18 16:54:54,337 | INFO | Training epoch 148, Batch 1000/1000: LR=9.87e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.54e-01
2025-10-18 16:54:54,417 | INFO | Epoch 148 Train Time 22.821707725524902s

2025-10-18 16:54:54,418 | INFO | [P1] saving best_model with loss 0.030626 at epoch 148
2025-10-18 16:55:17,665 | INFO | Training epoch 149, Batch 1000/1000: LR=9.87e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-18 16:55:17,743 | INFO | Epoch 149 Train Time 23.30985188484192s

2025-10-18 16:55:41,930 | INFO | Training epoch 150, Batch 1000/1000: LR=9.87e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.56e-01
2025-10-18 16:55:42,020 | INFO | Epoch 150 Train Time 24.275710582733154s

2025-10-18 16:56:04,951 | INFO | Training epoch 151, Batch 1000/1000: LR=9.86e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-18 16:56:05,033 | INFO | Epoch 151 Train Time 23.01223874092102s

2025-10-18 16:56:27,938 | INFO | Training epoch 152, Batch 1000/1000: LR=9.86e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-18 16:56:28,029 | INFO | Epoch 152 Train Time 22.993684768676758s

2025-10-18 16:56:50,941 | INFO | Training epoch 153, Batch 1000/1000: LR=9.86e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-18 16:56:51,021 | INFO | Epoch 153 Train Time 22.990696668624878s

2025-10-18 16:57:14,917 | INFO | Training epoch 154, Batch 1000/1000: LR=9.86e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-18 16:57:15,014 | INFO | Epoch 154 Train Time 23.99200415611267s

2025-10-18 16:57:38,140 | INFO | Training epoch 155, Batch 1000/1000: LR=9.86e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.54e-01
2025-10-18 16:57:38,219 | INFO | Epoch 155 Train Time 23.20402956008911s

2025-10-18 16:58:01,214 | INFO | Training epoch 156, Batch 1000/1000: LR=9.85e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-18 16:58:01,290 | INFO | Epoch 156 Train Time 23.06874990463257s

2025-10-18 16:58:24,643 | INFO | Training epoch 157, Batch 1000/1000: LR=9.85e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.54e-01
2025-10-18 16:58:24,732 | INFO | Epoch 157 Train Time 23.440998554229736s

2025-10-18 16:58:47,734 | INFO | Training epoch 158, Batch 1000/1000: LR=9.85e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.54e-01
2025-10-18 16:58:47,820 | INFO | Epoch 158 Train Time 23.086596488952637s

2025-10-18 16:58:47,820 | INFO | [P1] saving best_model with loss 0.030522 at epoch 158
2025-10-18 16:59:11,225 | INFO | Training epoch 159, Batch 1000/1000: LR=9.85e-05, Loss=3.08e-02 BER=1.17e-02 FER=1.55e-01
2025-10-18 16:59:11,303 | INFO | Epoch 159 Train Time 23.45999574661255s

2025-10-18 16:59:34,240 | INFO | Training epoch 160, Batch 1000/1000: LR=9.85e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-18 16:59:34,314 | INFO | Epoch 160 Train Time 23.009738206863403s

2025-10-18 16:59:57,932 | INFO | Training epoch 161, Batch 1000/1000: LR=9.84e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-18 16:59:58,023 | INFO | Epoch 161 Train Time 23.70811653137207s

2025-10-18 17:00:21,247 | INFO | Training epoch 162, Batch 1000/1000: LR=9.84e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-18 17:00:21,337 | INFO | Epoch 162 Train Time 23.313605546951294s

2025-10-18 17:00:44,061 | INFO | Training epoch 163, Batch 1000/1000: LR=9.84e-05, Loss=3.09e-02 BER=1.17e-02 FER=1.55e-01
2025-10-18 17:00:44,154 | INFO | Epoch 163 Train Time 22.815130472183228s

2025-10-18 17:01:06,807 | INFO | Training epoch 164, Batch 1000/1000: LR=9.84e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.52e-01
2025-10-18 17:01:06,878 | INFO | Epoch 164 Train Time 22.721970558166504s

2025-10-18 17:01:06,878 | INFO | [P1] saving best_model with loss 0.030362 at epoch 164
2025-10-18 17:01:27,734 | INFO | Training epoch 165, Batch 1000/1000: LR=9.84e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.53e-01
2025-10-18 17:01:27,808 | INFO | Epoch 165 Train Time 20.917375802993774s

2025-10-18 17:01:51,008 | INFO | Training epoch 166, Batch 1000/1000: LR=9.83e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.54e-01
2025-10-18 17:01:51,092 | INFO | Epoch 166 Train Time 23.281603574752808s

2025-10-18 17:02:15,493 | INFO | Training epoch 167, Batch 1000/1000: LR=9.83e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.53e-01
2025-10-18 17:02:15,585 | INFO | Epoch 167 Train Time 24.49151921272278s

2025-10-18 17:02:38,150 | INFO | Training epoch 168, Batch 1000/1000: LR=9.83e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.52e-01
2025-10-18 17:02:38,220 | INFO | Epoch 168 Train Time 22.633516550064087s

2025-10-18 17:03:00,233 | INFO | Training epoch 169, Batch 1000/1000: LR=9.83e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-18 17:03:00,306 | INFO | Epoch 169 Train Time 22.085326671600342s

2025-10-18 17:03:23,938 | INFO | Training epoch 170, Batch 1000/1000: LR=9.83e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-18 17:03:24,016 | INFO | Epoch 170 Train Time 23.708390474319458s

2025-10-18 17:03:46,053 | INFO | Training epoch 171, Batch 1000/1000: LR=9.82e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.52e-01
2025-10-18 17:03:46,128 | INFO | Epoch 171 Train Time 22.11099076271057s

2025-10-18 17:04:07,834 | INFO | Training epoch 172, Batch 1000/1000: LR=9.82e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.53e-01
2025-10-18 17:04:07,930 | INFO | Epoch 172 Train Time 21.801310062408447s

2025-10-18 17:04:31,642 | INFO | Training epoch 173, Batch 1000/1000: LR=9.82e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:04:31,731 | INFO | Epoch 173 Train Time 23.800011157989502s

2025-10-18 17:04:31,732 | INFO | [P1] saving best_model with loss 0.030289 at epoch 173
2025-10-18 17:04:55,110 | INFO | Training epoch 174, Batch 1000/1000: LR=9.82e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-18 17:04:55,191 | INFO | Epoch 174 Train Time 23.438637495040894s

2025-10-18 17:05:19,010 | INFO | Training epoch 175, Batch 1000/1000: LR=9.82e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-18 17:05:19,092 | INFO | Epoch 175 Train Time 23.900126457214355s

2025-10-18 17:05:42,658 | INFO | Training epoch 176, Batch 1000/1000: LR=9.81e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.54e-01
2025-10-18 17:05:42,742 | INFO | Epoch 176 Train Time 23.64842391014099s

2025-10-18 17:06:07,008 | INFO | Training epoch 177, Batch 1000/1000: LR=9.81e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-18 17:06:07,079 | INFO | Epoch 177 Train Time 24.336522102355957s

2025-10-18 17:06:30,925 | INFO | Training epoch 178, Batch 1000/1000: LR=9.81e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-18 17:06:31,020 | INFO | Epoch 178 Train Time 23.939621210098267s

2025-10-18 17:06:54,830 | INFO | Training epoch 179, Batch 1000/1000: LR=9.81e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-18 17:06:54,934 | INFO | Epoch 179 Train Time 23.912001848220825s

2025-10-18 17:07:15,640 | INFO | Training epoch 180, Batch 1000/1000: LR=9.81e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-18 17:07:15,711 | INFO | Epoch 180 Train Time 20.776076793670654s

2025-10-18 17:07:38,630 | INFO | Training epoch 181, Batch 1000/1000: LR=9.80e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:07:38,721 | INFO | Epoch 181 Train Time 23.008997440338135s

2025-10-18 17:07:38,722 | INFO | [P1] saving best_model with loss 0.030107 at epoch 181
2025-10-18 17:08:02,456 | INFO | Training epoch 182, Batch 1000/1000: LR=9.80e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:08:02,542 | INFO | Epoch 182 Train Time 23.804966926574707s

2025-10-18 17:08:25,918 | INFO | Training epoch 183, Batch 1000/1000: LR=9.80e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.52e-01
2025-10-18 17:08:25,985 | INFO | Epoch 183 Train Time 23.442045211791992s

2025-10-18 17:08:49,843 | INFO | Training epoch 184, Batch 1000/1000: LR=9.80e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-18 17:08:49,909 | INFO | Epoch 184 Train Time 23.922940015792847s

2025-10-18 17:09:13,118 | INFO | Training epoch 185, Batch 1000/1000: LR=9.79e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.51e-01
2025-10-18 17:09:13,195 | INFO | Epoch 185 Train Time 23.28505825996399s

2025-10-18 17:09:13,197 | INFO | [P1] saving best_model with loss 0.030093 at epoch 185
2025-10-18 17:09:36,650 | INFO | Training epoch 186, Batch 1000/1000: LR=9.79e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-18 17:09:36,734 | INFO | Epoch 186 Train Time 23.523370265960693s

2025-10-18 17:10:01,233 | INFO | Training epoch 187, Batch 1000/1000: LR=9.79e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-18 17:10:01,313 | INFO | Epoch 187 Train Time 24.578505516052246s

2025-10-18 17:10:24,438 | INFO | Training epoch 188, Batch 1000/1000: LR=9.79e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:10:24,519 | INFO | Epoch 188 Train Time 23.20431160926819s

2025-10-18 17:10:46,553 | INFO | Training epoch 189, Batch 1000/1000: LR=9.79e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.52e-01
2025-10-18 17:10:46,632 | INFO | Epoch 189 Train Time 22.11192750930786s

2025-10-18 17:11:09,734 | INFO | Training epoch 190, Batch 1000/1000: LR=9.78e-05, Loss=3.06e-02 BER=1.16e-02 FER=1.52e-01
2025-10-18 17:11:09,807 | INFO | Epoch 190 Train Time 23.17367696762085s

2025-10-18 17:11:33,636 | INFO | Training epoch 191, Batch 1000/1000: LR=9.78e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-18 17:11:33,720 | INFO | Epoch 191 Train Time 23.91148042678833s

2025-10-18 17:11:56,818 | INFO | Training epoch 192, Batch 1000/1000: LR=9.78e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:11:56,886 | INFO | Epoch 192 Train Time 23.164741039276123s

2025-10-18 17:12:20,316 | INFO | Training epoch 193, Batch 1000/1000: LR=9.78e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-18 17:12:20,416 | INFO | Epoch 193 Train Time 23.528971910476685s

2025-10-18 17:12:43,863 | INFO | Training epoch 194, Batch 1000/1000: LR=9.77e-05, Loss=3.04e-02 BER=1.15e-02 FER=1.51e-01
2025-10-18 17:12:43,964 | INFO | Epoch 194 Train Time 23.54633641242981s

2025-10-18 17:13:05,230 | INFO | Training epoch 195, Batch 1000/1000: LR=9.77e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-18 17:13:05,315 | INFO | Epoch 195 Train Time 21.349841117858887s

2025-10-18 17:13:29,202 | INFO | Training epoch 196, Batch 1000/1000: LR=9.77e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:13:29,270 | INFO | Epoch 196 Train Time 23.954236268997192s

2025-10-18 17:13:29,271 | INFO | [P1] saving best_model with loss 0.029768 at epoch 196
2025-10-18 17:13:53,136 | INFO | Training epoch 197, Batch 1000/1000: LR=9.77e-05, Loss=3.04e-02 BER=1.15e-02 FER=1.51e-01
2025-10-18 17:13:53,221 | INFO | Epoch 197 Train Time 23.93154740333557s

2025-10-18 17:14:16,557 | INFO | Training epoch 198, Batch 1000/1000: LR=9.76e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-18 17:14:16,650 | INFO | Epoch 198 Train Time 23.42793869972229s

2025-10-18 17:14:40,143 | INFO | Training epoch 199, Batch 1000/1000: LR=9.76e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:14:40,237 | INFO | Epoch 199 Train Time 23.58620047569275s

2025-10-18 17:15:03,340 | INFO | Training epoch 200, Batch 1000/1000: LR=9.76e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.52e-01
2025-10-18 17:15:03,422 | INFO | Epoch 200 Train Time 23.183745622634888s

2025-10-18 17:15:25,856 | INFO | Training epoch 201, Batch 1000/1000: LR=9.76e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:15:25,947 | INFO | Epoch 201 Train Time 22.523733377456665s

2025-10-18 17:15:49,834 | INFO | Training epoch 202, Batch 1000/1000: LR=9.76e-05, Loss=2.97e-02 BER=1.13e-02 FER=1.48e-01
2025-10-18 17:15:49,929 | INFO | Epoch 202 Train Time 23.980332612991333s

2025-10-18 17:15:49,930 | INFO | [P1] saving best_model with loss 0.029726 at epoch 202
2025-10-18 17:16:12,617 | INFO | Training epoch 203, Batch 1000/1000: LR=9.75e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:16:12,695 | INFO | Epoch 203 Train Time 22.75141954421997s

2025-10-18 17:16:35,518 | INFO | Training epoch 204, Batch 1000/1000: LR=9.75e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:16:35,590 | INFO | Epoch 204 Train Time 22.8932945728302s

2025-10-18 17:16:59,525 | INFO | Training epoch 205, Batch 1000/1000: LR=9.75e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:16:59,611 | INFO | Epoch 205 Train Time 24.019604682922363s

2025-10-18 17:17:23,421 | INFO | Training epoch 206, Batch 1000/1000: LR=9.75e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-18 17:17:23,504 | INFO | Epoch 206 Train Time 23.891979217529297s

2025-10-18 17:17:45,847 | INFO | Training epoch 207, Batch 1000/1000: LR=9.74e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:17:45,922 | INFO | Epoch 207 Train Time 22.41742181777954s

2025-10-18 17:18:09,033 | INFO | Training epoch 208, Batch 1000/1000: LR=9.74e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-18 17:18:09,099 | INFO | Epoch 208 Train Time 23.17602777481079s

2025-10-18 17:18:32,465 | INFO | Training epoch 209, Batch 1000/1000: LR=9.74e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:18:32,552 | INFO | Epoch 209 Train Time 23.452616214752197s

2025-10-18 17:18:55,554 | INFO | Training epoch 210, Batch 1000/1000: LR=9.74e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.51e-01
2025-10-18 17:18:55,640 | INFO | Epoch 210 Train Time 23.08667802810669s

2025-10-18 17:19:19,418 | INFO | Training epoch 211, Batch 1000/1000: LR=9.73e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:19:19,501 | INFO | Epoch 211 Train Time 23.859663248062134s

2025-10-18 17:19:42,408 | INFO | Training epoch 212, Batch 1000/1000: LR=9.73e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-18 17:19:42,476 | INFO | Epoch 212 Train Time 22.974987268447876s

2025-10-18 17:20:05,515 | INFO | Training epoch 213, Batch 1000/1000: LR=9.73e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:20:05,612 | INFO | Epoch 213 Train Time 23.13418412208557s

2025-10-18 17:20:29,415 | INFO | Training epoch 214, Batch 1000/1000: LR=9.73e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:20:29,507 | INFO | Epoch 214 Train Time 23.89466667175293s

2025-10-18 17:20:53,220 | INFO | Training epoch 215, Batch 1000/1000: LR=9.72e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.46e-01
2025-10-18 17:20:53,297 | INFO | Epoch 215 Train Time 23.78876233100891s

2025-10-18 17:20:53,298 | INFO | [P1] saving best_model with loss 0.029589 at epoch 215
2025-10-18 17:21:16,037 | INFO | Training epoch 216, Batch 1000/1000: LR=9.72e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-18 17:21:16,125 | INFO | Epoch 216 Train Time 22.80758786201477s

2025-10-18 17:21:39,223 | INFO | Training epoch 217, Batch 1000/1000: LR=9.72e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:21:39,304 | INFO | Epoch 217 Train Time 23.178383827209473s

2025-10-18 17:22:02,429 | INFO | Training epoch 218, Batch 1000/1000: LR=9.72e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:22:02,509 | INFO | Epoch 218 Train Time 23.204030990600586s

2025-10-18 17:22:26,151 | INFO | Training epoch 219, Batch 1000/1000: LR=9.71e-05, Loss=3.01e-02 BER=1.14e-02 FER=1.49e-01
2025-10-18 17:22:26,232 | INFO | Epoch 219 Train Time 23.72268509864807s

2025-10-18 17:22:50,353 | INFO | Training epoch 220, Batch 1000/1000: LR=9.71e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.50e-01
2025-10-18 17:22:50,424 | INFO | Epoch 220 Train Time 24.189669609069824s

2025-10-18 17:23:12,821 | INFO | Training epoch 221, Batch 1000/1000: LR=9.71e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:23:12,892 | INFO | Epoch 221 Train Time 22.465593099594116s

2025-10-18 17:23:36,316 | INFO | Training epoch 222, Batch 1000/1000: LR=9.70e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:23:36,399 | INFO | Epoch 222 Train Time 23.505875825881958s

2025-10-18 17:23:59,417 | INFO | Training epoch 223, Batch 1000/1000: LR=9.70e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:23:59,494 | INFO | Epoch 223 Train Time 23.093589782714844s

2025-10-18 17:24:22,546 | INFO | Training epoch 224, Batch 1000/1000: LR=9.70e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:24:22,630 | INFO | Epoch 224 Train Time 23.13487482070923s

2025-10-18 17:24:46,031 | INFO | Training epoch 225, Batch 1000/1000: LR=9.70e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:24:46,114 | INFO | Epoch 225 Train Time 23.48369550704956s

2025-10-18 17:25:09,321 | INFO | Training epoch 226, Batch 1000/1000: LR=9.69e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:25:09,400 | INFO | Epoch 226 Train Time 23.284626245498657s

2025-10-18 17:25:32,252 | INFO | Training epoch 227, Batch 1000/1000: LR=9.69e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:25:32,338 | INFO | Epoch 227 Train Time 22.936573266983032s

2025-10-18 17:25:56,152 | INFO | Training epoch 228, Batch 1000/1000: LR=9.69e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-18 17:25:56,220 | INFO | Epoch 228 Train Time 23.881041288375854s

2025-10-18 17:26:19,428 | INFO | Training epoch 229, Batch 1000/1000: LR=9.69e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:26:19,505 | INFO | Epoch 229 Train Time 23.283991813659668s

2025-10-18 17:26:42,542 | INFO | Training epoch 230, Batch 1000/1000: LR=9.68e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:26:42,614 | INFO | Epoch 230 Train Time 23.107728958129883s

2025-10-18 17:27:06,326 | INFO | Training epoch 231, Batch 1000/1000: LR=9.68e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:27:06,408 | INFO | Epoch 231 Train Time 23.792609691619873s

2025-10-18 17:27:29,528 | INFO | Training epoch 232, Batch 1000/1000: LR=9.68e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:27:29,611 | INFO | Epoch 232 Train Time 23.20184302330017s

2025-10-18 17:27:53,545 | INFO | Training epoch 233, Batch 1000/1000: LR=9.67e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-18 17:27:53,608 | INFO | Epoch 233 Train Time 23.99569058418274s

2025-10-18 17:28:17,102 | INFO | Training epoch 234, Batch 1000/1000: LR=9.67e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:28:17,160 | INFO | Epoch 234 Train Time 23.551187753677368s

2025-10-18 17:28:17,160 | INFO | [P1] saving best_model with loss 0.029565 at epoch 234
2025-10-18 17:28:40,745 | INFO | Training epoch 235, Batch 1000/1000: LR=9.67e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-18 17:28:40,816 | INFO | Epoch 235 Train Time 23.64364457130432s

2025-10-18 17:29:04,322 | INFO | Training epoch 236, Batch 1000/1000: LR=9.67e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.48e-01
2025-10-18 17:29:04,405 | INFO | Epoch 236 Train Time 23.588236331939697s

2025-10-18 17:29:27,219 | INFO | Training epoch 237, Batch 1000/1000: LR=9.66e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:29:27,303 | INFO | Epoch 237 Train Time 22.896055698394775s

2025-10-18 17:29:49,706 | INFO | Training epoch 238, Batch 1000/1000: LR=9.66e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:29:49,796 | INFO | Epoch 238 Train Time 22.492223978042603s

2025-10-18 17:30:14,157 | INFO | Training epoch 239, Batch 1000/1000: LR=9.66e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:30:14,224 | INFO | Epoch 239 Train Time 24.426795482635498s

2025-10-18 17:30:36,303 | INFO | Training epoch 240, Batch 1000/1000: LR=9.66e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:30:36,390 | INFO | Epoch 240 Train Time 22.164595127105713s

2025-10-18 17:30:57,927 | INFO | Training epoch 241, Batch 1000/1000: LR=9.65e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:30:58,009 | INFO | Epoch 241 Train Time 21.618807077407837s

2025-10-18 17:31:21,303 | INFO | Training epoch 242, Batch 1000/1000: LR=9.65e-05, Loss=3.01e-02 BER=1.14e-02 FER=1.49e-01
2025-10-18 17:31:21,386 | INFO | Epoch 242 Train Time 23.374716997146606s

2025-10-18 17:31:44,626 | INFO | Training epoch 243, Batch 1000/1000: LR=9.65e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.46e-01
2025-10-18 17:31:44,697 | INFO | Epoch 243 Train Time 23.309311866760254s

2025-10-18 17:31:44,698 | INFO | [P1] saving best_model with loss 0.029337 at epoch 243
2025-10-18 17:32:07,728 | INFO | Training epoch 244, Batch 1000/1000: LR=9.64e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.46e-01
2025-10-18 17:32:07,816 | INFO | Epoch 244 Train Time 23.102876663208008s

2025-10-18 17:32:31,221 | INFO | Training epoch 245, Batch 1000/1000: LR=9.64e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-18 17:32:31,300 | INFO | Epoch 245 Train Time 23.48199486732483s

2025-10-18 17:32:55,123 | INFO | Training epoch 246, Batch 1000/1000: LR=9.64e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.47e-01
2025-10-18 17:32:55,189 | INFO | Epoch 246 Train Time 23.887321949005127s

2025-10-18 17:33:18,423 | INFO | Training epoch 247, Batch 1000/1000: LR=9.64e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:33:18,491 | INFO | Epoch 247 Train Time 23.300248384475708s

2025-10-18 17:33:41,450 | INFO | Training epoch 248, Batch 1000/1000: LR=9.63e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:33:41,512 | INFO | Epoch 248 Train Time 23.020371198654175s

2025-10-18 17:34:04,938 | INFO | Training epoch 249, Batch 1000/1000: LR=9.63e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:34:05,011 | INFO | Epoch 249 Train Time 23.496041297912598s

2025-10-18 17:34:27,921 | INFO | Training epoch 250, Batch 1000/1000: LR=9.63e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.47e-01
2025-10-18 17:34:27,985 | INFO | Epoch 250 Train Time 22.972859144210815s

2025-10-18 17:34:51,254 | INFO | Training epoch 251, Batch 1000/1000: LR=9.62e-05, Loss=2.96e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:34:51,323 | INFO | Epoch 251 Train Time 23.33747673034668s

2025-10-18 17:35:14,512 | INFO | Training epoch 252, Batch 1000/1000: LR=9.62e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:35:14,582 | INFO | Epoch 252 Train Time 23.25759983062744s

2025-10-18 17:35:38,021 | INFO | Training epoch 253, Batch 1000/1000: LR=9.62e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:35:38,086 | INFO | Epoch 253 Train Time 23.50216245651245s

2025-10-18 17:36:01,641 | INFO | Training epoch 254, Batch 1000/1000: LR=9.61e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-18 17:36:01,706 | INFO | Epoch 254 Train Time 23.618441820144653s

2025-10-18 17:36:25,569 | INFO | Training epoch 255, Batch 1000/1000: LR=9.61e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:36:25,650 | INFO | Epoch 255 Train Time 23.942920446395874s

2025-10-18 17:36:49,530 | INFO | Training epoch 256, Batch 1000/1000: LR=9.61e-05, Loss=2.96e-02 BER=1.12e-02 FER=1.46e-01
2025-10-18 17:36:49,622 | INFO | Epoch 256 Train Time 23.970699548721313s

2025-10-18 17:37:13,519 | INFO | Training epoch 257, Batch 1000/1000: LR=9.61e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:37:13,602 | INFO | Epoch 257 Train Time 23.978944540023804s

2025-10-18 17:37:37,154 | INFO | Training epoch 258, Batch 1000/1000: LR=9.60e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-18 17:37:37,228 | INFO | Epoch 258 Train Time 23.623319625854492s

2025-10-18 17:38:00,244 | INFO | Training epoch 259, Batch 1000/1000: LR=9.60e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-18 17:38:00,322 | INFO | Epoch 259 Train Time 23.093657970428467s

2025-10-18 17:38:23,819 | INFO | Training epoch 260, Batch 1000/1000: LR=9.60e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:38:23,886 | INFO | Epoch 260 Train Time 23.563247442245483s

2025-10-18 17:38:48,104 | INFO | Training epoch 261, Batch 1000/1000: LR=9.59e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:38:48,173 | INFO | Epoch 261 Train Time 24.285860538482666s

2025-10-18 17:39:11,535 | INFO | Training epoch 262, Batch 1000/1000: LR=9.59e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:39:11,615 | INFO | Epoch 262 Train Time 23.440004348754883s

2025-10-18 17:39:34,854 | INFO | Training epoch 263, Batch 1000/1000: LR=9.59e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:39:34,949 | INFO | Epoch 263 Train Time 23.332799673080444s

2025-10-18 17:39:58,365 | INFO | Training epoch 264, Batch 1000/1000: LR=9.58e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:39:58,433 | INFO | Epoch 264 Train Time 23.482418537139893s

2025-10-18 17:40:21,224 | INFO | Training epoch 265, Batch 1000/1000: LR=9.58e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:40:21,294 | INFO | Epoch 265 Train Time 22.860738515853882s

2025-10-18 17:40:44,615 | INFO | Training epoch 266, Batch 1000/1000: LR=9.58e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.46e-01
2025-10-18 17:40:44,683 | INFO | Epoch 266 Train Time 23.38740634918213s

2025-10-18 17:41:07,035 | INFO | Training epoch 267, Batch 1000/1000: LR=9.57e-05, Loss=2.96e-02 BER=1.12e-02 FER=1.46e-01
2025-10-18 17:41:07,124 | INFO | Epoch 267 Train Time 22.439375638961792s

2025-10-18 17:41:31,003 | INFO | Training epoch 268, Batch 1000/1000: LR=9.57e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:41:31,089 | INFO | Epoch 268 Train Time 23.96378517150879s

2025-10-18 17:41:55,139 | INFO | Training epoch 269, Batch 1000/1000: LR=9.57e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:41:55,220 | INFO | Epoch 269 Train Time 24.12927269935608s

2025-10-18 17:42:19,331 | INFO | Training epoch 270, Batch 1000/1000: LR=9.56e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:42:19,395 | INFO | Epoch 270 Train Time 24.17407536506653s

2025-10-18 17:42:42,735 | INFO | Training epoch 271, Batch 1000/1000: LR=9.56e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.46e-01
2025-10-18 17:42:42,825 | INFO | Epoch 271 Train Time 23.428957223892212s

2025-10-18 17:43:05,537 | INFO | Training epoch 272, Batch 1000/1000: LR=9.56e-05, Loss=2.95e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:43:05,624 | INFO | Epoch 272 Train Time 22.797686100006104s

2025-10-18 17:43:27,137 | INFO | Training epoch 273, Batch 1000/1000: LR=9.56e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:43:27,205 | INFO | Epoch 273 Train Time 21.580724239349365s

2025-10-18 17:43:50,539 | INFO | Training epoch 274, Batch 1000/1000: LR=9.55e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:43:50,604 | INFO | Epoch 274 Train Time 23.398000240325928s

2025-10-18 17:44:14,137 | INFO | Training epoch 275, Batch 1000/1000: LR=9.55e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:44:14,205 | INFO | Epoch 275 Train Time 23.599287748336792s

2025-10-18 17:44:37,003 | INFO | Training epoch 276, Batch 1000/1000: LR=9.55e-05, Loss=2.94e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:44:37,087 | INFO | Epoch 276 Train Time 22.880399465560913s

2025-10-18 17:45:00,539 | INFO | Training epoch 277, Batch 1000/1000: LR=9.54e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:45:00,606 | INFO | Epoch 277 Train Time 23.51831293106079s

2025-10-18 17:45:00,607 | INFO | [P1] saving best_model with loss 0.029282 at epoch 277
2025-10-18 17:45:24,142 | INFO | Training epoch 278, Batch 1000/1000: LR=9.54e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:45:24,226 | INFO | Epoch 278 Train Time 23.59244728088379s

2025-10-18 17:45:46,453 | INFO | Training epoch 279, Batch 1000/1000: LR=9.54e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:45:46,543 | INFO | Epoch 279 Train Time 22.315288066864014s

2025-10-18 17:46:09,847 | INFO | Training epoch 280, Batch 1000/1000: LR=9.53e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.47e-01
2025-10-18 17:46:09,936 | INFO | Epoch 280 Train Time 23.391323804855347s

2025-10-18 17:46:32,822 | INFO | Training epoch 281, Batch 1000/1000: LR=9.53e-05, Loss=2.96e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:46:32,888 | INFO | Epoch 281 Train Time 22.951354026794434s

2025-10-18 17:46:55,452 | INFO | Training epoch 282, Batch 1000/1000: LR=9.53e-05, Loss=2.94e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:46:55,529 | INFO | Epoch 282 Train Time 22.638263702392578s

2025-10-18 17:47:19,014 | INFO | Training epoch 283, Batch 1000/1000: LR=9.52e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:47:19,093 | INFO | Epoch 283 Train Time 23.563332557678223s

2025-10-18 17:47:19,094 | INFO | [P1] saving best_model with loss 0.029271 at epoch 283
2025-10-18 17:47:42,416 | INFO | Training epoch 284, Batch 1000/1000: LR=9.52e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-18 17:47:42,518 | INFO | Epoch 284 Train Time 23.409921646118164s

2025-10-18 17:48:04,863 | INFO | Training epoch 285, Batch 1000/1000: LR=9.52e-05, Loss=2.90e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 17:48:04,926 | INFO | Epoch 285 Train Time 22.40711998939514s

2025-10-18 17:48:04,926 | INFO | [P1] saving best_model with loss 0.029026 at epoch 285
2025-10-18 17:48:28,330 | INFO | Training epoch 286, Batch 1000/1000: LR=9.51e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 17:48:28,404 | INFO | Epoch 286 Train Time 23.46253228187561s

2025-10-18 17:48:50,956 | INFO | Training epoch 287, Batch 1000/1000: LR=9.51e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:48:51,030 | INFO | Epoch 287 Train Time 22.6245596408844s

2025-10-18 17:49:14,064 | INFO | Training epoch 288, Batch 1000/1000: LR=9.51e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:49:14,143 | INFO | Epoch 288 Train Time 23.112140417099s

2025-10-18 17:49:37,700 | INFO | Training epoch 289, Batch 1000/1000: LR=9.50e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.45e-01
2025-10-18 17:49:37,782 | INFO | Epoch 289 Train Time 23.637887001037598s

2025-10-18 17:50:01,032 | INFO | Training epoch 290, Batch 1000/1000: LR=9.50e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 17:50:01,099 | INFO | Epoch 290 Train Time 23.31570267677307s

2025-10-18 17:50:25,112 | INFO | Training epoch 291, Batch 1000/1000: LR=9.50e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:50:25,203 | INFO | Epoch 291 Train Time 24.102672815322876s

2025-10-18 17:50:48,644 | INFO | Training epoch 292, Batch 1000/1000: LR=9.49e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 17:50:48,724 | INFO | Epoch 292 Train Time 23.519983053207397s

2025-10-18 17:51:11,317 | INFO | Training epoch 293, Batch 1000/1000: LR=9.49e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 17:51:11,384 | INFO | Epoch 293 Train Time 22.659090280532837s

2025-10-18 17:51:34,678 | INFO | Training epoch 294, Batch 1000/1000: LR=9.48e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:51:34,761 | INFO | Epoch 294 Train Time 23.375328540802002s

2025-10-18 17:51:58,182 | INFO | Training epoch 295, Batch 1000/1000: LR=9.48e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.43e-01
2025-10-18 17:51:58,267 | INFO | Epoch 295 Train Time 23.505122423171997s

2025-10-18 17:51:58,268 | INFO | [P1] saving best_model with loss 0.028741 at epoch 295
2025-10-18 17:52:21,426 | INFO | Training epoch 296, Batch 1000/1000: LR=9.48e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:52:21,494 | INFO | Epoch 296 Train Time 23.20415711402893s

2025-10-18 17:52:44,962 | INFO | Training epoch 297, Batch 1000/1000: LR=9.47e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:52:45,044 | INFO | Epoch 297 Train Time 23.549195528030396s

2025-10-18 17:53:07,750 | INFO | Training epoch 298, Batch 1000/1000: LR=9.47e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.46e-01
2025-10-18 17:53:07,816 | INFO | Epoch 298 Train Time 22.771264791488647s

2025-10-18 17:53:30,751 | INFO | Training epoch 299, Batch 1000/1000: LR=9.47e-05, Loss=2.93e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:53:30,820 | INFO | Epoch 299 Train Time 23.00277018547058s

2025-10-18 17:53:52,055 | INFO | Training epoch 300, Batch 1000/1000: LR=9.46e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.47e-01
2025-10-18 17:53:52,126 | INFO | Epoch 300 Train Time 21.304985284805298s

2025-10-18 17:54:14,321 | INFO | Training epoch 301, Batch 1000/1000: LR=9.46e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 17:54:14,382 | INFO | Epoch 301 Train Time 22.25517725944519s

2025-10-18 17:54:37,143 | INFO | Training epoch 302, Batch 1000/1000: LR=9.46e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:54:37,213 | INFO | Epoch 302 Train Time 22.829745292663574s

2025-10-18 17:54:59,914 | INFO | Training epoch 303, Batch 1000/1000: LR=9.45e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:54:59,999 | INFO | Epoch 303 Train Time 22.784208059310913s

2025-10-18 17:55:23,238 | INFO | Training epoch 304, Batch 1000/1000: LR=9.45e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:55:23,304 | INFO | Epoch 304 Train Time 23.303987503051758s

2025-10-18 17:55:46,551 | INFO | Training epoch 305, Batch 1000/1000: LR=9.45e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:55:46,613 | INFO | Epoch 305 Train Time 23.30795931816101s

2025-10-18 17:56:07,431 | INFO | Training epoch 306, Batch 1000/1000: LR=9.44e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 17:56:07,495 | INFO | Epoch 306 Train Time 20.879961252212524s

2025-10-18 17:56:31,324 | INFO | Training epoch 307, Batch 1000/1000: LR=9.44e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 17:56:31,392 | INFO | Epoch 307 Train Time 23.895795106887817s

2025-10-18 17:56:55,208 | INFO | Training epoch 308, Batch 1000/1000: LR=9.44e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:56:55,291 | INFO | Epoch 308 Train Time 23.897524118423462s

2025-10-18 17:57:19,113 | INFO | Training epoch 309, Batch 1000/1000: LR=9.43e-05, Loss=2.97e-02 BER=1.12e-02 FER=1.46e-01
2025-10-18 17:57:19,192 | INFO | Epoch 309 Train Time 23.900779008865356s

2025-10-18 17:57:41,754 | INFO | Training epoch 310, Batch 1000/1000: LR=9.43e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.46e-01
2025-10-18 17:57:41,838 | INFO | Epoch 310 Train Time 22.644941568374634s

2025-10-18 17:58:04,987 | INFO | Training epoch 311, Batch 1000/1000: LR=9.42e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 17:58:05,074 | INFO | Epoch 311 Train Time 23.234809398651123s

2025-10-18 17:58:27,816 | INFO | Training epoch 312, Batch 1000/1000: LR=9.42e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 17:58:27,901 | INFO | Epoch 312 Train Time 22.826464891433716s

2025-10-18 17:58:50,900 | INFO | Training epoch 313, Batch 1000/1000: LR=9.42e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 17:58:50,978 | INFO | Epoch 313 Train Time 23.075129508972168s

2025-10-18 17:59:14,548 | INFO | Training epoch 314, Batch 1000/1000: LR=9.41e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 17:59:14,645 | INFO | Epoch 314 Train Time 23.66521692276001s

2025-10-18 17:59:34,634 | INFO | Training epoch 315, Batch 1000/1000: LR=9.41e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 17:59:34,693 | INFO | Epoch 315 Train Time 20.04719853401184s

2025-10-18 17:59:58,226 | INFO | Training epoch 316, Batch 1000/1000: LR=9.41e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 17:59:58,312 | INFO | Epoch 316 Train Time 23.617892265319824s

2025-10-18 18:00:21,348 | INFO | Training epoch 317, Batch 1000/1000: LR=9.40e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:00:21,432 | INFO | Epoch 317 Train Time 23.118342876434326s

2025-10-18 18:00:44,240 | INFO | Training epoch 318, Batch 1000/1000: LR=9.40e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:00:44,311 | INFO | Epoch 318 Train Time 22.878186225891113s

2025-10-18 18:01:07,057 | INFO | Training epoch 319, Batch 1000/1000: LR=9.40e-05, Loss=2.94e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 18:01:07,126 | INFO | Epoch 319 Train Time 22.813244581222534s

2025-10-18 18:01:30,438 | INFO | Training epoch 320, Batch 1000/1000: LR=9.39e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 18:01:30,514 | INFO | Epoch 320 Train Time 23.38711929321289s

2025-10-18 18:01:53,756 | INFO | Training epoch 321, Batch 1000/1000: LR=9.39e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:01:53,836 | INFO | Epoch 321 Train Time 23.321706295013428s

2025-10-18 18:02:16,253 | INFO | Training epoch 322, Batch 1000/1000: LR=9.38e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:02:16,316 | INFO | Epoch 322 Train Time 22.4784197807312s

2025-10-18 18:02:39,598 | INFO | Training epoch 323, Batch 1000/1000: LR=9.38e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 18:02:39,675 | INFO | Epoch 323 Train Time 23.357995748519897s

2025-10-18 18:02:39,676 | INFO | [P1] saving best_model with loss 0.028652 at epoch 323
2025-10-18 18:03:03,130 | INFO | Training epoch 324, Batch 1000/1000: LR=9.38e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:03:03,210 | INFO | Epoch 324 Train Time 23.52159357070923s

2025-10-18 18:03:25,843 | INFO | Training epoch 325, Batch 1000/1000: LR=9.37e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:03:25,921 | INFO | Epoch 325 Train Time 22.710392713546753s

2025-10-18 18:03:48,896 | INFO | Training epoch 326, Batch 1000/1000: LR=9.37e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:03:48,964 | INFO | Epoch 326 Train Time 23.042399644851685s

2025-10-18 18:04:11,230 | INFO | Training epoch 327, Batch 1000/1000: LR=9.37e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:04:11,308 | INFO | Epoch 327 Train Time 22.341540575027466s

2025-10-18 18:04:33,957 | INFO | Training epoch 328, Batch 1000/1000: LR=9.36e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:04:34,041 | INFO | Epoch 328 Train Time 22.731749296188354s

2025-10-18 18:04:56,432 | INFO | Training epoch 329, Batch 1000/1000: LR=9.36e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:04:56,500 | INFO | Epoch 329 Train Time 22.45694851875305s

2025-10-18 18:05:19,430 | INFO | Training epoch 330, Batch 1000/1000: LR=9.35e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:05:19,498 | INFO | Epoch 330 Train Time 22.996684789657593s

2025-10-18 18:05:42,443 | INFO | Training epoch 331, Batch 1000/1000: LR=9.35e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 18:05:42,509 | INFO | Epoch 331 Train Time 23.009521961212158s

2025-10-18 18:06:05,330 | INFO | Training epoch 332, Batch 1000/1000: LR=9.35e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:06:05,411 | INFO | Epoch 332 Train Time 22.9002628326416s

2025-10-18 18:06:29,330 | INFO | Training epoch 333, Batch 1000/1000: LR=9.34e-05, Loss=2.92e-02 BER=1.11e-02 FER=1.45e-01
2025-10-18 18:06:29,398 | INFO | Epoch 333 Train Time 23.98637628555298s

2025-10-18 18:06:51,853 | INFO | Training epoch 334, Batch 1000/1000: LR=9.34e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.44e-01
2025-10-18 18:06:51,926 | INFO | Epoch 334 Train Time 22.526124954223633s

2025-10-18 18:07:15,842 | INFO | Training epoch 335, Batch 1000/1000: LR=9.33e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:07:15,929 | INFO | Epoch 335 Train Time 24.001312732696533s

2025-10-18 18:07:38,826 | INFO | Training epoch 336, Batch 1000/1000: LR=9.33e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:07:38,908 | INFO | Epoch 336 Train Time 22.978373765945435s

2025-10-18 18:08:02,090 | INFO | Training epoch 337, Batch 1000/1000: LR=9.33e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:08:02,167 | INFO | Epoch 337 Train Time 23.257819652557373s

2025-10-18 18:08:25,225 | INFO | Training epoch 338, Batch 1000/1000: LR=9.32e-05, Loss=2.94e-02 BER=1.11e-02 FER=1.44e-01
2025-10-18 18:08:25,296 | INFO | Epoch 338 Train Time 23.12795901298523s

2025-10-18 18:08:48,042 | INFO | Training epoch 339, Batch 1000/1000: LR=9.32e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.43e-01
2025-10-18 18:08:48,109 | INFO | Epoch 339 Train Time 22.812200784683228s

2025-10-18 18:09:11,833 | INFO | Training epoch 340, Batch 1000/1000: LR=9.31e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:09:11,919 | INFO | Epoch 340 Train Time 23.808606147766113s

2025-10-18 18:09:35,639 | INFO | Training epoch 341, Batch 1000/1000: LR=9.31e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 18:09:35,715 | INFO | Epoch 341 Train Time 23.79412603378296s

2025-10-18 18:09:59,027 | INFO | Training epoch 342, Batch 1000/1000: LR=9.31e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:09:59,090 | INFO | Epoch 342 Train Time 23.3746919631958s

2025-10-18 18:10:22,264 | INFO | Training epoch 343, Batch 1000/1000: LR=9.30e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 18:10:22,350 | INFO | Epoch 343 Train Time 23.258391857147217s

2025-10-18 18:10:44,400 | INFO | Training epoch 344, Batch 1000/1000: LR=9.30e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:10:44,497 | INFO | Epoch 344 Train Time 22.146207571029663s

2025-10-18 18:11:07,636 | INFO | Training epoch 345, Batch 1000/1000: LR=9.29e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:11:07,722 | INFO | Epoch 345 Train Time 23.223970413208008s

2025-10-18 18:11:30,835 | INFO | Training epoch 346, Batch 1000/1000: LR=9.29e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:11:30,903 | INFO | Epoch 346 Train Time 23.17932152748108s

2025-10-18 18:11:53,517 | INFO | Training epoch 347, Batch 1000/1000: LR=9.29e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 18:11:53,598 | INFO | Epoch 347 Train Time 22.694010972976685s

2025-10-18 18:12:17,433 | INFO | Training epoch 348, Batch 1000/1000: LR=9.28e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:12:17,507 | INFO | Epoch 348 Train Time 23.908377170562744s

2025-10-18 18:12:39,951 | INFO | Training epoch 349, Batch 1000/1000: LR=9.28e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:12:40,022 | INFO | Epoch 349 Train Time 22.513319730758667s

2025-10-18 18:13:03,107 | INFO | Training epoch 350, Batch 1000/1000: LR=9.27e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.44e-01
2025-10-18 18:13:03,197 | INFO | Epoch 350 Train Time 23.17386269569397s

2025-10-18 18:13:27,818 | INFO | Training epoch 351, Batch 1000/1000: LR=9.27e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:13:27,892 | INFO | Epoch 351 Train Time 24.693432331085205s

2025-10-18 18:13:50,906 | INFO | Training epoch 352, Batch 1000/1000: LR=9.27e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:13:50,985 | INFO | Epoch 352 Train Time 23.090633392333984s

2025-10-18 18:14:14,696 | INFO | Training epoch 353, Batch 1000/1000: LR=9.26e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:14:14,782 | INFO | Epoch 353 Train Time 23.796252012252808s

2025-10-18 18:14:38,451 | INFO | Training epoch 354, Batch 1000/1000: LR=9.26e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:14:38,533 | INFO | Epoch 354 Train Time 23.74909496307373s

2025-10-18 18:15:00,912 | INFO | Training epoch 355, Batch 1000/1000: LR=9.25e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.45e-01
2025-10-18 18:15:01,001 | INFO | Epoch 355 Train Time 22.46634268760681s

2025-10-18 18:15:23,950 | INFO | Training epoch 356, Batch 1000/1000: LR=9.25e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:15:24,037 | INFO | Epoch 356 Train Time 23.034050703048706s

2025-10-18 18:15:47,369 | INFO | Training epoch 357, Batch 1000/1000: LR=9.25e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:15:47,434 | INFO | Epoch 357 Train Time 23.39536452293396s

2025-10-18 18:16:10,320 | INFO | Training epoch 358, Batch 1000/1000: LR=9.24e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.43e-01
2025-10-18 18:16:10,389 | INFO | Epoch 358 Train Time 22.953404903411865s

2025-10-18 18:16:33,525 | INFO | Training epoch 359, Batch 1000/1000: LR=9.24e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:16:33,591 | INFO | Epoch 359 Train Time 23.201564073562622s

2025-10-18 18:16:56,207 | INFO | Training epoch 360, Batch 1000/1000: LR=9.23e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:16:56,295 | INFO | Epoch 360 Train Time 22.702609539031982s

2025-10-18 18:17:20,543 | INFO | Training epoch 361, Batch 1000/1000: LR=9.23e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:17:20,612 | INFO | Epoch 361 Train Time 24.316113471984863s

2025-10-18 18:17:44,751 | INFO | Training epoch 362, Batch 1000/1000: LR=9.23e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.43e-01
2025-10-18 18:17:44,821 | INFO | Epoch 362 Train Time 24.207984447479248s

2025-10-18 18:18:08,037 | INFO | Training epoch 363, Batch 1000/1000: LR=9.22e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:18:08,122 | INFO | Epoch 363 Train Time 23.29955792427063s

2025-10-18 18:18:30,453 | INFO | Training epoch 364, Batch 1000/1000: LR=9.22e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:18:30,520 | INFO | Epoch 364 Train Time 22.396849870681763s

2025-10-18 18:18:53,415 | INFO | Training epoch 365, Batch 1000/1000: LR=9.21e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:18:53,499 | INFO | Epoch 365 Train Time 22.976776838302612s

2025-10-18 18:19:14,765 | INFO | Training epoch 366, Batch 1000/1000: LR=9.21e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:19:14,825 | INFO | Epoch 366 Train Time 21.326068878173828s

2025-10-18 18:19:38,339 | INFO | Training epoch 367, Batch 1000/1000: LR=9.20e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.44e-01
2025-10-18 18:19:38,402 | INFO | Epoch 367 Train Time 23.575342178344727s

2025-10-18 18:20:01,709 | INFO | Training epoch 368, Batch 1000/1000: LR=9.20e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-18 18:20:01,780 | INFO | Epoch 368 Train Time 23.377244472503662s

2025-10-18 18:20:25,653 | INFO | Training epoch 369, Batch 1000/1000: LR=9.20e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.42e-01
2025-10-18 18:20:25,741 | INFO | Epoch 369 Train Time 23.959584951400757s

2025-10-18 18:20:49,248 | INFO | Training epoch 370, Batch 1000/1000: LR=9.19e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:20:49,315 | INFO | Epoch 370 Train Time 23.572510480880737s

2025-10-18 18:21:13,146 | INFO | Training epoch 371, Batch 1000/1000: LR=9.19e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:21:13,225 | INFO | Epoch 371 Train Time 23.909531831741333s

2025-10-18 18:21:36,155 | INFO | Training epoch 372, Batch 1000/1000: LR=9.18e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:21:36,241 | INFO | Epoch 372 Train Time 23.014566898345947s

2025-10-18 18:21:36,242 | INFO | [P1] saving best_model with loss 0.028628 at epoch 372
2025-10-18 18:21:59,232 | INFO | Training epoch 373, Batch 1000/1000: LR=9.18e-05, Loss=2.89e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:21:59,306 | INFO | Epoch 373 Train Time 23.04718589782715s

2025-10-18 18:22:22,643 | INFO | Training epoch 374, Batch 1000/1000: LR=9.17e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:22:22,732 | INFO | Epoch 374 Train Time 23.42422366142273s

2025-10-18 18:22:46,133 | INFO | Training epoch 375, Batch 1000/1000: LR=9.17e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.42e-01
2025-10-18 18:22:46,236 | INFO | Epoch 375 Train Time 23.502846717834473s

2025-10-18 18:23:09,825 | INFO | Training epoch 376, Batch 1000/1000: LR=9.17e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:23:09,901 | INFO | Epoch 376 Train Time 23.66477870941162s

2025-10-18 18:23:09,902 | INFO | [P1] saving best_model with loss 0.028609 at epoch 376
2025-10-18 18:23:34,136 | INFO | Training epoch 377, Batch 1000/1000: LR=9.16e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:23:34,210 | INFO | Epoch 377 Train Time 24.291118383407593s

2025-10-18 18:23:34,212 | INFO | [P1] saving best_model with loss 0.028555 at epoch 377
2025-10-18 18:23:58,056 | INFO | Training epoch 378, Batch 1000/1000: LR=9.16e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:23:58,139 | INFO | Epoch 378 Train Time 23.912450075149536s

2025-10-18 18:23:58,140 | INFO | [P1] saving best_model with loss 0.028552 at epoch 378
2025-10-18 18:24:21,424 | INFO | Training epoch 379, Batch 1000/1000: LR=9.15e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:24:21,507 | INFO | Epoch 379 Train Time 23.348644256591797s

2025-10-18 18:24:45,207 | INFO | Training epoch 380, Batch 1000/1000: LR=9.15e-05, Loss=2.88e-02 BER=1.09e-02 FER=1.41e-01
2025-10-18 18:24:45,289 | INFO | Epoch 380 Train Time 23.781258583068848s

2025-10-18 18:25:07,843 | INFO | Training epoch 381, Batch 1000/1000: LR=9.14e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:25:07,919 | INFO | Epoch 381 Train Time 22.628490686416626s

2025-10-18 18:25:32,345 | INFO | Training epoch 382, Batch 1000/1000: LR=9.14e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:25:32,428 | INFO | Epoch 382 Train Time 24.50861430168152s

2025-10-18 18:25:54,891 | INFO | Training epoch 383, Batch 1000/1000: LR=9.14e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:25:54,967 | INFO | Epoch 383 Train Time 22.53750729560852s

2025-10-18 18:26:17,726 | INFO | Training epoch 384, Batch 1000/1000: LR=9.13e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.43e-01
2025-10-18 18:26:17,807 | INFO | Epoch 384 Train Time 22.839832305908203s

2025-10-18 18:26:41,236 | INFO | Training epoch 385, Batch 1000/1000: LR=9.13e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.40e-01
2025-10-18 18:26:41,311 | INFO | Epoch 385 Train Time 23.502988576889038s

2025-10-18 18:26:41,312 | INFO | [P1] saving best_model with loss 0.028406 at epoch 385
2025-10-18 18:27:05,314 | INFO | Training epoch 386, Batch 1000/1000: LR=9.12e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:27:05,394 | INFO | Epoch 386 Train Time 24.065918922424316s

2025-10-18 18:27:28,756 | INFO | Training epoch 387, Batch 1000/1000: LR=9.12e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:27:28,838 | INFO | Epoch 387 Train Time 23.44270348548889s

2025-10-18 18:27:52,435 | INFO | Training epoch 388, Batch 1000/1000: LR=9.11e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.41e-01
2025-10-18 18:27:52,505 | INFO | Epoch 388 Train Time 23.66555905342102s

2025-10-18 18:28:16,455 | INFO | Training epoch 389, Batch 1000/1000: LR=9.11e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:28:16,533 | INFO | Epoch 389 Train Time 24.02735996246338s

2025-10-18 18:28:37,047 | INFO | Training epoch 390, Batch 1000/1000: LR=9.10e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:28:37,123 | INFO | Epoch 390 Train Time 20.58842420578003s

2025-10-18 18:29:01,412 | INFO | Training epoch 391, Batch 1000/1000: LR=9.10e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:29:01,486 | INFO | Epoch 391 Train Time 24.36200451850891s

2025-10-18 18:29:25,349 | INFO | Training epoch 392, Batch 1000/1000: LR=9.10e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:29:25,437 | INFO | Epoch 392 Train Time 23.949598789215088s

2025-10-18 18:29:49,127 | INFO | Training epoch 393, Batch 1000/1000: LR=9.09e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:29:49,210 | INFO | Epoch 393 Train Time 23.77211594581604s

2025-10-18 18:30:12,918 | INFO | Training epoch 394, Batch 1000/1000: LR=9.09e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 18:30:13,007 | INFO | Epoch 394 Train Time 23.79697608947754s

2025-10-18 18:30:36,805 | INFO | Training epoch 395, Batch 1000/1000: LR=9.08e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.42e-01
2025-10-18 18:30:36,890 | INFO | Epoch 395 Train Time 23.88218379020691s

2025-10-18 18:31:00,225 | INFO | Training epoch 396, Batch 1000/1000: LR=9.08e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:31:00,300 | INFO | Epoch 396 Train Time 23.408610105514526s

2025-10-18 18:31:24,046 | INFO | Training epoch 397, Batch 1000/1000: LR=9.07e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:31:24,136 | INFO | Epoch 397 Train Time 23.834816455841064s

2025-10-18 18:31:48,142 | INFO | Training epoch 398, Batch 1000/1000: LR=9.07e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 18:31:48,217 | INFO | Epoch 398 Train Time 24.079917430877686s

2025-10-18 18:32:12,459 | INFO | Training epoch 399, Batch 1000/1000: LR=9.06e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:32:12,533 | INFO | Epoch 399 Train Time 24.31472396850586s

2025-10-18 18:32:35,619 | INFO | Training epoch 400, Batch 1000/1000: LR=9.06e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:32:35,694 | INFO | Epoch 400 Train Time 23.160164833068848s

2025-10-18 18:32:59,724 | INFO | Training epoch 401, Batch 1000/1000: LR=9.05e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:32:59,803 | INFO | Epoch 401 Train Time 24.10767412185669s

2025-10-18 18:33:23,315 | INFO | Training epoch 402, Batch 1000/1000: LR=9.05e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:33:23,399 | INFO | Epoch 402 Train Time 23.593894243240356s

2025-10-18 18:33:46,851 | INFO | Training epoch 403, Batch 1000/1000: LR=9.05e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:33:46,925 | INFO | Epoch 403 Train Time 23.525651931762695s

2025-10-18 18:34:11,543 | INFO | Training epoch 404, Batch 1000/1000: LR=9.04e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:34:11,621 | INFO | Epoch 404 Train Time 24.69449543952942s

2025-10-18 18:34:35,938 | INFO | Training epoch 405, Batch 1000/1000: LR=9.04e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.43e-01
2025-10-18 18:34:36,018 | INFO | Epoch 405 Train Time 24.395912647247314s

2025-10-18 18:34:59,037 | INFO | Training epoch 406, Batch 1000/1000: LR=9.03e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.40e-01
2025-10-18 18:34:59,103 | INFO | Epoch 406 Train Time 23.08345651626587s

2025-10-18 18:34:59,103 | INFO | [P1] saving best_model with loss 0.028202 at epoch 406
2025-10-18 18:35:22,719 | INFO | Training epoch 407, Batch 1000/1000: LR=9.03e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 18:35:22,803 | INFO | Epoch 407 Train Time 23.686870574951172s

2025-10-18 18:35:45,439 | INFO | Training epoch 408, Batch 1000/1000: LR=9.02e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:35:45,516 | INFO | Epoch 408 Train Time 22.71246361732483s

2025-10-18 18:36:09,746 | INFO | Training epoch 409, Batch 1000/1000: LR=9.02e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:36:09,831 | INFO | Epoch 409 Train Time 24.313782215118408s

2025-10-18 18:36:33,125 | INFO | Training epoch 410, Batch 1000/1000: LR=9.01e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:36:33,200 | INFO | Epoch 410 Train Time 23.36814022064209s

2025-10-18 18:36:57,833 | INFO | Training epoch 411, Batch 1000/1000: LR=9.01e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:36:57,915 | INFO | Epoch 411 Train Time 24.713732957839966s

2025-10-18 18:37:20,453 | INFO | Training epoch 412, Batch 1000/1000: LR=9.00e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 18:37:20,539 | INFO | Epoch 412 Train Time 22.622952461242676s

2025-10-18 18:37:44,293 | INFO | Training epoch 413, Batch 1000/1000: LR=9.00e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:37:44,360 | INFO | Epoch 413 Train Time 23.819562435150146s

2025-10-18 18:38:08,130 | INFO | Training epoch 414, Batch 1000/1000: LR=8.99e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.42e-01
2025-10-18 18:38:08,219 | INFO | Epoch 414 Train Time 23.858104944229126s

2025-10-18 18:38:31,341 | INFO | Training epoch 415, Batch 1000/1000: LR=8.99e-05, Loss=2.88e-02 BER=1.09e-02 FER=1.41e-01
2025-10-18 18:38:31,431 | INFO | Epoch 415 Train Time 23.209823608398438s

2025-10-18 18:38:54,352 | INFO | Training epoch 416, Batch 1000/1000: LR=8.98e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:38:54,435 | INFO | Epoch 416 Train Time 23.001418590545654s

2025-10-18 18:39:13,102 | INFO | Training epoch 417, Batch 1000/1000: LR=8.98e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:39:13,165 | INFO | Epoch 417 Train Time 18.72989845275879s

2025-10-18 18:39:35,433 | INFO | Training epoch 418, Batch 1000/1000: LR=8.98e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 18:39:35,498 | INFO | Epoch 418 Train Time 22.331963777542114s

2025-10-18 18:39:58,241 | INFO | Training epoch 419, Batch 1000/1000: LR=8.97e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:39:58,327 | INFO | Epoch 419 Train Time 22.827038049697876s

2025-10-18 18:40:22,154 | INFO | Training epoch 420, Batch 1000/1000: LR=8.97e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 18:40:22,226 | INFO | Epoch 420 Train Time 23.897510290145874s

2025-10-18 18:40:44,218 | INFO | Training epoch 421, Batch 1000/1000: LR=8.96e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:40:44,283 | INFO | Epoch 421 Train Time 22.05483913421631s

2025-10-18 18:41:07,903 | INFO | Training epoch 422, Batch 1000/1000: LR=8.96e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:41:07,968 | INFO | Epoch 422 Train Time 23.685086965560913s

2025-10-18 18:41:31,127 | INFO | Training epoch 423, Batch 1000/1000: LR=8.95e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:41:31,195 | INFO | Epoch 423 Train Time 23.225650548934937s

2025-10-18 18:41:54,052 | INFO | Training epoch 424, Batch 1000/1000: LR=8.95e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:41:54,123 | INFO | Epoch 424 Train Time 22.92716693878174s

2025-10-18 18:42:17,259 | INFO | Training epoch 425, Batch 1000/1000: LR=8.94e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:42:17,359 | INFO | Epoch 425 Train Time 23.23529887199402s

2025-10-18 18:42:40,021 | INFO | Training epoch 426, Batch 1000/1000: LR=8.94e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:42:40,127 | INFO | Epoch 426 Train Time 22.76559853553772s

2025-10-18 18:43:04,144 | INFO | Training epoch 427, Batch 1000/1000: LR=8.93e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:43:04,217 | INFO | Epoch 427 Train Time 24.089370727539062s

2025-10-18 18:43:27,155 | INFO | Training epoch 428, Batch 1000/1000: LR=8.93e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-18 18:43:27,240 | INFO | Epoch 428 Train Time 23.020827770233154s

2025-10-18 18:43:50,406 | INFO | Training epoch 429, Batch 1000/1000: LR=8.92e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:43:50,498 | INFO | Epoch 429 Train Time 23.256778955459595s

2025-10-18 18:44:13,519 | INFO | Training epoch 430, Batch 1000/1000: LR=8.92e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:44:13,599 | INFO | Epoch 430 Train Time 23.099757194519043s

2025-10-18 18:44:36,843 | INFO | Training epoch 431, Batch 1000/1000: LR=8.91e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:44:36,924 | INFO | Epoch 431 Train Time 23.324267625808716s

2025-10-18 18:45:00,218 | INFO | Training epoch 432, Batch 1000/1000: LR=8.91e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 18:45:00,285 | INFO | Epoch 432 Train Time 23.36021113395691s

2025-10-18 18:45:23,436 | INFO | Training epoch 433, Batch 1000/1000: LR=8.90e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:45:23,509 | INFO | Epoch 433 Train Time 23.222078323364258s

2025-10-18 18:45:47,135 | INFO | Training epoch 434, Batch 1000/1000: LR=8.90e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:45:47,202 | INFO | Epoch 434 Train Time 23.69261860847473s

2025-10-18 18:46:10,760 | INFO | Training epoch 435, Batch 1000/1000: LR=8.89e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:46:10,854 | INFO | Epoch 435 Train Time 23.65032124519348s

2025-10-18 18:46:35,051 | INFO | Training epoch 436, Batch 1000/1000: LR=8.89e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:46:35,134 | INFO | Epoch 436 Train Time 24.2786386013031s

2025-10-18 18:46:58,255 | INFO | Training epoch 437, Batch 1000/1000: LR=8.88e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:46:58,335 | INFO | Epoch 437 Train Time 23.20060110092163s

2025-10-18 18:47:20,456 | INFO | Training epoch 438, Batch 1000/1000: LR=8.88e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:47:20,538 | INFO | Epoch 438 Train Time 22.201356172561646s

2025-10-18 18:47:43,636 | INFO | Training epoch 439, Batch 1000/1000: LR=8.87e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:47:43,719 | INFO | Epoch 439 Train Time 23.180737733840942s

2025-10-18 18:48:06,474 | INFO | Training epoch 440, Batch 1000/1000: LR=8.87e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:48:06,550 | INFO | Epoch 440 Train Time 22.82992458343506s

2025-10-18 18:48:30,565 | INFO | Training epoch 441, Batch 1000/1000: LR=8.86e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:48:30,637 | INFO | Epoch 441 Train Time 24.08517360687256s

2025-10-18 18:48:53,119 | INFO | Training epoch 442, Batch 1000/1000: LR=8.86e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:48:53,183 | INFO | Epoch 442 Train Time 22.544111490249634s

2025-10-18 18:49:12,309 | INFO | Training epoch 443, Batch 1000/1000: LR=8.85e-05, Loss=2.88e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:49:12,373 | INFO | Epoch 443 Train Time 19.189314126968384s

2025-10-18 18:49:36,357 | INFO | Training epoch 444, Batch 1000/1000: LR=8.85e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:49:36,422 | INFO | Epoch 444 Train Time 24.04724669456482s

2025-10-18 18:50:00,035 | INFO | Training epoch 445, Batch 1000/1000: LR=8.84e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:50:00,100 | INFO | Epoch 445 Train Time 23.677040815353394s

2025-10-18 18:50:23,314 | INFO | Training epoch 446, Batch 1000/1000: LR=8.84e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:50:23,387 | INFO | Epoch 446 Train Time 23.28637671470642s

2025-10-18 18:50:46,945 | INFO | Training epoch 447, Batch 1000/1000: LR=8.83e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:50:47,014 | INFO | Epoch 447 Train Time 23.626009225845337s

2025-10-18 18:51:10,541 | INFO | Training epoch 448, Batch 1000/1000: LR=8.83e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:51:10,611 | INFO | Epoch 448 Train Time 23.595333337783813s

2025-10-18 18:51:33,655 | INFO | Training epoch 449, Batch 1000/1000: LR=8.82e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:51:33,740 | INFO | Epoch 449 Train Time 23.12769079208374s

2025-10-18 18:51:56,472 | INFO | Training epoch 450, Batch 1000/1000: LR=8.82e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:51:56,554 | INFO | Epoch 450 Train Time 22.8129780292511s

2025-10-18 18:52:20,518 | INFO | Training epoch 451, Batch 1000/1000: LR=8.81e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:52:20,601 | INFO | Epoch 451 Train Time 24.046355485916138s

2025-10-18 18:52:45,425 | INFO | Training epoch 452, Batch 1000/1000: LR=8.81e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 18:52:45,503 | INFO | Epoch 452 Train Time 24.90031933784485s

2025-10-18 18:53:08,021 | INFO | Training epoch 453, Batch 1000/1000: LR=8.80e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 18:53:08,104 | INFO | Epoch 453 Train Time 22.599737882614136s

2025-10-18 18:53:31,621 | INFO | Training epoch 454, Batch 1000/1000: LR=8.80e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:53:31,683 | INFO | Epoch 454 Train Time 23.57707953453064s

2025-10-18 18:53:55,124 | INFO | Training epoch 455, Batch 1000/1000: LR=8.79e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.39e-01
2025-10-18 18:53:55,190 | INFO | Epoch 455 Train Time 23.50674343109131s

2025-10-18 18:53:55,191 | INFO | [P1] saving best_model with loss 0.028012 at epoch 455
2025-10-18 18:54:18,003 | INFO | Training epoch 456, Batch 1000/1000: LR=8.79e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:54:18,066 | INFO | Epoch 456 Train Time 22.861578226089478s

2025-10-18 18:54:40,529 | INFO | Training epoch 457, Batch 1000/1000: LR=8.78e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 18:54:40,597 | INFO | Epoch 457 Train Time 22.529191255569458s

2025-10-18 18:55:04,345 | INFO | Training epoch 458, Batch 1000/1000: LR=8.78e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 18:55:04,430 | INFO | Epoch 458 Train Time 23.832275867462158s

2025-10-18 18:55:27,452 | INFO | Training epoch 459, Batch 1000/1000: LR=8.77e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 18:55:27,528 | INFO | Epoch 459 Train Time 23.096369981765747s

2025-10-18 18:55:50,745 | INFO | Training epoch 460, Batch 1000/1000: LR=8.77e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:55:50,808 | INFO | Epoch 460 Train Time 23.27858567237854s

2025-10-18 18:56:13,943 | INFO | Training epoch 461, Batch 1000/1000: LR=8.76e-05, Loss=2.88e-02 BER=1.09e-02 FER=1.41e-01
2025-10-18 18:56:14,018 | INFO | Epoch 461 Train Time 23.208391904830933s

2025-10-18 18:56:36,419 | INFO | Training epoch 462, Batch 1000/1000: LR=8.76e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-18 18:56:36,501 | INFO | Epoch 462 Train Time 22.481175422668457s

2025-10-18 18:57:00,367 | INFO | Training epoch 463, Batch 1000/1000: LR=8.75e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:57:00,456 | INFO | Epoch 463 Train Time 23.954121589660645s

2025-10-18 18:57:23,241 | INFO | Training epoch 464, Batch 1000/1000: LR=8.75e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 18:57:23,303 | INFO | Epoch 464 Train Time 22.84558916091919s

2025-10-18 18:57:45,014 | INFO | Training epoch 465, Batch 1000/1000: LR=8.74e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 18:57:45,083 | INFO | Epoch 465 Train Time 21.778897285461426s

2025-10-18 18:58:08,545 | INFO | Training epoch 466, Batch 1000/1000: LR=8.74e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 18:58:08,611 | INFO | Epoch 466 Train Time 23.527043104171753s

2025-10-18 18:58:31,549 | INFO | Training epoch 467, Batch 1000/1000: LR=8.73e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.39e-01
2025-10-18 18:58:31,631 | INFO | Epoch 467 Train Time 23.01934003829956s

2025-10-18 18:58:54,354 | INFO | Training epoch 468, Batch 1000/1000: LR=8.73e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:58:54,423 | INFO | Epoch 468 Train Time 22.79056406021118s

2025-10-18 18:59:17,635 | INFO | Training epoch 469, Batch 1000/1000: LR=8.72e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 18:59:17,724 | INFO | Epoch 469 Train Time 23.30017614364624s

2025-10-18 18:59:40,698 | INFO | Training epoch 470, Batch 1000/1000: LR=8.72e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 18:59:40,785 | INFO | Epoch 470 Train Time 23.059367656707764s

2025-10-18 19:00:03,722 | INFO | Training epoch 471, Batch 1000/1000: LR=8.71e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:00:03,784 | INFO | Epoch 471 Train Time 22.998367071151733s

2025-10-18 19:00:27,516 | INFO | Training epoch 472, Batch 1000/1000: LR=8.71e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:00:27,578 | INFO | Epoch 472 Train Time 23.791541576385498s

2025-10-18 19:00:50,127 | INFO | Training epoch 473, Batch 1000/1000: LR=8.70e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:00:50,197 | INFO | Epoch 473 Train Time 22.617838621139526s

2025-10-18 19:01:12,935 | INFO | Training epoch 474, Batch 1000/1000: LR=8.70e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-18 19:01:12,994 | INFO | Epoch 474 Train Time 22.79616403579712s

2025-10-18 19:01:36,248 | INFO | Training epoch 475, Batch 1000/1000: LR=8.69e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:01:36,319 | INFO | Epoch 475 Train Time 23.324041604995728s

2025-10-18 19:02:00,749 | INFO | Training epoch 476, Batch 1000/1000: LR=8.68e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:02:00,819 | INFO | Epoch 476 Train Time 24.49912714958191s

2025-10-18 19:02:22,100 | INFO | Training epoch 477, Batch 1000/1000: LR=8.68e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:02:22,169 | INFO | Epoch 477 Train Time 21.349427461624146s

2025-10-18 19:02:45,723 | INFO | Training epoch 478, Batch 1000/1000: LR=8.67e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 19:02:45,814 | INFO | Epoch 478 Train Time 23.643086671829224s

2025-10-18 19:03:09,027 | INFO | Training epoch 479, Batch 1000/1000: LR=8.67e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:03:09,094 | INFO | Epoch 479 Train Time 23.278469562530518s

2025-10-18 19:03:31,715 | INFO | Training epoch 480, Batch 1000/1000: LR=8.66e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:03:31,785 | INFO | Epoch 480 Train Time 22.689804315567017s

2025-10-18 19:03:55,835 | INFO | Training epoch 481, Batch 1000/1000: LR=8.66e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:03:55,921 | INFO | Epoch 481 Train Time 24.13584613800049s

2025-10-18 19:04:17,110 | INFO | Training epoch 482, Batch 1000/1000: LR=8.65e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:04:17,185 | INFO | Epoch 482 Train Time 21.261672019958496s

2025-10-18 19:04:40,894 | INFO | Training epoch 483, Batch 1000/1000: LR=8.65e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:04:40,974 | INFO | Epoch 483 Train Time 23.789028882980347s

2025-10-18 19:05:05,347 | INFO | Training epoch 484, Batch 1000/1000: LR=8.64e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:05:05,417 | INFO | Epoch 484 Train Time 24.441906213760376s

2025-10-18 19:05:29,016 | INFO | Training epoch 485, Batch 1000/1000: LR=8.64e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:05:29,100 | INFO | Epoch 485 Train Time 23.680611848831177s

2025-10-18 19:05:52,754 | INFO | Training epoch 486, Batch 1000/1000: LR=8.63e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:05:52,836 | INFO | Epoch 486 Train Time 23.735278129577637s

2025-10-18 19:06:14,057 | INFO | Training epoch 487, Batch 1000/1000: LR=8.63e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.38e-01
2025-10-18 19:06:14,148 | INFO | Epoch 487 Train Time 21.311502695083618s

2025-10-18 19:06:36,871 | INFO | Training epoch 488, Batch 1000/1000: LR=8.62e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:06:36,955 | INFO | Epoch 488 Train Time 22.80574870109558s

2025-10-18 19:07:00,209 | INFO | Training epoch 489, Batch 1000/1000: LR=8.62e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 19:07:00,274 | INFO | Epoch 489 Train Time 23.31799340248108s

2025-10-18 19:07:23,427 | INFO | Training epoch 490, Batch 1000/1000: LR=8.61e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.37e-01
2025-10-18 19:07:23,493 | INFO | Epoch 490 Train Time 23.218123197555542s

2025-10-18 19:07:23,494 | INFO | [P1] saving best_model with loss 0.027820 at epoch 490
2025-10-18 19:07:46,872 | INFO | Training epoch 491, Batch 1000/1000: LR=8.60e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:07:46,939 | INFO | Epoch 491 Train Time 23.427159070968628s

2025-10-18 19:08:10,647 | INFO | Training epoch 492, Batch 1000/1000: LR=8.60e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 19:08:10,719 | INFO | Epoch 492 Train Time 23.77946448326111s

2025-10-18 19:08:33,831 | INFO | Training epoch 493, Batch 1000/1000: LR=8.59e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:08:33,905 | INFO | Epoch 493 Train Time 23.18382740020752s

2025-10-18 19:08:56,503 | INFO | Training epoch 494, Batch 1000/1000: LR=8.59e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:08:56,583 | INFO | Epoch 494 Train Time 22.676661491394043s

2025-10-18 19:09:19,131 | INFO | Training epoch 495, Batch 1000/1000: LR=8.58e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 19:09:19,215 | INFO | Epoch 495 Train Time 22.630802631378174s

2025-10-18 19:09:41,808 | INFO | Training epoch 496, Batch 1000/1000: LR=8.58e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:09:41,882 | INFO | Epoch 496 Train Time 22.666553020477295s

2025-10-18 19:10:04,625 | INFO | Training epoch 497, Batch 1000/1000: LR=8.57e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:10:04,724 | INFO | Epoch 497 Train Time 22.84074592590332s

2025-10-18 19:10:27,342 | INFO | Training epoch 498, Batch 1000/1000: LR=8.57e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:10:27,409 | INFO | Epoch 498 Train Time 22.68352484703064s

2025-10-18 19:10:49,036 | INFO | Training epoch 499, Batch 1000/1000: LR=8.56e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:10:49,105 | INFO | Epoch 499 Train Time 21.694747924804688s

2025-10-18 19:11:12,838 | INFO | Training epoch 500, Batch 1000/1000: LR=8.56e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:11:12,908 | INFO | Epoch 500 Train Time 23.801433086395264s

2025-10-18 19:11:35,521 | INFO | Training epoch 501, Batch 1000/1000: LR=8.55e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:11:35,585 | INFO | Epoch 501 Train Time 22.675755262374878s

2025-10-18 19:11:59,009 | INFO | Training epoch 502, Batch 1000/1000: LR=8.54e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:11:59,093 | INFO | Epoch 502 Train Time 23.506285667419434s

2025-10-18 19:12:22,869 | INFO | Training epoch 503, Batch 1000/1000: LR=8.54e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:12:22,960 | INFO | Epoch 503 Train Time 23.865718603134155s

2025-10-18 19:12:47,437 | INFO | Training epoch 504, Batch 1000/1000: LR=8.53e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:12:47,511 | INFO | Epoch 504 Train Time 24.54989790916443s

2025-10-18 19:13:10,048 | INFO | Training epoch 505, Batch 1000/1000: LR=8.53e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 19:13:10,132 | INFO | Epoch 505 Train Time 22.620086193084717s

2025-10-18 19:13:10,133 | INFO | [P1] saving best_model with loss 0.027783 at epoch 505
2025-10-18 19:13:33,347 | INFO | Training epoch 506, Batch 1000/1000: LR=8.52e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.39e-01
2025-10-18 19:13:33,418 | INFO | Epoch 506 Train Time 23.264649629592896s

2025-10-18 19:13:56,345 | INFO | Training epoch 507, Batch 1000/1000: LR=8.52e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:13:56,420 | INFO | Epoch 507 Train Time 23.000675439834595s

2025-10-18 19:14:20,739 | INFO | Training epoch 508, Batch 1000/1000: LR=8.51e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 19:14:20,814 | INFO | Epoch 508 Train Time 24.393197774887085s

2025-10-18 19:14:41,104 | INFO | Training epoch 509, Batch 1000/1000: LR=8.51e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:14:41,185 | INFO | Epoch 509 Train Time 20.36968731880188s

2025-10-18 19:15:04,723 | INFO | Training epoch 510, Batch 1000/1000: LR=8.50e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:15:04,796 | INFO | Epoch 510 Train Time 23.609882593154907s

2025-10-18 19:15:27,514 | INFO | Training epoch 511, Batch 1000/1000: LR=8.49e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:15:27,600 | INFO | Epoch 511 Train Time 22.80325698852539s

2025-10-18 19:15:50,350 | INFO | Training epoch 512, Batch 1000/1000: LR=8.49e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:15:50,428 | INFO | Epoch 512 Train Time 22.8262619972229s

2025-10-18 19:16:13,652 | INFO | Training epoch 513, Batch 1000/1000: LR=8.48e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:16:13,733 | INFO | Epoch 513 Train Time 23.304611682891846s

2025-10-18 19:16:37,025 | INFO | Training epoch 514, Batch 1000/1000: LR=8.48e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:16:37,108 | INFO | Epoch 514 Train Time 23.37376070022583s

2025-10-18 19:17:00,251 | INFO | Training epoch 515, Batch 1000/1000: LR=8.47e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:17:00,333 | INFO | Epoch 515 Train Time 23.22406578063965s

2025-10-18 19:17:24,009 | INFO | Training epoch 516, Batch 1000/1000: LR=8.47e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:17:24,078 | INFO | Epoch 516 Train Time 23.74427103996277s

2025-10-18 19:17:47,634 | INFO | Training epoch 517, Batch 1000/1000: LR=8.46e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:17:47,720 | INFO | Epoch 517 Train Time 23.640217304229736s

2025-10-18 19:18:11,759 | INFO | Training epoch 518, Batch 1000/1000: LR=8.46e-05, Loss=2.82e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:18:11,832 | INFO | Epoch 518 Train Time 24.110626935958862s

2025-10-18 19:18:34,450 | INFO | Training epoch 519, Batch 1000/1000: LR=8.45e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:18:34,528 | INFO | Epoch 519 Train Time 22.693961143493652s

2025-10-18 19:18:56,923 | INFO | Training epoch 520, Batch 1000/1000: LR=8.44e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:18:57,000 | INFO | Epoch 520 Train Time 22.469106435775757s

2025-10-18 19:19:19,222 | INFO | Training epoch 521, Batch 1000/1000: LR=8.44e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.41e-01
2025-10-18 19:19:19,305 | INFO | Epoch 521 Train Time 22.30411696434021s

2025-10-18 19:19:42,426 | INFO | Training epoch 522, Batch 1000/1000: LR=8.43e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:19:42,515 | INFO | Epoch 522 Train Time 23.209583282470703s

2025-10-18 19:20:06,144 | INFO | Training epoch 523, Batch 1000/1000: LR=8.43e-05, Loss=2.83e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:20:06,216 | INFO | Epoch 523 Train Time 23.699371099472046s

2025-10-18 19:20:27,513 | INFO | Training epoch 524, Batch 1000/1000: LR=8.42e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.39e-01
2025-10-18 19:20:27,580 | INFO | Epoch 524 Train Time 21.36342692375183s

2025-10-18 19:20:50,756 | INFO | Training epoch 525, Batch 1000/1000: LR=8.42e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:20:50,836 | INFO | Epoch 525 Train Time 23.254368782043457s

2025-10-18 19:21:14,120 | INFO | Training epoch 526, Batch 1000/1000: LR=8.41e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:21:14,203 | INFO | Epoch 526 Train Time 23.3663809299469s

2025-10-18 19:21:37,242 | INFO | Training epoch 527, Batch 1000/1000: LR=8.40e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:21:37,339 | INFO | Epoch 527 Train Time 23.134738206863403s

2025-10-18 19:22:00,233 | INFO | Training epoch 528, Batch 1000/1000: LR=8.40e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.40e-01
2025-10-18 19:22:00,302 | INFO | Epoch 528 Train Time 22.961588859558105s

2025-10-18 19:22:23,635 | INFO | Training epoch 529, Batch 1000/1000: LR=8.39e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:22:23,717 | INFO | Epoch 529 Train Time 23.41287112236023s

2025-10-18 19:22:47,232 | INFO | Training epoch 530, Batch 1000/1000: LR=8.39e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:22:47,308 | INFO | Epoch 530 Train Time 23.58969497680664s

2025-10-18 19:23:10,414 | INFO | Training epoch 531, Batch 1000/1000: LR=8.38e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 19:23:10,487 | INFO | Epoch 531 Train Time 23.178079843521118s

2025-10-18 19:23:33,555 | INFO | Training epoch 532, Batch 1000/1000: LR=8.38e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-18 19:23:33,620 | INFO | Epoch 532 Train Time 23.1325900554657s

2025-10-18 19:23:56,057 | INFO | Training epoch 533, Batch 1000/1000: LR=8.37e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:23:56,122 | INFO | Epoch 533 Train Time 22.500771045684814s

2025-10-18 19:24:20,253 | INFO | Training epoch 534, Batch 1000/1000: LR=8.36e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:24:20,344 | INFO | Epoch 534 Train Time 24.220750093460083s

2025-10-18 19:24:44,143 | INFO | Training epoch 535, Batch 1000/1000: LR=8.36e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:24:44,225 | INFO | Epoch 535 Train Time 23.878410816192627s

2025-10-18 19:25:07,113 | INFO | Training epoch 536, Batch 1000/1000: LR=8.35e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:25:07,197 | INFO | Epoch 536 Train Time 22.97052264213562s

2025-10-18 19:25:30,155 | INFO | Training epoch 537, Batch 1000/1000: LR=8.35e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:25:30,234 | INFO | Epoch 537 Train Time 23.037101984024048s

2025-10-18 19:25:53,097 | INFO | Training epoch 538, Batch 1000/1000: LR=8.34e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:25:53,185 | INFO | Epoch 538 Train Time 22.949523448944092s

2025-10-18 19:26:16,554 | INFO | Training epoch 539, Batch 1000/1000: LR=8.34e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:26:16,642 | INFO | Epoch 539 Train Time 23.45587468147278s

2025-10-18 19:26:40,416 | INFO | Training epoch 540, Batch 1000/1000: LR=8.33e-05, Loss=2.85e-02 BER=1.08e-02 FER=1.40e-01
2025-10-18 19:26:40,495 | INFO | Epoch 540 Train Time 23.85184073448181s

2025-10-18 19:27:03,599 | INFO | Training epoch 541, Batch 1000/1000: LR=8.32e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:27:03,680 | INFO | Epoch 541 Train Time 23.18433928489685s

2025-10-18 19:27:27,411 | INFO | Training epoch 542, Batch 1000/1000: LR=8.32e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:27:27,475 | INFO | Epoch 542 Train Time 23.79407286643982s

2025-10-18 19:27:50,622 | INFO | Training epoch 543, Batch 1000/1000: LR=8.31e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:27:50,694 | INFO | Epoch 543 Train Time 23.217835426330566s

2025-10-18 19:28:13,320 | INFO | Training epoch 544, Batch 1000/1000: LR=8.31e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:28:13,404 | INFO | Epoch 544 Train Time 22.70878577232361s

2025-10-18 19:28:36,726 | INFO | Training epoch 545, Batch 1000/1000: LR=8.30e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:28:36,809 | INFO | Epoch 545 Train Time 23.40459656715393s

2025-10-18 19:29:00,154 | INFO | Training epoch 546, Batch 1000/1000: LR=8.29e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.38e-01
2025-10-18 19:29:00,235 | INFO | Epoch 546 Train Time 23.424302339553833s

2025-10-18 19:29:23,500 | INFO | Training epoch 547, Batch 1000/1000: LR=8.29e-05, Loss=2.82e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:29:23,574 | INFO | Epoch 547 Train Time 23.33763575553894s

2025-10-18 19:29:47,154 | INFO | Training epoch 548, Batch 1000/1000: LR=8.28e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:29:47,220 | INFO | Epoch 548 Train Time 23.644279718399048s

2025-10-18 19:30:09,737 | INFO | Training epoch 549, Batch 1000/1000: LR=8.28e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 19:30:09,805 | INFO | Epoch 549 Train Time 22.584034204483032s

2025-10-18 19:30:33,650 | INFO | Training epoch 550, Batch 1000/1000: LR=8.27e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:30:33,737 | INFO | Epoch 550 Train Time 23.93080186843872s

2025-10-18 19:30:56,947 | INFO | Training epoch 551, Batch 1000/1000: LR=8.26e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.39e-01
2025-10-18 19:30:57,023 | INFO | Epoch 551 Train Time 23.28444242477417s

2025-10-18 19:31:20,451 | INFO | Training epoch 552, Batch 1000/1000: LR=8.26e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:31:20,539 | INFO | Epoch 552 Train Time 23.515252113342285s

2025-10-18 19:31:44,021 | INFO | Training epoch 553, Batch 1000/1000: LR=8.25e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 19:31:44,105 | INFO | Epoch 553 Train Time 23.564050674438477s

2025-10-18 19:32:05,447 | INFO | Training epoch 554, Batch 1000/1000: LR=8.25e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:32:05,532 | INFO | Epoch 554 Train Time 21.426486253738403s

2025-10-18 19:32:29,213 | INFO | Training epoch 555, Batch 1000/1000: LR=8.24e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:32:29,283 | INFO | Epoch 555 Train Time 23.749788522720337s

2025-10-18 19:32:52,457 | INFO | Training epoch 556, Batch 1000/1000: LR=8.24e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:32:52,544 | INFO | Epoch 556 Train Time 23.260042667388916s

2025-10-18 19:33:13,511 | INFO | Training epoch 557, Batch 1000/1000: LR=8.23e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:33:13,602 | INFO | Epoch 557 Train Time 21.057283401489258s

2025-10-18 19:33:36,734 | INFO | Training epoch 558, Batch 1000/1000: LR=8.22e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:33:36,813 | INFO | Epoch 558 Train Time 23.2096049785614s

2025-10-18 19:33:59,303 | INFO | Training epoch 559, Batch 1000/1000: LR=8.22e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.38e-01
2025-10-18 19:33:59,382 | INFO | Epoch 559 Train Time 22.568472623825073s

2025-10-18 19:34:22,416 | INFO | Training epoch 560, Batch 1000/1000: LR=8.21e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:34:22,493 | INFO | Epoch 560 Train Time 23.10940432548523s

2025-10-18 19:34:45,639 | INFO | Training epoch 561, Batch 1000/1000: LR=8.21e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:34:45,726 | INFO | Epoch 561 Train Time 23.232036352157593s

2025-10-18 19:35:09,251 | INFO | Training epoch 562, Batch 1000/1000: LR=8.20e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.38e-01
2025-10-18 19:35:09,319 | INFO | Epoch 562 Train Time 23.591724634170532s

2025-10-18 19:35:33,036 | INFO | Training epoch 563, Batch 1000/1000: LR=8.19e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:35:33,101 | INFO | Epoch 563 Train Time 23.780259370803833s

2025-10-18 19:35:56,351 | INFO | Training epoch 564, Batch 1000/1000: LR=8.19e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:35:56,416 | INFO | Epoch 564 Train Time 23.31283187866211s

2025-10-18 19:35:56,417 | INFO | [P1] saving best_model with loss 0.027714 at epoch 564
2025-10-18 19:36:18,641 | INFO | Training epoch 565, Batch 1000/1000: LR=8.18e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:36:18,731 | INFO | Epoch 565 Train Time 22.298253774642944s

2025-10-18 19:36:42,560 | INFO | Training epoch 566, Batch 1000/1000: LR=8.18e-05, Loss=2.82e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:36:42,660 | INFO | Epoch 566 Train Time 23.92803382873535s

2025-10-18 19:37:02,319 | INFO | Training epoch 567, Batch 1000/1000: LR=8.17e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:37:02,383 | INFO | Epoch 567 Train Time 19.721458911895752s

2025-10-18 19:37:25,520 | INFO | Training epoch 568, Batch 1000/1000: LR=8.16e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:37:25,601 | INFO | Epoch 568 Train Time 23.216484546661377s

2025-10-18 19:37:48,844 | INFO | Training epoch 569, Batch 1000/1000: LR=8.16e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:37:48,927 | INFO | Epoch 569 Train Time 23.324532985687256s

2025-10-18 19:38:11,426 | INFO | Training epoch 570, Batch 1000/1000: LR=8.15e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:38:11,510 | INFO | Epoch 570 Train Time 22.58204984664917s

2025-10-18 19:38:32,924 | INFO | Training epoch 571, Batch 1000/1000: LR=8.14e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:38:32,990 | INFO | Epoch 571 Train Time 21.479401111602783s

2025-10-18 19:38:56,457 | INFO | Training epoch 572, Batch 1000/1000: LR=8.14e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-18 19:38:56,536 | INFO | Epoch 572 Train Time 23.544599533081055s

2025-10-18 19:39:19,232 | INFO | Training epoch 573, Batch 1000/1000: LR=8.13e-05, Loss=2.83e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:39:19,308 | INFO | Epoch 573 Train Time 22.76998805999756s

2025-10-18 19:39:42,908 | INFO | Training epoch 574, Batch 1000/1000: LR=8.13e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:39:42,984 | INFO | Epoch 574 Train Time 23.67576026916504s

2025-10-18 19:40:06,752 | INFO | Training epoch 575, Batch 1000/1000: LR=8.12e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:40:06,837 | INFO | Epoch 575 Train Time 23.851864099502563s

2025-10-18 19:40:29,936 | INFO | Training epoch 576, Batch 1000/1000: LR=8.11e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:40:30,009 | INFO | Epoch 576 Train Time 23.1709885597229s

2025-10-18 19:40:52,940 | INFO | Training epoch 577, Batch 1000/1000: LR=8.11e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:40:53,005 | INFO | Epoch 577 Train Time 22.995310306549072s

2025-10-18 19:41:16,847 | INFO | Training epoch 578, Batch 1000/1000: LR=8.10e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:41:16,929 | INFO | Epoch 578 Train Time 23.922763347625732s

2025-10-18 19:41:40,746 | INFO | Training epoch 579, Batch 1000/1000: LR=8.10e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:41:40,834 | INFO | Epoch 579 Train Time 23.903585195541382s

2025-10-18 19:42:03,868 | INFO | Training epoch 580, Batch 1000/1000: LR=8.09e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:42:03,936 | INFO | Epoch 580 Train Time 23.101384162902832s

2025-10-18 19:42:24,145 | INFO | Training epoch 581, Batch 1000/1000: LR=8.08e-05, Loss=2.82e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:42:24,208 | INFO | Epoch 581 Train Time 20.27017879486084s

2025-10-18 19:42:47,523 | INFO | Training epoch 582, Batch 1000/1000: LR=8.08e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:42:47,594 | INFO | Epoch 582 Train Time 23.38511300086975s

2025-10-18 19:43:07,402 | INFO | Training epoch 583, Batch 1000/1000: LR=8.07e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:43:07,460 | INFO | Epoch 583 Train Time 19.865722179412842s

2025-10-18 19:43:30,901 | INFO | Training epoch 584, Batch 1000/1000: LR=8.07e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:43:30,966 | INFO | Epoch 584 Train Time 23.504645824432373s

2025-10-18 19:43:54,332 | INFO | Training epoch 585, Batch 1000/1000: LR=8.06e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:43:54,397 | INFO | Epoch 585 Train Time 23.43026566505432s

2025-10-18 19:44:18,440 | INFO | Training epoch 586, Batch 1000/1000: LR=8.05e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:44:18,526 | INFO | Epoch 586 Train Time 24.128137826919556s

2025-10-18 19:44:41,602 | INFO | Training epoch 587, Batch 1000/1000: LR=8.05e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:44:41,665 | INFO | Epoch 587 Train Time 23.138002157211304s

2025-10-18 19:45:04,586 | INFO | Training epoch 588, Batch 1000/1000: LR=8.04e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:45:04,650 | INFO | Epoch 588 Train Time 22.983577251434326s

2025-10-18 19:45:27,626 | INFO | Training epoch 589, Batch 1000/1000: LR=8.03e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:45:27,714 | INFO | Epoch 589 Train Time 23.061880588531494s

2025-10-18 19:45:51,020 | INFO | Training epoch 590, Batch 1000/1000: LR=8.03e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:45:51,106 | INFO | Epoch 590 Train Time 23.39090609550476s

2025-10-18 19:46:14,646 | INFO | Training epoch 591, Batch 1000/1000: LR=8.02e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:46:14,722 | INFO | Epoch 591 Train Time 23.614648580551147s

2025-10-18 19:46:38,528 | INFO | Training epoch 592, Batch 1000/1000: LR=8.02e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:46:38,605 | INFO | Epoch 592 Train Time 23.882518529891968s

2025-10-18 19:47:02,952 | INFO | Training epoch 593, Batch 1000/1000: LR=8.01e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:47:03,016 | INFO | Epoch 593 Train Time 24.409974813461304s

2025-10-18 19:47:25,838 | INFO | Training epoch 594, Batch 1000/1000: LR=8.00e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:47:25,913 | INFO | Epoch 594 Train Time 22.895947217941284s

2025-10-18 19:47:48,639 | INFO | Training epoch 595, Batch 1000/1000: LR=8.00e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:47:48,725 | INFO | Epoch 595 Train Time 22.811620950698853s

2025-10-18 19:48:12,203 | INFO | Training epoch 596, Batch 1000/1000: LR=7.99e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:48:12,288 | INFO | Epoch 596 Train Time 23.56118655204773s

2025-10-18 19:48:36,430 | INFO | Training epoch 597, Batch 1000/1000: LR=7.98e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:48:36,520 | INFO | Epoch 597 Train Time 24.230409383773804s

2025-10-18 19:48:58,926 | INFO | Training epoch 598, Batch 1000/1000: LR=7.98e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:48:59,018 | INFO | Epoch 598 Train Time 22.496708869934082s

2025-10-18 19:49:21,640 | INFO | Training epoch 599, Batch 1000/1000: LR=7.97e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-18 19:49:21,725 | INFO | Epoch 599 Train Time 22.705855131149292s

2025-10-18 19:49:45,142 | INFO | Training epoch 600, Batch 1000/1000: LR=7.97e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 19:49:45,210 | INFO | Epoch 600 Train Time 23.483969926834106s

2025-10-18 19:50:08,829 | INFO | Training epoch 601, Batch 1000/1000: LR=7.96e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:50:08,895 | INFO | Epoch 601 Train Time 23.68380308151245s

2025-10-18 19:50:31,051 | INFO | Training epoch 602, Batch 1000/1000: LR=7.95e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.39e-01
2025-10-18 19:50:31,129 | INFO | Epoch 602 Train Time 22.232526540756226s

2025-10-18 19:50:54,219 | INFO | Training epoch 603, Batch 1000/1000: LR=7.95e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:50:54,306 | INFO | Epoch 603 Train Time 23.17480158805847s

2025-10-18 19:51:15,029 | INFO | Training epoch 604, Batch 1000/1000: LR=7.94e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:51:15,094 | INFO | Epoch 604 Train Time 20.785621881484985s

2025-10-18 19:51:38,729 | INFO | Training epoch 605, Batch 1000/1000: LR=7.93e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:51:38,792 | INFO | Epoch 605 Train Time 23.69705629348755s

2025-10-18 19:52:01,629 | INFO | Training epoch 606, Batch 1000/1000: LR=7.93e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 19:52:01,704 | INFO | Epoch 606 Train Time 22.911466598510742s

2025-10-18 19:52:25,749 | INFO | Training epoch 607, Batch 1000/1000: LR=7.92e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:52:25,828 | INFO | Epoch 607 Train Time 24.121177196502686s

2025-10-18 19:52:49,719 | INFO | Training epoch 608, Batch 1000/1000: LR=7.92e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:52:49,793 | INFO | Epoch 608 Train Time 23.96438193321228s

2025-10-18 19:53:10,333 | INFO | Training epoch 609, Batch 1000/1000: LR=7.91e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:53:10,419 | INFO | Epoch 609 Train Time 20.623717546463013s

2025-10-18 19:53:31,223 | INFO | Training epoch 610, Batch 1000/1000: LR=7.90e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:53:31,301 | INFO | Epoch 610 Train Time 20.881527185440063s

2025-10-18 19:53:54,442 | INFO | Training epoch 611, Batch 1000/1000: LR=7.90e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 19:53:54,508 | INFO | Epoch 611 Train Time 23.205121278762817s

2025-10-18 19:53:54,508 | INFO | [P1] saving best_model with loss 0.027578 at epoch 611
2025-10-18 19:54:16,847 | INFO | Training epoch 612, Batch 1000/1000: LR=7.89e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:54:16,922 | INFO | Epoch 612 Train Time 22.393691062927246s

2025-10-18 19:54:41,117 | INFO | Training epoch 613, Batch 1000/1000: LR=7.88e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:54:41,201 | INFO | Epoch 613 Train Time 24.27790904045105s

2025-10-18 19:55:03,949 | INFO | Training epoch 614, Batch 1000/1000: LR=7.88e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:55:04,030 | INFO | Epoch 614 Train Time 22.82750654220581s

2025-10-18 19:55:25,301 | INFO | Training epoch 615, Batch 1000/1000: LR=7.87e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:55:25,370 | INFO | Epoch 615 Train Time 21.338292360305786s

2025-10-18 19:55:47,652 | INFO | Training epoch 616, Batch 1000/1000: LR=7.86e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:55:47,722 | INFO | Epoch 616 Train Time 22.351232767105103s

2025-10-18 19:56:10,649 | INFO | Training epoch 617, Batch 1000/1000: LR=7.86e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:56:10,721 | INFO | Epoch 617 Train Time 22.99845790863037s

2025-10-18 19:56:34,341 | INFO | Training epoch 618, Batch 1000/1000: LR=7.85e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 19:56:34,427 | INFO | Epoch 618 Train Time 23.704522132873535s

2025-10-18 19:56:56,008 | INFO | Training epoch 619, Batch 1000/1000: LR=7.85e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:56:56,084 | INFO | Epoch 619 Train Time 21.656033992767334s

2025-10-18 19:57:19,313 | INFO | Training epoch 620, Batch 1000/1000: LR=7.84e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 19:57:19,378 | INFO | Epoch 620 Train Time 23.293753623962402s

2025-10-18 19:57:43,850 | INFO | Training epoch 621, Batch 1000/1000: LR=7.83e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 19:57:43,934 | INFO | Epoch 621 Train Time 24.55417013168335s

2025-10-18 19:58:07,537 | INFO | Training epoch 622, Batch 1000/1000: LR=7.83e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 19:58:07,599 | INFO | Epoch 622 Train Time 23.663671255111694s

2025-10-18 19:58:31,513 | INFO | Training epoch 623, Batch 1000/1000: LR=7.82e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:58:31,590 | INFO | Epoch 623 Train Time 23.99033212661743s

2025-10-18 19:58:54,827 | INFO | Training epoch 624, Batch 1000/1000: LR=7.81e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 19:58:54,909 | INFO | Epoch 624 Train Time 23.318456172943115s

2025-10-18 19:59:17,013 | INFO | Training epoch 625, Batch 1000/1000: LR=7.81e-05, Loss=2.79e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 19:59:17,090 | INFO | Epoch 625 Train Time 22.17971706390381s

2025-10-18 19:59:40,855 | INFO | Training epoch 626, Batch 1000/1000: LR=7.80e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 19:59:40,937 | INFO | Epoch 626 Train Time 23.845417976379395s

2025-10-18 20:00:04,451 | INFO | Training epoch 627, Batch 1000/1000: LR=7.79e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:00:04,523 | INFO | Epoch 627 Train Time 23.585184812545776s

2025-10-18 20:00:28,060 | INFO | Training epoch 628, Batch 1000/1000: LR=7.79e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:00:28,150 | INFO | Epoch 628 Train Time 23.62583589553833s

2025-10-18 20:00:50,816 | INFO | Training epoch 629, Batch 1000/1000: LR=7.78e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:00:50,879 | INFO | Epoch 629 Train Time 22.727931261062622s

2025-10-18 20:01:13,423 | INFO | Training epoch 630, Batch 1000/1000: LR=7.77e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:01:13,489 | INFO | Epoch 630 Train Time 22.60939121246338s

2025-10-18 20:01:13,490 | INFO | [P1] saving best_model with loss 0.027542 at epoch 630
2025-10-18 20:01:36,054 | INFO | Training epoch 631, Batch 1000/1000: LR=7.77e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.38e-01
2025-10-18 20:01:36,118 | INFO | Epoch 631 Train Time 22.61388397216797s

2025-10-18 20:01:58,700 | INFO | Training epoch 632, Batch 1000/1000: LR=7.76e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:01:58,785 | INFO | Epoch 632 Train Time 22.666505336761475s

2025-10-18 20:02:22,030 | INFO | Training epoch 633, Batch 1000/1000: LR=7.75e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:02:22,117 | INFO | Epoch 633 Train Time 23.3310284614563s

2025-10-18 20:02:45,613 | INFO | Training epoch 634, Batch 1000/1000: LR=7.75e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 20:02:45,673 | INFO | Epoch 634 Train Time 23.555278539657593s

2025-10-18 20:03:08,145 | INFO | Training epoch 635, Batch 1000/1000: LR=7.74e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:03:08,224 | INFO | Epoch 635 Train Time 22.54892897605896s

2025-10-18 20:03:30,737 | INFO | Training epoch 636, Batch 1000/1000: LR=7.74e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:03:30,808 | INFO | Epoch 636 Train Time 22.582719802856445s

2025-10-18 20:03:54,157 | INFO | Training epoch 637, Batch 1000/1000: LR=7.73e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:03:54,225 | INFO | Epoch 637 Train Time 23.416457176208496s

2025-10-18 20:04:17,936 | INFO | Training epoch 638, Batch 1000/1000: LR=7.72e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:04:18,008 | INFO | Epoch 638 Train Time 23.7819881439209s

2025-10-18 20:04:41,246 | INFO | Training epoch 639, Batch 1000/1000: LR=7.72e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.38e-01
2025-10-18 20:04:41,313 | INFO | Epoch 639 Train Time 23.30366063117981s

2025-10-18 20:05:04,822 | INFO | Training epoch 640, Batch 1000/1000: LR=7.71e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:05:04,902 | INFO | Epoch 640 Train Time 23.588335752487183s

2025-10-18 20:05:27,559 | INFO | Training epoch 641, Batch 1000/1000: LR=7.70e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:05:27,632 | INFO | Epoch 641 Train Time 22.729520559310913s

2025-10-18 20:05:49,932 | INFO | Training epoch 642, Batch 1000/1000: LR=7.70e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:05:50,004 | INFO | Epoch 642 Train Time 22.371171236038208s

2025-10-18 20:06:12,819 | INFO | Training epoch 643, Batch 1000/1000: LR=7.69e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:06:12,890 | INFO | Epoch 643 Train Time 22.884031772613525s

2025-10-18 20:06:36,252 | INFO | Training epoch 644, Batch 1000/1000: LR=7.68e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:06:36,323 | INFO | Epoch 644 Train Time 23.43281078338623s

2025-10-18 20:06:59,640 | INFO | Training epoch 645, Batch 1000/1000: LR=7.68e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:06:59,709 | INFO | Epoch 645 Train Time 23.385066032409668s

2025-10-18 20:07:22,977 | INFO | Training epoch 646, Batch 1000/1000: LR=7.67e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:07:23,069 | INFO | Epoch 646 Train Time 23.358742475509644s

2025-10-18 20:07:47,347 | INFO | Training epoch 647, Batch 1000/1000: LR=7.66e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:07:47,414 | INFO | Epoch 647 Train Time 24.34260869026184s

2025-10-18 20:08:10,829 | INFO | Training epoch 648, Batch 1000/1000: LR=7.66e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:08:10,905 | INFO | Epoch 648 Train Time 23.490299701690674s

2025-10-18 20:08:34,421 | INFO | Training epoch 649, Batch 1000/1000: LR=7.65e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:08:34,505 | INFO | Epoch 649 Train Time 23.59916877746582s

2025-10-18 20:08:57,337 | INFO | Training epoch 650, Batch 1000/1000: LR=7.64e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:08:57,410 | INFO | Epoch 650 Train Time 22.904557943344116s

2025-10-18 20:09:21,005 | INFO | Training epoch 651, Batch 1000/1000: LR=7.64e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 20:09:21,070 | INFO | Epoch 651 Train Time 23.659167051315308s

2025-10-18 20:09:40,839 | INFO | Training epoch 652, Batch 1000/1000: LR=7.63e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:09:40,906 | INFO | Epoch 652 Train Time 19.834545612335205s

2025-10-18 20:10:04,052 | INFO | Training epoch 653, Batch 1000/1000: LR=7.62e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:10:04,117 | INFO | Epoch 653 Train Time 23.20916247367859s

2025-10-18 20:10:04,118 | INFO | [P1] saving best_model with loss 0.027514 at epoch 653
2025-10-18 20:10:28,658 | INFO | Training epoch 654, Batch 1000/1000: LR=7.62e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:10:28,746 | INFO | Epoch 654 Train Time 24.611258029937744s

2025-10-18 20:10:52,352 | INFO | Training epoch 655, Batch 1000/1000: LR=7.61e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-18 20:10:52,436 | INFO | Epoch 655 Train Time 23.68913173675537s

2025-10-18 20:11:14,739 | INFO | Training epoch 656, Batch 1000/1000: LR=7.60e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:11:14,814 | INFO | Epoch 656 Train Time 22.375932693481445s

2025-10-18 20:11:38,136 | INFO | Training epoch 657, Batch 1000/1000: LR=7.60e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:11:38,220 | INFO | Epoch 657 Train Time 23.404144763946533s

2025-10-18 20:12:00,408 | INFO | Training epoch 658, Batch 1000/1000: LR=7.59e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:12:00,491 | INFO | Epoch 658 Train Time 22.270308256149292s

2025-10-18 20:12:22,522 | INFO | Training epoch 659, Batch 1000/1000: LR=7.58e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:12:22,587 | INFO | Epoch 659 Train Time 22.095531225204468s

2025-10-18 20:12:45,546 | INFO | Training epoch 660, Batch 1000/1000: LR=7.58e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:12:45,606 | INFO | Epoch 660 Train Time 23.01780605316162s

2025-10-18 20:13:08,626 | INFO | Training epoch 661, Batch 1000/1000: LR=7.57e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:13:08,700 | INFO | Epoch 661 Train Time 23.092560052871704s

2025-10-18 20:13:31,747 | INFO | Training epoch 662, Batch 1000/1000: LR=7.56e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:13:31,834 | INFO | Epoch 662 Train Time 23.133163452148438s

2025-10-18 20:13:55,149 | INFO | Training epoch 663, Batch 1000/1000: LR=7.56e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:13:55,229 | INFO | Epoch 663 Train Time 23.394233226776123s

2025-10-18 20:14:16,987 | INFO | Training epoch 664, Batch 1000/1000: LR=7.55e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:14:17,053 | INFO | Epoch 664 Train Time 21.82147216796875s

2025-10-18 20:14:36,554 | INFO | Training epoch 665, Batch 1000/1000: LR=7.54e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:14:36,622 | INFO | Epoch 665 Train Time 19.56781005859375s

2025-10-18 20:14:59,427 | INFO | Training epoch 666, Batch 1000/1000: LR=7.54e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:14:59,514 | INFO | Epoch 666 Train Time 22.89125919342041s

2025-10-18 20:15:19,819 | INFO | Training epoch 667, Batch 1000/1000: LR=7.53e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:15:19,883 | INFO | Epoch 667 Train Time 20.367136001586914s

2025-10-18 20:15:43,341 | INFO | Training epoch 668, Batch 1000/1000: LR=7.52e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:15:43,404 | INFO | Epoch 668 Train Time 23.520094394683838s

2025-10-18 20:16:07,095 | INFO | Training epoch 669, Batch 1000/1000: LR=7.52e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:16:07,161 | INFO | Epoch 669 Train Time 23.755347967147827s

2025-10-18 20:16:30,337 | INFO | Training epoch 670, Batch 1000/1000: LR=7.51e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:16:30,404 | INFO | Epoch 670 Train Time 23.242223739624023s

2025-10-18 20:16:53,033 | INFO | Training epoch 671, Batch 1000/1000: LR=7.50e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:16:53,108 | INFO | Epoch 671 Train Time 22.702856063842773s

2025-10-18 20:17:16,652 | INFO | Training epoch 672, Batch 1000/1000: LR=7.50e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:17:16,736 | INFO | Epoch 672 Train Time 23.626553297042847s

2025-10-18 20:17:40,242 | INFO | Training epoch 673, Batch 1000/1000: LR=7.49e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.36e-01
2025-10-18 20:17:40,309 | INFO | Epoch 673 Train Time 23.571125507354736s

2025-10-18 20:18:03,554 | INFO | Training epoch 674, Batch 1000/1000: LR=7.48e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:18:03,641 | INFO | Epoch 674 Train Time 23.330326557159424s

2025-10-18 20:18:24,273 | INFO | Training epoch 675, Batch 1000/1000: LR=7.48e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:18:24,341 | INFO | Epoch 675 Train Time 20.698806285858154s

2025-10-18 20:18:46,905 | INFO | Training epoch 676, Batch 1000/1000: LR=7.47e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:18:46,968 | INFO | Epoch 676 Train Time 22.624814748764038s

2025-10-18 20:19:09,835 | INFO | Training epoch 677, Batch 1000/1000: LR=7.46e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:19:09,923 | INFO | Epoch 677 Train Time 22.95368242263794s

2025-10-18 20:19:31,240 | INFO | Training epoch 678, Batch 1000/1000: LR=7.46e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:19:31,348 | INFO | Epoch 678 Train Time 21.422797679901123s

2025-10-18 20:19:54,017 | INFO | Training epoch 679, Batch 1000/1000: LR=7.45e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:19:54,094 | INFO | Epoch 679 Train Time 22.743691444396973s

2025-10-18 20:20:17,530 | INFO | Training epoch 680, Batch 1000/1000: LR=7.44e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:20:17,593 | INFO | Epoch 680 Train Time 23.497708797454834s

2025-10-18 20:20:40,962 | INFO | Training epoch 681, Batch 1000/1000: LR=7.43e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:20:41,047 | INFO | Epoch 681 Train Time 23.453166961669922s

2025-10-18 20:21:05,937 | INFO | Training epoch 682, Batch 1000/1000: LR=7.43e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.36e-01
2025-10-18 20:21:06,009 | INFO | Epoch 682 Train Time 24.96070146560669s

2025-10-18 20:21:29,533 | INFO | Training epoch 683, Batch 1000/1000: LR=7.42e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:21:29,626 | INFO | Epoch 683 Train Time 23.615792274475098s

2025-10-18 20:21:53,268 | INFO | Training epoch 684, Batch 1000/1000: LR=7.41e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:21:53,368 | INFO | Epoch 684 Train Time 23.739230632781982s

2025-10-18 20:22:15,998 | INFO | Training epoch 685, Batch 1000/1000: LR=7.41e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:22:16,075 | INFO | Epoch 685 Train Time 22.70585799217224s

2025-10-18 20:22:39,017 | INFO | Training epoch 686, Batch 1000/1000: LR=7.40e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 20:22:39,100 | INFO | Epoch 686 Train Time 23.02378249168396s

2025-10-18 20:23:02,917 | INFO | Training epoch 687, Batch 1000/1000: LR=7.39e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:23:03,006 | INFO | Epoch 687 Train Time 23.90437602996826s

2025-10-18 20:23:26,562 | INFO | Training epoch 688, Batch 1000/1000: LR=7.39e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:23:26,630 | INFO | Epoch 688 Train Time 23.623217582702637s

2025-10-18 20:23:26,631 | INFO | [P1] saving best_model with loss 0.027363 at epoch 688
2025-10-18 20:23:51,041 | INFO | Training epoch 689, Batch 1000/1000: LR=7.38e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:23:51,115 | INFO | Epoch 689 Train Time 24.470247268676758s

2025-10-18 20:24:13,916 | INFO | Training epoch 690, Batch 1000/1000: LR=7.37e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.35e-01
2025-10-18 20:24:13,990 | INFO | Epoch 690 Train Time 22.87394905090332s

2025-10-18 20:24:37,024 | INFO | Training epoch 691, Batch 1000/1000: LR=7.37e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:24:37,112 | INFO | Epoch 691 Train Time 23.121583700180054s

2025-10-18 20:25:00,910 | INFO | Training epoch 692, Batch 1000/1000: LR=7.36e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:25:00,988 | INFO | Epoch 692 Train Time 23.8739972114563s

2025-10-18 20:25:24,535 | INFO | Training epoch 693, Batch 1000/1000: LR=7.35e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:25:24,620 | INFO | Epoch 693 Train Time 23.63150715827942s

2025-10-18 20:25:47,438 | INFO | Training epoch 694, Batch 1000/1000: LR=7.35e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:25:47,508 | INFO | Epoch 694 Train Time 22.88678789138794s

2025-10-18 20:26:10,633 | INFO | Training epoch 695, Batch 1000/1000: LR=7.34e-05, Loss=2.82e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 20:26:10,719 | INFO | Epoch 695 Train Time 23.209595203399658s

2025-10-18 20:26:34,036 | INFO | Training epoch 696, Batch 1000/1000: LR=7.33e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:26:34,102 | INFO | Epoch 696 Train Time 23.382094144821167s

2025-10-18 20:26:56,895 | INFO | Training epoch 697, Batch 1000/1000: LR=7.32e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:26:56,959 | INFO | Epoch 697 Train Time 22.85514760017395s

2025-10-18 20:27:19,811 | INFO | Training epoch 698, Batch 1000/1000: LR=7.32e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 20:27:19,892 | INFO | Epoch 698 Train Time 22.93130874633789s

2025-10-18 20:27:42,455 | INFO | Training epoch 699, Batch 1000/1000: LR=7.31e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:27:42,522 | INFO | Epoch 699 Train Time 22.630149602890015s

2025-10-18 20:28:03,305 | INFO | Training epoch 700, Batch 1000/1000: LR=7.30e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:28:03,369 | INFO | Epoch 700 Train Time 20.845797777175903s

2025-10-18 20:28:26,360 | INFO | Training epoch 701, Batch 1000/1000: LR=7.30e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:28:26,441 | INFO | Epoch 701 Train Time 23.07035756111145s

2025-10-18 20:28:49,720 | INFO | Training epoch 702, Batch 1000/1000: LR=7.29e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:28:49,812 | INFO | Epoch 702 Train Time 23.370086908340454s

2025-10-18 20:29:12,643 | INFO | Training epoch 703, Batch 1000/1000: LR=7.28e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:29:12,717 | INFO | Epoch 703 Train Time 22.903911590576172s

2025-10-18 20:29:36,033 | INFO | Training epoch 704, Batch 1000/1000: LR=7.28e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:29:36,100 | INFO | Epoch 704 Train Time 23.38190746307373s

2025-10-18 20:29:59,235 | INFO | Training epoch 705, Batch 1000/1000: LR=7.27e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:29:59,303 | INFO | Epoch 705 Train Time 23.201302528381348s

2025-10-18 20:30:22,502 | INFO | Training epoch 706, Batch 1000/1000: LR=7.26e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 20:30:22,571 | INFO | Epoch 706 Train Time 23.266145706176758s

2025-10-18 20:30:45,517 | INFO | Training epoch 707, Batch 1000/1000: LR=7.26e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:30:45,584 | INFO | Epoch 707 Train Time 23.012155771255493s

2025-10-18 20:31:06,836 | INFO | Training epoch 708, Batch 1000/1000: LR=7.25e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:31:06,903 | INFO | Epoch 708 Train Time 21.319011926651s

2025-10-18 20:31:30,319 | INFO | Training epoch 709, Batch 1000/1000: LR=7.24e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:31:30,389 | INFO | Epoch 709 Train Time 23.48410201072693s

2025-10-18 20:31:54,059 | INFO | Training epoch 710, Batch 1000/1000: LR=7.23e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:31:54,140 | INFO | Epoch 710 Train Time 23.74924612045288s

2025-10-18 20:32:17,432 | INFO | Training epoch 711, Batch 1000/1000: LR=7.23e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:32:17,508 | INFO | Epoch 711 Train Time 23.36514401435852s

2025-10-18 20:32:41,427 | INFO | Training epoch 712, Batch 1000/1000: LR=7.22e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:32:41,502 | INFO | Epoch 712 Train Time 23.993274211883545s

2025-10-18 20:33:05,430 | INFO | Training epoch 713, Batch 1000/1000: LR=7.21e-05, Loss=2.83e-02 BER=1.07e-02 FER=1.38e-01
2025-10-18 20:33:05,525 | INFO | Epoch 713 Train Time 24.02116584777832s

2025-10-18 20:33:29,459 | INFO | Training epoch 714, Batch 1000/1000: LR=7.21e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:33:29,522 | INFO | Epoch 714 Train Time 23.99568462371826s

2025-10-18 20:33:53,380 | INFO | Training epoch 715, Batch 1000/1000: LR=7.20e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 20:33:53,449 | INFO | Epoch 715 Train Time 23.925123691558838s

2025-10-18 20:34:16,019 | INFO | Training epoch 716, Batch 1000/1000: LR=7.19e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:34:16,085 | INFO | Epoch 716 Train Time 22.635416507720947s

2025-10-18 20:34:40,397 | INFO | Training epoch 717, Batch 1000/1000: LR=7.19e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:34:40,480 | INFO | Epoch 717 Train Time 24.393768548965454s

2025-10-18 20:35:00,732 | INFO | Training epoch 718, Batch 1000/1000: LR=7.18e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 20:35:00,796 | INFO | Epoch 718 Train Time 20.315457820892334s

2025-10-18 20:35:23,315 | INFO | Training epoch 719, Batch 1000/1000: LR=7.17e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:35:23,397 | INFO | Epoch 719 Train Time 22.5991849899292s

2025-10-18 20:35:46,047 | INFO | Training epoch 720, Batch 1000/1000: LR=7.16e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:35:46,143 | INFO | Epoch 720 Train Time 22.74479389190674s

2025-10-18 20:36:08,328 | INFO | Training epoch 721, Batch 1000/1000: LR=7.16e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:36:08,422 | INFO | Epoch 721 Train Time 22.277653217315674s

2025-10-18 20:36:31,016 | INFO | Training epoch 722, Batch 1000/1000: LR=7.15e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:36:31,092 | INFO | Epoch 722 Train Time 22.667290925979614s

2025-10-18 20:36:54,434 | INFO | Training epoch 723, Batch 1000/1000: LR=7.14e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:36:54,506 | INFO | Epoch 723 Train Time 23.411186695098877s

2025-10-18 20:37:18,046 | INFO | Training epoch 724, Batch 1000/1000: LR=7.14e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 20:37:18,116 | INFO | Epoch 724 Train Time 23.60882806777954s

2025-10-18 20:37:41,158 | INFO | Training epoch 725, Batch 1000/1000: LR=7.13e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:37:41,224 | INFO | Epoch 725 Train Time 23.107022523880005s

2025-10-18 20:38:03,952 | INFO | Training epoch 726, Batch 1000/1000: LR=7.12e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:38:04,022 | INFO | Epoch 726 Train Time 22.79708504676819s

2025-10-18 20:38:28,357 | INFO | Training epoch 727, Batch 1000/1000: LR=7.12e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:38:28,445 | INFO | Epoch 727 Train Time 24.42213487625122s

2025-10-18 20:38:51,853 | INFO | Training epoch 728, Batch 1000/1000: LR=7.11e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:38:51,949 | INFO | Epoch 728 Train Time 23.502748250961304s

2025-10-18 20:39:14,848 | INFO | Training epoch 729, Batch 1000/1000: LR=7.10e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:39:14,934 | INFO | Epoch 729 Train Time 22.982545852661133s

2025-10-18 20:39:38,445 | INFO | Training epoch 730, Batch 1000/1000: LR=7.09e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:39:38,520 | INFO | Epoch 730 Train Time 23.58470869064331s

2025-10-18 20:39:38,521 | INFO | [P1] saving best_model with loss 0.027328 at epoch 730
2025-10-18 20:40:01,961 | INFO | Training epoch 731, Batch 1000/1000: LR=7.09e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:40:02,031 | INFO | Epoch 731 Train Time 23.494093656539917s

2025-10-18 20:40:23,410 | INFO | Training epoch 732, Batch 1000/1000: LR=7.08e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.38e-01
2025-10-18 20:40:23,492 | INFO | Epoch 732 Train Time 21.458322048187256s

2025-10-18 20:40:46,745 | INFO | Training epoch 733, Batch 1000/1000: LR=7.07e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 20:40:46,831 | INFO | Epoch 733 Train Time 23.338736057281494s

2025-10-18 20:41:10,026 | INFO | Training epoch 734, Batch 1000/1000: LR=7.07e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:41:10,088 | INFO | Epoch 734 Train Time 23.256053686141968s

2025-10-18 20:41:33,419 | INFO | Training epoch 735, Batch 1000/1000: LR=7.06e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:41:33,495 | INFO | Epoch 735 Train Time 23.404508352279663s

2025-10-18 20:41:57,558 | INFO | Training epoch 736, Batch 1000/1000: LR=7.05e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:41:57,627 | INFO | Epoch 736 Train Time 24.130245685577393s

2025-10-18 20:42:21,237 | INFO | Training epoch 737, Batch 1000/1000: LR=7.04e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:42:21,304 | INFO | Epoch 737 Train Time 23.676669120788574s

2025-10-18 20:42:45,356 | INFO | Training epoch 738, Batch 1000/1000: LR=7.04e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:42:45,418 | INFO | Epoch 738 Train Time 24.113128900527954s

2025-10-18 20:43:07,928 | INFO | Training epoch 739, Batch 1000/1000: LR=7.03e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:43:08,007 | INFO | Epoch 739 Train Time 22.58811116218567s

2025-10-18 20:43:31,662 | INFO | Training epoch 740, Batch 1000/1000: LR=7.02e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:43:31,728 | INFO | Epoch 740 Train Time 23.71976351737976s

2025-10-18 20:43:55,030 | INFO | Training epoch 741, Batch 1000/1000: LR=7.02e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:43:55,097 | INFO | Epoch 741 Train Time 23.368223905563354s

2025-10-18 20:44:18,448 | INFO | Training epoch 742, Batch 1000/1000: LR=7.01e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:44:18,531 | INFO | Epoch 742 Train Time 23.432574033737183s

2025-10-18 20:44:42,061 | INFO | Training epoch 743, Batch 1000/1000: LR=7.00e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:44:42,150 | INFO | Epoch 743 Train Time 23.618083000183105s

2025-10-18 20:45:05,845 | INFO | Training epoch 744, Batch 1000/1000: LR=6.99e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:45:05,910 | INFO | Epoch 744 Train Time 23.75916337966919s

2025-10-18 20:45:29,120 | INFO | Training epoch 745, Batch 1000/1000: LR=6.99e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:45:29,188 | INFO | Epoch 745 Train Time 23.27651333808899s

2025-10-18 20:45:51,129 | INFO | Training epoch 746, Batch 1000/1000: LR=6.98e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:45:51,215 | INFO | Epoch 746 Train Time 22.02665400505066s

2025-10-18 20:46:13,432 | INFO | Training epoch 747, Batch 1000/1000: LR=6.97e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.34e-01
2025-10-18 20:46:13,518 | INFO | Epoch 747 Train Time 22.300859689712524s

2025-10-18 20:46:37,324 | INFO | Training epoch 748, Batch 1000/1000: LR=6.97e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:46:37,406 | INFO | Epoch 748 Train Time 23.887168169021606s

2025-10-18 20:47:00,251 | INFO | Training epoch 749, Batch 1000/1000: LR=6.96e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:47:00,317 | INFO | Epoch 749 Train Time 22.909589529037476s

2025-10-18 20:47:22,644 | INFO | Training epoch 750, Batch 1000/1000: LR=6.95e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:47:22,720 | INFO | Epoch 750 Train Time 22.402419567108154s

2025-10-18 20:47:45,698 | INFO | Training epoch 751, Batch 1000/1000: LR=6.94e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:47:45,783 | INFO | Epoch 751 Train Time 23.061597108840942s

2025-10-18 20:48:09,423 | INFO | Training epoch 752, Batch 1000/1000: LR=6.94e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:48:09,512 | INFO | Epoch 752 Train Time 23.726786613464355s

2025-10-18 20:48:32,555 | INFO | Training epoch 753, Batch 1000/1000: LR=6.93e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:48:32,632 | INFO | Epoch 753 Train Time 23.119054317474365s

2025-10-18 20:48:55,910 | INFO | Training epoch 754, Batch 1000/1000: LR=6.92e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-18 20:48:55,991 | INFO | Epoch 754 Train Time 23.35855984687805s

2025-10-18 20:49:16,009 | INFO | Training epoch 755, Batch 1000/1000: LR=6.92e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:49:16,093 | INFO | Epoch 755 Train Time 20.100727081298828s

2025-10-18 20:49:38,663 | INFO | Training epoch 756, Batch 1000/1000: LR=6.91e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:49:38,736 | INFO | Epoch 756 Train Time 22.6414577960968s

2025-10-18 20:50:01,030 | INFO | Training epoch 757, Batch 1000/1000: LR=6.90e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:50:01,114 | INFO | Epoch 757 Train Time 22.37748122215271s

2025-10-18 20:50:23,116 | INFO | Training epoch 758, Batch 1000/1000: LR=6.89e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:50:23,196 | INFO | Epoch 758 Train Time 22.080034494400024s

2025-10-18 20:50:45,857 | INFO | Training epoch 759, Batch 1000/1000: LR=6.89e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:50:45,941 | INFO | Epoch 759 Train Time 22.744049549102783s

2025-10-18 20:51:09,217 | INFO | Training epoch 760, Batch 1000/1000: LR=6.88e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:51:09,303 | INFO | Epoch 760 Train Time 23.359664916992188s

2025-10-18 20:51:32,751 | INFO | Training epoch 761, Batch 1000/1000: LR=6.87e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:51:32,824 | INFO | Epoch 761 Train Time 23.520169496536255s

2025-10-18 20:51:56,385 | INFO | Training epoch 762, Batch 1000/1000: LR=6.86e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 20:51:56,469 | INFO | Epoch 762 Train Time 23.643412590026855s

2025-10-18 20:52:19,051 | INFO | Training epoch 763, Batch 1000/1000: LR=6.86e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:52:19,120 | INFO | Epoch 763 Train Time 22.649978637695312s

2025-10-18 20:52:41,872 | INFO | Training epoch 764, Batch 1000/1000: LR=6.85e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:52:41,953 | INFO | Epoch 764 Train Time 22.832147359848022s

2025-10-18 20:53:04,742 | INFO | Training epoch 765, Batch 1000/1000: LR=6.84e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 20:53:04,819 | INFO | Epoch 765 Train Time 22.86430549621582s

2025-10-18 20:53:28,451 | INFO | Training epoch 766, Batch 1000/1000: LR=6.84e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:53:28,543 | INFO | Epoch 766 Train Time 23.723646640777588s

2025-10-18 20:53:52,330 | INFO | Training epoch 767, Batch 1000/1000: LR=6.83e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:53:52,412 | INFO | Epoch 767 Train Time 23.868011236190796s

2025-10-18 20:54:16,229 | INFO | Training epoch 768, Batch 1000/1000: LR=6.82e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:54:16,307 | INFO | Epoch 768 Train Time 23.89297342300415s

2025-10-18 20:54:40,067 | INFO | Training epoch 769, Batch 1000/1000: LR=6.81e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 20:54:40,157 | INFO | Epoch 769 Train Time 23.848474740982056s

2025-10-18 20:55:02,738 | INFO | Training epoch 770, Batch 1000/1000: LR=6.81e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:55:02,827 | INFO | Epoch 770 Train Time 22.668787717819214s

2025-10-18 20:55:26,355 | INFO | Training epoch 771, Batch 1000/1000: LR=6.80e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 20:55:26,445 | INFO | Epoch 771 Train Time 23.615110158920288s

2025-10-18 20:55:49,438 | INFO | Training epoch 772, Batch 1000/1000: LR=6.79e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:55:49,512 | INFO | Epoch 772 Train Time 23.065674304962158s

2025-10-18 20:56:11,540 | INFO | Training epoch 773, Batch 1000/1000: LR=6.79e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 20:56:11,612 | INFO | Epoch 773 Train Time 22.099607467651367s

2025-10-18 20:56:34,676 | INFO | Training epoch 774, Batch 1000/1000: LR=6.78e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:56:34,755 | INFO | Epoch 774 Train Time 23.141566514968872s

2025-10-18 20:56:57,435 | INFO | Training epoch 775, Batch 1000/1000: LR=6.77e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:56:57,521 | INFO | Epoch 775 Train Time 22.765995025634766s

2025-10-18 20:57:21,147 | INFO | Training epoch 776, Batch 1000/1000: LR=6.76e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 20:57:21,232 | INFO | Epoch 776 Train Time 23.709519624710083s

2025-10-18 20:57:44,530 | INFO | Training epoch 777, Batch 1000/1000: LR=6.76e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 20:57:44,606 | INFO | Epoch 777 Train Time 23.373878240585327s

2025-10-18 20:58:06,904 | INFO | Training epoch 778, Batch 1000/1000: LR=6.75e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:58:06,966 | INFO | Epoch 778 Train Time 22.359045028686523s

2025-10-18 20:58:30,535 | INFO | Training epoch 779, Batch 1000/1000: LR=6.74e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 20:58:30,624 | INFO | Epoch 779 Train Time 23.656278133392334s

2025-10-18 20:58:53,732 | INFO | Training epoch 780, Batch 1000/1000: LR=6.73e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:58:53,816 | INFO | Epoch 780 Train Time 23.190792560577393s

2025-10-18 20:59:17,723 | INFO | Training epoch 781, Batch 1000/1000: LR=6.73e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 20:59:17,800 | INFO | Epoch 781 Train Time 23.982306957244873s

2025-10-18 20:59:41,140 | INFO | Training epoch 782, Batch 1000/1000: LR=6.72e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 20:59:41,222 | INFO | Epoch 782 Train Time 23.41899824142456s

2025-10-18 21:00:03,412 | INFO | Training epoch 783, Batch 1000/1000: LR=6.71e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:00:03,497 | INFO | Epoch 783 Train Time 22.27387809753418s

2025-10-18 21:00:27,651 | INFO | Training epoch 784, Batch 1000/1000: LR=6.70e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 21:00:27,736 | INFO | Epoch 784 Train Time 24.237759828567505s

2025-10-18 21:00:51,339 | INFO | Training epoch 785, Batch 1000/1000: LR=6.70e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:00:51,414 | INFO | Epoch 785 Train Time 23.67530608177185s

2025-10-18 21:01:15,276 | INFO | Training epoch 786, Batch 1000/1000: LR=6.69e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:01:15,356 | INFO | Epoch 786 Train Time 23.94046974182129s

2025-10-18 21:01:38,721 | INFO | Training epoch 787, Batch 1000/1000: LR=6.68e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 21:01:38,808 | INFO | Epoch 787 Train Time 23.451775312423706s

2025-10-18 21:02:01,440 | INFO | Training epoch 788, Batch 1000/1000: LR=6.68e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:02:01,516 | INFO | Epoch 788 Train Time 22.706836223602295s

2025-10-18 21:02:24,009 | INFO | Training epoch 789, Batch 1000/1000: LR=6.67e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:02:24,072 | INFO | Epoch 789 Train Time 22.554731369018555s

2025-10-18 21:02:47,914 | INFO | Training epoch 790, Batch 1000/1000: LR=6.66e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:02:47,999 | INFO | Epoch 790 Train Time 23.926213026046753s

2025-10-18 21:03:10,845 | INFO | Training epoch 791, Batch 1000/1000: LR=6.65e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:03:10,923 | INFO | Epoch 791 Train Time 22.923760890960693s

2025-10-18 21:03:34,613 | INFO | Training epoch 792, Batch 1000/1000: LR=6.65e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:03:34,680 | INFO | Epoch 792 Train Time 23.75537371635437s

2025-10-18 21:03:57,328 | INFO | Training epoch 793, Batch 1000/1000: LR=6.64e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 21:03:57,411 | INFO | Epoch 793 Train Time 22.730692863464355s

2025-10-18 21:04:20,559 | INFO | Training epoch 794, Batch 1000/1000: LR=6.63e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:04:20,645 | INFO | Epoch 794 Train Time 23.23315143585205s

2025-10-18 21:04:44,099 | INFO | Training epoch 795, Batch 1000/1000: LR=6.62e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:04:44,174 | INFO | Epoch 795 Train Time 23.528075695037842s

2025-10-18 21:05:05,757 | INFO | Training epoch 796, Batch 1000/1000: LR=6.62e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 21:05:05,842 | INFO | Epoch 796 Train Time 21.6663715839386s

2025-10-18 21:05:28,435 | INFO | Training epoch 797, Batch 1000/1000: LR=6.61e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:05:28,499 | INFO | Epoch 797 Train Time 22.656136512756348s

2025-10-18 21:05:52,223 | INFO | Training epoch 798, Batch 1000/1000: LR=6.60e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:05:52,303 | INFO | Epoch 798 Train Time 23.803072929382324s

2025-10-18 21:05:52,305 | INFO | [P1] saving best_model with loss 0.027191 at epoch 798
2025-10-18 21:06:16,134 | INFO | Training epoch 799, Batch 1000/1000: LR=6.59e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:06:16,228 | INFO | Epoch 799 Train Time 23.904475212097168s

2025-10-18 21:06:39,533 | INFO | Training epoch 800, Batch 1000/1000: LR=6.59e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:06:39,603 | INFO | Epoch 800 Train Time 23.374216318130493s

2025-10-18 21:07:02,843 | INFO | Training epoch 801, Batch 1000/1000: LR=6.58e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:07:02,918 | INFO | Epoch 801 Train Time 23.31405019760132s

2025-10-18 21:07:26,157 | INFO | Training epoch 802, Batch 1000/1000: LR=6.57e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:07:26,241 | INFO | Epoch 802 Train Time 23.321803331375122s

2025-10-18 21:07:49,250 | INFO | Training epoch 803, Batch 1000/1000: LR=6.56e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:07:49,319 | INFO | Epoch 803 Train Time 23.077279090881348s

2025-10-18 21:08:12,545 | INFO | Training epoch 804, Batch 1000/1000: LR=6.56e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:08:12,628 | INFO | Epoch 804 Train Time 23.308225393295288s

2025-10-18 21:08:36,901 | INFO | Training epoch 805, Batch 1000/1000: LR=6.55e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:08:36,974 | INFO | Epoch 805 Train Time 24.34179425239563s

2025-10-18 21:08:59,813 | INFO | Training epoch 806, Batch 1000/1000: LR=6.54e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:08:59,901 | INFO | Epoch 806 Train Time 22.92600989341736s

2025-10-18 21:09:24,067 | INFO | Training epoch 807, Batch 1000/1000: LR=6.54e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:09:24,149 | INFO | Epoch 807 Train Time 24.24667453765869s

2025-10-18 21:09:47,802 | INFO | Training epoch 808, Batch 1000/1000: LR=6.53e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:09:47,867 | INFO | Epoch 808 Train Time 23.716792821884155s

2025-10-18 21:10:11,147 | INFO | Training epoch 809, Batch 1000/1000: LR=6.52e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:10:11,230 | INFO | Epoch 809 Train Time 23.361635446548462s

2025-10-18 21:10:35,023 | INFO | Training epoch 810, Batch 1000/1000: LR=6.51e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:10:35,090 | INFO | Epoch 810 Train Time 23.859039545059204s

2025-10-18 21:10:58,414 | INFO | Training epoch 811, Batch 1000/1000: LR=6.51e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:10:58,482 | INFO | Epoch 811 Train Time 23.390084266662598s

2025-10-18 21:11:21,396 | INFO | Training epoch 812, Batch 1000/1000: LR=6.50e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:11:21,474 | INFO | Epoch 812 Train Time 22.991836309432983s

2025-10-18 21:11:45,234 | INFO | Training epoch 813, Batch 1000/1000: LR=6.49e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:11:45,311 | INFO | Epoch 813 Train Time 23.836123943328857s

2025-10-18 21:12:08,133 | INFO | Training epoch 814, Batch 1000/1000: LR=6.48e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:12:08,222 | INFO | Epoch 814 Train Time 22.910612106323242s

2025-10-18 21:12:30,619 | INFO | Training epoch 815, Batch 1000/1000: LR=6.48e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 21:12:30,701 | INFO | Epoch 815 Train Time 22.47638177871704s

2025-10-18 21:12:54,510 | INFO | Training epoch 816, Batch 1000/1000: LR=6.47e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 21:12:54,595 | INFO | Epoch 816 Train Time 23.89298915863037s

2025-10-18 21:13:18,085 | INFO | Training epoch 817, Batch 1000/1000: LR=6.46e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:13:18,165 | INFO | Epoch 817 Train Time 23.56866192817688s

2025-10-18 21:13:41,855 | INFO | Training epoch 818, Batch 1000/1000: LR=6.45e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:13:41,928 | INFO | Epoch 818 Train Time 23.76219868659973s

2025-10-18 21:14:05,138 | INFO | Training epoch 819, Batch 1000/1000: LR=6.45e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:14:05,216 | INFO | Epoch 819 Train Time 23.285674333572388s

2025-10-18 21:14:27,351 | INFO | Training epoch 820, Batch 1000/1000: LR=6.44e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:14:27,420 | INFO | Epoch 820 Train Time 22.202861547470093s

2025-10-18 21:14:51,346 | INFO | Training epoch 821, Batch 1000/1000: LR=6.43e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:14:51,415 | INFO | Epoch 821 Train Time 23.994199514389038s

2025-10-18 21:15:14,730 | INFO | Training epoch 822, Batch 1000/1000: LR=6.42e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:15:14,815 | INFO | Epoch 822 Train Time 23.399107217788696s

2025-10-18 21:15:37,852 | INFO | Training epoch 823, Batch 1000/1000: LR=6.42e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:15:37,944 | INFO | Epoch 823 Train Time 23.12815809249878s

2025-10-18 21:15:59,297 | INFO | Training epoch 824, Batch 1000/1000: LR=6.41e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:15:59,367 | INFO | Epoch 824 Train Time 21.42088294029236s

2025-10-18 21:16:22,329 | INFO | Training epoch 825, Batch 1000/1000: LR=6.40e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:16:22,397 | INFO | Epoch 825 Train Time 23.03002119064331s

2025-10-18 21:16:46,118 | INFO | Training epoch 826, Batch 1000/1000: LR=6.39e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:16:46,203 | INFO | Epoch 826 Train Time 23.805009365081787s

2025-10-18 21:17:09,408 | INFO | Training epoch 827, Batch 1000/1000: LR=6.39e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:17:09,500 | INFO | Epoch 827 Train Time 23.295904636383057s

2025-10-18 21:17:32,459 | INFO | Training epoch 828, Batch 1000/1000: LR=6.38e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-18 21:17:32,545 | INFO | Epoch 828 Train Time 23.04430055618286s

2025-10-18 21:17:56,161 | INFO | Training epoch 829, Batch 1000/1000: LR=6.37e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:17:56,241 | INFO | Epoch 829 Train Time 23.695338487625122s

2025-10-18 21:18:19,012 | INFO | Training epoch 830, Batch 1000/1000: LR=6.36e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:18:19,096 | INFO | Epoch 830 Train Time 22.8533296585083s

2025-10-18 21:18:42,653 | INFO | Training epoch 831, Batch 1000/1000: LR=6.36e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:18:42,743 | INFO | Epoch 831 Train Time 23.64637279510498s

2025-10-18 21:19:05,419 | INFO | Training epoch 832, Batch 1000/1000: LR=6.35e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:19:05,517 | INFO | Epoch 832 Train Time 22.772752046585083s

2025-10-18 21:19:28,728 | INFO | Training epoch 833, Batch 1000/1000: LR=6.34e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:19:28,800 | INFO | Epoch 833 Train Time 23.281918048858643s

2025-10-18 21:19:51,122 | INFO | Training epoch 834, Batch 1000/1000: LR=6.33e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 21:19:51,206 | INFO | Epoch 834 Train Time 22.404610872268677s

2025-10-18 21:20:14,836 | INFO | Training epoch 835, Batch 1000/1000: LR=6.33e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:20:14,930 | INFO | Epoch 835 Train Time 23.722944259643555s

2025-10-18 21:20:38,939 | INFO | Training epoch 836, Batch 1000/1000: LR=6.32e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:20:39,013 | INFO | Epoch 836 Train Time 24.08158779144287s

2025-10-18 21:21:01,843 | INFO | Training epoch 837, Batch 1000/1000: LR=6.31e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:21:01,928 | INFO | Epoch 837 Train Time 22.91395378112793s

2025-10-18 21:21:25,310 | INFO | Training epoch 838, Batch 1000/1000: LR=6.30e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:21:25,387 | INFO | Epoch 838 Train Time 23.45715045928955s

2025-10-18 21:21:48,752 | INFO | Training epoch 839, Batch 1000/1000: LR=6.30e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:21:48,831 | INFO | Epoch 839 Train Time 23.441569089889526s

2025-10-18 21:22:13,016 | INFO | Training epoch 840, Batch 1000/1000: LR=6.29e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:22:13,087 | INFO | Epoch 840 Train Time 24.254921197891235s

2025-10-18 21:22:36,516 | INFO | Training epoch 841, Batch 1000/1000: LR=6.28e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:22:36,584 | INFO | Epoch 841 Train Time 23.496140241622925s

2025-10-18 21:22:59,934 | INFO | Training epoch 842, Batch 1000/1000: LR=6.27e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:23:00,012 | INFO | Epoch 842 Train Time 23.426153421401978s

2025-10-18 21:23:23,669 | INFO | Training epoch 843, Batch 1000/1000: LR=6.27e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:23:23,749 | INFO | Epoch 843 Train Time 23.73580813407898s

2025-10-18 21:23:47,232 | INFO | Training epoch 844, Batch 1000/1000: LR=6.26e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 21:23:47,310 | INFO | Epoch 844 Train Time 23.560169219970703s

2025-10-18 21:24:10,755 | INFO | Training epoch 845, Batch 1000/1000: LR=6.25e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 21:24:10,833 | INFO | Epoch 845 Train Time 23.522238731384277s

2025-10-18 21:24:33,756 | INFO | Training epoch 846, Batch 1000/1000: LR=6.24e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:24:33,851 | INFO | Epoch 846 Train Time 23.016162157058716s

2025-10-18 21:24:33,851 | INFO | [P1] saving best_model with loss 0.027090 at epoch 846
2025-10-18 21:24:57,923 | INFO | Training epoch 847, Batch 1000/1000: LR=6.24e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:24:58,006 | INFO | Epoch 847 Train Time 24.128177165985107s

2025-10-18 21:25:21,924 | INFO | Training epoch 848, Batch 1000/1000: LR=6.23e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:25:22,025 | INFO | Epoch 848 Train Time 24.018762826919556s

2025-10-18 21:25:45,301 | INFO | Training epoch 849, Batch 1000/1000: LR=6.22e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:25:45,368 | INFO | Epoch 849 Train Time 23.340617656707764s

2025-10-18 21:26:08,647 | INFO | Training epoch 850, Batch 1000/1000: LR=6.21e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:26:08,729 | INFO | Epoch 850 Train Time 23.360254287719727s

2025-10-18 21:26:32,645 | INFO | Training epoch 851, Batch 1000/1000: LR=6.21e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:26:32,740 | INFO | Epoch 851 Train Time 24.00868821144104s

2025-10-18 21:26:56,948 | INFO | Training epoch 852, Batch 1000/1000: LR=6.20e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:26:57,037 | INFO | Epoch 852 Train Time 24.29542827606201s

2025-10-18 21:27:19,958 | INFO | Training epoch 853, Batch 1000/1000: LR=6.19e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:27:20,048 | INFO | Epoch 853 Train Time 23.010974884033203s

2025-10-18 21:27:43,303 | INFO | Training epoch 854, Batch 1000/1000: LR=6.18e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 21:27:43,387 | INFO | Epoch 854 Train Time 23.337342977523804s

2025-10-18 21:28:06,995 | INFO | Training epoch 855, Batch 1000/1000: LR=6.18e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:28:07,072 | INFO | Epoch 855 Train Time 23.684368133544922s

2025-10-18 21:28:30,063 | INFO | Training epoch 856, Batch 1000/1000: LR=6.17e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:28:30,138 | INFO | Epoch 856 Train Time 23.065587520599365s

2025-10-18 21:28:52,456 | INFO | Training epoch 857, Batch 1000/1000: LR=6.16e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:28:52,527 | INFO | Epoch 857 Train Time 22.387641429901123s

2025-10-18 21:29:14,457 | INFO | Training epoch 858, Batch 1000/1000: LR=6.15e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:29:14,578 | INFO | Epoch 858 Train Time 22.050472259521484s

2025-10-18 21:29:37,038 | INFO | Training epoch 859, Batch 1000/1000: LR=6.14e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:29:37,112 | INFO | Epoch 859 Train Time 22.531825065612793s

2025-10-18 21:30:00,546 | INFO | Training epoch 860, Batch 1000/1000: LR=6.14e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:30:00,624 | INFO | Epoch 860 Train Time 23.50953435897827s

2025-10-18 21:30:23,139 | INFO | Training epoch 861, Batch 1000/1000: LR=6.13e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:30:23,209 | INFO | Epoch 861 Train Time 22.5845730304718s

2025-10-18 21:30:46,429 | INFO | Training epoch 862, Batch 1000/1000: LR=6.12e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:30:46,498 | INFO | Epoch 862 Train Time 23.287153244018555s

2025-10-18 21:31:09,937 | INFO | Training epoch 863, Batch 1000/1000: LR=6.11e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:31:10,017 | INFO | Epoch 863 Train Time 23.51719093322754s

2025-10-18 21:31:33,637 | INFO | Training epoch 864, Batch 1000/1000: LR=6.11e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:31:33,708 | INFO | Epoch 864 Train Time 23.689005136489868s

2025-10-18 21:31:56,545 | INFO | Training epoch 865, Batch 1000/1000: LR=6.10e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:31:56,625 | INFO | Epoch 865 Train Time 22.916505098342896s

2025-10-18 21:32:16,749 | INFO | Training epoch 866, Batch 1000/1000: LR=6.09e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 21:32:16,822 | INFO | Epoch 866 Train Time 20.195237398147583s

2025-10-18 21:32:40,247 | INFO | Training epoch 867, Batch 1000/1000: LR=6.08e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:32:40,330 | INFO | Epoch 867 Train Time 23.505949020385742s

2025-10-18 21:33:03,240 | INFO | Training epoch 868, Batch 1000/1000: LR=6.08e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:33:03,326 | INFO | Epoch 868 Train Time 22.993525505065918s

2025-10-18 21:33:25,635 | INFO | Training epoch 869, Batch 1000/1000: LR=6.07e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:33:25,711 | INFO | Epoch 869 Train Time 22.384926319122314s

2025-10-18 21:33:48,361 | INFO | Training epoch 870, Batch 1000/1000: LR=6.06e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:33:48,435 | INFO | Epoch 870 Train Time 22.72249436378479s

2025-10-18 21:34:12,246 | INFO | Training epoch 871, Batch 1000/1000: LR=6.05e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:34:12,347 | INFO | Epoch 871 Train Time 23.911224842071533s

2025-10-18 21:34:34,843 | INFO | Training epoch 872, Batch 1000/1000: LR=6.05e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:34:34,936 | INFO | Epoch 872 Train Time 22.587836980819702s

2025-10-18 21:34:58,145 | INFO | Training epoch 873, Batch 1000/1000: LR=6.04e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:34:58,236 | INFO | Epoch 873 Train Time 23.298264980316162s

2025-10-18 21:35:20,938 | INFO | Training epoch 874, Batch 1000/1000: LR=6.03e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:35:21,030 | INFO | Epoch 874 Train Time 22.792951345443726s

2025-10-18 21:35:45,660 | INFO | Training epoch 875, Batch 1000/1000: LR=6.02e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:35:45,758 | INFO | Epoch 875 Train Time 24.726683139801025s

2025-10-18 21:36:09,931 | INFO | Training epoch 876, Batch 1000/1000: LR=6.02e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:36:10,009 | INFO | Epoch 876 Train Time 24.250046014785767s

2025-10-18 21:36:32,527 | INFO | Training epoch 877, Batch 1000/1000: LR=6.01e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:36:32,590 | INFO | Epoch 877 Train Time 22.579971313476562s

2025-10-18 21:36:55,630 | INFO | Training epoch 878, Batch 1000/1000: LR=6.00e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:36:55,723 | INFO | Epoch 878 Train Time 23.130563259124756s

2025-10-18 21:37:19,231 | INFO | Training epoch 879, Batch 1000/1000: LR=5.99e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:37:19,323 | INFO | Epoch 879 Train Time 23.59931468963623s

2025-10-18 21:37:43,555 | INFO | Training epoch 880, Batch 1000/1000: LR=5.99e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:37:43,640 | INFO | Epoch 880 Train Time 24.314573049545288s

2025-10-18 21:38:05,409 | INFO | Training epoch 881, Batch 1000/1000: LR=5.98e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:38:05,497 | INFO | Epoch 881 Train Time 21.85525393486023s

2025-10-18 21:38:29,541 | INFO | Training epoch 882, Batch 1000/1000: LR=5.97e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:38:29,632 | INFO | Epoch 882 Train Time 24.133856773376465s

2025-10-18 21:38:52,997 | INFO | Training epoch 883, Batch 1000/1000: LR=5.96e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:38:53,065 | INFO | Epoch 883 Train Time 23.432045936584473s

2025-10-18 21:39:15,966 | INFO | Training epoch 884, Batch 1000/1000: LR=5.95e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:39:16,043 | INFO | Epoch 884 Train Time 22.977085828781128s

2025-10-18 21:39:39,125 | INFO | Training epoch 885, Batch 1000/1000: LR=5.95e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:39:39,204 | INFO | Epoch 885 Train Time 23.159018754959106s

2025-10-18 21:40:02,422 | INFO | Training epoch 886, Batch 1000/1000: LR=5.94e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:40:02,496 | INFO | Epoch 886 Train Time 23.289419412612915s

2025-10-18 21:40:25,546 | INFO | Training epoch 887, Batch 1000/1000: LR=5.93e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:40:25,625 | INFO | Epoch 887 Train Time 23.127951860427856s

2025-10-18 21:40:48,345 | INFO | Training epoch 888, Batch 1000/1000: LR=5.92e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:40:48,433 | INFO | Epoch 888 Train Time 22.80684542655945s

2025-10-18 21:41:10,748 | INFO | Training epoch 889, Batch 1000/1000: LR=5.92e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:41:10,815 | INFO | Epoch 889 Train Time 22.380497694015503s

2025-10-18 21:41:34,555 | INFO | Training epoch 890, Batch 1000/1000: LR=5.91e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 21:41:34,641 | INFO | Epoch 890 Train Time 23.825045347213745s

2025-10-18 21:41:57,515 | INFO | Training epoch 891, Batch 1000/1000: LR=5.90e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:41:57,603 | INFO | Epoch 891 Train Time 22.960542917251587s

2025-10-18 21:42:20,243 | INFO | Training epoch 892, Batch 1000/1000: LR=5.89e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:42:20,338 | INFO | Epoch 892 Train Time 22.733581066131592s

2025-10-18 21:42:42,833 | INFO | Training epoch 893, Batch 1000/1000: LR=5.89e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:42:42,919 | INFO | Epoch 893 Train Time 22.579196214675903s

2025-10-18 21:43:05,811 | INFO | Training epoch 894, Batch 1000/1000: LR=5.88e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:43:05,896 | INFO | Epoch 894 Train Time 22.976097106933594s

2025-10-18 21:43:28,844 | INFO | Training epoch 895, Batch 1000/1000: LR=5.87e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:43:28,933 | INFO | Epoch 895 Train Time 23.035841703414917s

2025-10-18 21:43:51,757 | INFO | Training epoch 896, Batch 1000/1000: LR=5.86e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:43:51,832 | INFO | Epoch 896 Train Time 22.898730039596558s

2025-10-18 21:44:14,910 | INFO | Training epoch 897, Batch 1000/1000: LR=5.86e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:44:14,998 | INFO | Epoch 897 Train Time 23.163787126541138s

2025-10-18 21:44:38,160 | INFO | Training epoch 898, Batch 1000/1000: LR=5.85e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:44:38,256 | INFO | Epoch 898 Train Time 23.25745940208435s

2025-10-18 21:45:01,945 | INFO | Training epoch 899, Batch 1000/1000: LR=5.84e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:45:02,017 | INFO | Epoch 899 Train Time 23.757875204086304s

2025-10-18 21:45:24,749 | INFO | Training epoch 900, Batch 1000/1000: LR=5.83e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:45:24,827 | INFO | Epoch 900 Train Time 22.808674097061157s

2025-10-18 21:45:47,029 | INFO | Training epoch 901, Batch 1000/1000: LR=5.82e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:45:47,119 | INFO | Epoch 901 Train Time 22.29186511039734s

2025-10-18 21:46:10,256 | INFO | Training epoch 902, Batch 1000/1000: LR=5.82e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:46:10,343 | INFO | Epoch 902 Train Time 23.22247838973999s

2025-10-18 21:46:32,834 | INFO | Training epoch 903, Batch 1000/1000: LR=5.81e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:46:32,916 | INFO | Epoch 903 Train Time 22.571735858917236s

2025-10-18 21:46:56,151 | INFO | Training epoch 904, Batch 1000/1000: LR=5.80e-05, Loss=2.74e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:46:56,229 | INFO | Epoch 904 Train Time 23.311689615249634s

2025-10-18 21:47:19,127 | INFO | Training epoch 905, Batch 1000/1000: LR=5.79e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:47:19,203 | INFO | Epoch 905 Train Time 22.973016500473022s

2025-10-18 21:47:19,204 | INFO | [P1] saving best_model with loss 0.027008 at epoch 905
2025-10-18 21:47:40,026 | INFO | Training epoch 906, Batch 1000/1000: LR=5.79e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:47:40,113 | INFO | Epoch 906 Train Time 20.890658617019653s

2025-10-18 21:48:04,814 | INFO | Training epoch 907, Batch 1000/1000: LR=5.78e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:48:04,907 | INFO | Epoch 907 Train Time 24.792446851730347s

2025-10-18 21:48:29,841 | INFO | Training epoch 908, Batch 1000/1000: LR=5.77e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:48:29,929 | INFO | Epoch 908 Train Time 25.020395278930664s

2025-10-18 21:48:53,440 | INFO | Training epoch 909, Batch 1000/1000: LR=5.76e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:48:53,524 | INFO | Epoch 909 Train Time 23.593199968338013s

2025-10-18 21:49:17,548 | INFO | Training epoch 910, Batch 1000/1000: LR=5.76e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:49:17,636 | INFO | Epoch 910 Train Time 24.111544609069824s

2025-10-18 21:49:41,836 | INFO | Training epoch 911, Batch 1000/1000: LR=5.75e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.34e-01
2025-10-18 21:49:41,922 | INFO | Epoch 911 Train Time 24.284316301345825s

2025-10-18 21:50:05,255 | INFO | Training epoch 912, Batch 1000/1000: LR=5.74e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:50:05,339 | INFO | Epoch 912 Train Time 23.415724754333496s

2025-10-18 21:50:28,729 | INFO | Training epoch 913, Batch 1000/1000: LR=5.73e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:50:28,805 | INFO | Epoch 913 Train Time 23.464472770690918s

2025-10-18 21:50:52,023 | INFO | Training epoch 914, Batch 1000/1000: LR=5.72e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:50:52,111 | INFO | Epoch 914 Train Time 23.305508852005005s

2025-10-18 21:51:16,457 | INFO | Training epoch 915, Batch 1000/1000: LR=5.72e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:51:16,552 | INFO | Epoch 915 Train Time 24.43973398208618s

2025-10-18 21:51:40,233 | INFO | Training epoch 916, Batch 1000/1000: LR=5.71e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 21:51:40,323 | INFO | Epoch 916 Train Time 23.770700693130493s

2025-10-18 21:52:04,545 | INFO | Training epoch 917, Batch 1000/1000: LR=5.70e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:52:04,648 | INFO | Epoch 917 Train Time 24.322733879089355s

2025-10-18 21:52:27,303 | INFO | Training epoch 918, Batch 1000/1000: LR=5.69e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 21:52:27,390 | INFO | Epoch 918 Train Time 22.740517139434814s

2025-10-18 21:52:50,037 | INFO | Training epoch 919, Batch 1000/1000: LR=5.69e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:52:50,118 | INFO | Epoch 919 Train Time 22.726467847824097s

2025-10-18 21:53:13,436 | INFO | Training epoch 920, Batch 1000/1000: LR=5.68e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 21:53:13,511 | INFO | Epoch 920 Train Time 23.39198136329651s

2025-10-18 21:53:35,961 | INFO | Training epoch 921, Batch 1000/1000: LR=5.67e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:53:36,052 | INFO | Epoch 921 Train Time 22.540581464767456s

2025-10-18 21:53:58,019 | INFO | Training epoch 922, Batch 1000/1000: LR=5.66e-05, Loss=2.74e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:53:58,091 | INFO | Epoch 922 Train Time 22.036922931671143s

2025-10-18 21:54:20,437 | INFO | Training epoch 923, Batch 1000/1000: LR=5.65e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:54:20,518 | INFO | Epoch 923 Train Time 22.425848960876465s

2025-10-18 21:54:43,678 | INFO | Training epoch 924, Batch 1000/1000: LR=5.65e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:54:43,788 | INFO | Epoch 924 Train Time 23.26738452911377s

2025-10-18 21:55:07,837 | INFO | Training epoch 925, Batch 1000/1000: LR=5.64e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:55:07,917 | INFO | Epoch 925 Train Time 24.12749481201172s

2025-10-18 21:55:31,091 | INFO | Training epoch 926, Batch 1000/1000: LR=5.63e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:55:31,178 | INFO | Epoch 926 Train Time 23.26035189628601s

2025-10-18 21:55:53,945 | INFO | Training epoch 927, Batch 1000/1000: LR=5.62e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 21:55:54,037 | INFO | Epoch 927 Train Time 22.857704401016235s

2025-10-18 21:56:17,243 | INFO | Training epoch 928, Batch 1000/1000: LR=5.62e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 21:56:17,313 | INFO | Epoch 928 Train Time 23.274794816970825s

2025-10-18 21:56:40,454 | INFO | Training epoch 929, Batch 1000/1000: LR=5.61e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:56:40,540 | INFO | Epoch 929 Train Time 23.225724697113037s

2025-10-18 21:57:03,445 | INFO | Training epoch 930, Batch 1000/1000: LR=5.60e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:57:03,534 | INFO | Epoch 930 Train Time 22.99247145652771s

2025-10-18 21:57:27,023 | INFO | Training epoch 931, Batch 1000/1000: LR=5.59e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:57:27,109 | INFO | Epoch 931 Train Time 23.572852849960327s

2025-10-18 21:57:50,935 | INFO | Training epoch 932, Batch 1000/1000: LR=5.59e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 21:57:51,014 | INFO | Epoch 932 Train Time 23.903700828552246s

2025-10-18 21:58:14,116 | INFO | Training epoch 933, Batch 1000/1000: LR=5.58e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:58:14,187 | INFO | Epoch 933 Train Time 23.171674966812134s

2025-10-18 21:58:38,042 | INFO | Training epoch 934, Batch 1000/1000: LR=5.57e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 21:58:38,137 | INFO | Epoch 934 Train Time 23.949384927749634s

2025-10-18 21:59:01,826 | INFO | Training epoch 935, Batch 1000/1000: LR=5.56e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 21:59:01,912 | INFO | Epoch 935 Train Time 23.772831201553345s

2025-10-18 21:59:26,198 | INFO | Training epoch 936, Batch 1000/1000: LR=5.55e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 21:59:26,267 | INFO | Epoch 936 Train Time 24.35464072227478s

2025-10-18 21:59:49,512 | INFO | Training epoch 937, Batch 1000/1000: LR=5.55e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 21:59:49,580 | INFO | Epoch 937 Train Time 23.312339305877686s

2025-10-18 22:00:13,312 | INFO | Training epoch 938, Batch 1000/1000: LR=5.54e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:00:13,387 | INFO | Epoch 938 Train Time 23.804569959640503s

2025-10-18 22:00:34,844 | INFO | Training epoch 939, Batch 1000/1000: LR=5.53e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:00:34,939 | INFO | Epoch 939 Train Time 21.55058979988098s

2025-10-18 22:00:58,438 | INFO | Training epoch 940, Batch 1000/1000: LR=5.52e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:00:58,520 | INFO | Epoch 940 Train Time 23.57877779006958s

2025-10-18 22:01:21,416 | INFO | Training epoch 941, Batch 1000/1000: LR=5.52e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:01:21,507 | INFO | Epoch 941 Train Time 22.986167192459106s

2025-10-18 22:01:44,998 | INFO | Training epoch 942, Batch 1000/1000: LR=5.51e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:01:45,064 | INFO | Epoch 942 Train Time 23.556360960006714s

2025-10-18 22:02:09,222 | INFO | Training epoch 943, Batch 1000/1000: LR=5.50e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:02:09,295 | INFO | Epoch 943 Train Time 24.229169130325317s

2025-10-18 22:02:32,148 | INFO | Training epoch 944, Batch 1000/1000: LR=5.49e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 22:02:32,222 | INFO | Epoch 944 Train Time 22.924497842788696s

2025-10-18 22:02:55,459 | INFO | Training epoch 945, Batch 1000/1000: LR=5.48e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:02:55,537 | INFO | Epoch 945 Train Time 23.313855171203613s

2025-10-18 22:03:18,316 | INFO | Training epoch 946, Batch 1000/1000: LR=5.48e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:03:18,382 | INFO | Epoch 946 Train Time 22.842596530914307s

2025-10-18 22:03:40,547 | INFO | Training epoch 947, Batch 1000/1000: LR=5.47e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:03:40,641 | INFO | Epoch 947 Train Time 22.25848126411438s

2025-10-18 22:04:03,653 | INFO | Training epoch 948, Batch 1000/1000: LR=5.46e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:04:03,740 | INFO | Epoch 948 Train Time 23.098795175552368s

2025-10-18 22:04:27,430 | INFO | Training epoch 949, Batch 1000/1000: LR=5.45e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:04:27,515 | INFO | Epoch 949 Train Time 23.773330450057983s

2025-10-18 22:04:50,634 | INFO | Training epoch 950, Batch 1000/1000: LR=5.45e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:04:50,724 | INFO | Epoch 950 Train Time 23.20863389968872s

2025-10-18 22:05:13,234 | INFO | Training epoch 951, Batch 1000/1000: LR=5.44e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:05:13,318 | INFO | Epoch 951 Train Time 22.591635704040527s

2025-10-18 22:05:36,646 | INFO | Training epoch 952, Batch 1000/1000: LR=5.43e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:05:36,725 | INFO | Epoch 952 Train Time 23.40664768218994s

2025-10-18 22:05:59,934 | INFO | Training epoch 953, Batch 1000/1000: LR=5.42e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:06:00,005 | INFO | Epoch 953 Train Time 23.277562856674194s

2025-10-18 22:06:21,435 | INFO | Training epoch 954, Batch 1000/1000: LR=5.42e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:06:21,521 | INFO | Epoch 954 Train Time 21.515440464019775s

2025-10-18 22:06:45,309 | INFO | Training epoch 955, Batch 1000/1000: LR=5.41e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:06:45,397 | INFO | Epoch 955 Train Time 23.87510848045349s

2025-10-18 22:07:09,234 | INFO | Training epoch 956, Batch 1000/1000: LR=5.40e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:07:09,324 | INFO | Epoch 956 Train Time 23.92505669593811s

2025-10-18 22:07:32,732 | INFO | Training epoch 957, Batch 1000/1000: LR=5.39e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 22:07:32,818 | INFO | Epoch 957 Train Time 23.492834091186523s

2025-10-18 22:07:32,820 | INFO | [P1] saving best_model with loss 0.026996 at epoch 957
2025-10-18 22:07:56,539 | INFO | Training epoch 958, Batch 1000/1000: LR=5.38e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:07:56,626 | INFO | Epoch 958 Train Time 23.790995121002197s

2025-10-18 22:08:19,946 | INFO | Training epoch 959, Batch 1000/1000: LR=5.38e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:08:20,033 | INFO | Epoch 959 Train Time 23.40584421157837s

2025-10-18 22:08:44,055 | INFO | Training epoch 960, Batch 1000/1000: LR=5.37e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:08:44,130 | INFO | Epoch 960 Train Time 24.09558415412903s

2025-10-18 22:09:07,307 | INFO | Training epoch 961, Batch 1000/1000: LR=5.36e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-18 22:09:07,383 | INFO | Epoch 961 Train Time 23.250990390777588s

2025-10-18 22:09:30,407 | INFO | Training epoch 962, Batch 1000/1000: LR=5.35e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:09:30,491 | INFO | Epoch 962 Train Time 23.105550050735474s

2025-10-18 22:09:50,353 | INFO | Training epoch 963, Batch 1000/1000: LR=5.35e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:09:50,444 | INFO | Epoch 963 Train Time 19.951202154159546s

2025-10-18 22:10:14,027 | INFO | Training epoch 964, Batch 1000/1000: LR=5.34e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:10:14,119 | INFO | Epoch 964 Train Time 23.67384672164917s

2025-10-18 22:10:37,554 | INFO | Training epoch 965, Batch 1000/1000: LR=5.33e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:10:37,636 | INFO | Epoch 965 Train Time 23.51608896255493s

2025-10-18 22:11:00,935 | INFO | Training epoch 966, Batch 1000/1000: LR=5.32e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.34e-01
2025-10-18 22:11:01,024 | INFO | Epoch 966 Train Time 23.386236667633057s

2025-10-18 22:11:24,750 | INFO | Training epoch 967, Batch 1000/1000: LR=5.31e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:11:24,846 | INFO | Epoch 967 Train Time 23.821672439575195s

2025-10-18 22:11:49,015 | INFO | Training epoch 968, Batch 1000/1000: LR=5.31e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:11:49,094 | INFO | Epoch 968 Train Time 24.24583101272583s

2025-10-18 22:12:12,656 | INFO | Training epoch 969, Batch 1000/1000: LR=5.30e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 22:12:12,743 | INFO | Epoch 969 Train Time 23.648028135299683s

2025-10-18 22:12:35,828 | INFO | Training epoch 970, Batch 1000/1000: LR=5.29e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:12:35,895 | INFO | Epoch 970 Train Time 23.151337385177612s

2025-10-18 22:12:59,312 | INFO | Training epoch 971, Batch 1000/1000: LR=5.28e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:12:59,411 | INFO | Epoch 971 Train Time 23.513384103775024s

2025-10-18 22:13:23,641 | INFO | Training epoch 972, Batch 1000/1000: LR=5.28e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:13:23,734 | INFO | Epoch 972 Train Time 24.321473836898804s

2025-10-18 22:13:46,399 | INFO | Training epoch 973, Batch 1000/1000: LR=5.27e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:13:46,470 | INFO | Epoch 973 Train Time 22.734607696533203s

2025-10-18 22:14:10,336 | INFO | Training epoch 974, Batch 1000/1000: LR=5.26e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:14:10,416 | INFO | Epoch 974 Train Time 23.94538164138794s

2025-10-18 22:14:32,462 | INFO | Training epoch 975, Batch 1000/1000: LR=5.25e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:14:32,542 | INFO | Epoch 975 Train Time 22.123907566070557s

2025-10-18 22:14:54,447 | INFO | Training epoch 976, Batch 1000/1000: LR=5.24e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:14:54,519 | INFO | Epoch 976 Train Time 21.976321935653687s

2025-10-18 22:15:18,058 | INFO | Training epoch 977, Batch 1000/1000: LR=5.24e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:15:18,146 | INFO | Epoch 977 Train Time 23.625609636306763s

2025-10-18 22:15:42,036 | INFO | Training epoch 978, Batch 1000/1000: LR=5.23e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:15:42,123 | INFO | Epoch 978 Train Time 23.97482919692993s

2025-10-18 22:16:05,338 | INFO | Training epoch 979, Batch 1000/1000: LR=5.22e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:16:05,419 | INFO | Epoch 979 Train Time 23.294724225997925s

2025-10-18 22:16:29,428 | INFO | Training epoch 980, Batch 1000/1000: LR=5.21e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:16:29,500 | INFO | Epoch 980 Train Time 24.079611778259277s

2025-10-18 22:16:52,548 | INFO | Training epoch 981, Batch 1000/1000: LR=5.21e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:16:52,630 | INFO | Epoch 981 Train Time 23.128730297088623s

2025-10-18 22:17:16,252 | INFO | Training epoch 982, Batch 1000/1000: LR=5.20e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:17:16,326 | INFO | Epoch 982 Train Time 23.695021867752075s

2025-10-18 22:17:38,557 | INFO | Training epoch 983, Batch 1000/1000: LR=5.19e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 22:17:38,635 | INFO | Epoch 983 Train Time 22.30782461166382s

2025-10-18 22:18:02,743 | INFO | Training epoch 984, Batch 1000/1000: LR=5.18e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:18:02,828 | INFO | Epoch 984 Train Time 24.19198989868164s

2025-10-18 22:18:26,250 | INFO | Training epoch 985, Batch 1000/1000: LR=5.17e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:18:26,330 | INFO | Epoch 985 Train Time 23.49958109855652s

2025-10-18 22:18:51,221 | INFO | Training epoch 986, Batch 1000/1000: LR=5.17e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:18:51,309 | INFO | Epoch 986 Train Time 24.977403163909912s

2025-10-18 22:19:13,424 | INFO | Training epoch 987, Batch 1000/1000: LR=5.16e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:19:13,509 | INFO | Epoch 987 Train Time 22.19896960258484s

2025-10-18 22:19:36,355 | INFO | Training epoch 988, Batch 1000/1000: LR=5.15e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:19:36,442 | INFO | Epoch 988 Train Time 22.931606769561768s

2025-10-18 22:19:56,654 | INFO | Training epoch 989, Batch 1000/1000: LR=5.14e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 22:19:56,728 | INFO | Epoch 989 Train Time 20.284528255462646s

2025-10-18 22:20:19,652 | INFO | Training epoch 990, Batch 1000/1000: LR=5.14e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:20:19,732 | INFO | Epoch 990 Train Time 23.003010511398315s

2025-10-18 22:20:43,931 | INFO | Training epoch 991, Batch 1000/1000: LR=5.13e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 22:20:44,009 | INFO | Epoch 991 Train Time 24.276087999343872s

2025-10-18 22:20:44,010 | INFO | [P1] saving best_model with loss 0.026952 at epoch 991
2025-10-18 22:21:07,411 | INFO | Training epoch 992, Batch 1000/1000: LR=5.12e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:21:07,476 | INFO | Epoch 992 Train Time 23.452788591384888s

2025-10-18 22:21:30,032 | INFO | Training epoch 993, Batch 1000/1000: LR=5.11e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:21:30,124 | INFO | Epoch 993 Train Time 22.647544860839844s

2025-10-18 22:21:52,742 | INFO | Training epoch 994, Batch 1000/1000: LR=5.10e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:21:52,840 | INFO | Epoch 994 Train Time 22.715290069580078s

2025-10-18 22:22:16,537 | INFO | Training epoch 995, Batch 1000/1000: LR=5.10e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:22:16,621 | INFO | Epoch 995 Train Time 23.779427528381348s

2025-10-18 22:22:40,517 | INFO | Training epoch 996, Batch 1000/1000: LR=5.09e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:22:40,600 | INFO | Epoch 996 Train Time 23.977726936340332s

2025-10-18 22:23:02,905 | INFO | Training epoch 997, Batch 1000/1000: LR=5.08e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:23:02,975 | INFO | Epoch 997 Train Time 22.374189138412476s

2025-10-18 22:23:25,527 | INFO | Training epoch 998, Batch 1000/1000: LR=5.07e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:23:25,600 | INFO | Epoch 998 Train Time 22.62342405319214s

2025-10-18 22:23:49,524 | INFO | Training epoch 999, Batch 1000/1000: LR=5.07e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 22:23:49,608 | INFO | Epoch 999 Train Time 24.006577730178833s

2025-10-18 22:24:13,227 | INFO | Training epoch 1000, Batch 1000/1000: LR=5.06e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:24:13,307 | INFO | Epoch 1000 Train Time 23.69765067100525s

2025-10-18 22:24:36,429 | INFO | Training epoch 1001, Batch 1000/1000: LR=5.05e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 22:24:36,503 | INFO | Epoch 1001 Train Time 23.19487977027893s

2025-10-18 22:24:58,978 | INFO | Training epoch 1002, Batch 1000/1000: LR=5.04e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:24:59,066 | INFO | Epoch 1002 Train Time 22.56184434890747s

2025-10-18 22:25:20,548 | INFO | Training epoch 1003, Batch 1000/1000: LR=5.03e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 22:25:20,637 | INFO | Epoch 1003 Train Time 21.57036852836609s

2025-10-18 22:25:41,321 | INFO | Training epoch 1004, Batch 1000/1000: LR=5.03e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:25:41,387 | INFO | Epoch 1004 Train Time 20.748888969421387s

2025-10-18 22:26:04,445 | INFO | Training epoch 1005, Batch 1000/1000: LR=5.02e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:26:04,523 | INFO | Epoch 1005 Train Time 23.13446068763733s

2025-10-18 22:26:28,349 | INFO | Training epoch 1006, Batch 1000/1000: LR=5.01e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:26:28,413 | INFO | Epoch 1006 Train Time 23.887919902801514s

2025-10-18 22:26:51,203 | INFO | Training epoch 1007, Batch 1000/1000: LR=5.00e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:26:51,290 | INFO | Epoch 1007 Train Time 22.87577986717224s

2025-10-18 22:27:14,442 | INFO | Training epoch 1008, Batch 1000/1000: LR=5.00e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:27:14,522 | INFO | Epoch 1008 Train Time 23.23105549812317s

2025-10-18 22:27:37,113 | INFO | Training epoch 1009, Batch 1000/1000: LR=4.99e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.35e-01
2025-10-18 22:27:37,195 | INFO | Epoch 1009 Train Time 22.671574592590332s

2025-10-18 22:28:00,054 | INFO | Training epoch 1010, Batch 1000/1000: LR=4.98e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:28:00,127 | INFO | Epoch 1010 Train Time 22.929394483566284s

2025-10-18 22:28:22,856 | INFO | Training epoch 1011, Batch 1000/1000: LR=4.97e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:28:22,950 | INFO | Epoch 1011 Train Time 22.821693420410156s

2025-10-18 22:28:46,822 | INFO | Training epoch 1012, Batch 1000/1000: LR=4.96e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:28:46,918 | INFO | Epoch 1012 Train Time 23.966564893722534s

2025-10-18 22:29:09,321 | INFO | Training epoch 1013, Batch 1000/1000: LR=4.96e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:29:09,394 | INFO | Epoch 1013 Train Time 22.473942041397095s

2025-10-18 22:29:31,833 | INFO | Training epoch 1014, Batch 1000/1000: LR=4.95e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-18 22:29:31,918 | INFO | Epoch 1014 Train Time 22.523826360702515s

2025-10-18 22:29:54,741 | INFO | Training epoch 1015, Batch 1000/1000: LR=4.94e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:29:54,812 | INFO | Epoch 1015 Train Time 22.89237928390503s

2025-10-18 22:30:19,120 | INFO | Training epoch 1016, Batch 1000/1000: LR=4.93e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:30:19,195 | INFO | Epoch 1016 Train Time 24.38249373435974s

2025-10-18 22:30:42,362 | INFO | Training epoch 1017, Batch 1000/1000: LR=4.93e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:30:42,444 | INFO | Epoch 1017 Train Time 23.24728560447693s

2025-10-18 22:31:05,520 | INFO | Training epoch 1018, Batch 1000/1000: LR=4.92e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:31:05,591 | INFO | Epoch 1018 Train Time 23.145566940307617s

2025-10-18 22:31:28,912 | INFO | Training epoch 1019, Batch 1000/1000: LR=4.91e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:31:28,998 | INFO | Epoch 1019 Train Time 23.406667232513428s

2025-10-18 22:31:51,399 | INFO | Training epoch 1020, Batch 1000/1000: LR=4.90e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:31:51,489 | INFO | Epoch 1020 Train Time 22.489994287490845s

2025-10-18 22:32:15,244 | INFO | Training epoch 1021, Batch 1000/1000: LR=4.89e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:32:15,323 | INFO | Epoch 1021 Train Time 23.832146644592285s

2025-10-18 22:32:38,123 | INFO | Training epoch 1022, Batch 1000/1000: LR=4.89e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:32:38,199 | INFO | Epoch 1022 Train Time 22.87536072731018s

2025-10-18 22:33:00,310 | INFO | Training epoch 1023, Batch 1000/1000: LR=4.88e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:33:00,397 | INFO | Epoch 1023 Train Time 22.19747233390808s

2025-10-18 22:33:24,060 | INFO | Training epoch 1024, Batch 1000/1000: LR=4.87e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:33:24,148 | INFO | Epoch 1024 Train Time 23.749167442321777s

2025-10-18 22:33:46,664 | INFO | Training epoch 1025, Batch 1000/1000: LR=4.86e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:33:46,742 | INFO | Epoch 1025 Train Time 22.592981815338135s

2025-10-18 22:34:09,548 | INFO | Training epoch 1026, Batch 1000/1000: LR=4.86e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:34:09,624 | INFO | Epoch 1026 Train Time 22.880768060684204s

2025-10-18 22:34:32,542 | INFO | Training epoch 1027, Batch 1000/1000: LR=4.85e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:34:32,626 | INFO | Epoch 1027 Train Time 23.000070810317993s

2025-10-18 22:34:53,919 | INFO | Training epoch 1028, Batch 1000/1000: LR=4.84e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:34:53,999 | INFO | Epoch 1028 Train Time 21.372632265090942s

2025-10-18 22:35:18,458 | INFO | Training epoch 1029, Batch 1000/1000: LR=4.83e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:35:18,527 | INFO | Epoch 1029 Train Time 24.527099609375s

2025-10-18 22:35:41,871 | INFO | Training epoch 1030, Batch 1000/1000: LR=4.82e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 22:35:41,956 | INFO | Epoch 1030 Train Time 23.425792694091797s

2025-10-18 22:36:05,538 | INFO | Training epoch 1031, Batch 1000/1000: LR=4.82e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:36:05,613 | INFO | Epoch 1031 Train Time 23.656831741333008s

2025-10-18 22:36:28,744 | INFO | Training epoch 1032, Batch 1000/1000: LR=4.81e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:36:28,834 | INFO | Epoch 1032 Train Time 23.219605445861816s

2025-10-18 22:36:51,950 | INFO | Training epoch 1033, Batch 1000/1000: LR=4.80e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:36:52,035 | INFO | Epoch 1033 Train Time 23.200031280517578s

2025-10-18 22:37:15,235 | INFO | Training epoch 1034, Batch 1000/1000: LR=4.79e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.35e-01
2025-10-18 22:37:15,329 | INFO | Epoch 1034 Train Time 23.292577981948853s

2025-10-18 22:37:38,214 | INFO | Training epoch 1035, Batch 1000/1000: LR=4.79e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:37:38,290 | INFO | Epoch 1035 Train Time 22.958298444747925s

2025-10-18 22:38:00,925 | INFO | Training epoch 1036, Batch 1000/1000: LR=4.78e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:38:00,993 | INFO | Epoch 1036 Train Time 22.70222282409668s

2025-10-18 22:38:24,966 | INFO | Training epoch 1037, Batch 1000/1000: LR=4.77e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:38:25,047 | INFO | Epoch 1037 Train Time 24.053075551986694s

2025-10-18 22:38:47,235 | INFO | Training epoch 1038, Batch 1000/1000: LR=4.76e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 22:38:47,329 | INFO | Epoch 1038 Train Time 22.27981948852539s

2025-10-18 22:39:10,449 | INFO | Training epoch 1039, Batch 1000/1000: LR=4.75e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:39:10,526 | INFO | Epoch 1039 Train Time 23.19609546661377s

2025-10-18 22:39:32,346 | INFO | Training epoch 1040, Batch 1000/1000: LR=4.75e-05, Loss=2.79e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 22:39:32,425 | INFO | Epoch 1040 Train Time 21.89829111099243s

2025-10-18 22:39:55,323 | INFO | Training epoch 1041, Batch 1000/1000: LR=4.74e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 22:39:55,397 | INFO | Epoch 1041 Train Time 22.971396923065186s

2025-10-18 22:40:17,755 | INFO | Training epoch 1042, Batch 1000/1000: LR=4.73e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:40:17,835 | INFO | Epoch 1042 Train Time 22.436997890472412s

2025-10-18 22:40:41,121 | INFO | Training epoch 1043, Batch 1000/1000: LR=4.72e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:40:41,191 | INFO | Epoch 1043 Train Time 23.35463523864746s

2025-10-18 22:41:04,124 | INFO | Training epoch 1044, Batch 1000/1000: LR=4.72e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 22:41:04,208 | INFO | Epoch 1044 Train Time 23.015122175216675s

2025-10-18 22:41:27,425 | INFO | Training epoch 1045, Batch 1000/1000: LR=4.71e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:41:27,512 | INFO | Epoch 1045 Train Time 23.30272388458252s

2025-10-18 22:41:49,933 | INFO | Training epoch 1046, Batch 1000/1000: LR=4.70e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:41:50,007 | INFO | Epoch 1046 Train Time 22.49421739578247s

2025-10-18 22:42:13,498 | INFO | Training epoch 1047, Batch 1000/1000: LR=4.69e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:42:13,584 | INFO | Epoch 1047 Train Time 23.575294017791748s

2025-10-18 22:42:36,844 | INFO | Training epoch 1048, Batch 1000/1000: LR=4.68e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:42:36,924 | INFO | Epoch 1048 Train Time 23.339327812194824s

2025-10-18 22:43:00,739 | INFO | Training epoch 1049, Batch 1000/1000: LR=4.68e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:43:00,820 | INFO | Epoch 1049 Train Time 23.89478588104248s

2025-10-18 22:43:25,061 | INFO | Training epoch 1050, Batch 1000/1000: LR=4.67e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:43:25,136 | INFO | Epoch 1050 Train Time 24.315473318099976s

2025-10-18 22:43:47,940 | INFO | Training epoch 1051, Batch 1000/1000: LR=4.66e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:43:48,026 | INFO | Epoch 1051 Train Time 22.88945960998535s

2025-10-18 22:44:10,844 | INFO | Training epoch 1052, Batch 1000/1000: LR=4.65e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:44:10,929 | INFO | Epoch 1052 Train Time 22.901257514953613s

2025-10-18 22:44:34,432 | INFO | Training epoch 1053, Batch 1000/1000: LR=4.65e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 22:44:34,506 | INFO | Epoch 1053 Train Time 23.576218366622925s

2025-10-18 22:44:57,138 | INFO | Training epoch 1054, Batch 1000/1000: LR=4.64e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:44:57,222 | INFO | Epoch 1054 Train Time 22.714818239212036s

2025-10-18 22:45:21,273 | INFO | Training epoch 1055, Batch 1000/1000: LR=4.63e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:45:21,362 | INFO | Epoch 1055 Train Time 24.138922452926636s

2025-10-18 22:45:44,057 | INFO | Training epoch 1056, Batch 1000/1000: LR=4.62e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:45:44,155 | INFO | Epoch 1056 Train Time 22.792027711868286s

2025-10-18 22:46:07,701 | INFO | Training epoch 1057, Batch 1000/1000: LR=4.62e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:46:07,790 | INFO | Epoch 1057 Train Time 23.634243965148926s

2025-10-18 22:46:32,143 | INFO | Training epoch 1058, Batch 1000/1000: LR=4.61e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:46:32,230 | INFO | Epoch 1058 Train Time 24.438754558563232s

2025-10-18 22:46:55,014 | INFO | Training epoch 1059, Batch 1000/1000: LR=4.60e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:46:55,115 | INFO | Epoch 1059 Train Time 22.88272523880005s

2025-10-18 22:47:18,054 | INFO | Training epoch 1060, Batch 1000/1000: LR=4.59e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:47:18,144 | INFO | Epoch 1060 Train Time 23.02723741531372s

2025-10-18 22:47:41,554 | INFO | Training epoch 1061, Batch 1000/1000: LR=4.58e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:47:41,651 | INFO | Epoch 1061 Train Time 23.506258487701416s

2025-10-18 22:48:04,062 | INFO | Training epoch 1062, Batch 1000/1000: LR=4.58e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:48:04,147 | INFO | Epoch 1062 Train Time 22.49515175819397s

2025-10-18 22:48:27,536 | INFO | Training epoch 1063, Batch 1000/1000: LR=4.57e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:48:27,625 | INFO | Epoch 1063 Train Time 23.477089643478394s

2025-10-18 22:48:50,914 | INFO | Training epoch 1064, Batch 1000/1000: LR=4.56e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:48:51,002 | INFO | Epoch 1064 Train Time 23.374408721923828s

2025-10-18 22:49:14,125 | INFO | Training epoch 1065, Batch 1000/1000: LR=4.55e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 22:49:14,206 | INFO | Epoch 1065 Train Time 23.20188570022583s

2025-10-18 22:49:37,938 | INFO | Training epoch 1066, Batch 1000/1000: LR=4.55e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:49:38,019 | INFO | Epoch 1066 Train Time 23.811931133270264s

2025-10-18 22:50:02,526 | INFO | Training epoch 1067, Batch 1000/1000: LR=4.54e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 22:50:02,634 | INFO | Epoch 1067 Train Time 24.614010095596313s

2025-10-18 22:50:25,340 | INFO | Training epoch 1068, Batch 1000/1000: LR=4.53e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:50:25,423 | INFO | Epoch 1068 Train Time 22.788619995117188s

2025-10-18 22:50:48,751 | INFO | Training epoch 1069, Batch 1000/1000: LR=4.52e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:50:48,846 | INFO | Epoch 1069 Train Time 23.421452522277832s

2025-10-18 22:51:12,762 | INFO | Training epoch 1070, Batch 1000/1000: LR=4.51e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:51:12,837 | INFO | Epoch 1070 Train Time 23.988725185394287s

2025-10-18 22:51:37,043 | INFO | Training epoch 1071, Batch 1000/1000: LR=4.51e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 22:51:37,131 | INFO | Epoch 1071 Train Time 24.29213571548462s

2025-10-18 22:52:00,465 | INFO | Training epoch 1072, Batch 1000/1000: LR=4.50e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:52:00,560 | INFO | Epoch 1072 Train Time 23.427729845046997s

2025-10-18 22:52:23,438 | INFO | Training epoch 1073, Batch 1000/1000: LR=4.49e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:52:23,513 | INFO | Epoch 1073 Train Time 22.952264308929443s

2025-10-18 22:52:45,947 | INFO | Training epoch 1074, Batch 1000/1000: LR=4.48e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:52:46,034 | INFO | Epoch 1074 Train Time 22.51988387107849s

2025-10-18 22:53:09,072 | INFO | Training epoch 1075, Batch 1000/1000: LR=4.48e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 22:53:09,152 | INFO | Epoch 1075 Train Time 23.11649751663208s

2025-10-18 22:53:29,240 | INFO | Training epoch 1076, Batch 1000/1000: LR=4.47e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 22:53:29,313 | INFO | Epoch 1076 Train Time 20.160574913024902s

2025-10-18 22:53:52,255 | INFO | Training epoch 1077, Batch 1000/1000: LR=4.46e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 22:53:52,342 | INFO | Epoch 1077 Train Time 23.026092052459717s

2025-10-18 22:54:15,553 | INFO | Training epoch 1078, Batch 1000/1000: LR=4.45e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 22:54:15,641 | INFO | Epoch 1078 Train Time 23.297847747802734s

2025-10-18 22:54:37,360 | INFO | Training epoch 1079, Batch 1000/1000: LR=4.45e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 22:54:37,454 | INFO | Epoch 1079 Train Time 21.81218409538269s

2025-10-18 22:55:00,133 | INFO | Training epoch 1080, Batch 1000/1000: LR=4.44e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:55:00,211 | INFO | Epoch 1080 Train Time 22.75551199913025s

2025-10-18 22:55:23,420 | INFO | Training epoch 1081, Batch 1000/1000: LR=4.43e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 22:55:23,500 | INFO | Epoch 1081 Train Time 23.28760290145874s

2025-10-18 22:55:46,953 | INFO | Training epoch 1082, Batch 1000/1000: LR=4.42e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:55:47,041 | INFO | Epoch 1082 Train Time 23.53946018218994s

2025-10-18 22:56:10,632 | INFO | Training epoch 1083, Batch 1000/1000: LR=4.41e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:56:10,719 | INFO | Epoch 1083 Train Time 23.67668080329895s

2025-10-18 22:56:34,106 | INFO | Training epoch 1084, Batch 1000/1000: LR=4.41e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:56:34,186 | INFO | Epoch 1084 Train Time 23.465436458587646s

2025-10-18 22:56:57,938 | INFO | Training epoch 1085, Batch 1000/1000: LR=4.40e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.35e-01
2025-10-18 22:56:58,023 | INFO | Epoch 1085 Train Time 23.835668563842773s

2025-10-18 22:57:21,656 | INFO | Training epoch 1086, Batch 1000/1000: LR=4.39e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:57:21,747 | INFO | Epoch 1086 Train Time 23.72260046005249s

2025-10-18 22:57:44,934 | INFO | Training epoch 1087, Batch 1000/1000: LR=4.38e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 22:57:45,008 | INFO | Epoch 1087 Train Time 23.260460376739502s

2025-10-18 22:58:09,748 | INFO | Training epoch 1088, Batch 1000/1000: LR=4.38e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:58:09,843 | INFO | Epoch 1088 Train Time 24.832960844039917s

2025-10-18 22:58:31,916 | INFO | Training epoch 1089, Batch 1000/1000: LR=4.37e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 22:58:32,007 | INFO | Epoch 1089 Train Time 22.163000106811523s

2025-10-18 22:58:55,621 | INFO | Training epoch 1090, Batch 1000/1000: LR=4.36e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-18 22:58:55,712 | INFO | Epoch 1090 Train Time 23.704583644866943s

2025-10-18 22:59:18,544 | INFO | Training epoch 1091, Batch 1000/1000: LR=4.35e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 22:59:18,613 | INFO | Epoch 1091 Train Time 22.89935564994812s

2025-10-18 22:59:42,056 | INFO | Training epoch 1092, Batch 1000/1000: LR=4.34e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 22:59:42,148 | INFO | Epoch 1092 Train Time 23.53333282470703s

2025-10-18 22:59:42,150 | INFO | [P1] saving best_model with loss 0.026895 at epoch 1092
2025-10-18 23:00:04,574 | INFO | Training epoch 1093, Batch 1000/1000: LR=4.34e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:00:04,666 | INFO | Epoch 1093 Train Time 22.48981475830078s

2025-10-18 23:00:28,358 | INFO | Training epoch 1094, Batch 1000/1000: LR=4.33e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:00:28,435 | INFO | Epoch 1094 Train Time 23.767761707305908s

2025-10-18 23:00:52,352 | INFO | Training epoch 1095, Batch 1000/1000: LR=4.32e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:00:52,450 | INFO | Epoch 1095 Train Time 24.01296854019165s

2025-10-18 23:01:15,640 | INFO | Training epoch 1096, Batch 1000/1000: LR=4.31e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 23:01:15,723 | INFO | Epoch 1096 Train Time 23.27241063117981s

2025-10-18 23:01:38,971 | INFO | Training epoch 1097, Batch 1000/1000: LR=4.31e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:01:39,058 | INFO | Epoch 1097 Train Time 23.333712339401245s

2025-10-18 23:02:01,837 | INFO | Training epoch 1098, Batch 1000/1000: LR=4.30e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:02:01,943 | INFO | Epoch 1098 Train Time 22.884238481521606s

2025-10-18 23:02:21,500 | INFO | Training epoch 1099, Batch 1000/1000: LR=4.29e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:02:21,572 | INFO | Epoch 1099 Train Time 19.62810969352722s

2025-10-18 23:02:45,349 | INFO | Training epoch 1100, Batch 1000/1000: LR=4.28e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:02:45,425 | INFO | Epoch 1100 Train Time 23.851566791534424s

2025-10-18 23:03:07,159 | INFO | Training epoch 1101, Batch 1000/1000: LR=4.28e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:03:07,249 | INFO | Epoch 1101 Train Time 21.823174476623535s

2025-10-18 23:03:30,321 | INFO | Training epoch 1102, Batch 1000/1000: LR=4.27e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:03:30,408 | INFO | Epoch 1102 Train Time 23.15803861618042s

2025-10-18 23:03:53,431 | INFO | Training epoch 1103, Batch 1000/1000: LR=4.26e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:03:53,515 | INFO | Epoch 1103 Train Time 23.104796409606934s

2025-10-18 23:04:16,910 | INFO | Training epoch 1104, Batch 1000/1000: LR=4.25e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:04:16,994 | INFO | Epoch 1104 Train Time 23.478453874588013s

2025-10-18 23:04:40,134 | INFO | Training epoch 1105, Batch 1000/1000: LR=4.24e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:04:40,215 | INFO | Epoch 1105 Train Time 23.219388961791992s

2025-10-18 23:05:04,040 | INFO | Training epoch 1106, Batch 1000/1000: LR=4.24e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:05:04,116 | INFO | Epoch 1106 Train Time 23.900818586349487s

2025-10-18 23:05:27,915 | INFO | Training epoch 1107, Batch 1000/1000: LR=4.23e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:05:27,992 | INFO | Epoch 1107 Train Time 23.87471580505371s

2025-10-18 23:05:50,547 | INFO | Training epoch 1108, Batch 1000/1000: LR=4.22e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:05:50,643 | INFO | Epoch 1108 Train Time 22.64924955368042s

2025-10-18 23:06:13,512 | INFO | Training epoch 1109, Batch 1000/1000: LR=4.21e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:06:13,601 | INFO | Epoch 1109 Train Time 22.956223964691162s

2025-10-18 23:06:36,954 | INFO | Training epoch 1110, Batch 1000/1000: LR=4.21e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:06:37,028 | INFO | Epoch 1110 Train Time 23.427140951156616s

2025-10-18 23:07:00,952 | INFO | Training epoch 1111, Batch 1000/1000: LR=4.20e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:07:01,048 | INFO | Epoch 1111 Train Time 24.018948793411255s

2025-10-18 23:07:23,649 | INFO | Training epoch 1112, Batch 1000/1000: LR=4.19e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:07:23,722 | INFO | Epoch 1112 Train Time 22.672613382339478s

2025-10-18 23:07:47,466 | INFO | Training epoch 1113, Batch 1000/1000: LR=4.18e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:07:47,558 | INFO | Epoch 1113 Train Time 23.834413528442383s

2025-10-18 23:07:47,558 | INFO | [P1] saving best_model with loss 0.026883 at epoch 1113
2025-10-18 23:08:11,356 | INFO | Training epoch 1114, Batch 1000/1000: LR=4.18e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 23:08:11,438 | INFO | Epoch 1114 Train Time 23.85535955429077s

2025-10-18 23:08:33,834 | INFO | Training epoch 1115, Batch 1000/1000: LR=4.17e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:08:33,911 | INFO | Epoch 1115 Train Time 22.4712917804718s

2025-10-18 23:08:57,124 | INFO | Training epoch 1116, Batch 1000/1000: LR=4.16e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:08:57,202 | INFO | Epoch 1116 Train Time 23.28976058959961s

2025-10-18 23:09:21,446 | INFO | Training epoch 1117, Batch 1000/1000: LR=4.15e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:09:21,532 | INFO | Epoch 1117 Train Time 24.32952117919922s

2025-10-18 23:09:44,340 | INFO | Training epoch 1118, Batch 1000/1000: LR=4.15e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:09:44,422 | INFO | Epoch 1118 Train Time 22.887343168258667s

2025-10-18 23:10:07,545 | INFO | Training epoch 1119, Batch 1000/1000: LR=4.14e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:10:07,627 | INFO | Epoch 1119 Train Time 23.204445838928223s

2025-10-18 23:10:31,211 | INFO | Training epoch 1120, Batch 1000/1000: LR=4.13e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:10:31,295 | INFO | Epoch 1120 Train Time 23.665496349334717s

2025-10-18 23:10:54,454 | INFO | Training epoch 1121, Batch 1000/1000: LR=4.12e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:10:54,542 | INFO | Epoch 1121 Train Time 23.246602058410645s

2025-10-18 23:11:17,857 | INFO | Training epoch 1122, Batch 1000/1000: LR=4.11e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:11:17,938 | INFO | Epoch 1122 Train Time 23.394700288772583s

2025-10-18 23:11:40,822 | INFO | Training epoch 1123, Batch 1000/1000: LR=4.11e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:11:40,922 | INFO | Epoch 1123 Train Time 22.982471704483032s

2025-10-18 23:12:03,655 | INFO | Training epoch 1124, Batch 1000/1000: LR=4.10e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:12:03,721 | INFO | Epoch 1124 Train Time 22.798619270324707s

2025-10-18 23:12:25,548 | INFO | Training epoch 1125, Batch 1000/1000: LR=4.09e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:12:25,614 | INFO | Epoch 1125 Train Time 21.891357421875s

2025-10-18 23:12:49,046 | INFO | Training epoch 1126, Batch 1000/1000: LR=4.08e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:12:49,116 | INFO | Epoch 1126 Train Time 23.50170350074768s

2025-10-18 23:13:11,036 | INFO | Training epoch 1127, Batch 1000/1000: LR=4.08e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:13:11,131 | INFO | Epoch 1127 Train Time 22.013535976409912s

2025-10-18 23:13:33,838 | INFO | Training epoch 1128, Batch 1000/1000: LR=4.07e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:13:33,905 | INFO | Epoch 1128 Train Time 22.773119688034058s

2025-10-18 23:13:57,247 | INFO | Training epoch 1129, Batch 1000/1000: LR=4.06e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:13:57,328 | INFO | Epoch 1129 Train Time 23.421874284744263s

2025-10-18 23:14:19,430 | INFO | Training epoch 1130, Batch 1000/1000: LR=4.05e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:14:19,499 | INFO | Epoch 1130 Train Time 22.169843196868896s

2025-10-18 23:14:42,423 | INFO | Training epoch 1131, Batch 1000/1000: LR=4.05e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:14:42,529 | INFO | Epoch 1131 Train Time 23.029611110687256s

2025-10-18 23:15:05,811 | INFO | Training epoch 1132, Batch 1000/1000: LR=4.04e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:15:05,886 | INFO | Epoch 1132 Train Time 23.35485577583313s

2025-10-18 23:15:29,006 | INFO | Training epoch 1133, Batch 1000/1000: LR=4.03e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:15:29,071 | INFO | Epoch 1133 Train Time 23.182642459869385s

2025-10-18 23:15:51,302 | INFO | Training epoch 1134, Batch 1000/1000: LR=4.02e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:15:51,368 | INFO | Epoch 1134 Train Time 22.294363260269165s

2025-10-18 23:16:15,204 | INFO | Training epoch 1135, Batch 1000/1000: LR=4.02e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:16:15,292 | INFO | Epoch 1135 Train Time 23.923222303390503s

2025-10-18 23:16:38,846 | INFO | Training epoch 1136, Batch 1000/1000: LR=4.01e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.34e-01
2025-10-18 23:16:38,930 | INFO | Epoch 1136 Train Time 23.637383460998535s

2025-10-18 23:17:02,046 | INFO | Training epoch 1137, Batch 1000/1000: LR=4.00e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:17:02,132 | INFO | Epoch 1137 Train Time 23.19997477531433s

2025-10-18 23:17:25,625 | INFO | Training epoch 1138, Batch 1000/1000: LR=3.99e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:17:25,708 | INFO | Epoch 1138 Train Time 23.57545566558838s

2025-10-18 23:17:48,526 | INFO | Training epoch 1139, Batch 1000/1000: LR=3.99e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:17:48,595 | INFO | Epoch 1139 Train Time 22.885560750961304s

2025-10-18 23:18:12,151 | INFO | Training epoch 1140, Batch 1000/1000: LR=3.98e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 23:18:12,235 | INFO | Epoch 1140 Train Time 23.637556314468384s

2025-10-18 23:18:35,345 | INFO | Training epoch 1141, Batch 1000/1000: LR=3.97e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:18:35,434 | INFO | Epoch 1141 Train Time 23.197612524032593s

2025-10-18 23:18:58,855 | INFO | Training epoch 1142, Batch 1000/1000: LR=3.96e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:18:58,933 | INFO | Epoch 1142 Train Time 23.498571634292603s

2025-10-18 23:19:22,014 | INFO | Training epoch 1143, Batch 1000/1000: LR=3.96e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-18 23:19:22,096 | INFO | Epoch 1143 Train Time 23.160890102386475s

2025-10-18 23:19:45,354 | INFO | Training epoch 1144, Batch 1000/1000: LR=3.95e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:19:45,440 | INFO | Epoch 1144 Train Time 23.34184169769287s

2025-10-18 23:20:07,603 | INFO | Training epoch 1145, Batch 1000/1000: LR=3.94e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:20:07,670 | INFO | Epoch 1145 Train Time 22.22809386253357s

2025-10-18 23:20:30,856 | INFO | Training epoch 1146, Batch 1000/1000: LR=3.93e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:20:30,927 | INFO | Epoch 1146 Train Time 23.256580352783203s

2025-10-18 23:20:30,928 | INFO | [P1] saving best_model with loss 0.026756 at epoch 1146
2025-10-18 23:20:54,450 | INFO | Training epoch 1147, Batch 1000/1000: LR=3.92e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:20:54,533 | INFO | Epoch 1147 Train Time 23.588213682174683s

2025-10-18 23:21:18,159 | INFO | Training epoch 1148, Batch 1000/1000: LR=3.92e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:21:18,245 | INFO | Epoch 1148 Train Time 23.710232734680176s

2025-10-18 23:21:41,261 | INFO | Training epoch 1149, Batch 1000/1000: LR=3.91e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:21:41,344 | INFO | Epoch 1149 Train Time 23.096785306930542s

2025-10-18 23:22:03,625 | INFO | Training epoch 1150, Batch 1000/1000: LR=3.90e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:22:03,693 | INFO | Epoch 1150 Train Time 22.348726511001587s

2025-10-18 23:22:26,949 | INFO | Training epoch 1151, Batch 1000/1000: LR=3.89e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:22:27,015 | INFO | Epoch 1151 Train Time 23.320855617523193s

2025-10-18 23:22:49,592 | INFO | Training epoch 1152, Batch 1000/1000: LR=3.89e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:22:49,669 | INFO | Epoch 1152 Train Time 22.652389526367188s

2025-10-18 23:23:12,940 | INFO | Training epoch 1153, Batch 1000/1000: LR=3.88e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:23:13,026 | INFO | Epoch 1153 Train Time 23.355040550231934s

2025-10-18 23:23:36,252 | INFO | Training epoch 1154, Batch 1000/1000: LR=3.87e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.34e-01
2025-10-18 23:23:36,340 | INFO | Epoch 1154 Train Time 23.312304258346558s

2025-10-18 23:23:59,119 | INFO | Training epoch 1155, Batch 1000/1000: LR=3.86e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:23:59,201 | INFO | Epoch 1155 Train Time 22.860042333602905s

2025-10-18 23:24:22,731 | INFO | Training epoch 1156, Batch 1000/1000: LR=3.86e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:24:22,801 | INFO | Epoch 1156 Train Time 23.599202156066895s

2025-10-18 23:24:45,611 | INFO | Training epoch 1157, Batch 1000/1000: LR=3.85e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 23:24:45,676 | INFO | Epoch 1157 Train Time 22.873539924621582s

2025-10-18 23:25:08,400 | INFO | Training epoch 1158, Batch 1000/1000: LR=3.84e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:25:08,469 | INFO | Epoch 1158 Train Time 22.79148268699646s

2025-10-18 23:25:30,458 | INFO | Training epoch 1159, Batch 1000/1000: LR=3.83e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:25:30,543 | INFO | Epoch 1159 Train Time 22.071886777877808s

2025-10-18 23:25:50,305 | INFO | Training epoch 1160, Batch 1000/1000: LR=3.83e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:25:50,387 | INFO | Epoch 1160 Train Time 19.842989444732666s

2025-10-18 23:26:13,958 | INFO | Training epoch 1161, Batch 1000/1000: LR=3.82e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:26:14,059 | INFO | Epoch 1161 Train Time 23.671530723571777s

2025-10-18 23:26:36,814 | INFO | Training epoch 1162, Batch 1000/1000: LR=3.81e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:26:36,894 | INFO | Epoch 1162 Train Time 22.833651542663574s

2025-10-18 23:26:59,501 | INFO | Training epoch 1163, Batch 1000/1000: LR=3.80e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:26:59,580 | INFO | Epoch 1163 Train Time 22.684667348861694s

2025-10-18 23:27:21,821 | INFO | Training epoch 1164, Batch 1000/1000: LR=3.80e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:27:21,889 | INFO | Epoch 1164 Train Time 22.307206392288208s

2025-10-18 23:27:45,025 | INFO | Training epoch 1165, Batch 1000/1000: LR=3.79e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:27:45,092 | INFO | Epoch 1165 Train Time 23.20259737968445s

2025-10-18 23:28:08,533 | INFO | Training epoch 1166, Batch 1000/1000: LR=3.78e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:28:08,595 | INFO | Epoch 1166 Train Time 23.50154423713684s

2025-10-18 23:28:28,703 | INFO | Training epoch 1167, Batch 1000/1000: LR=3.77e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:28:28,800 | INFO | Epoch 1167 Train Time 20.202218770980835s

2025-10-18 23:28:51,334 | INFO | Training epoch 1168, Batch 1000/1000: LR=3.77e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:28:51,415 | INFO | Epoch 1168 Train Time 22.614896774291992s

2025-10-18 23:29:14,761 | INFO | Training epoch 1169, Batch 1000/1000: LR=3.76e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:29:14,829 | INFO | Epoch 1169 Train Time 23.413089752197266s

2025-10-18 23:29:38,546 | INFO | Training epoch 1170, Batch 1000/1000: LR=3.75e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-18 23:29:38,608 | INFO | Epoch 1170 Train Time 23.777096033096313s

2025-10-18 23:30:01,127 | INFO | Training epoch 1171, Batch 1000/1000: LR=3.74e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:30:01,209 | INFO | Epoch 1171 Train Time 22.598662853240967s

2025-10-18 23:30:23,411 | INFO | Training epoch 1172, Batch 1000/1000: LR=3.74e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:30:23,506 | INFO | Epoch 1172 Train Time 22.294296979904175s

2025-10-18 23:30:46,653 | INFO | Training epoch 1173, Batch 1000/1000: LR=3.73e-05, Loss=2.69e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:30:46,726 | INFO | Epoch 1173 Train Time 23.219228267669678s

2025-10-18 23:31:08,645 | INFO | Training epoch 1174, Batch 1000/1000: LR=3.72e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:31:08,715 | INFO | Epoch 1174 Train Time 21.986056089401245s

2025-10-18 23:31:32,217 | INFO | Training epoch 1175, Batch 1000/1000: LR=3.71e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:31:32,291 | INFO | Epoch 1175 Train Time 23.575876712799072s

2025-10-18 23:31:55,038 | INFO | Training epoch 1176, Batch 1000/1000: LR=3.71e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:31:55,124 | INFO | Epoch 1176 Train Time 22.831764221191406s

2025-10-18 23:32:18,145 | INFO | Training epoch 1177, Batch 1000/1000: LR=3.70e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:32:18,242 | INFO | Epoch 1177 Train Time 23.11746382713318s

2025-10-18 23:32:40,321 | INFO | Training epoch 1178, Batch 1000/1000: LR=3.69e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:32:40,388 | INFO | Epoch 1178 Train Time 22.144317865371704s

2025-10-18 23:33:03,135 | INFO | Training epoch 1179, Batch 1000/1000: LR=3.68e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:33:03,205 | INFO | Epoch 1179 Train Time 22.815577030181885s

2025-10-18 23:33:25,052 | INFO | Training epoch 1180, Batch 1000/1000: LR=3.68e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:33:25,135 | INFO | Epoch 1180 Train Time 21.92930293083191s

2025-10-18 23:33:48,724 | INFO | Training epoch 1181, Batch 1000/1000: LR=3.67e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:33:48,797 | INFO | Epoch 1181 Train Time 23.661420106887817s

2025-10-18 23:34:12,236 | INFO | Training epoch 1182, Batch 1000/1000: LR=3.66e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:34:12,328 | INFO | Epoch 1182 Train Time 23.529983282089233s

2025-10-18 23:34:34,178 | INFO | Training epoch 1183, Batch 1000/1000: LR=3.65e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:34:34,269 | INFO | Epoch 1183 Train Time 21.940518856048584s

2025-10-18 23:34:56,141 | INFO | Training epoch 1184, Batch 1000/1000: LR=3.65e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:34:56,210 | INFO | Epoch 1184 Train Time 21.939658641815186s

2025-10-18 23:35:19,727 | INFO | Training epoch 1185, Batch 1000/1000: LR=3.64e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:35:19,803 | INFO | Epoch 1185 Train Time 23.591861248016357s

2025-10-18 23:35:43,404 | INFO | Training epoch 1186, Batch 1000/1000: LR=3.63e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:35:43,484 | INFO | Epoch 1186 Train Time 23.68033456802368s

2025-10-18 23:36:05,862 | INFO | Training epoch 1187, Batch 1000/1000: LR=3.62e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:36:05,940 | INFO | Epoch 1187 Train Time 22.45547604560852s

2025-10-18 23:36:28,141 | INFO | Training epoch 1188, Batch 1000/1000: LR=3.62e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:36:28,215 | INFO | Epoch 1188 Train Time 22.274173736572266s

2025-10-18 23:36:51,559 | INFO | Training epoch 1189, Batch 1000/1000: LR=3.61e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:36:51,631 | INFO | Epoch 1189 Train Time 23.414056539535522s

2025-10-18 23:37:15,211 | INFO | Training epoch 1190, Batch 1000/1000: LR=3.60e-05, Loss=2.69e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:37:15,299 | INFO | Epoch 1190 Train Time 23.667216300964355s

2025-10-18 23:37:37,651 | INFO | Training epoch 1191, Batch 1000/1000: LR=3.59e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:37:37,731 | INFO | Epoch 1191 Train Time 22.43033242225647s

2025-10-18 23:38:01,059 | INFO | Training epoch 1192, Batch 1000/1000: LR=3.59e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:38:01,131 | INFO | Epoch 1192 Train Time 23.399235486984253s

2025-10-18 23:38:24,631 | INFO | Training epoch 1193, Batch 1000/1000: LR=3.58e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:38:24,703 | INFO | Epoch 1193 Train Time 23.570616483688354s

2025-10-18 23:38:47,949 | INFO | Training epoch 1194, Batch 1000/1000: LR=3.57e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:38:48,030 | INFO | Epoch 1194 Train Time 23.326759815216064s

2025-10-18 23:39:11,455 | INFO | Training epoch 1195, Batch 1000/1000: LR=3.56e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:39:11,551 | INFO | Epoch 1195 Train Time 23.519373416900635s

2025-10-18 23:39:34,038 | INFO | Training epoch 1196, Batch 1000/1000: LR=3.56e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:39:34,101 | INFO | Epoch 1196 Train Time 22.5489764213562s

2025-10-18 23:39:56,198 | INFO | Training epoch 1197, Batch 1000/1000: LR=3.55e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:39:56,280 | INFO | Epoch 1197 Train Time 22.178598642349243s

2025-10-18 23:40:19,048 | INFO | Training epoch 1198, Batch 1000/1000: LR=3.54e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:40:19,125 | INFO | Epoch 1198 Train Time 22.842684507369995s

2025-10-18 23:40:40,343 | INFO | Training epoch 1199, Batch 1000/1000: LR=3.54e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:40:40,410 | INFO | Epoch 1199 Train Time 21.284231662750244s

2025-10-18 23:41:03,119 | INFO | Training epoch 1200, Batch 1000/1000: LR=3.53e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:41:03,187 | INFO | Epoch 1200 Train Time 22.776208877563477s

2025-10-18 23:41:25,334 | INFO | Training epoch 1201, Batch 1000/1000: LR=3.52e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:41:25,420 | INFO | Epoch 1201 Train Time 22.23173761367798s

2025-10-18 23:41:48,151 | INFO | Training epoch 1202, Batch 1000/1000: LR=3.51e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:41:48,216 | INFO | Epoch 1202 Train Time 22.794989824295044s

2025-10-18 23:42:11,207 | INFO | Training epoch 1203, Batch 1000/1000: LR=3.51e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:42:11,274 | INFO | Epoch 1203 Train Time 23.056006908416748s

2025-10-18 23:42:33,126 | INFO | Training epoch 1204, Batch 1000/1000: LR=3.50e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:42:33,197 | INFO | Epoch 1204 Train Time 21.92165517807007s

2025-10-18 23:42:52,148 | INFO | Training epoch 1205, Batch 1000/1000: LR=3.49e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:42:52,213 | INFO | Epoch 1205 Train Time 19.014731407165527s

2025-10-18 23:43:15,633 | INFO | Training epoch 1206, Batch 1000/1000: LR=3.48e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:43:15,737 | INFO | Epoch 1206 Train Time 23.521775007247925s

2025-10-18 23:43:38,556 | INFO | Training epoch 1207, Batch 1000/1000: LR=3.48e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:43:38,635 | INFO | Epoch 1207 Train Time 22.896968603134155s

2025-10-18 23:43:58,723 | INFO | Training epoch 1208, Batch 1000/1000: LR=3.47e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:43:58,814 | INFO | Epoch 1208 Train Time 20.178263902664185s

2025-10-18 23:44:22,548 | INFO | Training epoch 1209, Batch 1000/1000: LR=3.46e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:44:22,630 | INFO | Epoch 1209 Train Time 23.814441204071045s

2025-10-18 23:44:44,886 | INFO | Training epoch 1210, Batch 1000/1000: LR=3.45e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:44:44,949 | INFO | Epoch 1210 Train Time 22.31827735900879s

2025-10-18 23:45:07,648 | INFO | Training epoch 1211, Batch 1000/1000: LR=3.45e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:45:07,737 | INFO | Epoch 1211 Train Time 22.78708791732788s

2025-10-18 23:45:29,610 | INFO | Training epoch 1212, Batch 1000/1000: LR=3.44e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-18 23:45:29,694 | INFO | Epoch 1212 Train Time 21.955572366714478s

2025-10-18 23:45:53,320 | INFO | Training epoch 1213, Batch 1000/1000: LR=3.43e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:45:53,384 | INFO | Epoch 1213 Train Time 23.688907146453857s

2025-10-18 23:46:13,493 | INFO | Training epoch 1214, Batch 1000/1000: LR=3.42e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:46:13,561 | INFO | Epoch 1214 Train Time 20.176921844482422s

2025-10-18 23:46:36,052 | INFO | Training epoch 1215, Batch 1000/1000: LR=3.42e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:46:36,120 | INFO | Epoch 1215 Train Time 22.557857275009155s

2025-10-18 23:46:59,034 | INFO | Training epoch 1216, Batch 1000/1000: LR=3.41e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:46:59,115 | INFO | Epoch 1216 Train Time 22.993086099624634s

2025-10-18 23:47:22,903 | INFO | Training epoch 1217, Batch 1000/1000: LR=3.40e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:47:22,967 | INFO | Epoch 1217 Train Time 23.851284980773926s

2025-10-18 23:47:46,143 | INFO | Training epoch 1218, Batch 1000/1000: LR=3.40e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:47:46,220 | INFO | Epoch 1218 Train Time 23.25237798690796s

2025-10-18 23:48:09,109 | INFO | Training epoch 1219, Batch 1000/1000: LR=3.39e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:48:09,192 | INFO | Epoch 1219 Train Time 22.970417022705078s

2025-10-18 23:48:29,170 | INFO | Training epoch 1220, Batch 1000/1000: LR=3.38e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:48:29,232 | INFO | Epoch 1220 Train Time 20.038340091705322s

2025-10-18 23:48:52,024 | INFO | Training epoch 1221, Batch 1000/1000: LR=3.37e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:48:52,089 | INFO | Epoch 1221 Train Time 22.855758666992188s

2025-10-18 23:49:15,041 | INFO | Training epoch 1222, Batch 1000/1000: LR=3.37e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:49:15,107 | INFO | Epoch 1222 Train Time 23.01749348640442s

2025-10-18 23:49:38,136 | INFO | Training epoch 1223, Batch 1000/1000: LR=3.36e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:49:38,214 | INFO | Epoch 1223 Train Time 23.106385469436646s

2025-10-18 23:50:00,901 | INFO | Training epoch 1224, Batch 1000/1000: LR=3.35e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:50:00,963 | INFO | Epoch 1224 Train Time 22.747411012649536s

2025-10-18 23:50:23,738 | INFO | Training epoch 1225, Batch 1000/1000: LR=3.34e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:50:23,814 | INFO | Epoch 1225 Train Time 22.849823236465454s

2025-10-18 23:50:46,997 | INFO | Training epoch 1226, Batch 1000/1000: LR=3.34e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:50:47,063 | INFO | Epoch 1226 Train Time 23.247552156448364s

2025-10-18 23:51:10,947 | INFO | Training epoch 1227, Batch 1000/1000: LR=3.33e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:51:11,017 | INFO | Epoch 1227 Train Time 23.952933073043823s

2025-10-18 23:51:32,701 | INFO | Training epoch 1228, Batch 1000/1000: LR=3.32e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:51:32,781 | INFO | Epoch 1228 Train Time 21.763898134231567s

2025-10-18 23:51:55,751 | INFO | Training epoch 1229, Batch 1000/1000: LR=3.31e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-18 23:51:55,840 | INFO | Epoch 1229 Train Time 23.057721853256226s

2025-10-18 23:52:19,001 | INFO | Training epoch 1230, Batch 1000/1000: LR=3.31e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:52:19,084 | INFO | Epoch 1230 Train Time 23.241222620010376s

2025-10-18 23:52:42,551 | INFO | Training epoch 1231, Batch 1000/1000: LR=3.30e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:52:42,616 | INFO | Epoch 1231 Train Time 23.531447172164917s

2025-10-18 23:53:05,245 | INFO | Training epoch 1232, Batch 1000/1000: LR=3.29e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:53:05,315 | INFO | Epoch 1232 Train Time 22.697895765304565s

2025-10-18 23:53:28,557 | INFO | Training epoch 1233, Batch 1000/1000: LR=3.29e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:53:28,625 | INFO | Epoch 1233 Train Time 23.3074734210968s

2025-10-18 23:53:52,106 | INFO | Training epoch 1234, Batch 1000/1000: LR=3.28e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:53:52,174 | INFO | Epoch 1234 Train Time 23.5479793548584s

2025-10-18 23:54:14,802 | INFO | Training epoch 1235, Batch 1000/1000: LR=3.27e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-18 23:54:14,869 | INFO | Epoch 1235 Train Time 22.6938693523407s

2025-10-18 23:54:37,860 | INFO | Training epoch 1236, Batch 1000/1000: LR=3.26e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-18 23:54:37,929 | INFO | Epoch 1236 Train Time 23.05895161628723s

2025-10-18 23:55:00,435 | INFO | Training epoch 1237, Batch 1000/1000: LR=3.26e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-18 23:55:00,505 | INFO | Epoch 1237 Train Time 22.57490301132202s

2025-10-18 23:55:21,853 | INFO | Training epoch 1238, Batch 1000/1000: LR=3.25e-05, Loss=2.74e-02 BER=1.04e-02 FER=1.33e-01
2025-10-18 23:55:21,917 | INFO | Epoch 1238 Train Time 21.411391258239746s

2025-10-18 23:55:45,045 | INFO | Training epoch 1239, Batch 1000/1000: LR=3.24e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-18 23:55:45,126 | INFO | Epoch 1239 Train Time 23.206956386566162s

2025-10-18 23:56:08,813 | INFO | Training epoch 1240, Batch 1000/1000: LR=3.24e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:56:08,895 | INFO | Epoch 1240 Train Time 23.768059492111206s

2025-10-18 23:56:32,056 | INFO | Training epoch 1241, Batch 1000/1000: LR=3.23e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:56:32,127 | INFO | Epoch 1241 Train Time 23.231375455856323s

2025-10-18 23:56:55,140 | INFO | Training epoch 1242, Batch 1000/1000: LR=3.22e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:56:55,226 | INFO | Epoch 1242 Train Time 23.097333192825317s

2025-10-18 23:57:18,982 | INFO | Training epoch 1243, Batch 1000/1000: LR=3.21e-05, Loss=2.77e-02 BER=1.05e-02 FER=1.34e-01
2025-10-18 23:57:19,052 | INFO | Epoch 1243 Train Time 23.825073957443237s

2025-10-18 23:57:41,311 | INFO | Training epoch 1244, Batch 1000/1000: LR=3.21e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.32e-01
2025-10-18 23:57:41,383 | INFO | Epoch 1244 Train Time 22.33018946647644s

2025-10-18 23:58:04,148 | INFO | Training epoch 1245, Batch 1000/1000: LR=3.20e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:58:04,245 | INFO | Epoch 1245 Train Time 22.860684394836426s

2025-10-18 23:58:27,852 | INFO | Training epoch 1246, Batch 1000/1000: LR=3.19e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-18 23:58:27,939 | INFO | Epoch 1246 Train Time 23.692512035369873s

2025-10-18 23:58:51,015 | INFO | Training epoch 1247, Batch 1000/1000: LR=3.18e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:58:51,100 | INFO | Epoch 1247 Train Time 23.15947437286377s

2025-10-18 23:59:14,520 | INFO | Training epoch 1248, Batch 1000/1000: LR=3.18e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-18 23:59:14,596 | INFO | Epoch 1248 Train Time 23.493708848953247s

2025-10-18 23:59:38,142 | INFO | Training epoch 1249, Batch 1000/1000: LR=3.17e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-18 23:59:38,232 | INFO | Epoch 1249 Train Time 23.635359525680542s

2025-10-19 00:00:01,410 | INFO | Training epoch 1250, Batch 1000/1000: LR=3.16e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:00:01,492 | INFO | Epoch 1250 Train Time 23.258113622665405s

2025-10-19 00:00:24,749 | INFO | Training epoch 1251, Batch 1000/1000: LR=3.16e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:00:24,820 | INFO | Epoch 1251 Train Time 23.32681703567505s

2025-10-19 00:00:47,358 | INFO | Training epoch 1252, Batch 1000/1000: LR=3.15e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:00:47,417 | INFO | Epoch 1252 Train Time 22.59561324119568s

2025-10-19 00:01:09,830 | INFO | Training epoch 1253, Batch 1000/1000: LR=3.14e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:01:09,913 | INFO | Epoch 1253 Train Time 22.495862245559692s

2025-10-19 00:01:33,455 | INFO | Training epoch 1254, Batch 1000/1000: LR=3.13e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:01:33,521 | INFO | Epoch 1254 Train Time 23.60507321357727s

2025-10-19 00:01:56,738 | INFO | Training epoch 1255, Batch 1000/1000: LR=3.13e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:01:56,807 | INFO | Epoch 1255 Train Time 23.28537631034851s

2025-10-19 00:02:19,429 | INFO | Training epoch 1256, Batch 1000/1000: LR=3.12e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:02:19,507 | INFO | Epoch 1256 Train Time 22.698760509490967s

2025-10-19 00:02:43,334 | INFO | Training epoch 1257, Batch 1000/1000: LR=3.11e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.31e-01
2025-10-19 00:02:43,413 | INFO | Epoch 1257 Train Time 23.905189990997314s

2025-10-19 00:03:05,326 | INFO | Training epoch 1258, Batch 1000/1000: LR=3.11e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:03:05,416 | INFO | Epoch 1258 Train Time 22.000574111938477s

2025-10-19 00:03:27,708 | INFO | Training epoch 1259, Batch 1000/1000: LR=3.10e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-19 00:03:27,776 | INFO | Epoch 1259 Train Time 22.359349012374878s

2025-10-19 00:03:52,065 | INFO | Training epoch 1260, Batch 1000/1000: LR=3.09e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:03:52,135 | INFO | Epoch 1260 Train Time 24.356605529785156s

2025-10-19 00:04:15,143 | INFO | Training epoch 1261, Batch 1000/1000: LR=3.08e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:04:15,209 | INFO | Epoch 1261 Train Time 23.073243856430054s

2025-10-19 00:04:38,561 | INFO | Training epoch 1262, Batch 1000/1000: LR=3.08e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:04:38,648 | INFO | Epoch 1262 Train Time 23.438637733459473s

2025-10-19 00:05:01,355 | INFO | Training epoch 1263, Batch 1000/1000: LR=3.07e-05, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 00:05:01,442 | INFO | Epoch 1263 Train Time 22.792755365371704s

2025-10-19 00:05:01,443 | INFO | [P1] saving best_model with loss 0.026666 at epoch 1263
2025-10-19 00:05:23,760 | INFO | Training epoch 1264, Batch 1000/1000: LR=3.06e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.32e-01
2025-10-19 00:05:23,825 | INFO | Epoch 1264 Train Time 22.365883827209473s

2025-10-19 00:05:45,523 | INFO | Training epoch 1265, Batch 1000/1000: LR=3.06e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:05:45,585 | INFO | Epoch 1265 Train Time 21.758437871932983s

2025-10-19 00:06:08,548 | INFO | Training epoch 1266, Batch 1000/1000: LR=3.05e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:06:08,612 | INFO | Epoch 1266 Train Time 23.02594828605652s

2025-10-19 00:06:31,132 | INFO | Training epoch 1267, Batch 1000/1000: LR=3.04e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:06:31,223 | INFO | Epoch 1267 Train Time 22.609741926193237s

2025-10-19 00:06:54,292 | INFO | Training epoch 1268, Batch 1000/1000: LR=3.03e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 00:06:54,370 | INFO | Epoch 1268 Train Time 23.145915031433105s

2025-10-19 00:07:14,528 | INFO | Training epoch 1269, Batch 1000/1000: LR=3.03e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:07:14,591 | INFO | Epoch 1269 Train Time 20.219969272613525s

2025-10-19 00:07:37,310 | INFO | Training epoch 1270, Batch 1000/1000: LR=3.02e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:07:37,392 | INFO | Epoch 1270 Train Time 22.79999303817749s

2025-10-19 00:08:01,234 | INFO | Training epoch 1271, Batch 1000/1000: LR=3.01e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:08:01,316 | INFO | Epoch 1271 Train Time 23.923033714294434s

2025-10-19 00:08:24,236 | INFO | Training epoch 1272, Batch 1000/1000: LR=3.01e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:08:24,315 | INFO | Epoch 1272 Train Time 22.997901916503906s

2025-10-19 00:08:47,207 | INFO | Training epoch 1273, Batch 1000/1000: LR=3.00e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:08:47,291 | INFO | Epoch 1273 Train Time 22.97488570213318s

2025-10-19 00:09:09,629 | INFO | Training epoch 1274, Batch 1000/1000: LR=2.99e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:09:09,713 | INFO | Epoch 1274 Train Time 22.421680212020874s

2025-10-19 00:09:30,648 | INFO | Training epoch 1275, Batch 1000/1000: LR=2.98e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-19 00:09:30,730 | INFO | Epoch 1275 Train Time 21.015260696411133s

2025-10-19 00:09:54,359 | INFO | Training epoch 1276, Batch 1000/1000: LR=2.98e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:09:54,444 | INFO | Epoch 1276 Train Time 23.712721347808838s

2025-10-19 00:10:17,226 | INFO | Training epoch 1277, Batch 1000/1000: LR=2.97e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:10:17,308 | INFO | Epoch 1277 Train Time 22.863195419311523s

2025-10-19 00:10:41,070 | INFO | Training epoch 1278, Batch 1000/1000: LR=2.96e-05, Loss=2.75e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:10:41,151 | INFO | Epoch 1278 Train Time 23.840989351272583s

2025-10-19 00:11:05,420 | INFO | Training epoch 1279, Batch 1000/1000: LR=2.96e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:11:05,498 | INFO | Epoch 1279 Train Time 24.345760822296143s

2025-10-19 00:11:29,112 | INFO | Training epoch 1280, Batch 1000/1000: LR=2.95e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:11:29,212 | INFO | Epoch 1280 Train Time 23.712893962860107s

2025-10-19 00:11:52,030 | INFO | Training epoch 1281, Batch 1000/1000: LR=2.94e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:11:52,104 | INFO | Epoch 1281 Train Time 22.891345500946045s

2025-10-19 00:12:15,025 | INFO | Training epoch 1282, Batch 1000/1000: LR=2.94e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:12:15,102 | INFO | Epoch 1282 Train Time 22.99716305732727s

2025-10-19 00:12:39,273 | INFO | Training epoch 1283, Batch 1000/1000: LR=2.93e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-19 00:12:39,340 | INFO | Epoch 1283 Train Time 24.236873865127563s

2025-10-19 00:13:03,390 | INFO | Training epoch 1284, Batch 1000/1000: LR=2.92e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:13:03,482 | INFO | Epoch 1284 Train Time 24.13927960395813s

2025-10-19 00:13:26,933 | INFO | Training epoch 1285, Batch 1000/1000: LR=2.91e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:13:27,031 | INFO | Epoch 1285 Train Time 23.54870295524597s

2025-10-19 00:13:50,030 | INFO | Training epoch 1286, Batch 1000/1000: LR=2.91e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:13:50,111 | INFO | Epoch 1286 Train Time 23.078959941864014s

2025-10-19 00:14:13,493 | INFO | Training epoch 1287, Batch 1000/1000: LR=2.90e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 00:14:13,568 | INFO | Epoch 1287 Train Time 23.45647668838501s

2025-10-19 00:14:36,403 | INFO | Training epoch 1288, Batch 1000/1000: LR=2.89e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:14:36,486 | INFO | Epoch 1288 Train Time 22.917211771011353s

2025-10-19 00:14:59,780 | INFO | Training epoch 1289, Batch 1000/1000: LR=2.89e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:14:59,855 | INFO | Epoch 1289 Train Time 23.367421865463257s

2025-10-19 00:15:23,069 | INFO | Training epoch 1290, Batch 1000/1000: LR=2.88e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:15:23,154 | INFO | Epoch 1290 Train Time 23.298282384872437s

2025-10-19 00:15:45,364 | INFO | Training epoch 1291, Batch 1000/1000: LR=2.87e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:15:45,428 | INFO | Epoch 1291 Train Time 22.272884368896484s

2025-10-19 00:16:08,849 | INFO | Training epoch 1292, Batch 1000/1000: LR=2.87e-05, Loss=2.74e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:16:08,918 | INFO | Epoch 1292 Train Time 23.488849639892578s

2025-10-19 00:16:31,604 | INFO | Training epoch 1293, Batch 1000/1000: LR=2.86e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:16:31,679 | INFO | Epoch 1293 Train Time 22.760196447372437s

2025-10-19 00:16:55,249 | INFO | Training epoch 1294, Batch 1000/1000: LR=2.85e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:16:55,335 | INFO | Epoch 1294 Train Time 23.653942823410034s

2025-10-19 00:17:18,900 | INFO | Training epoch 1295, Batch 1000/1000: LR=2.84e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:17:18,977 | INFO | Epoch 1295 Train Time 23.6408908367157s

2025-10-19 00:17:42,452 | INFO | Training epoch 1296, Batch 1000/1000: LR=2.84e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:17:42,523 | INFO | Epoch 1296 Train Time 23.5446298122406s

2025-10-19 00:18:05,746 | INFO | Training epoch 1297, Batch 1000/1000: LR=2.83e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:18:05,830 | INFO | Epoch 1297 Train Time 23.305070400238037s

2025-10-19 00:18:29,161 | INFO | Training epoch 1298, Batch 1000/1000: LR=2.82e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.35e-01
2025-10-19 00:18:29,228 | INFO | Epoch 1298 Train Time 23.396933555603027s

2025-10-19 00:18:52,859 | INFO | Training epoch 1299, Batch 1000/1000: LR=2.82e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:18:52,937 | INFO | Epoch 1299 Train Time 23.707982063293457s

2025-10-19 00:19:16,639 | INFO | Training epoch 1300, Batch 1000/1000: LR=2.81e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:19:16,724 | INFO | Epoch 1300 Train Time 23.785695552825928s

2025-10-19 00:19:41,361 | INFO | Training epoch 1301, Batch 1000/1000: LR=2.80e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:19:41,432 | INFO | Epoch 1301 Train Time 24.706896543502808s

2025-10-19 00:20:05,148 | INFO | Training epoch 1302, Batch 1000/1000: LR=2.80e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:20:05,229 | INFO | Epoch 1302 Train Time 23.794125080108643s

2025-10-19 00:20:29,331 | INFO | Training epoch 1303, Batch 1000/1000: LR=2.79e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:20:29,417 | INFO | Epoch 1303 Train Time 24.18681025505066s

2025-10-19 00:20:52,118 | INFO | Training epoch 1304, Batch 1000/1000: LR=2.78e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:20:52,201 | INFO | Epoch 1304 Train Time 22.78271174430847s

2025-10-19 00:21:15,743 | INFO | Training epoch 1305, Batch 1000/1000: LR=2.78e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:21:15,809 | INFO | Epoch 1305 Train Time 23.6072940826416s

2025-10-19 00:21:38,651 | INFO | Training epoch 1306, Batch 1000/1000: LR=2.77e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:21:38,719 | INFO | Epoch 1306 Train Time 22.908497095108032s

2025-10-19 00:22:00,729 | INFO | Training epoch 1307, Batch 1000/1000: LR=2.76e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 00:22:00,789 | INFO | Epoch 1307 Train Time 22.068978309631348s

2025-10-19 00:22:24,849 | INFO | Training epoch 1308, Batch 1000/1000: LR=2.75e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:22:24,916 | INFO | Epoch 1308 Train Time 24.126346826553345s

2025-10-19 00:22:48,356 | INFO | Training epoch 1309, Batch 1000/1000: LR=2.75e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:22:48,435 | INFO | Epoch 1309 Train Time 23.517004013061523s

2025-10-19 00:23:11,545 | INFO | Training epoch 1310, Batch 1000/1000: LR=2.74e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:23:11,629 | INFO | Epoch 1310 Train Time 23.193358898162842s

2025-10-19 00:23:34,673 | INFO | Training epoch 1311, Batch 1000/1000: LR=2.73e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:23:34,740 | INFO | Epoch 1311 Train Time 23.109164714813232s

2025-10-19 00:23:57,020 | INFO | Training epoch 1312, Batch 1000/1000: LR=2.73e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 00:23:57,084 | INFO | Epoch 1312 Train Time 22.343759059906006s

2025-10-19 00:24:20,811 | INFO | Training epoch 1313, Batch 1000/1000: LR=2.72e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:24:20,901 | INFO | Epoch 1313 Train Time 23.813681602478027s

2025-10-19 00:24:44,940 | INFO | Training epoch 1314, Batch 1000/1000: LR=2.71e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:24:45,013 | INFO | Epoch 1314 Train Time 24.111170530319214s

2025-10-19 00:25:07,418 | INFO | Training epoch 1315, Batch 1000/1000: LR=2.71e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:25:07,484 | INFO | Epoch 1315 Train Time 22.46925401687622s

2025-10-19 00:25:30,806 | INFO | Training epoch 1316, Batch 1000/1000: LR=2.70e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:25:30,882 | INFO | Epoch 1316 Train Time 23.397613286972046s

2025-10-19 00:25:54,043 | INFO | Training epoch 1317, Batch 1000/1000: LR=2.69e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 00:25:54,125 | INFO | Epoch 1317 Train Time 23.24239182472229s

2025-10-19 00:26:16,737 | INFO | Training epoch 1318, Batch 1000/1000: LR=2.69e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:26:16,822 | INFO | Epoch 1318 Train Time 22.695894956588745s

2025-10-19 00:26:40,048 | INFO | Training epoch 1319, Batch 1000/1000: LR=2.68e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:26:40,134 | INFO | Epoch 1319 Train Time 23.310686588287354s

2025-10-19 00:27:02,360 | INFO | Training epoch 1320, Batch 1000/1000: LR=2.67e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:27:02,445 | INFO | Epoch 1320 Train Time 22.310322523117065s

2025-10-19 00:27:25,409 | INFO | Training epoch 1321, Batch 1000/1000: LR=2.67e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:27:25,497 | INFO | Epoch 1321 Train Time 23.050984382629395s

2025-10-19 00:27:47,946 | INFO | Training epoch 1322, Batch 1000/1000: LR=2.66e-05, Loss=2.67e-02 BER=9.98e-03 FER=1.30e-01
2025-10-19 00:27:48,012 | INFO | Epoch 1322 Train Time 22.513887882232666s

2025-10-19 00:28:11,431 | INFO | Training epoch 1323, Batch 1000/1000: LR=2.65e-05, Loss=2.67e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:28:11,505 | INFO | Epoch 1323 Train Time 23.492381811141968s

2025-10-19 00:28:35,046 | INFO | Training epoch 1324, Batch 1000/1000: LR=2.64e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:28:35,130 | INFO | Epoch 1324 Train Time 23.62267017364502s

2025-10-19 00:28:58,009 | INFO | Training epoch 1325, Batch 1000/1000: LR=2.64e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-19 00:28:58,083 | INFO | Epoch 1325 Train Time 22.951896905899048s

2025-10-19 00:29:22,231 | INFO | Training epoch 1326, Batch 1000/1000: LR=2.63e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 00:29:22,298 | INFO | Epoch 1326 Train Time 24.213322639465332s

2025-10-19 00:29:45,432 | INFO | Training epoch 1327, Batch 1000/1000: LR=2.62e-05, Loss=2.74e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 00:29:45,502 | INFO | Epoch 1327 Train Time 23.203402996063232s

2025-10-19 00:30:08,165 | INFO | Training epoch 1328, Batch 1000/1000: LR=2.62e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:30:08,237 | INFO | Epoch 1328 Train Time 22.733906269073486s

2025-10-19 00:30:30,747 | INFO | Training epoch 1329, Batch 1000/1000: LR=2.61e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:30:30,823 | INFO | Epoch 1329 Train Time 22.585015773773193s

2025-10-19 00:30:53,715 | INFO | Training epoch 1330, Batch 1000/1000: LR=2.60e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:30:53,781 | INFO | Epoch 1330 Train Time 22.957223176956177s

2025-10-19 00:31:13,423 | INFO | Training epoch 1331, Batch 1000/1000: LR=2.60e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:31:13,489 | INFO | Epoch 1331 Train Time 19.7064368724823s

2025-10-19 00:31:36,456 | INFO | Training epoch 1332, Batch 1000/1000: LR=2.59e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:31:36,537 | INFO | Epoch 1332 Train Time 23.04691767692566s

2025-10-19 00:31:59,830 | INFO | Training epoch 1333, Batch 1000/1000: LR=2.58e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:31:59,913 | INFO | Epoch 1333 Train Time 23.374382495880127s

2025-10-19 00:32:23,609 | INFO | Training epoch 1334, Batch 1000/1000: LR=2.58e-05, Loss=2.71e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:32:23,693 | INFO | Epoch 1334 Train Time 23.779043912887573s

2025-10-19 00:32:46,056 | INFO | Training epoch 1335, Batch 1000/1000: LR=2.57e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:32:46,146 | INFO | Epoch 1335 Train Time 22.45208501815796s

2025-10-19 00:33:09,235 | INFO | Training epoch 1336, Batch 1000/1000: LR=2.56e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:33:09,305 | INFO | Epoch 1336 Train Time 23.15696930885315s

2025-10-19 00:33:33,435 | INFO | Training epoch 1337, Batch 1000/1000: LR=2.56e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:33:33,520 | INFO | Epoch 1337 Train Time 24.213406801223755s

2025-10-19 00:33:57,023 | INFO | Training epoch 1338, Batch 1000/1000: LR=2.55e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:33:57,111 | INFO | Epoch 1338 Train Time 23.589821815490723s

2025-10-19 00:34:20,534 | INFO | Training epoch 1339, Batch 1000/1000: LR=2.54e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:34:20,610 | INFO | Epoch 1339 Train Time 23.497310876846313s

2025-10-19 00:34:45,043 | INFO | Training epoch 1340, Batch 1000/1000: LR=2.54e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:34:45,117 | INFO | Epoch 1340 Train Time 24.506444454193115s

2025-10-19 00:35:09,923 | INFO | Training epoch 1341, Batch 1000/1000: LR=2.53e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:35:10,023 | INFO | Epoch 1341 Train Time 24.903696537017822s

2025-10-19 00:35:33,415 | INFO | Training epoch 1342, Batch 1000/1000: LR=2.52e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:35:33,492 | INFO | Epoch 1342 Train Time 23.467007875442505s

2025-10-19 00:35:56,770 | INFO | Training epoch 1343, Batch 1000/1000: LR=2.52e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-19 00:35:56,840 | INFO | Epoch 1343 Train Time 23.345768690109253s

2025-10-19 00:36:19,836 | INFO | Training epoch 1344, Batch 1000/1000: LR=2.51e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:36:19,902 | INFO | Epoch 1344 Train Time 23.061166763305664s

2025-10-19 00:36:42,907 | INFO | Training epoch 1345, Batch 1000/1000: LR=2.50e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:36:42,974 | INFO | Epoch 1345 Train Time 23.07088804244995s

2025-10-19 00:37:05,739 | INFO | Training epoch 1346, Batch 1000/1000: LR=2.50e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:37:05,806 | INFO | Epoch 1346 Train Time 22.831204414367676s

2025-10-19 00:37:28,976 | INFO | Training epoch 1347, Batch 1000/1000: LR=2.49e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:37:29,065 | INFO | Epoch 1347 Train Time 23.257890462875366s

2025-10-19 00:37:52,527 | INFO | Training epoch 1348, Batch 1000/1000: LR=2.48e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:37:52,610 | INFO | Epoch 1348 Train Time 23.544434309005737s

2025-10-19 00:38:15,432 | INFO | Training epoch 1349, Batch 1000/1000: LR=2.48e-05, Loss=2.70e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:38:15,502 | INFO | Epoch 1349 Train Time 22.890480041503906s

2025-10-19 00:38:38,962 | INFO | Training epoch 1350, Batch 1000/1000: LR=2.47e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:38:39,044 | INFO | Epoch 1350 Train Time 23.53941249847412s

2025-10-19 00:39:02,353 | INFO | Training epoch 1351, Batch 1000/1000: LR=2.46e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:39:02,440 | INFO | Epoch 1351 Train Time 23.395076274871826s

2025-10-19 00:39:25,632 | INFO | Training epoch 1352, Batch 1000/1000: LR=2.46e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:39:25,698 | INFO | Epoch 1352 Train Time 23.257006645202637s

2025-10-19 00:39:48,130 | INFO | Training epoch 1353, Batch 1000/1000: LR=2.45e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:39:48,195 | INFO | Epoch 1353 Train Time 22.494813680648804s

2025-10-19 00:40:11,747 | INFO | Training epoch 1354, Batch 1000/1000: LR=2.44e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:40:11,810 | INFO | Epoch 1354 Train Time 23.614691019058228s

2025-10-19 00:40:35,041 | INFO | Training epoch 1355, Batch 1000/1000: LR=2.44e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:40:35,129 | INFO | Epoch 1355 Train Time 23.31755542755127s

2025-10-19 00:40:58,363 | INFO | Training epoch 1356, Batch 1000/1000: LR=2.43e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:40:58,442 | INFO | Epoch 1356 Train Time 23.312407732009888s

2025-10-19 00:41:21,610 | INFO | Training epoch 1357, Batch 1000/1000: LR=2.42e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:41:21,679 | INFO | Epoch 1357 Train Time 23.23497176170349s

2025-10-19 00:41:45,023 | INFO | Training epoch 1358, Batch 1000/1000: LR=2.42e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:41:45,091 | INFO | Epoch 1358 Train Time 23.4098482131958s

2025-10-19 00:42:07,948 | INFO | Training epoch 1359, Batch 1000/1000: LR=2.41e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:42:08,029 | INFO | Epoch 1359 Train Time 22.93661093711853s

2025-10-19 00:42:31,436 | INFO | Training epoch 1360, Batch 1000/1000: LR=2.40e-05, Loss=2.71e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:42:31,503 | INFO | Epoch 1360 Train Time 23.472830057144165s

2025-10-19 00:42:54,440 | INFO | Training epoch 1361, Batch 1000/1000: LR=2.40e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:42:54,514 | INFO | Epoch 1361 Train Time 23.0098557472229s

2025-10-19 00:43:18,457 | INFO | Training epoch 1362, Batch 1000/1000: LR=2.39e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:43:18,543 | INFO | Epoch 1362 Train Time 24.027973413467407s

2025-10-19 00:43:42,242 | INFO | Training epoch 1363, Batch 1000/1000: LR=2.38e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.30e-01
2025-10-19 00:43:42,325 | INFO | Epoch 1363 Train Time 23.7805757522583s

2025-10-19 00:44:05,746 | INFO | Training epoch 1364, Batch 1000/1000: LR=2.38e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:44:05,813 | INFO | Epoch 1364 Train Time 23.48690104484558s

2025-10-19 00:44:28,627 | INFO | Training epoch 1365, Batch 1000/1000: LR=2.37e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:44:28,695 | INFO | Epoch 1365 Train Time 22.880510568618774s

2025-10-19 00:44:52,053 | INFO | Training epoch 1366, Batch 1000/1000: LR=2.36e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:44:52,141 | INFO | Epoch 1366 Train Time 23.444698572158813s

2025-10-19 00:45:14,952 | INFO | Training epoch 1367, Batch 1000/1000: LR=2.36e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:45:15,035 | INFO | Epoch 1367 Train Time 22.892290353775024s

2025-10-19 00:45:37,446 | INFO | Training epoch 1368, Batch 1000/1000: LR=2.35e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:45:37,528 | INFO | Epoch 1368 Train Time 22.49072504043579s

2025-10-19 00:46:02,442 | INFO | Training epoch 1369, Batch 1000/1000: LR=2.35e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:46:02,535 | INFO | Epoch 1369 Train Time 25.005353212356567s

2025-10-19 00:46:25,645 | INFO | Training epoch 1370, Batch 1000/1000: LR=2.34e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:46:25,725 | INFO | Epoch 1370 Train Time 23.189517736434937s

2025-10-19 00:46:48,843 | INFO | Training epoch 1371, Batch 1000/1000: LR=2.33e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:46:48,928 | INFO | Epoch 1371 Train Time 23.201364517211914s

2025-10-19 00:47:12,975 | INFO | Training epoch 1372, Batch 1000/1000: LR=2.33e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:47:13,041 | INFO | Epoch 1372 Train Time 24.11155867576599s

2025-10-19 00:47:36,556 | INFO | Training epoch 1373, Batch 1000/1000: LR=2.32e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:47:36,622 | INFO | Epoch 1373 Train Time 23.580446004867554s

2025-10-19 00:47:59,839 | INFO | Training epoch 1374, Batch 1000/1000: LR=2.31e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:47:59,905 | INFO | Epoch 1374 Train Time 23.281612634658813s

2025-10-19 00:48:23,109 | INFO | Training epoch 1375, Batch 1000/1000: LR=2.31e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:48:23,174 | INFO | Epoch 1375 Train Time 23.267970085144043s

2025-10-19 00:48:47,154 | INFO | Training epoch 1376, Batch 1000/1000: LR=2.30e-05, Loss=2.72e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:48:47,224 | INFO | Epoch 1376 Train Time 24.048821687698364s

2025-10-19 00:49:11,024 | INFO | Training epoch 1377, Batch 1000/1000: LR=2.29e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:49:11,104 | INFO | Epoch 1377 Train Time 23.87911343574524s

2025-10-19 00:49:34,805 | INFO | Training epoch 1378, Batch 1000/1000: LR=2.29e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:49:34,870 | INFO | Epoch 1378 Train Time 23.76530623435974s

2025-10-19 00:49:57,746 | INFO | Training epoch 1379, Batch 1000/1000: LR=2.28e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:49:57,814 | INFO | Epoch 1379 Train Time 22.94219207763672s

2025-10-19 00:50:21,008 | INFO | Training epoch 1380, Batch 1000/1000: LR=2.27e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:50:21,101 | INFO | Epoch 1380 Train Time 23.285924911499023s

2025-10-19 00:50:43,981 | INFO | Training epoch 1381, Batch 1000/1000: LR=2.27e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:50:44,048 | INFO | Epoch 1381 Train Time 22.946508646011353s

2025-10-19 00:51:07,315 | INFO | Training epoch 1382, Batch 1000/1000: LR=2.26e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 00:51:07,398 | INFO | Epoch 1382 Train Time 23.34910225868225s

2025-10-19 00:51:30,029 | INFO | Training epoch 1383, Batch 1000/1000: LR=2.25e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:51:30,112 | INFO | Epoch 1383 Train Time 22.712679147720337s

2025-10-19 00:51:53,110 | INFO | Training epoch 1384, Batch 1000/1000: LR=2.25e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 00:51:53,195 | INFO | Epoch 1384 Train Time 23.081522941589355s

2025-10-19 00:52:15,952 | INFO | Training epoch 1385, Batch 1000/1000: LR=2.24e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:52:16,016 | INFO | Epoch 1385 Train Time 22.820831775665283s

2025-10-19 00:52:39,312 | INFO | Training epoch 1386, Batch 1000/1000: LR=2.24e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:52:39,376 | INFO | Epoch 1386 Train Time 23.35890293121338s

2025-10-19 00:53:02,355 | INFO | Training epoch 1387, Batch 1000/1000: LR=2.23e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:53:02,424 | INFO | Epoch 1387 Train Time 23.046563625335693s

2025-10-19 00:53:25,103 | INFO | Training epoch 1388, Batch 1000/1000: LR=2.22e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:53:25,186 | INFO | Epoch 1388 Train Time 22.760437488555908s

2025-10-19 00:53:49,214 | INFO | Training epoch 1389, Batch 1000/1000: LR=2.22e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:53:49,279 | INFO | Epoch 1389 Train Time 24.09243369102478s

2025-10-19 00:54:13,143 | INFO | Training epoch 1390, Batch 1000/1000: LR=2.21e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 00:54:13,211 | INFO | Epoch 1390 Train Time 23.930649757385254s

2025-10-19 00:54:36,327 | INFO | Training epoch 1391, Batch 1000/1000: LR=2.20e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:54:36,415 | INFO | Epoch 1391 Train Time 23.20289134979248s

2025-10-19 00:54:58,527 | INFO | Training epoch 1392, Batch 1000/1000: LR=2.20e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 00:54:58,593 | INFO | Epoch 1392 Train Time 22.177087783813477s

2025-10-19 00:55:22,143 | INFO | Training epoch 1393, Batch 1000/1000: LR=2.19e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:55:22,216 | INFO | Epoch 1393 Train Time 23.621347665786743s

2025-10-19 00:55:45,724 | INFO | Training epoch 1394, Batch 1000/1000: LR=2.18e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:55:45,828 | INFO | Epoch 1394 Train Time 23.610345125198364s

2025-10-19 00:56:06,026 | INFO | Training epoch 1395, Batch 1000/1000: LR=2.18e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:56:06,110 | INFO | Epoch 1395 Train Time 20.28102421760559s

2025-10-19 00:56:28,861 | INFO | Training epoch 1396, Batch 1000/1000: LR=2.17e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:56:28,926 | INFO | Epoch 1396 Train Time 22.81488800048828s

2025-10-19 00:56:52,463 | INFO | Training epoch 1397, Batch 1000/1000: LR=2.17e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:56:52,526 | INFO | Epoch 1397 Train Time 23.598628759384155s

2025-10-19 00:57:15,331 | INFO | Training epoch 1398, Batch 1000/1000: LR=2.16e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:57:15,414 | INFO | Epoch 1398 Train Time 22.887571096420288s

2025-10-19 00:57:38,735 | INFO | Training epoch 1399, Batch 1000/1000: LR=2.15e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 00:57:38,805 | INFO | Epoch 1399 Train Time 23.389533042907715s

2025-10-19 00:58:01,518 | INFO | Training epoch 1400, Batch 1000/1000: LR=2.15e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 00:58:01,585 | INFO | Epoch 1400 Train Time 22.778480529785156s

2025-10-19 00:58:24,238 | INFO | Training epoch 1401, Batch 1000/1000: LR=2.14e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 00:58:24,303 | INFO | Epoch 1401 Train Time 22.716980695724487s

2025-10-19 00:58:47,139 | INFO | Training epoch 1402, Batch 1000/1000: LR=2.13e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 00:58:47,218 | INFO | Epoch 1402 Train Time 22.913769960403442s

2025-10-19 00:59:11,029 | INFO | Training epoch 1403, Batch 1000/1000: LR=2.13e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:59:11,115 | INFO | Epoch 1403 Train Time 23.895659923553467s

2025-10-19 00:59:35,555 | INFO | Training epoch 1404, Batch 1000/1000: LR=2.12e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 00:59:35,628 | INFO | Epoch 1404 Train Time 24.511562824249268s

2025-10-19 00:59:57,846 | INFO | Training epoch 1405, Batch 1000/1000: LR=2.12e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 00:59:57,921 | INFO | Epoch 1405 Train Time 22.292722463607788s

2025-10-19 01:00:19,934 | INFO | Training epoch 1406, Batch 1000/1000: LR=2.11e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:00:20,006 | INFO | Epoch 1406 Train Time 22.083020210266113s

2025-10-19 01:00:40,526 | INFO | Training epoch 1407, Batch 1000/1000: LR=2.10e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:00:40,593 | INFO | Epoch 1407 Train Time 20.586352586746216s

2025-10-19 01:01:02,773 | INFO | Training epoch 1408, Batch 1000/1000: LR=2.10e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 01:01:02,836 | INFO | Epoch 1408 Train Time 22.24162483215332s

2025-10-19 01:01:26,255 | INFO | Training epoch 1409, Batch 1000/1000: LR=2.09e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:01:26,360 | INFO | Epoch 1409 Train Time 23.523217916488647s

2025-10-19 01:01:49,442 | INFO | Training epoch 1410, Batch 1000/1000: LR=2.08e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 01:01:49,511 | INFO | Epoch 1410 Train Time 23.150449514389038s

2025-10-19 01:02:11,459 | INFO | Training epoch 1411, Batch 1000/1000: LR=2.08e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:02:11,531 | INFO | Epoch 1411 Train Time 22.01967144012451s

2025-10-19 01:02:35,042 | INFO | Training epoch 1412, Batch 1000/1000: LR=2.07e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 01:02:35,123 | INFO | Epoch 1412 Train Time 23.591225147247314s

2025-10-19 01:02:59,318 | INFO | Training epoch 1413, Batch 1000/1000: LR=2.07e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:02:59,383 | INFO | Epoch 1413 Train Time 24.25844669342041s

2025-10-19 01:03:19,940 | INFO | Training epoch 1414, Batch 1000/1000: LR=2.06e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:03:20,023 | INFO | Epoch 1414 Train Time 20.63779306411743s

2025-10-19 01:03:43,216 | INFO | Training epoch 1415, Batch 1000/1000: LR=2.05e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:03:43,299 | INFO | Epoch 1415 Train Time 23.273714065551758s

2025-10-19 01:04:05,943 | INFO | Training epoch 1416, Batch 1000/1000: LR=2.05e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:04:06,017 | INFO | Epoch 1416 Train Time 22.71490216255188s

2025-10-19 01:04:29,027 | INFO | Training epoch 1417, Batch 1000/1000: LR=2.04e-05, Loss=2.67e-02 BER=9.97e-03 FER=1.30e-01
2025-10-19 01:04:29,126 | INFO | Epoch 1417 Train Time 23.106931447982788s

2025-10-19 01:04:52,332 | INFO | Training epoch 1418, Batch 1000/1000: LR=2.03e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:04:52,398 | INFO | Epoch 1418 Train Time 23.27056646347046s

2025-10-19 01:05:15,424 | INFO | Training epoch 1419, Batch 1000/1000: LR=2.03e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:05:15,507 | INFO | Epoch 1419 Train Time 23.108412504196167s

2025-10-19 01:05:35,134 | INFO | Training epoch 1420, Batch 1000/1000: LR=2.02e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 01:05:35,212 | INFO | Epoch 1420 Train Time 19.7041232585907s

2025-10-19 01:05:57,957 | INFO | Training epoch 1421, Batch 1000/1000: LR=2.02e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:05:58,035 | INFO | Epoch 1421 Train Time 22.82113528251648s

2025-10-19 01:06:21,137 | INFO | Training epoch 1422, Batch 1000/1000: LR=2.01e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:06:21,215 | INFO | Epoch 1422 Train Time 23.178669929504395s

2025-10-19 01:06:44,641 | INFO | Training epoch 1423, Batch 1000/1000: LR=2.00e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:06:44,730 | INFO | Epoch 1423 Train Time 23.51400923728943s

2025-10-19 01:07:07,850 | INFO | Training epoch 1424, Batch 1000/1000: LR=2.00e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:07:07,927 | INFO | Epoch 1424 Train Time 23.194778203964233s

2025-10-19 01:07:31,056 | INFO | Training epoch 1425, Batch 1000/1000: LR=1.99e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 01:07:31,136 | INFO | Epoch 1425 Train Time 23.208672523498535s

2025-10-19 01:07:53,109 | INFO | Training epoch 1426, Batch 1000/1000: LR=1.99e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:07:53,178 | INFO | Epoch 1426 Train Time 22.041003942489624s

2025-10-19 01:08:15,744 | INFO | Training epoch 1427, Batch 1000/1000: LR=1.98e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 01:08:15,811 | INFO | Epoch 1427 Train Time 22.63066077232361s

2025-10-19 01:08:38,547 | INFO | Training epoch 1428, Batch 1000/1000: LR=1.97e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 01:08:38,618 | INFO | Epoch 1428 Train Time 22.805017232894897s

2025-10-19 01:09:01,653 | INFO | Training epoch 1429, Batch 1000/1000: LR=1.97e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:09:01,737 | INFO | Epoch 1429 Train Time 23.11826205253601s

2025-10-19 01:09:24,115 | INFO | Training epoch 1430, Batch 1000/1000: LR=1.96e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:09:24,194 | INFO | Epoch 1430 Train Time 22.455167770385742s

2025-10-19 01:09:47,730 | INFO | Training epoch 1431, Batch 1000/1000: LR=1.96e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:09:47,816 | INFO | Epoch 1431 Train Time 23.621633052825928s

2025-10-19 01:10:10,230 | INFO | Training epoch 1432, Batch 1000/1000: LR=1.95e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:10:10,299 | INFO | Epoch 1432 Train Time 22.4808030128479s

2025-10-19 01:10:32,349 | INFO | Training epoch 1433, Batch 1000/1000: LR=1.94e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:10:32,421 | INFO | Epoch 1433 Train Time 22.121408462524414s

2025-10-19 01:10:52,658 | INFO | Training epoch 1434, Batch 1000/1000: LR=1.94e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:10:52,740 | INFO | Epoch 1434 Train Time 20.31678342819214s

2025-10-19 01:11:15,618 | INFO | Training epoch 1435, Batch 1000/1000: LR=1.93e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:11:15,683 | INFO | Epoch 1435 Train Time 22.940473318099976s

2025-10-19 01:11:39,646 | INFO | Training epoch 1436, Batch 1000/1000: LR=1.92e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:11:39,723 | INFO | Epoch 1436 Train Time 24.038888931274414s

2025-10-19 01:12:02,833 | INFO | Training epoch 1437, Batch 1000/1000: LR=1.92e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:12:02,895 | INFO | Epoch 1437 Train Time 23.1707501411438s

2025-10-19 01:12:25,139 | INFO | Training epoch 1438, Batch 1000/1000: LR=1.91e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:12:25,205 | INFO | Epoch 1438 Train Time 22.309558153152466s

2025-10-19 01:12:48,060 | INFO | Training epoch 1439, Batch 1000/1000: LR=1.91e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:12:48,145 | INFO | Epoch 1439 Train Time 22.93873143196106s

2025-10-19 01:13:12,014 | INFO | Training epoch 1440, Batch 1000/1000: LR=1.90e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:13:12,114 | INFO | Epoch 1440 Train Time 23.967236042022705s

2025-10-19 01:13:35,029 | INFO | Training epoch 1441, Batch 1000/1000: LR=1.89e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:13:35,107 | INFO | Epoch 1441 Train Time 22.991493701934814s

2025-10-19 01:13:57,967 | INFO | Training epoch 1442, Batch 1000/1000: LR=1.89e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:13:58,040 | INFO | Epoch 1442 Train Time 22.932044506072998s

2025-10-19 01:14:20,619 | INFO | Training epoch 1443, Batch 1000/1000: LR=1.88e-05, Loss=2.69e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:14:20,693 | INFO | Epoch 1443 Train Time 22.65207052230835s

2025-10-19 01:14:43,035 | INFO | Training epoch 1444, Batch 1000/1000: LR=1.88e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:14:43,105 | INFO | Epoch 1444 Train Time 22.408968925476074s

2025-10-19 01:15:05,411 | INFO | Training epoch 1445, Batch 1000/1000: LR=1.87e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:15:05,480 | INFO | Epoch 1445 Train Time 22.37434983253479s

2025-10-19 01:15:28,542 | INFO | Training epoch 1446, Batch 1000/1000: LR=1.86e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:15:28,617 | INFO | Epoch 1446 Train Time 23.13525152206421s

2025-10-19 01:15:51,646 | INFO | Training epoch 1447, Batch 1000/1000: LR=1.86e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:15:51,728 | INFO | Epoch 1447 Train Time 23.109836101531982s

2025-10-19 01:16:14,843 | INFO | Training epoch 1448, Batch 1000/1000: LR=1.85e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:16:14,909 | INFO | Epoch 1448 Train Time 23.178900957107544s

2025-10-19 01:16:35,416 | INFO | Training epoch 1449, Batch 1000/1000: LR=1.85e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:16:35,487 | INFO | Epoch 1449 Train Time 20.577006816864014s

2025-10-19 01:16:59,130 | INFO | Training epoch 1450, Batch 1000/1000: LR=1.84e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:16:59,199 | INFO | Epoch 1450 Train Time 23.711575031280518s

2025-10-19 01:17:22,039 | INFO | Training epoch 1451, Batch 1000/1000: LR=1.84e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:17:22,125 | INFO | Epoch 1451 Train Time 22.924619436264038s

2025-10-19 01:17:45,250 | INFO | Training epoch 1452, Batch 1000/1000: LR=1.83e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:17:45,322 | INFO | Epoch 1452 Train Time 23.196051359176636s

2025-10-19 01:18:09,050 | INFO | Training epoch 1453, Batch 1000/1000: LR=1.82e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:18:09,141 | INFO | Epoch 1453 Train Time 23.817440271377563s

2025-10-19 01:18:32,525 | INFO | Training epoch 1454, Batch 1000/1000: LR=1.82e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:18:32,616 | INFO | Epoch 1454 Train Time 23.474045276641846s

2025-10-19 01:18:51,925 | INFO | Training epoch 1455, Batch 1000/1000: LR=1.81e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:18:52,002 | INFO | Epoch 1455 Train Time 19.38612723350525s

2025-10-19 01:19:14,837 | INFO | Training epoch 1456, Batch 1000/1000: LR=1.81e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:19:14,934 | INFO | Epoch 1456 Train Time 22.930455923080444s

2025-10-19 01:19:38,224 | INFO | Training epoch 1457, Batch 1000/1000: LR=1.80e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:19:38,306 | INFO | Epoch 1457 Train Time 23.37100124359131s

2025-10-19 01:20:02,522 | INFO | Training epoch 1458, Batch 1000/1000: LR=1.79e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:20:02,622 | INFO | Epoch 1458 Train Time 24.314331769943237s

2025-10-19 01:20:25,645 | INFO | Training epoch 1459, Batch 1000/1000: LR=1.79e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:20:25,715 | INFO | Epoch 1459 Train Time 23.09032964706421s

2025-10-19 01:20:49,037 | INFO | Training epoch 1460, Batch 1000/1000: LR=1.78e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:20:49,120 | INFO | Epoch 1460 Train Time 23.403550386428833s

2025-10-19 01:21:09,105 | INFO | Training epoch 1461, Batch 1000/1000: LR=1.78e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:21:09,170 | INFO | Epoch 1461 Train Time 20.048330783843994s

2025-10-19 01:21:31,725 | INFO | Training epoch 1462, Batch 1000/1000: LR=1.77e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:21:31,790 | INFO | Epoch 1462 Train Time 22.61942219734192s

2025-10-19 01:21:55,531 | INFO | Training epoch 1463, Batch 1000/1000: LR=1.76e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:21:55,597 | INFO | Epoch 1463 Train Time 23.8062481880188s

2025-10-19 01:22:18,947 | INFO | Training epoch 1464, Batch 1000/1000: LR=1.76e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:22:19,037 | INFO | Epoch 1464 Train Time 23.43809962272644s

2025-10-19 01:22:43,261 | INFO | Training epoch 1465, Batch 1000/1000: LR=1.75e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:22:43,351 | INFO | Epoch 1465 Train Time 24.312747955322266s

2025-10-19 01:23:06,629 | INFO | Training epoch 1466, Batch 1000/1000: LR=1.75e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.31e-01
2025-10-19 01:23:06,694 | INFO | Epoch 1466 Train Time 23.340850591659546s

2025-10-19 01:23:30,562 | INFO | Training epoch 1467, Batch 1000/1000: LR=1.74e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:23:30,631 | INFO | Epoch 1467 Train Time 23.93625807762146s

2025-10-19 01:23:52,437 | INFO | Training epoch 1468, Batch 1000/1000: LR=1.74e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:23:52,517 | INFO | Epoch 1468 Train Time 21.884300231933594s

2025-10-19 01:24:15,405 | INFO | Training epoch 1469, Batch 1000/1000: LR=1.73e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:24:15,482 | INFO | Epoch 1469 Train Time 22.963942766189575s

2025-10-19 01:24:39,428 | INFO | Training epoch 1470, Batch 1000/1000: LR=1.72e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:24:39,545 | INFO | Epoch 1470 Train Time 24.062268257141113s

2025-10-19 01:25:02,144 | INFO | Training epoch 1471, Batch 1000/1000: LR=1.72e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 01:25:02,224 | INFO | Epoch 1471 Train Time 22.67875051498413s

2025-10-19 01:25:24,420 | INFO | Training epoch 1472, Batch 1000/1000: LR=1.71e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:25:24,498 | INFO | Epoch 1472 Train Time 22.272419214248657s

2025-10-19 01:25:47,225 | INFO | Training epoch 1473, Batch 1000/1000: LR=1.71e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 01:25:47,309 | INFO | Epoch 1473 Train Time 22.81021237373352s

2025-10-19 01:26:09,523 | INFO | Training epoch 1474, Batch 1000/1000: LR=1.70e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:26:09,593 | INFO | Epoch 1474 Train Time 22.283161401748657s

2025-10-19 01:26:33,852 | INFO | Training epoch 1475, Batch 1000/1000: LR=1.70e-05, Loss=2.69e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:26:33,931 | INFO | Epoch 1475 Train Time 24.336508750915527s

2025-10-19 01:26:57,231 | INFO | Training epoch 1476, Batch 1000/1000: LR=1.69e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:26:57,322 | INFO | Epoch 1476 Train Time 23.389347791671753s

2025-10-19 01:27:20,952 | INFO | Training epoch 1477, Batch 1000/1000: LR=1.68e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:27:21,034 | INFO | Epoch 1477 Train Time 23.710877418518066s

2025-10-19 01:27:43,136 | INFO | Training epoch 1478, Batch 1000/1000: LR=1.68e-05, Loss=2.67e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:27:43,200 | INFO | Epoch 1478 Train Time 22.16508150100708s

2025-10-19 01:28:05,975 | INFO | Training epoch 1479, Batch 1000/1000: LR=1.67e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 01:28:06,047 | INFO | Epoch 1479 Train Time 22.846302270889282s

2025-10-19 01:28:30,027 | INFO | Training epoch 1480, Batch 1000/1000: LR=1.67e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:28:30,097 | INFO | Epoch 1480 Train Time 24.048540830612183s

2025-10-19 01:28:52,920 | INFO | Training epoch 1481, Batch 1000/1000: LR=1.66e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:28:53,000 | INFO | Epoch 1481 Train Time 22.90289855003357s

2025-10-19 01:29:16,356 | INFO | Training epoch 1482, Batch 1000/1000: LR=1.66e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:29:16,467 | INFO | Epoch 1482 Train Time 23.464500188827515s

2025-10-19 01:29:39,749 | INFO | Training epoch 1483, Batch 1000/1000: LR=1.65e-05, Loss=2.66e-02 BER=9.97e-03 FER=1.30e-01
2025-10-19 01:29:39,842 | INFO | Epoch 1483 Train Time 23.37420415878296s

2025-10-19 01:29:39,843 | INFO | [P1] saving best_model with loss 0.026574 at epoch 1483
2025-10-19 01:30:03,362 | INFO | Training epoch 1484, Batch 1000/1000: LR=1.64e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:30:03,432 | INFO | Epoch 1484 Train Time 23.566426992416382s

2025-10-19 01:30:26,552 | INFO | Training epoch 1485, Batch 1000/1000: LR=1.64e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:30:26,622 | INFO | Epoch 1485 Train Time 23.189300298690796s

2025-10-19 01:30:48,833 | INFO | Training epoch 1486, Batch 1000/1000: LR=1.63e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:30:48,916 | INFO | Epoch 1486 Train Time 22.292386054992676s

2025-10-19 01:31:10,824 | INFO | Training epoch 1487, Batch 1000/1000: LR=1.63e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-19 01:31:10,910 | INFO | Epoch 1487 Train Time 21.99299383163452s

2025-10-19 01:31:33,906 | INFO | Training epoch 1488, Batch 1000/1000: LR=1.62e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:31:33,980 | INFO | Epoch 1488 Train Time 23.0673611164093s

2025-10-19 01:31:57,210 | INFO | Training epoch 1489, Batch 1000/1000: LR=1.62e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:31:57,301 | INFO | Epoch 1489 Train Time 23.320112943649292s

2025-10-19 01:32:20,849 | INFO | Training epoch 1490, Batch 1000/1000: LR=1.61e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:32:20,928 | INFO | Epoch 1490 Train Time 23.625535488128662s

2025-10-19 01:32:44,890 | INFO | Training epoch 1491, Batch 1000/1000: LR=1.61e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:32:44,965 | INFO | Epoch 1491 Train Time 24.036011934280396s

2025-10-19 01:33:08,340 | INFO | Training epoch 1492, Batch 1000/1000: LR=1.60e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:33:08,425 | INFO | Epoch 1492 Train Time 23.458435535430908s

2025-10-19 01:33:31,736 | INFO | Training epoch 1493, Batch 1000/1000: LR=1.59e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:33:31,820 | INFO | Epoch 1493 Train Time 23.39258360862732s

2025-10-19 01:33:55,632 | INFO | Training epoch 1494, Batch 1000/1000: LR=1.59e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:33:55,703 | INFO | Epoch 1494 Train Time 23.88251495361328s

2025-10-19 01:34:18,131 | INFO | Training epoch 1495, Batch 1000/1000: LR=1.58e-05, Loss=2.67e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:34:18,220 | INFO | Epoch 1495 Train Time 22.515213012695312s

2025-10-19 01:34:42,044 | INFO | Training epoch 1496, Batch 1000/1000: LR=1.58e-05, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:34:42,128 | INFO | Epoch 1496 Train Time 23.907466888427734s

2025-10-19 01:35:05,531 | INFO | Training epoch 1497, Batch 1000/1000: LR=1.57e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:35:05,625 | INFO | Epoch 1497 Train Time 23.49464988708496s

2025-10-19 01:35:28,440 | INFO | Training epoch 1498, Batch 1000/1000: LR=1.57e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:35:28,532 | INFO | Epoch 1498 Train Time 22.90522074699402s

2025-10-19 01:35:51,659 | INFO | Training epoch 1499, Batch 1000/1000: LR=1.56e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 01:35:51,730 | INFO | Epoch 1499 Train Time 23.196082592010498s

2025-10-19 01:36:14,054 | INFO | Training epoch 1500, Batch 1000/1000: LR=1.56e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 01:36:14,135 | INFO | Epoch 1500 Train Time 22.402865409851074s

2025-10-19 01:36:36,552 | INFO | Training epoch 1501, Batch 1000/1000: LR=1.55e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:36:36,639 | INFO | Epoch 1501 Train Time 22.502443075180054s

2025-10-19 01:36:58,911 | INFO | Training epoch 1502, Batch 1000/1000: LR=1.54e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:36:58,990 | INFO | Epoch 1502 Train Time 22.35049033164978s

2025-10-19 01:37:22,341 | INFO | Training epoch 1503, Batch 1000/1000: LR=1.54e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:37:22,428 | INFO | Epoch 1503 Train Time 23.435941219329834s

2025-10-19 01:37:45,965 | INFO | Training epoch 1504, Batch 1000/1000: LR=1.53e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 01:37:46,055 | INFO | Epoch 1504 Train Time 23.626859664916992s

2025-10-19 01:38:08,713 | INFO | Training epoch 1505, Batch 1000/1000: LR=1.53e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:38:08,784 | INFO | Epoch 1505 Train Time 22.72614860534668s

2025-10-19 01:38:30,649 | INFO | Training epoch 1506, Batch 1000/1000: LR=1.52e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:38:30,730 | INFO | Epoch 1506 Train Time 21.944560527801514s

2025-10-19 01:38:54,062 | INFO | Training epoch 1507, Batch 1000/1000: LR=1.52e-05, Loss=2.67e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:38:54,140 | INFO | Epoch 1507 Train Time 23.409015655517578s

2025-10-19 01:39:14,513 | INFO | Training epoch 1508, Batch 1000/1000: LR=1.51e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:39:14,588 | INFO | Epoch 1508 Train Time 20.44655680656433s

2025-10-19 01:39:37,429 | INFO | Training epoch 1509, Batch 1000/1000: LR=1.51e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:39:37,496 | INFO | Epoch 1509 Train Time 22.907025814056396s

2025-10-19 01:40:00,780 | INFO | Training epoch 1510, Batch 1000/1000: LR=1.50e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:40:00,842 | INFO | Epoch 1510 Train Time 23.34406876564026s

2025-10-19 01:40:23,948 | INFO | Training epoch 1511, Batch 1000/1000: LR=1.50e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:40:24,017 | INFO | Epoch 1511 Train Time 23.172691345214844s

2025-10-19 01:40:48,052 | INFO | Training epoch 1512, Batch 1000/1000: LR=1.49e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:40:48,118 | INFO | Epoch 1512 Train Time 24.100895643234253s

2025-10-19 01:41:12,317 | INFO | Training epoch 1513, Batch 1000/1000: LR=1.48e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:41:12,385 | INFO | Epoch 1513 Train Time 24.265215396881104s

2025-10-19 01:41:34,051 | INFO | Training epoch 1514, Batch 1000/1000: LR=1.48e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:41:34,126 | INFO | Epoch 1514 Train Time 21.74105405807495s

2025-10-19 01:41:56,921 | INFO | Training epoch 1515, Batch 1000/1000: LR=1.47e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:41:56,988 | INFO | Epoch 1515 Train Time 22.860275506973267s

2025-10-19 01:42:19,448 | INFO | Training epoch 1516, Batch 1000/1000: LR=1.47e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:42:19,511 | INFO | Epoch 1516 Train Time 22.522956132888794s

2025-10-19 01:42:43,139 | INFO | Training epoch 1517, Batch 1000/1000: LR=1.46e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:42:43,211 | INFO | Epoch 1517 Train Time 23.699150800704956s

2025-10-19 01:43:06,207 | INFO | Training epoch 1518, Batch 1000/1000: LR=1.46e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:43:06,287 | INFO | Epoch 1518 Train Time 23.074047565460205s

2025-10-19 01:43:30,353 | INFO | Training epoch 1519, Batch 1000/1000: LR=1.45e-05, Loss=2.68e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 01:43:30,432 | INFO | Epoch 1519 Train Time 24.143519401550293s

2025-10-19 01:43:53,847 | INFO | Training epoch 1520, Batch 1000/1000: LR=1.45e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 01:43:53,912 | INFO | Epoch 1520 Train Time 23.47953963279724s

2025-10-19 01:44:15,734 | INFO | Training epoch 1521, Batch 1000/1000: LR=1.44e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:44:15,821 | INFO | Epoch 1521 Train Time 21.907541036605835s

2025-10-19 01:44:36,740 | INFO | Training epoch 1522, Batch 1000/1000: LR=1.44e-05, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 01:44:36,815 | INFO | Epoch 1522 Train Time 20.99368405342102s

2025-10-19 01:45:00,255 | INFO | Training epoch 1523, Batch 1000/1000: LR=1.43e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:45:00,326 | INFO | Epoch 1523 Train Time 23.510363578796387s

2025-10-19 01:45:23,036 | INFO | Training epoch 1524, Batch 1000/1000: LR=1.43e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:45:23,112 | INFO | Epoch 1524 Train Time 22.78492569923401s

2025-10-19 01:45:45,738 | INFO | Training epoch 1525, Batch 1000/1000: LR=1.42e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 01:45:45,807 | INFO | Epoch 1525 Train Time 22.694128274917603s

2025-10-19 01:46:09,464 | INFO | Training epoch 1526, Batch 1000/1000: LR=1.42e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:46:09,539 | INFO | Epoch 1526 Train Time 23.730709552764893s

2025-10-19 01:46:32,237 | INFO | Training epoch 1527, Batch 1000/1000: LR=1.41e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:46:32,304 | INFO | Epoch 1527 Train Time 22.76341414451599s

2025-10-19 01:46:55,843 | INFO | Training epoch 1528, Batch 1000/1000: LR=1.40e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:46:55,932 | INFO | Epoch 1528 Train Time 23.627354383468628s

2025-10-19 01:47:17,761 | INFO | Training epoch 1529, Batch 1000/1000: LR=1.40e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:47:17,836 | INFO | Epoch 1529 Train Time 21.903454065322876s

2025-10-19 01:47:41,256 | INFO | Training epoch 1530, Batch 1000/1000: LR=1.39e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:47:41,321 | INFO | Epoch 1530 Train Time 23.483107805252075s

2025-10-19 01:48:04,460 | INFO | Training epoch 1531, Batch 1000/1000: LR=1.39e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:48:04,528 | INFO | Epoch 1531 Train Time 23.206965923309326s

2025-10-19 01:48:28,715 | INFO | Training epoch 1532, Batch 1000/1000: LR=1.38e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 01:48:28,809 | INFO | Epoch 1532 Train Time 24.279768466949463s

2025-10-19 01:48:52,229 | INFO | Training epoch 1533, Batch 1000/1000: LR=1.38e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:48:52,295 | INFO | Epoch 1533 Train Time 23.48466968536377s

2025-10-19 01:49:15,215 | INFO | Training epoch 1534, Batch 1000/1000: LR=1.37e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:49:15,288 | INFO | Epoch 1534 Train Time 22.99272847175598s

2025-10-19 01:49:37,548 | INFO | Training epoch 1535, Batch 1000/1000: LR=1.37e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:49:37,634 | INFO | Epoch 1535 Train Time 22.344607830047607s

2025-10-19 01:50:00,549 | INFO | Training epoch 1536, Batch 1000/1000: LR=1.36e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 01:50:00,614 | INFO | Epoch 1536 Train Time 22.979057788848877s

2025-10-19 01:50:23,333 | INFO | Training epoch 1537, Batch 1000/1000: LR=1.36e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:50:23,397 | INFO | Epoch 1537 Train Time 22.781826734542847s

2025-10-19 01:50:46,445 | INFO | Training epoch 1538, Batch 1000/1000: LR=1.35e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:50:46,529 | INFO | Epoch 1538 Train Time 23.13045620918274s

2025-10-19 01:51:08,745 | INFO | Training epoch 1539, Batch 1000/1000: LR=1.35e-05, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:51:08,837 | INFO | Epoch 1539 Train Time 22.30765151977539s

2025-10-19 01:51:32,238 | INFO | Training epoch 1540, Batch 1000/1000: LR=1.34e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:51:32,309 | INFO | Epoch 1540 Train Time 23.470673084259033s

2025-10-19 01:51:55,643 | INFO | Training epoch 1541, Batch 1000/1000: LR=1.34e-05, Loss=2.67e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 01:51:55,743 | INFO | Epoch 1541 Train Time 23.433087587356567s

2025-10-19 01:52:19,247 | INFO | Training epoch 1542, Batch 1000/1000: LR=1.33e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:52:19,322 | INFO | Epoch 1542 Train Time 23.577445030212402s

2025-10-19 01:52:41,527 | INFO | Training epoch 1543, Batch 1000/1000: LR=1.33e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:52:41,593 | INFO | Epoch 1543 Train Time 22.270426273345947s

2025-10-19 01:53:05,708 | INFO | Training epoch 1544, Batch 1000/1000: LR=1.32e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:53:05,778 | INFO | Epoch 1544 Train Time 24.182643175125122s

2025-10-19 01:53:29,525 | INFO | Training epoch 1545, Batch 1000/1000: LR=1.32e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:53:29,614 | INFO | Epoch 1545 Train Time 23.8348605632782s

2025-10-19 01:53:52,536 | INFO | Training epoch 1546, Batch 1000/1000: LR=1.31e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:53:52,627 | INFO | Epoch 1546 Train Time 23.01194715499878s

2025-10-19 01:54:16,261 | INFO | Training epoch 1547, Batch 1000/1000: LR=1.31e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:54:16,325 | INFO | Epoch 1547 Train Time 23.696613073349s

2025-10-19 01:54:37,634 | INFO | Training epoch 1548, Batch 1000/1000: LR=1.30e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 01:54:37,711 | INFO | Epoch 1548 Train Time 21.385129690170288s

2025-10-19 01:55:00,355 | INFO | Training epoch 1549, Batch 1000/1000: LR=1.30e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.31e-01
2025-10-19 01:55:00,444 | INFO | Epoch 1549 Train Time 22.732539176940918s

2025-10-19 01:55:23,449 | INFO | Training epoch 1550, Batch 1000/1000: LR=1.29e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:55:23,518 | INFO | Epoch 1550 Train Time 23.07251501083374s

2025-10-19 01:55:47,115 | INFO | Training epoch 1551, Batch 1000/1000: LR=1.29e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:55:47,185 | INFO | Epoch 1551 Train Time 23.666533946990967s

2025-10-19 01:56:10,453 | INFO | Training epoch 1552, Batch 1000/1000: LR=1.28e-05, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 01:56:10,523 | INFO | Epoch 1552 Train Time 23.336586713790894s

2025-10-19 01:56:34,743 | INFO | Training epoch 1553, Batch 1000/1000: LR=1.28e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:56:34,830 | INFO | Epoch 1553 Train Time 24.305663585662842s

2025-10-19 01:56:56,017 | INFO | Training epoch 1554, Batch 1000/1000: LR=1.27e-05, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 01:56:56,088 | INFO | Epoch 1554 Train Time 21.256288528442383s

2025-10-19 01:57:19,510 | INFO | Training epoch 1555, Batch 1000/1000: LR=1.27e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 01:57:19,583 | INFO | Epoch 1555 Train Time 23.4948513507843s

2025-10-19 01:57:40,124 | INFO | Training epoch 1556, Batch 1000/1000: LR=1.26e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:57:40,208 | INFO | Epoch 1556 Train Time 20.623353004455566s

2025-10-19 01:58:03,266 | INFO | Training epoch 1557, Batch 1000/1000: LR=1.26e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 01:58:03,349 | INFO | Epoch 1557 Train Time 23.140881538391113s

2025-10-19 01:58:25,937 | INFO | Training epoch 1558, Batch 1000/1000: LR=1.25e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:58:26,021 | INFO | Epoch 1558 Train Time 22.670287132263184s

2025-10-19 01:58:49,557 | INFO | Training epoch 1559, Batch 1000/1000: LR=1.25e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 01:58:49,631 | INFO | Epoch 1559 Train Time 23.609215021133423s

2025-10-19 01:59:13,035 | INFO | Training epoch 1560, Batch 1000/1000: LR=1.24e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 01:59:13,105 | INFO | Epoch 1560 Train Time 23.47322964668274s

2025-10-19 01:59:35,916 | INFO | Training epoch 1561, Batch 1000/1000: LR=1.24e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:59:36,003 | INFO | Epoch 1561 Train Time 22.89643096923828s

2025-10-19 01:59:59,141 | INFO | Training epoch 1562, Batch 1000/1000: LR=1.23e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 01:59:59,213 | INFO | Epoch 1562 Train Time 23.208709716796875s

2025-10-19 02:00:20,253 | INFO | Training epoch 1563, Batch 1000/1000: LR=1.23e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:00:20,336 | INFO | Epoch 1563 Train Time 21.1228768825531s

2025-10-19 02:00:43,921 | INFO | Training epoch 1564, Batch 1000/1000: LR=1.22e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:00:44,004 | INFO | Epoch 1564 Train Time 23.666053533554077s

2025-10-19 02:01:07,819 | INFO | Training epoch 1565, Batch 1000/1000: LR=1.22e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:01:07,899 | INFO | Epoch 1565 Train Time 23.893771409988403s

2025-10-19 02:01:30,821 | INFO | Training epoch 1566, Batch 1000/1000: LR=1.21e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:01:30,885 | INFO | Epoch 1566 Train Time 22.984627962112427s

2025-10-19 02:01:54,339 | INFO | Training epoch 1567, Batch 1000/1000: LR=1.21e-05, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 02:01:54,404 | INFO | Epoch 1567 Train Time 23.517510175704956s

2025-10-19 02:02:18,224 | INFO | Training epoch 1568, Batch 1000/1000: LR=1.20e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:02:18,300 | INFO | Epoch 1568 Train Time 23.894827842712402s

2025-10-19 02:02:41,813 | INFO | Training epoch 1569, Batch 1000/1000: LR=1.20e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:02:41,885 | INFO | Epoch 1569 Train Time 23.583494663238525s

2025-10-19 02:03:04,955 | INFO | Training epoch 1570, Batch 1000/1000: LR=1.19e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:03:05,026 | INFO | Epoch 1570 Train Time 23.140413284301758s

2025-10-19 02:03:27,706 | INFO | Training epoch 1571, Batch 1000/1000: LR=1.19e-05, Loss=2.67e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 02:03:27,770 | INFO | Epoch 1571 Train Time 22.742789030075073s

2025-10-19 02:03:50,030 | INFO | Training epoch 1572, Batch 1000/1000: LR=1.18e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:03:50,104 | INFO | Epoch 1572 Train Time 22.332958459854126s

2025-10-19 02:04:13,122 | INFO | Training epoch 1573, Batch 1000/1000: LR=1.18e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:04:13,199 | INFO | Epoch 1573 Train Time 23.094600677490234s

2025-10-19 02:04:35,132 | INFO | Training epoch 1574, Batch 1000/1000: LR=1.17e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:04:35,192 | INFO | Epoch 1574 Train Time 21.99156141281128s

2025-10-19 02:04:59,061 | INFO | Training epoch 1575, Batch 1000/1000: LR=1.17e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:04:59,125 | INFO | Epoch 1575 Train Time 23.930957794189453s

2025-10-19 02:05:21,216 | INFO | Training epoch 1576, Batch 1000/1000: LR=1.16e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:05:21,314 | INFO | Epoch 1576 Train Time 22.18797993659973s

2025-10-19 02:05:44,012 | INFO | Training epoch 1577, Batch 1000/1000: LR=1.16e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:05:44,090 | INFO | Epoch 1577 Train Time 22.774715423583984s

2025-10-19 02:06:07,037 | INFO | Training epoch 1578, Batch 1000/1000: LR=1.15e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:06:07,105 | INFO | Epoch 1578 Train Time 23.012565851211548s

2025-10-19 02:06:31,054 | INFO | Training epoch 1579, Batch 1000/1000: LR=1.15e-05, Loss=2.66e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 02:06:31,119 | INFO | Epoch 1579 Train Time 24.013790369033813s

2025-10-19 02:06:54,629 | INFO | Training epoch 1580, Batch 1000/1000: LR=1.14e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:06:54,710 | INFO | Epoch 1580 Train Time 23.590303659439087s

2025-10-19 02:07:16,653 | INFO | Training epoch 1581, Batch 1000/1000: LR=1.14e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:07:16,721 | INFO | Epoch 1581 Train Time 22.009833097457886s

2025-10-19 02:07:39,605 | INFO | Training epoch 1582, Batch 1000/1000: LR=1.13e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:07:39,688 | INFO | Epoch 1582 Train Time 22.96579360961914s

2025-10-19 02:08:02,345 | INFO | Training epoch 1583, Batch 1000/1000: LR=1.13e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:08:02,429 | INFO | Epoch 1583 Train Time 22.740971565246582s

2025-10-19 02:08:25,211 | INFO | Training epoch 1584, Batch 1000/1000: LR=1.12e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:08:25,298 | INFO | Epoch 1584 Train Time 22.866872310638428s

2025-10-19 02:08:48,897 | INFO | Training epoch 1585, Batch 1000/1000: LR=1.12e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:08:48,979 | INFO | Epoch 1585 Train Time 23.67976689338684s

2025-10-19 02:09:11,018 | INFO | Training epoch 1586, Batch 1000/1000: LR=1.12e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:09:11,093 | INFO | Epoch 1586 Train Time 22.112765550613403s

2025-10-19 02:09:35,041 | INFO | Training epoch 1587, Batch 1000/1000: LR=1.11e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:09:35,116 | INFO | Epoch 1587 Train Time 24.021729707717896s

2025-10-19 02:09:59,009 | INFO | Training epoch 1588, Batch 1000/1000: LR=1.11e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-19 02:09:59,074 | INFO | Epoch 1588 Train Time 23.956631660461426s

2025-10-19 02:10:22,325 | INFO | Training epoch 1589, Batch 1000/1000: LR=1.10e-05, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 02:10:22,408 | INFO | Epoch 1589 Train Time 23.33275842666626s

2025-10-19 02:10:44,624 | INFO | Training epoch 1590, Batch 1000/1000: LR=1.10e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:10:44,707 | INFO | Epoch 1590 Train Time 22.296587228775024s

2025-10-19 02:11:07,935 | INFO | Training epoch 1591, Batch 1000/1000: LR=1.09e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:11:08,001 | INFO | Epoch 1591 Train Time 23.292524576187134s

2025-10-19 02:11:31,137 | INFO | Training epoch 1592, Batch 1000/1000: LR=1.09e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:11:31,209 | INFO | Epoch 1592 Train Time 23.20790123939514s

2025-10-19 02:11:54,256 | INFO | Training epoch 1593, Batch 1000/1000: LR=1.08e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:11:54,321 | INFO | Epoch 1593 Train Time 23.1107280254364s

2025-10-19 02:12:16,457 | INFO | Training epoch 1594, Batch 1000/1000: LR=1.08e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:12:16,528 | INFO | Epoch 1594 Train Time 22.205849409103394s

2025-10-19 02:12:40,328 | INFO | Training epoch 1595, Batch 1000/1000: LR=1.07e-05, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:12:40,403 | INFO | Epoch 1595 Train Time 23.874271392822266s

2025-10-19 02:13:04,356 | INFO | Training epoch 1596, Batch 1000/1000: LR=1.07e-05, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:13:04,446 | INFO | Epoch 1596 Train Time 24.042091131210327s

2025-10-19 02:13:26,911 | INFO | Training epoch 1597, Batch 1000/1000: LR=1.06e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:13:26,985 | INFO | Epoch 1597 Train Time 22.5371356010437s

2025-10-19 02:13:48,442 | INFO | Training epoch 1598, Batch 1000/1000: LR=1.06e-05, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 02:13:48,519 | INFO | Epoch 1598 Train Time 21.532179594039917s

2025-10-19 02:14:12,448 | INFO | Training epoch 1599, Batch 1000/1000: LR=1.05e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-19 02:14:12,530 | INFO | Epoch 1599 Train Time 24.01012921333313s

2025-10-19 02:14:36,032 | INFO | Training epoch 1600, Batch 1000/1000: LR=1.05e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 02:14:36,114 | INFO | Epoch 1600 Train Time 23.58242893218994s

2025-10-19 02:14:59,734 | INFO | Training epoch 1601, Batch 1000/1000: LR=1.05e-05, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:14:59,800 | INFO | Epoch 1601 Train Time 23.68524980545044s

2025-10-19 02:15:23,028 | INFO | Training epoch 1602, Batch 1000/1000: LR=1.04e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:15:23,089 | INFO | Epoch 1602 Train Time 23.286746501922607s

2025-10-19 02:15:46,350 | INFO | Training epoch 1603, Batch 1000/1000: LR=1.04e-05, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:15:46,423 | INFO | Epoch 1603 Train Time 23.333698511123657s

2025-10-19 02:16:06,655 | INFO | Training epoch 1604, Batch 1000/1000: LR=1.03e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:16:06,720 | INFO | Epoch 1604 Train Time 20.29581308364868s

2025-10-19 02:16:29,323 | INFO | Training epoch 1605, Batch 1000/1000: LR=1.03e-05, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 02:16:29,405 | INFO | Epoch 1605 Train Time 22.68213415145874s

2025-10-19 02:16:52,326 | INFO | Training epoch 1606, Batch 1000/1000: LR=1.02e-05, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:16:52,419 | INFO | Epoch 1606 Train Time 23.012526035308838s

2025-10-19 02:17:15,722 | INFO | Training epoch 1607, Batch 1000/1000: LR=1.02e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:17:15,787 | INFO | Epoch 1607 Train Time 23.36733078956604s

2025-10-19 02:17:39,522 | INFO | Training epoch 1608, Batch 1000/1000: LR=1.01e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:17:39,601 | INFO | Epoch 1608 Train Time 23.812752962112427s

2025-10-19 02:18:01,845 | INFO | Training epoch 1609, Batch 1000/1000: LR=1.01e-05, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:18:01,926 | INFO | Epoch 1609 Train Time 22.32399868965149s

2025-10-19 02:18:23,239 | INFO | Training epoch 1610, Batch 1000/1000: LR=1.00e-05, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:18:23,318 | INFO | Epoch 1610 Train Time 21.390705585479736s

2025-10-19 02:18:47,304 | INFO | Training epoch 1611, Batch 1000/1000: LR=1.00e-05, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:18:47,391 | INFO | Epoch 1611 Train Time 24.071218729019165s

2025-10-19 02:19:10,260 | INFO | Training epoch 1612, Batch 1000/1000: LR=9.96e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:19:10,351 | INFO | Epoch 1612 Train Time 22.959648609161377s

2025-10-19 02:19:31,804 | INFO | Training epoch 1613, Batch 1000/1000: LR=9.91e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:19:31,902 | INFO | Epoch 1613 Train Time 21.550169944763184s

2025-10-19 02:19:55,153 | INFO | Training epoch 1614, Batch 1000/1000: LR=9.87e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:19:55,230 | INFO | Epoch 1614 Train Time 23.325976371765137s

2025-10-19 02:20:19,322 | INFO | Training epoch 1615, Batch 1000/1000: LR=9.82e-06, Loss=2.66e-02 BER=9.97e-03 FER=1.29e-01
2025-10-19 02:20:19,411 | INFO | Epoch 1615 Train Time 24.179665565490723s

2025-10-19 02:20:43,628 | INFO | Training epoch 1616, Batch 1000/1000: LR=9.78e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:20:43,715 | INFO | Epoch 1616 Train Time 24.303678035736084s

2025-10-19 02:21:06,840 | INFO | Training epoch 1617, Batch 1000/1000: LR=9.74e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:21:06,935 | INFO | Epoch 1617 Train Time 23.219115257263184s

2025-10-19 02:21:30,754 | INFO | Training epoch 1618, Batch 1000/1000: LR=9.69e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:21:30,831 | INFO | Epoch 1618 Train Time 23.894670486450195s

2025-10-19 02:21:53,445 | INFO | Training epoch 1619, Batch 1000/1000: LR=9.65e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:21:53,528 | INFO | Epoch 1619 Train Time 22.695587158203125s

2025-10-19 02:22:16,747 | INFO | Training epoch 1620, Batch 1000/1000: LR=9.60e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:22:16,827 | INFO | Epoch 1620 Train Time 23.297898292541504s

2025-10-19 02:22:40,322 | INFO | Training epoch 1621, Batch 1000/1000: LR=9.56e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:22:40,387 | INFO | Epoch 1621 Train Time 23.558359146118164s

2025-10-19 02:23:03,946 | INFO | Training epoch 1622, Batch 1000/1000: LR=9.52e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:23:04,010 | INFO | Epoch 1622 Train Time 23.621546983718872s

2025-10-19 02:23:27,209 | INFO | Training epoch 1623, Batch 1000/1000: LR=9.47e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:23:27,273 | INFO | Epoch 1623 Train Time 23.262327671051025s

2025-10-19 02:23:51,816 | INFO | Training epoch 1624, Batch 1000/1000: LR=9.43e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:23:51,904 | INFO | Epoch 1624 Train Time 24.62979769706726s

2025-10-19 02:24:13,415 | INFO | Training epoch 1625, Batch 1000/1000: LR=9.39e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:24:13,501 | INFO | Epoch 1625 Train Time 21.596111536026s

2025-10-19 02:24:36,461 | INFO | Training epoch 1626, Batch 1000/1000: LR=9.34e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:24:36,531 | INFO | Epoch 1626 Train Time 23.029305934906006s

2025-10-19 02:24:58,644 | INFO | Training epoch 1627, Batch 1000/1000: LR=9.30e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:24:58,740 | INFO | Epoch 1627 Train Time 22.206884622573853s

2025-10-19 02:25:21,928 | INFO | Training epoch 1628, Batch 1000/1000: LR=9.26e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 02:25:22,009 | INFO | Epoch 1628 Train Time 23.267668962478638s

2025-10-19 02:25:45,111 | INFO | Training epoch 1629, Batch 1000/1000: LR=9.21e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:25:45,184 | INFO | Epoch 1629 Train Time 23.17372465133667s

2025-10-19 02:26:08,544 | INFO | Training epoch 1630, Batch 1000/1000: LR=9.17e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:26:08,632 | INFO | Epoch 1630 Train Time 23.447269201278687s

2025-10-19 02:26:31,744 | INFO | Training epoch 1631, Batch 1000/1000: LR=9.13e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:26:31,824 | INFO | Epoch 1631 Train Time 23.190982818603516s

2025-10-19 02:26:54,631 | INFO | Training epoch 1632, Batch 1000/1000: LR=9.08e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:26:54,717 | INFO | Epoch 1632 Train Time 22.892327070236206s

2025-10-19 02:27:18,798 | INFO | Training epoch 1633, Batch 1000/1000: LR=9.04e-06, Loss=2.70e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:27:18,868 | INFO | Epoch 1633 Train Time 24.149201154708862s

2025-10-19 02:27:43,021 | INFO | Training epoch 1634, Batch 1000/1000: LR=9.00e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:27:43,095 | INFO | Epoch 1634 Train Time 24.226128578186035s

2025-10-19 02:28:05,537 | INFO | Training epoch 1635, Batch 1000/1000: LR=8.96e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:28:05,621 | INFO | Epoch 1635 Train Time 22.524881839752197s

2025-10-19 02:28:29,026 | INFO | Training epoch 1636, Batch 1000/1000: LR=8.92e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 02:28:29,100 | INFO | Epoch 1636 Train Time 23.4780113697052s

2025-10-19 02:28:52,458 | INFO | Training epoch 1637, Batch 1000/1000: LR=8.87e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:28:52,540 | INFO | Epoch 1637 Train Time 23.43944239616394s

2025-10-19 02:29:15,807 | INFO | Training epoch 1638, Batch 1000/1000: LR=8.83e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:29:15,884 | INFO | Epoch 1638 Train Time 23.342219352722168s

2025-10-19 02:29:39,426 | INFO | Training epoch 1639, Batch 1000/1000: LR=8.79e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:29:39,509 | INFO | Epoch 1639 Train Time 23.624301433563232s

2025-10-19 02:30:02,649 | INFO | Training epoch 1640, Batch 1000/1000: LR=8.75e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:30:02,733 | INFO | Epoch 1640 Train Time 23.222712755203247s

2025-10-19 02:30:25,235 | INFO | Training epoch 1641, Batch 1000/1000: LR=8.71e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:30:25,320 | INFO | Epoch 1641 Train Time 22.584611415863037s

2025-10-19 02:30:47,830 | INFO | Training epoch 1642, Batch 1000/1000: LR=8.66e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 02:30:47,911 | INFO | Epoch 1642 Train Time 22.588990688323975s

2025-10-19 02:31:10,677 | INFO | Training epoch 1643, Batch 1000/1000: LR=8.62e-06, Loss=2.74e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 02:31:10,742 | INFO | Epoch 1643 Train Time 22.828455448150635s

2025-10-19 02:31:33,900 | INFO | Training epoch 1644, Batch 1000/1000: LR=8.58e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:31:33,976 | INFO | Epoch 1644 Train Time 23.23202133178711s

2025-10-19 02:31:57,538 | INFO | Training epoch 1645, Batch 1000/1000: LR=8.54e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:31:57,606 | INFO | Epoch 1645 Train Time 23.629209756851196s

2025-10-19 02:32:21,132 | INFO | Training epoch 1646, Batch 1000/1000: LR=8.50e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:32:21,218 | INFO | Epoch 1646 Train Time 23.60972833633423s

2025-10-19 02:32:41,899 | INFO | Training epoch 1647, Batch 1000/1000: LR=8.46e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:32:41,991 | INFO | Epoch 1647 Train Time 20.77159595489502s

2025-10-19 02:33:05,558 | INFO | Training epoch 1648, Batch 1000/1000: LR=8.42e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:33:05,646 | INFO | Epoch 1648 Train Time 23.654340267181396s

2025-10-19 02:33:27,531 | INFO | Training epoch 1649, Batch 1000/1000: LR=8.38e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:33:27,595 | INFO | Epoch 1649 Train Time 21.947740077972412s

2025-10-19 02:33:50,751 | INFO | Training epoch 1650, Batch 1000/1000: LR=8.33e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:33:50,820 | INFO | Epoch 1650 Train Time 23.222318172454834s

2025-10-19 02:34:13,942 | INFO | Training epoch 1651, Batch 1000/1000: LR=8.29e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:34:14,025 | INFO | Epoch 1651 Train Time 23.20374369621277s

2025-10-19 02:34:36,447 | INFO | Training epoch 1652, Batch 1000/1000: LR=8.25e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:34:36,532 | INFO | Epoch 1652 Train Time 22.505491971969604s

2025-10-19 02:34:59,824 | INFO | Training epoch 1653, Batch 1000/1000: LR=8.21e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:34:59,914 | INFO | Epoch 1653 Train Time 23.381274700164795s

2025-10-19 02:35:23,343 | INFO | Training epoch 1654, Batch 1000/1000: LR=8.17e-06, Loss=2.66e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 02:35:23,409 | INFO | Epoch 1654 Train Time 23.49372696876526s

2025-10-19 02:35:45,412 | INFO | Training epoch 1655, Batch 1000/1000: LR=8.13e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:35:45,475 | INFO | Epoch 1655 Train Time 22.064283847808838s

2025-10-19 02:36:08,354 | INFO | Training epoch 1656, Batch 1000/1000: LR=8.09e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:36:08,437 | INFO | Epoch 1656 Train Time 22.960703372955322s

2025-10-19 02:36:31,549 | INFO | Training epoch 1657, Batch 1000/1000: LR=8.05e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:36:31,646 | INFO | Epoch 1657 Train Time 23.207733869552612s

2025-10-19 02:36:55,211 | INFO | Training epoch 1658, Batch 1000/1000: LR=8.01e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:36:55,283 | INFO | Epoch 1658 Train Time 23.635212421417236s

2025-10-19 02:37:18,056 | INFO | Training epoch 1659, Batch 1000/1000: LR=7.97e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:37:18,139 | INFO | Epoch 1659 Train Time 22.85596799850464s

2025-10-19 02:37:41,198 | INFO | Training epoch 1660, Batch 1000/1000: LR=7.93e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 02:37:41,261 | INFO | Epoch 1660 Train Time 23.12133479118347s

2025-10-19 02:38:03,521 | INFO | Training epoch 1661, Batch 1000/1000: LR=7.89e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:38:03,586 | INFO | Epoch 1661 Train Time 22.323402166366577s

2025-10-19 02:38:26,615 | INFO | Training epoch 1662, Batch 1000/1000: LR=7.85e-06, Loss=2.66e-02 BER=9.99e-03 FER=1.30e-01
2025-10-19 02:38:26,692 | INFO | Epoch 1662 Train Time 23.104636669158936s

2025-10-19 02:38:50,404 | INFO | Training epoch 1663, Batch 1000/1000: LR=7.81e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:38:50,472 | INFO | Epoch 1663 Train Time 23.77728581428528s

2025-10-19 02:39:13,144 | INFO | Training epoch 1664, Batch 1000/1000: LR=7.78e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:39:13,211 | INFO | Epoch 1664 Train Time 22.73769450187683s

2025-10-19 02:39:34,249 | INFO | Training epoch 1665, Batch 1000/1000: LR=7.74e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 02:39:34,315 | INFO | Epoch 1665 Train Time 21.10292077064514s

2025-10-19 02:39:57,801 | INFO | Training epoch 1666, Batch 1000/1000: LR=7.70e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:39:57,885 | INFO | Epoch 1666 Train Time 23.56878161430359s

2025-10-19 02:40:20,951 | INFO | Training epoch 1667, Batch 1000/1000: LR=7.66e-06, Loss=2.69e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 02:40:21,034 | INFO | Epoch 1667 Train Time 23.14856791496277s

2025-10-19 02:40:44,857 | INFO | Training epoch 1668, Batch 1000/1000: LR=7.62e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:40:44,941 | INFO | Epoch 1668 Train Time 23.905128002166748s

2025-10-19 02:41:04,501 | INFO | Training epoch 1669, Batch 1000/1000: LR=7.58e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:41:04,566 | INFO | Epoch 1669 Train Time 19.62394881248474s

2025-10-19 02:41:27,212 | INFO | Training epoch 1670, Batch 1000/1000: LR=7.54e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:41:27,287 | INFO | Epoch 1670 Train Time 22.72052788734436s

2025-10-19 02:41:50,846 | INFO | Training epoch 1671, Batch 1000/1000: LR=7.50e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:41:50,914 | INFO | Epoch 1671 Train Time 23.626633882522583s

2025-10-19 02:42:11,760 | INFO | Training epoch 1672, Batch 1000/1000: LR=7.46e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:42:11,832 | INFO | Epoch 1672 Train Time 20.916245222091675s

2025-10-19 02:42:34,940 | INFO | Training epoch 1673, Batch 1000/1000: LR=7.43e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:42:35,012 | INFO | Epoch 1673 Train Time 23.178858995437622s

2025-10-19 02:42:57,633 | INFO | Training epoch 1674, Batch 1000/1000: LR=7.39e-06, Loss=2.66e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 02:42:57,705 | INFO | Epoch 1674 Train Time 22.69192600250244s

2025-10-19 02:43:19,616 | INFO | Training epoch 1675, Batch 1000/1000: LR=7.35e-06, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 02:43:19,689 | INFO | Epoch 1675 Train Time 21.982605695724487s

2025-10-19 02:43:42,155 | INFO | Training epoch 1676, Batch 1000/1000: LR=7.31e-06, Loss=2.67e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:43:42,243 | INFO | Epoch 1676 Train Time 22.55327534675598s

2025-10-19 02:44:05,832 | INFO | Training epoch 1677, Batch 1000/1000: LR=7.27e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:44:05,912 | INFO | Epoch 1677 Train Time 23.667118310928345s

2025-10-19 02:44:26,442 | INFO | Training epoch 1678, Batch 1000/1000: LR=7.24e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:44:26,527 | INFO | Epoch 1678 Train Time 20.613942623138428s

2025-10-19 02:44:48,816 | INFO | Training epoch 1679, Batch 1000/1000: LR=7.20e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:44:48,883 | INFO | Epoch 1679 Train Time 22.35391855239868s

2025-10-19 02:45:11,640 | INFO | Training epoch 1680, Batch 1000/1000: LR=7.16e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:45:11,705 | INFO | Epoch 1680 Train Time 22.82175064086914s

2025-10-19 02:45:35,029 | INFO | Training epoch 1681, Batch 1000/1000: LR=7.12e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:45:35,102 | INFO | Epoch 1681 Train Time 23.394182682037354s

2025-10-19 02:45:57,538 | INFO | Training epoch 1682, Batch 1000/1000: LR=7.09e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:45:57,625 | INFO | Epoch 1682 Train Time 22.522258281707764s

2025-10-19 02:46:20,152 | INFO | Training epoch 1683, Batch 1000/1000: LR=7.05e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:46:20,231 | INFO | Epoch 1683 Train Time 22.60489320755005s

2025-10-19 02:46:42,041 | INFO | Training epoch 1684, Batch 1000/1000: LR=7.01e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:46:42,103 | INFO | Epoch 1684 Train Time 21.871506214141846s

2025-10-19 02:47:05,146 | INFO | Training epoch 1685, Batch 1000/1000: LR=6.97e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:47:05,217 | INFO | Epoch 1685 Train Time 23.113574028015137s

2025-10-19 02:47:27,528 | INFO | Training epoch 1686, Batch 1000/1000: LR=6.94e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:47:27,612 | INFO | Epoch 1686 Train Time 22.39306330680847s

2025-10-19 02:47:49,931 | INFO | Training epoch 1687, Batch 1000/1000: LR=6.90e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:47:50,012 | INFO | Epoch 1687 Train Time 22.399588346481323s

2025-10-19 02:48:13,725 | INFO | Training epoch 1688, Batch 1000/1000: LR=6.86e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:48:13,812 | INFO | Epoch 1688 Train Time 23.798807859420776s

2025-10-19 02:48:32,813 | INFO | Training epoch 1689, Batch 1000/1000: LR=6.83e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 02:48:32,878 | INFO | Epoch 1689 Train Time 19.063333749771118s

2025-10-19 02:48:56,157 | INFO | Training epoch 1690, Batch 1000/1000: LR=6.79e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.31e-01
2025-10-19 02:48:56,248 | INFO | Epoch 1690 Train Time 23.369351625442505s

2025-10-19 02:49:19,144 | INFO | Training epoch 1691, Batch 1000/1000: LR=6.75e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:49:19,210 | INFO | Epoch 1691 Train Time 22.960134267807007s

2025-10-19 02:49:42,634 | INFO | Training epoch 1692, Batch 1000/1000: LR=6.72e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:49:42,709 | INFO | Epoch 1692 Train Time 23.497161865234375s

2025-10-19 02:50:05,849 | INFO | Training epoch 1693, Batch 1000/1000: LR=6.68e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 02:50:05,914 | INFO | Epoch 1693 Train Time 23.204313278198242s

2025-10-19 02:50:27,609 | INFO | Training epoch 1694, Batch 1000/1000: LR=6.64e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:50:27,678 | INFO | Epoch 1694 Train Time 21.762766361236572s

2025-10-19 02:50:51,642 | INFO | Training epoch 1695, Batch 1000/1000: LR=6.61e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:50:51,706 | INFO | Epoch 1695 Train Time 24.02727246284485s

2025-10-19 02:51:14,738 | INFO | Training epoch 1696, Batch 1000/1000: LR=6.57e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:51:14,804 | INFO | Epoch 1696 Train Time 23.096648931503296s

2025-10-19 02:51:37,420 | INFO | Training epoch 1697, Batch 1000/1000: LR=6.54e-06, Loss=2.75e-02 BER=1.04e-02 FER=1.32e-01
2025-10-19 02:51:37,505 | INFO | Epoch 1697 Train Time 22.700230598449707s

2025-10-19 02:52:01,209 | INFO | Training epoch 1698, Batch 1000/1000: LR=6.50e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:52:01,283 | INFO | Epoch 1698 Train Time 23.777270793914795s

2025-10-19 02:52:24,625 | INFO | Training epoch 1699, Batch 1000/1000: LR=6.47e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.31e-01
2025-10-19 02:52:24,688 | INFO | Epoch 1699 Train Time 23.403178215026855s

2025-10-19 02:52:48,164 | INFO | Training epoch 1700, Batch 1000/1000: LR=6.43e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:52:48,228 | INFO | Epoch 1700 Train Time 23.538581371307373s

2025-10-19 02:53:09,212 | INFO | Training epoch 1701, Batch 1000/1000: LR=6.40e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:53:09,297 | INFO | Epoch 1701 Train Time 21.068950176239014s

2025-10-19 02:53:31,856 | INFO | Training epoch 1702, Batch 1000/1000: LR=6.36e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:53:31,940 | INFO | Epoch 1702 Train Time 22.64008402824402s

2025-10-19 02:53:55,655 | INFO | Training epoch 1703, Batch 1000/1000: LR=6.32e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:53:55,740 | INFO | Epoch 1703 Train Time 23.799511432647705s

2025-10-19 02:54:16,345 | INFO | Training epoch 1704, Batch 1000/1000: LR=6.29e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:54:16,410 | INFO | Epoch 1704 Train Time 20.668652534484863s

2025-10-19 02:54:36,723 | INFO | Training epoch 1705, Batch 1000/1000: LR=6.25e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:54:36,788 | INFO | Epoch 1705 Train Time 20.37575387954712s

2025-10-19 02:55:00,413 | INFO | Training epoch 1706, Batch 1000/1000: LR=6.22e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:55:00,473 | INFO | Epoch 1706 Train Time 23.682603359222412s

2025-10-19 02:55:23,421 | INFO | Training epoch 1707, Batch 1000/1000: LR=6.19e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:55:23,508 | INFO | Epoch 1707 Train Time 23.034257411956787s

2025-10-19 02:55:45,853 | INFO | Training epoch 1708, Batch 1000/1000: LR=6.15e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:55:45,919 | INFO | Epoch 1708 Train Time 22.409088134765625s

2025-10-19 02:56:08,926 | INFO | Training epoch 1709, Batch 1000/1000: LR=6.12e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:56:09,003 | INFO | Epoch 1709 Train Time 23.083301305770874s

2025-10-19 02:56:32,044 | INFO | Training epoch 1710, Batch 1000/1000: LR=6.08e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:56:32,115 | INFO | Epoch 1710 Train Time 23.11055850982666s

2025-10-19 02:56:56,122 | INFO | Training epoch 1711, Batch 1000/1000: LR=6.05e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 02:56:56,188 | INFO | Epoch 1711 Train Time 24.072495937347412s

2025-10-19 02:57:19,106 | INFO | Training epoch 1712, Batch 1000/1000: LR=6.01e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:57:19,188 | INFO | Epoch 1712 Train Time 22.998180150985718s

2025-10-19 02:57:42,756 | INFO | Training epoch 1713, Batch 1000/1000: LR=5.98e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 02:57:42,839 | INFO | Epoch 1713 Train Time 23.649993658065796s

2025-10-19 02:58:06,209 | INFO | Training epoch 1714, Batch 1000/1000: LR=5.95e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 02:58:06,271 | INFO | Epoch 1714 Train Time 23.430963277816772s

2025-10-19 02:58:29,437 | INFO | Training epoch 1715, Batch 1000/1000: LR=5.91e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 02:58:29,518 | INFO | Epoch 1715 Train Time 23.24643301963806s

2025-10-19 02:58:52,460 | INFO | Training epoch 1716, Batch 1000/1000: LR=5.88e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 02:58:52,540 | INFO | Epoch 1716 Train Time 23.019513368606567s

2025-10-19 02:59:15,731 | INFO | Training epoch 1717, Batch 1000/1000: LR=5.84e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.31e-01
2025-10-19 02:59:15,810 | INFO | Epoch 1717 Train Time 23.268893241882324s

2025-10-19 02:59:39,231 | INFO | Training epoch 1718, Batch 1000/1000: LR=5.81e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 02:59:39,313 | INFO | Epoch 1718 Train Time 23.50165557861328s

2025-10-19 03:00:02,636 | INFO | Training epoch 1719, Batch 1000/1000: LR=5.78e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:00:02,704 | INFO | Epoch 1719 Train Time 23.389885187149048s

2025-10-19 03:00:25,514 | INFO | Training epoch 1720, Batch 1000/1000: LR=5.74e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.30e-01
2025-10-19 03:00:25,609 | INFO | Epoch 1720 Train Time 22.904661178588867s

2025-10-19 03:00:47,826 | INFO | Training epoch 1721, Batch 1000/1000: LR=5.71e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:00:47,911 | INFO | Epoch 1721 Train Time 22.300547122955322s

2025-10-19 03:01:10,465 | INFO | Training epoch 1722, Batch 1000/1000: LR=5.68e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:01:10,548 | INFO | Epoch 1722 Train Time 22.63562536239624s

2025-10-19 03:01:34,058 | INFO | Training epoch 1723, Batch 1000/1000: LR=5.65e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:01:34,158 | INFO | Epoch 1723 Train Time 23.608577489852905s

2025-10-19 03:01:57,017 | INFO | Training epoch 1724, Batch 1000/1000: LR=5.61e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:01:57,100 | INFO | Epoch 1724 Train Time 22.940274715423584s

2025-10-19 03:02:20,261 | INFO | Training epoch 1725, Batch 1000/1000: LR=5.58e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:02:20,333 | INFO | Epoch 1725 Train Time 23.231027126312256s

2025-10-19 03:02:42,922 | INFO | Training epoch 1726, Batch 1000/1000: LR=5.55e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:02:42,988 | INFO | Epoch 1726 Train Time 22.654430150985718s

2025-10-19 03:03:06,438 | INFO | Training epoch 1727, Batch 1000/1000: LR=5.51e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:03:06,520 | INFO | Epoch 1727 Train Time 23.530226945877075s

2025-10-19 03:03:29,229 | INFO | Training epoch 1728, Batch 1000/1000: LR=5.48e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:03:29,310 | INFO | Epoch 1728 Train Time 22.788825511932373s

2025-10-19 03:03:52,456 | INFO | Training epoch 1729, Batch 1000/1000: LR=5.45e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:03:52,542 | INFO | Epoch 1729 Train Time 23.229587078094482s

2025-10-19 03:04:13,232 | INFO | Training epoch 1730, Batch 1000/1000: LR=5.42e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:04:13,293 | INFO | Epoch 1730 Train Time 20.75003719329834s

2025-10-19 03:04:35,935 | INFO | Training epoch 1731, Batch 1000/1000: LR=5.39e-06, Loss=2.68e-02 BER=9.98e-03 FER=1.31e-01
2025-10-19 03:04:36,006 | INFO | Epoch 1731 Train Time 22.711145401000977s

2025-10-19 03:04:58,927 | INFO | Training epoch 1732, Batch 1000/1000: LR=5.35e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:04:59,003 | INFO | Epoch 1732 Train Time 22.99636483192444s

2025-10-19 03:05:21,818 | INFO | Training epoch 1733, Batch 1000/1000: LR=5.32e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:05:21,892 | INFO | Epoch 1733 Train Time 22.887207746505737s

2025-10-19 03:05:45,817 | INFO | Training epoch 1734, Batch 1000/1000: LR=5.29e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:05:45,902 | INFO | Epoch 1734 Train Time 24.00856614112854s

2025-10-19 03:06:09,337 | INFO | Training epoch 1735, Batch 1000/1000: LR=5.26e-06, Loss=2.64e-02 BER=9.95e-03 FER=1.29e-01
2025-10-19 03:06:09,408 | INFO | Epoch 1735 Train Time 23.50553321838379s

2025-10-19 03:06:09,409 | INFO | [P1] saving best_model with loss 0.026433 at epoch 1735
2025-10-19 03:06:32,026 | INFO | Training epoch 1736, Batch 1000/1000: LR=5.23e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:06:32,094 | INFO | Epoch 1736 Train Time 22.673384189605713s

2025-10-19 03:06:54,853 | INFO | Training epoch 1737, Batch 1000/1000: LR=5.20e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:06:54,931 | INFO | Epoch 1737 Train Time 22.835689306259155s

2025-10-19 03:07:18,456 | INFO | Training epoch 1738, Batch 1000/1000: LR=5.16e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:07:18,543 | INFO | Epoch 1738 Train Time 23.60923957824707s

2025-10-19 03:07:42,487 | INFO | Training epoch 1739, Batch 1000/1000: LR=5.13e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:07:42,558 | INFO | Epoch 1739 Train Time 24.01317596435547s

2025-10-19 03:08:05,902 | INFO | Training epoch 1740, Batch 1000/1000: LR=5.10e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:08:05,982 | INFO | Epoch 1740 Train Time 23.42242431640625s

2025-10-19 03:08:29,646 | INFO | Training epoch 1741, Batch 1000/1000: LR=5.07e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.30e-01
2025-10-19 03:08:29,733 | INFO | Epoch 1741 Train Time 23.750271558761597s

2025-10-19 03:08:53,236 | INFO | Training epoch 1742, Batch 1000/1000: LR=5.04e-06, Loss=2.67e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:08:53,310 | INFO | Epoch 1742 Train Time 23.576053619384766s

2025-10-19 03:09:16,736 | INFO | Training epoch 1743, Batch 1000/1000: LR=5.01e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:09:16,807 | INFO | Epoch 1743 Train Time 23.4961576461792s

2025-10-19 03:09:36,727 | INFO | Training epoch 1744, Batch 1000/1000: LR=4.98e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:09:36,804 | INFO | Epoch 1744 Train Time 19.995386838912964s

2025-10-19 03:10:00,263 | INFO | Training epoch 1745, Batch 1000/1000: LR=4.95e-06, Loss=2.68e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:10:00,339 | INFO | Epoch 1745 Train Time 23.53422498703003s

2025-10-19 03:10:23,937 | INFO | Training epoch 1746, Batch 1000/1000: LR=4.92e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:10:24,025 | INFO | Epoch 1746 Train Time 23.68315887451172s

2025-10-19 03:10:47,452 | INFO | Training epoch 1747, Batch 1000/1000: LR=4.89e-06, Loss=2.67e-02 BER=9.98e-03 FER=1.30e-01
2025-10-19 03:10:47,538 | INFO | Epoch 1747 Train Time 23.513075828552246s

2025-10-19 03:11:11,012 | INFO | Training epoch 1748, Batch 1000/1000: LR=4.86e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:11:11,107 | INFO | Epoch 1748 Train Time 23.56751775741577s

2025-10-19 03:11:34,023 | INFO | Training epoch 1749, Batch 1000/1000: LR=4.83e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:11:34,094 | INFO | Epoch 1749 Train Time 22.985458850860596s

2025-10-19 03:11:57,963 | INFO | Training epoch 1750, Batch 1000/1000: LR=4.80e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:11:58,052 | INFO | Epoch 1750 Train Time 23.957406044006348s

2025-10-19 03:12:21,538 | INFO | Training epoch 1751, Batch 1000/1000: LR=4.77e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:12:21,623 | INFO | Epoch 1751 Train Time 23.57002854347229s

2025-10-19 03:12:43,753 | INFO | Training epoch 1752, Batch 1000/1000: LR=4.74e-06, Loss=2.68e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:12:43,837 | INFO | Epoch 1752 Train Time 22.212859392166138s

2025-10-19 03:13:06,744 | INFO | Training epoch 1753, Batch 1000/1000: LR=4.71e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:13:06,816 | INFO | Epoch 1753 Train Time 22.977200746536255s

2025-10-19 03:13:28,528 | INFO | Training epoch 1754, Batch 1000/1000: LR=4.68e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:13:28,611 | INFO | Epoch 1754 Train Time 21.795037508010864s

2025-10-19 03:13:51,336 | INFO | Training epoch 1755, Batch 1000/1000: LR=4.65e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:13:51,419 | INFO | Epoch 1755 Train Time 22.806310415267944s

2025-10-19 03:14:14,246 | INFO | Training epoch 1756, Batch 1000/1000: LR=4.62e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:14:14,337 | INFO | Epoch 1756 Train Time 22.916855335235596s

2025-10-19 03:14:37,053 | INFO | Training epoch 1757, Batch 1000/1000: LR=4.59e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:14:37,122 | INFO | Epoch 1757 Train Time 22.784070014953613s

2025-10-19 03:15:01,133 | INFO | Training epoch 1758, Batch 1000/1000: LR=4.56e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:15:01,219 | INFO | Epoch 1758 Train Time 24.096094846725464s

2025-10-19 03:15:23,834 | INFO | Training epoch 1759, Batch 1000/1000: LR=4.53e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:15:23,926 | INFO | Epoch 1759 Train Time 22.705955028533936s

2025-10-19 03:15:46,728 | INFO | Training epoch 1760, Batch 1000/1000: LR=4.50e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:15:46,800 | INFO | Epoch 1760 Train Time 22.872225284576416s

2025-10-19 03:16:10,123 | INFO | Training epoch 1761, Batch 1000/1000: LR=4.48e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:16:10,194 | INFO | Epoch 1761 Train Time 23.39348554611206s

2025-10-19 03:16:33,651 | INFO | Training epoch 1762, Batch 1000/1000: LR=4.45e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:16:33,729 | INFO | Epoch 1762 Train Time 23.53370189666748s

2025-10-19 03:16:56,650 | INFO | Training epoch 1763, Batch 1000/1000: LR=4.42e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:16:56,735 | INFO | Epoch 1763 Train Time 23.004583597183228s

2025-10-19 03:17:19,813 | INFO | Training epoch 1764, Batch 1000/1000: LR=4.39e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:17:19,888 | INFO | Epoch 1764 Train Time 23.152388095855713s

2025-10-19 03:17:44,156 | INFO | Training epoch 1765, Batch 1000/1000: LR=4.36e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:17:44,232 | INFO | Epoch 1765 Train Time 24.341756343841553s

2025-10-19 03:18:08,147 | INFO | Training epoch 1766, Batch 1000/1000: LR=4.33e-06, Loss=2.75e-02 BER=1.04e-02 FER=1.32e-01
2025-10-19 03:18:08,238 | INFO | Epoch 1766 Train Time 24.005109071731567s

2025-10-19 03:18:30,953 | INFO | Training epoch 1767, Batch 1000/1000: LR=4.31e-06, Loss=2.65e-02 BER=9.95e-03 FER=1.30e-01
2025-10-19 03:18:31,048 | INFO | Epoch 1767 Train Time 22.807817697525024s

2025-10-19 03:18:54,030 | INFO | Training epoch 1768, Batch 1000/1000: LR=4.28e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:18:54,110 | INFO | Epoch 1768 Train Time 23.061169624328613s

2025-10-19 03:19:16,636 | INFO | Training epoch 1769, Batch 1000/1000: LR=4.25e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:19:16,711 | INFO | Epoch 1769 Train Time 22.599595308303833s

2025-10-19 03:19:39,012 | INFO | Training epoch 1770, Batch 1000/1000: LR=4.22e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:19:39,079 | INFO | Epoch 1770 Train Time 22.366323947906494s

2025-10-19 03:20:01,436 | INFO | Training epoch 1771, Batch 1000/1000: LR=4.20e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:20:01,532 | INFO | Epoch 1771 Train Time 22.451729774475098s

2025-10-19 03:20:24,637 | INFO | Training epoch 1772, Batch 1000/1000: LR=4.17e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.30e-01
2025-10-19 03:20:24,715 | INFO | Epoch 1772 Train Time 23.18067193031311s

2025-10-19 03:20:46,225 | INFO | Training epoch 1773, Batch 1000/1000: LR=4.14e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:20:46,299 | INFO | Epoch 1773 Train Time 21.582759380340576s

2025-10-19 03:21:08,842 | INFO | Training epoch 1774, Batch 1000/1000: LR=4.11e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:21:08,921 | INFO | Epoch 1774 Train Time 22.620908737182617s

2025-10-19 03:21:32,552 | INFO | Training epoch 1775, Batch 1000/1000: LR=4.09e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:21:32,643 | INFO | Epoch 1775 Train Time 23.718806266784668s

2025-10-19 03:21:55,541 | INFO | Training epoch 1776, Batch 1000/1000: LR=4.06e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:21:55,612 | INFO | Epoch 1776 Train Time 22.9686017036438s

2025-10-19 03:22:18,239 | INFO | Training epoch 1777, Batch 1000/1000: LR=4.03e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:22:18,312 | INFO | Epoch 1777 Train Time 22.699206352233887s

2025-10-19 03:22:41,622 | INFO | Training epoch 1778, Batch 1000/1000: LR=4.01e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:22:41,693 | INFO | Epoch 1778 Train Time 23.37856364250183s

2025-10-19 03:23:05,337 | INFO | Training epoch 1779, Batch 1000/1000: LR=3.98e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 03:23:05,424 | INFO | Epoch 1779 Train Time 23.730443954467773s

2025-10-19 03:23:28,645 | INFO | Training epoch 1780, Batch 1000/1000: LR=3.95e-06, Loss=2.71e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:23:28,717 | INFO | Epoch 1780 Train Time 23.29121446609497s

2025-10-19 03:23:52,448 | INFO | Training epoch 1781, Batch 1000/1000: LR=3.93e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:23:52,535 | INFO | Epoch 1781 Train Time 23.817180395126343s

2025-10-19 03:24:16,164 | INFO | Training epoch 1782, Batch 1000/1000: LR=3.90e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:24:16,256 | INFO | Epoch 1782 Train Time 23.72040605545044s

2025-10-19 03:24:39,339 | INFO | Training epoch 1783, Batch 1000/1000: LR=3.87e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:24:39,437 | INFO | Epoch 1783 Train Time 23.178572416305542s

2025-10-19 03:25:03,545 | INFO | Training epoch 1784, Batch 1000/1000: LR=3.85e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:25:03,624 | INFO | Epoch 1784 Train Time 24.185790300369263s

2025-10-19 03:25:26,131 | INFO | Training epoch 1785, Batch 1000/1000: LR=3.82e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:25:26,208 | INFO | Epoch 1785 Train Time 22.58259606361389s

2025-10-19 03:25:49,848 | INFO | Training epoch 1786, Batch 1000/1000: LR=3.80e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:25:49,926 | INFO | Epoch 1786 Train Time 23.717538595199585s

2025-10-19 03:26:13,847 | INFO | Training epoch 1787, Batch 1000/1000: LR=3.77e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:26:13,940 | INFO | Epoch 1787 Train Time 24.01295256614685s

2025-10-19 03:26:36,621 | INFO | Training epoch 1788, Batch 1000/1000: LR=3.74e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:26:36,714 | INFO | Epoch 1788 Train Time 22.77287459373474s

2025-10-19 03:26:59,917 | INFO | Training epoch 1789, Batch 1000/1000: LR=3.72e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:27:00,003 | INFO | Epoch 1789 Train Time 23.287940979003906s

2025-10-19 03:27:23,254 | INFO | Training epoch 1790, Batch 1000/1000: LR=3.69e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:27:23,325 | INFO | Epoch 1790 Train Time 23.320407152175903s

2025-10-19 03:27:46,710 | INFO | Training epoch 1791, Batch 1000/1000: LR=3.67e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:27:46,799 | INFO | Epoch 1791 Train Time 23.473835229873657s

2025-10-19 03:28:08,058 | INFO | Training epoch 1792, Batch 1000/1000: LR=3.64e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:28:08,140 | INFO | Epoch 1792 Train Time 21.339600801467896s

2025-10-19 03:28:31,508 | INFO | Training epoch 1793, Batch 1000/1000: LR=3.62e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-19 03:28:31,581 | INFO | Epoch 1793 Train Time 23.440417289733887s

2025-10-19 03:28:54,634 | INFO | Training epoch 1794, Batch 1000/1000: LR=3.59e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:28:54,702 | INFO | Epoch 1794 Train Time 23.119404077529907s

2025-10-19 03:29:17,116 | INFO | Training epoch 1795, Batch 1000/1000: LR=3.57e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:29:17,202 | INFO | Epoch 1795 Train Time 22.498443603515625s

2025-10-19 03:29:40,451 | INFO | Training epoch 1796, Batch 1000/1000: LR=3.54e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:29:40,523 | INFO | Epoch 1796 Train Time 23.31895422935486s

2025-10-19 03:30:03,222 | INFO | Training epoch 1797, Batch 1000/1000: LR=3.52e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:30:03,296 | INFO | Epoch 1797 Train Time 22.77268362045288s

2025-10-19 03:30:26,240 | INFO | Training epoch 1798, Batch 1000/1000: LR=3.50e-06, Loss=2.74e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 03:30:26,322 | INFO | Epoch 1798 Train Time 23.024269104003906s

2025-10-19 03:30:49,152 | INFO | Training epoch 1799, Batch 1000/1000: LR=3.47e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:30:49,232 | INFO | Epoch 1799 Train Time 22.909188747406006s

2025-10-19 03:31:12,412 | INFO | Training epoch 1800, Batch 1000/1000: LR=3.45e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:31:12,508 | INFO | Epoch 1800 Train Time 23.275073051452637s

2025-10-19 03:31:37,016 | INFO | Training epoch 1801, Batch 1000/1000: LR=3.42e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 03:31:37,100 | INFO | Epoch 1801 Train Time 24.591572523117065s

2025-10-19 03:32:00,544 | INFO | Training epoch 1802, Batch 1000/1000: LR=3.40e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:32:00,635 | INFO | Epoch 1802 Train Time 23.533597230911255s

2025-10-19 03:32:24,206 | INFO | Training epoch 1803, Batch 1000/1000: LR=3.37e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 03:32:24,294 | INFO | Epoch 1803 Train Time 23.6582453250885s

2025-10-19 03:32:47,243 | INFO | Training epoch 1804, Batch 1000/1000: LR=3.35e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:32:47,334 | INFO | Epoch 1804 Train Time 23.038034915924072s

2025-10-19 03:33:10,290 | INFO | Training epoch 1805, Batch 1000/1000: LR=3.33e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:33:10,369 | INFO | Epoch 1805 Train Time 23.034075260162354s

2025-10-19 03:33:33,855 | INFO | Training epoch 1806, Batch 1000/1000: LR=3.30e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:33:33,953 | INFO | Epoch 1806 Train Time 23.583366632461548s

2025-10-19 03:33:56,532 | INFO | Training epoch 1807, Batch 1000/1000: LR=3.28e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:33:56,609 | INFO | Epoch 1807 Train Time 22.65403985977173s

2025-10-19 03:34:19,930 | INFO | Training epoch 1808, Batch 1000/1000: LR=3.26e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:34:20,011 | INFO | Epoch 1808 Train Time 23.400354385375977s

2025-10-19 03:34:43,717 | INFO | Training epoch 1809, Batch 1000/1000: LR=3.23e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 03:34:43,806 | INFO | Epoch 1809 Train Time 23.792847394943237s

2025-10-19 03:35:08,517 | INFO | Training epoch 1810, Batch 1000/1000: LR=3.21e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:35:08,612 | INFO | Epoch 1810 Train Time 24.80429220199585s

2025-10-19 03:35:32,162 | INFO | Training epoch 1811, Batch 1000/1000: LR=3.19e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:35:32,235 | INFO | Epoch 1811 Train Time 23.620763540267944s

2025-10-19 03:35:54,941 | INFO | Training epoch 1812, Batch 1000/1000: LR=3.17e-06, Loss=2.65e-02 BER=9.97e-03 FER=1.30e-01
2025-10-19 03:35:55,024 | INFO | Epoch 1812 Train Time 22.788605213165283s

2025-10-19 03:36:18,540 | INFO | Training epoch 1813, Batch 1000/1000: LR=3.14e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:36:18,614 | INFO | Epoch 1813 Train Time 23.587350368499756s

2025-10-19 03:36:41,413 | INFO | Training epoch 1814, Batch 1000/1000: LR=3.12e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:36:41,512 | INFO | Epoch 1814 Train Time 22.89655828475952s

2025-10-19 03:37:04,252 | INFO | Training epoch 1815, Batch 1000/1000: LR=3.10e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:37:04,333 | INFO | Epoch 1815 Train Time 22.820104360580444s

2025-10-19 03:37:28,330 | INFO | Training epoch 1816, Batch 1000/1000: LR=3.08e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:37:28,403 | INFO | Epoch 1816 Train Time 24.068843841552734s

2025-10-19 03:37:52,249 | INFO | Training epoch 1817, Batch 1000/1000: LR=3.05e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:37:52,326 | INFO | Epoch 1817 Train Time 23.92186188697815s

2025-10-19 03:38:15,522 | INFO | Training epoch 1818, Batch 1000/1000: LR=3.03e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 03:38:15,594 | INFO | Epoch 1818 Train Time 23.267225980758667s

2025-10-19 03:38:39,427 | INFO | Training epoch 1819, Batch 1000/1000: LR=3.01e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:38:39,502 | INFO | Epoch 1819 Train Time 23.9068820476532s

2025-10-19 03:39:03,117 | INFO | Training epoch 1820, Batch 1000/1000: LR=2.99e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:39:03,201 | INFO | Epoch 1820 Train Time 23.698259353637695s

2025-10-19 03:39:27,353 | INFO | Training epoch 1821, Batch 1000/1000: LR=2.97e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:39:27,444 | INFO | Epoch 1821 Train Time 24.24184775352478s

2025-10-19 03:39:51,423 | INFO | Training epoch 1822, Batch 1000/1000: LR=2.94e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:39:51,530 | INFO | Epoch 1822 Train Time 24.084429502487183s

2025-10-19 03:40:15,116 | INFO | Training epoch 1823, Batch 1000/1000: LR=2.92e-06, Loss=2.65e-02 BER=9.95e-03 FER=1.29e-01
2025-10-19 03:40:15,195 | INFO | Epoch 1823 Train Time 23.66299295425415s

2025-10-19 03:40:38,249 | INFO | Training epoch 1824, Batch 1000/1000: LR=2.90e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:40:38,336 | INFO | Epoch 1824 Train Time 23.14040732383728s

2025-10-19 03:41:01,114 | INFO | Training epoch 1825, Batch 1000/1000: LR=2.88e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:41:01,210 | INFO | Epoch 1825 Train Time 22.872586011886597s

2025-10-19 03:41:25,434 | INFO | Training epoch 1826, Batch 1000/1000: LR=2.86e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:41:25,519 | INFO | Epoch 1826 Train Time 24.308527946472168s

2025-10-19 03:41:48,702 | INFO | Training epoch 1827, Batch 1000/1000: LR=2.84e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:41:48,786 | INFO | Epoch 1827 Train Time 23.265390872955322s

2025-10-19 03:42:12,845 | INFO | Training epoch 1828, Batch 1000/1000: LR=2.82e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:42:12,924 | INFO | Epoch 1828 Train Time 24.13754105567932s

2025-10-19 03:42:36,945 | INFO | Training epoch 1829, Batch 1000/1000: LR=2.80e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:42:37,022 | INFO | Epoch 1829 Train Time 24.09612011909485s

2025-10-19 03:43:01,316 | INFO | Training epoch 1830, Batch 1000/1000: LR=2.77e-06, Loss=2.69e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 03:43:01,399 | INFO | Epoch 1830 Train Time 24.375842809677124s

2025-10-19 03:43:24,020 | INFO | Training epoch 1831, Batch 1000/1000: LR=2.75e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:43:24,113 | INFO | Epoch 1831 Train Time 22.71311664581299s

2025-10-19 03:43:46,947 | INFO | Training epoch 1832, Batch 1000/1000: LR=2.73e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:43:47,030 | INFO | Epoch 1832 Train Time 22.916504621505737s

2025-10-19 03:44:10,134 | INFO | Training epoch 1833, Batch 1000/1000: LR=2.71e-06, Loss=2.66e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 03:44:10,216 | INFO | Epoch 1833 Train Time 23.184423446655273s

2025-10-19 03:44:33,664 | INFO | Training epoch 1834, Batch 1000/1000: LR=2.69e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:44:33,751 | INFO | Epoch 1834 Train Time 23.534229278564453s

2025-10-19 03:44:55,624 | INFO | Training epoch 1835, Batch 1000/1000: LR=2.67e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:44:55,705 | INFO | Epoch 1835 Train Time 21.95237374305725s

2025-10-19 03:45:18,876 | INFO | Training epoch 1836, Batch 1000/1000: LR=2.65e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:45:18,952 | INFO | Epoch 1836 Train Time 23.246312379837036s

2025-10-19 03:45:41,857 | INFO | Training epoch 1837, Batch 1000/1000: LR=2.63e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:45:41,956 | INFO | Epoch 1837 Train Time 23.00317931175232s

2025-10-19 03:46:05,554 | INFO | Training epoch 1838, Batch 1000/1000: LR=2.61e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:46:05,647 | INFO | Epoch 1838 Train Time 23.688349962234497s

2025-10-19 03:46:29,528 | INFO | Training epoch 1839, Batch 1000/1000: LR=2.59e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:46:29,619 | INFO | Epoch 1839 Train Time 23.971412420272827s

2025-10-19 03:46:53,241 | INFO | Training epoch 1840, Batch 1000/1000: LR=2.57e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:46:53,328 | INFO | Epoch 1840 Train Time 23.70757746696472s

2025-10-19 03:47:16,337 | INFO | Training epoch 1841, Batch 1000/1000: LR=2.56e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:47:16,412 | INFO | Epoch 1841 Train Time 23.082688093185425s

2025-10-19 03:47:39,754 | INFO | Training epoch 1842, Batch 1000/1000: LR=2.54e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:47:39,845 | INFO | Epoch 1842 Train Time 23.431134939193726s

2025-10-19 03:48:03,338 | INFO | Training epoch 1843, Batch 1000/1000: LR=2.52e-06, Loss=2.66e-02 BER=9.99e-03 FER=1.29e-01
2025-10-19 03:48:03,424 | INFO | Epoch 1843 Train Time 23.576815843582153s

2025-10-19 03:48:26,926 | INFO | Training epoch 1844, Batch 1000/1000: LR=2.50e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:48:26,998 | INFO | Epoch 1844 Train Time 23.573303937911987s

2025-10-19 03:48:50,823 | INFO | Training epoch 1845, Batch 1000/1000: LR=2.48e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:48:50,901 | INFO | Epoch 1845 Train Time 23.90186595916748s

2025-10-19 03:49:14,770 | INFO | Training epoch 1846, Batch 1000/1000: LR=2.46e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:49:14,853 | INFO | Epoch 1846 Train Time 23.950334072113037s

2025-10-19 03:49:38,417 | INFO | Training epoch 1847, Batch 1000/1000: LR=2.44e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:49:38,501 | INFO | Epoch 1847 Train Time 23.646660804748535s

2025-10-19 03:50:01,516 | INFO | Training epoch 1848, Batch 1000/1000: LR=2.42e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:50:01,592 | INFO | Epoch 1848 Train Time 23.089215755462646s

2025-10-19 03:50:25,123 | INFO | Training epoch 1849, Batch 1000/1000: LR=2.40e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:50:25,190 | INFO | Epoch 1849 Train Time 23.59722590446472s

2025-10-19 03:50:49,107 | INFO | Training epoch 1850, Batch 1000/1000: LR=2.39e-06, Loss=2.67e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:50:49,200 | INFO | Epoch 1850 Train Time 24.007873058319092s

2025-10-19 03:51:11,871 | INFO | Training epoch 1851, Batch 1000/1000: LR=2.37e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:51:11,968 | INFO | Epoch 1851 Train Time 22.76570177078247s

2025-10-19 03:51:35,052 | INFO | Training epoch 1852, Batch 1000/1000: LR=2.35e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:51:35,148 | INFO | Epoch 1852 Train Time 23.178630590438843s

2025-10-19 03:51:59,239 | INFO | Training epoch 1853, Batch 1000/1000: LR=2.33e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:51:59,319 | INFO | Epoch 1853 Train Time 24.16904902458191s

2025-10-19 03:52:22,768 | INFO | Training epoch 1854, Batch 1000/1000: LR=2.31e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 03:52:22,837 | INFO | Epoch 1854 Train Time 23.515945434570312s

2025-10-19 03:52:46,329 | INFO | Training epoch 1855, Batch 1000/1000: LR=2.30e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:52:46,431 | INFO | Epoch 1855 Train Time 23.593095064163208s

2025-10-19 03:53:10,652 | INFO | Training epoch 1856, Batch 1000/1000: LR=2.28e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:53:10,749 | INFO | Epoch 1856 Train Time 24.315876483917236s

2025-10-19 03:53:33,426 | INFO | Training epoch 1857, Batch 1000/1000: LR=2.26e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:53:33,493 | INFO | Epoch 1857 Train Time 22.741684436798096s

2025-10-19 03:53:57,020 | INFO | Training epoch 1858, Batch 1000/1000: LR=2.24e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:53:57,107 | INFO | Epoch 1858 Train Time 23.613142728805542s

2025-10-19 03:54:20,760 | INFO | Training epoch 1859, Batch 1000/1000: LR=2.23e-06, Loss=2.67e-02 BER=9.99e-03 FER=1.30e-01
2025-10-19 03:54:20,866 | INFO | Epoch 1859 Train Time 23.7575740814209s

2025-10-19 03:54:43,930 | INFO | Training epoch 1860, Batch 1000/1000: LR=2.21e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:54:44,016 | INFO | Epoch 1860 Train Time 23.149268627166748s

2025-10-19 03:55:07,752 | INFO | Training epoch 1861, Batch 1000/1000: LR=2.19e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:55:07,828 | INFO | Epoch 1861 Train Time 23.811391830444336s

2025-10-19 03:55:30,345 | INFO | Training epoch 1862, Batch 1000/1000: LR=2.18e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 03:55:30,428 | INFO | Epoch 1862 Train Time 22.59865164756775s

2025-10-19 03:55:50,860 | INFO | Training epoch 1863, Batch 1000/1000: LR=2.16e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:55:50,934 | INFO | Epoch 1863 Train Time 20.505273580551147s

2025-10-19 03:56:12,833 | INFO | Training epoch 1864, Batch 1000/1000: LR=2.14e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 03:56:12,902 | INFO | Epoch 1864 Train Time 21.96686363220215s

2025-10-19 03:56:36,010 | INFO | Training epoch 1865, Batch 1000/1000: LR=2.13e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:56:36,082 | INFO | Epoch 1865 Train Time 23.17876386642456s

2025-10-19 03:56:59,340 | INFO | Training epoch 1866, Batch 1000/1000: LR=2.11e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:56:59,410 | INFO | Epoch 1866 Train Time 23.325318574905396s

2025-10-19 03:57:21,943 | INFO | Training epoch 1867, Batch 1000/1000: LR=2.09e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 03:57:22,043 | INFO | Epoch 1867 Train Time 22.631168365478516s

2025-10-19 03:57:45,325 | INFO | Training epoch 1868, Batch 1000/1000: LR=2.08e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:57:45,404 | INFO | Epoch 1868 Train Time 23.360270023345947s

2025-10-19 03:58:08,540 | INFO | Training epoch 1869, Batch 1000/1000: LR=2.06e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:58:08,615 | INFO | Epoch 1869 Train Time 23.21046209335327s

2025-10-19 03:58:32,333 | INFO | Training epoch 1870, Batch 1000/1000: LR=2.04e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:58:32,421 | INFO | Epoch 1870 Train Time 23.80454158782959s

2025-10-19 03:58:56,154 | INFO | Training epoch 1871, Batch 1000/1000: LR=2.03e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:58:56,230 | INFO | Epoch 1871 Train Time 23.807092905044556s

2025-10-19 03:59:18,423 | INFO | Training epoch 1872, Batch 1000/1000: LR=2.01e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 03:59:18,498 | INFO | Epoch 1872 Train Time 22.267453908920288s

2025-10-19 03:59:41,634 | INFO | Training epoch 1873, Batch 1000/1000: LR=2.00e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 03:59:41,704 | INFO | Epoch 1873 Train Time 23.203255653381348s

2025-10-19 04:00:05,209 | INFO | Training epoch 1874, Batch 1000/1000: LR=1.98e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:00:05,288 | INFO | Epoch 1874 Train Time 23.582055807113647s

2025-10-19 04:00:28,757 | INFO | Training epoch 1875, Batch 1000/1000: LR=1.97e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 04:00:28,845 | INFO | Epoch 1875 Train Time 23.555174112319946s

2025-10-19 04:00:52,539 | INFO | Training epoch 1876, Batch 1000/1000: LR=1.95e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:00:52,634 | INFO | Epoch 1876 Train Time 23.787811279296875s

2025-10-19 04:01:14,656 | INFO | Training epoch 1877, Batch 1000/1000: LR=1.94e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:01:14,756 | INFO | Epoch 1877 Train Time 22.121642589569092s

2025-10-19 04:01:37,215 | INFO | Training epoch 1878, Batch 1000/1000: LR=1.92e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:01:37,300 | INFO | Epoch 1878 Train Time 22.541680574417114s

2025-10-19 04:02:01,024 | INFO | Training epoch 1879, Batch 1000/1000: LR=1.91e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:02:01,112 | INFO | Epoch 1879 Train Time 23.810728073120117s

2025-10-19 04:02:24,619 | INFO | Training epoch 1880, Batch 1000/1000: LR=1.89e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:02:24,692 | INFO | Epoch 1880 Train Time 23.577245712280273s

2025-10-19 04:02:48,252 | INFO | Training epoch 1881, Batch 1000/1000: LR=1.88e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:02:48,348 | INFO | Epoch 1881 Train Time 23.655043125152588s

2025-10-19 04:03:11,524 | INFO | Training epoch 1882, Batch 1000/1000: LR=1.86e-06, Loss=2.67e-02 BER=9.98e-03 FER=1.29e-01
2025-10-19 04:03:11,594 | INFO | Epoch 1882 Train Time 23.245095252990723s

2025-10-19 04:03:34,824 | INFO | Training epoch 1883, Batch 1000/1000: LR=1.85e-06, Loss=2.66e-02 BER=9.93e-03 FER=1.30e-01
2025-10-19 04:03:34,900 | INFO | Epoch 1883 Train Time 23.30442190170288s

2025-10-19 04:03:57,667 | INFO | Training epoch 1884, Batch 1000/1000: LR=1.83e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:03:57,739 | INFO | Epoch 1884 Train Time 22.83760952949524s

2025-10-19 04:04:21,215 | INFO | Training epoch 1885, Batch 1000/1000: LR=1.82e-06, Loss=2.65e-02 BER=9.97e-03 FER=1.30e-01
2025-10-19 04:04:21,290 | INFO | Epoch 1885 Train Time 23.550727605819702s

2025-10-19 04:04:44,943 | INFO | Training epoch 1886, Batch 1000/1000: LR=1.81e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:04:45,032 | INFO | Epoch 1886 Train Time 23.739912271499634s

2025-10-19 04:05:08,180 | INFO | Training epoch 1887, Batch 1000/1000: LR=1.79e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:05:08,251 | INFO | Epoch 1887 Train Time 23.217915058135986s

2025-10-19 04:05:30,933 | INFO | Training epoch 1888, Batch 1000/1000: LR=1.78e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:05:31,013 | INFO | Epoch 1888 Train Time 22.76086664199829s

2025-10-19 04:05:54,249 | INFO | Training epoch 1889, Batch 1000/1000: LR=1.76e-06, Loss=2.72e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:05:54,336 | INFO | Epoch 1889 Train Time 23.320346117019653s

2025-10-19 04:06:18,047 | INFO | Training epoch 1890, Batch 1000/1000: LR=1.75e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:06:18,132 | INFO | Epoch 1890 Train Time 23.79440665245056s

2025-10-19 04:06:41,940 | INFO | Training epoch 1891, Batch 1000/1000: LR=1.74e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:06:42,047 | INFO | Epoch 1891 Train Time 23.913923501968384s

2025-10-19 04:07:06,527 | INFO | Training epoch 1892, Batch 1000/1000: LR=1.72e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:07:06,615 | INFO | Epoch 1892 Train Time 24.567850351333618s

2025-10-19 04:07:27,556 | INFO | Training epoch 1893, Batch 1000/1000: LR=1.71e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:07:27,649 | INFO | Epoch 1893 Train Time 21.033127784729004s

2025-10-19 04:07:50,146 | INFO | Training epoch 1894, Batch 1000/1000: LR=1.70e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:07:50,229 | INFO | Epoch 1894 Train Time 22.577961921691895s

2025-10-19 04:08:13,068 | INFO | Training epoch 1895, Batch 1000/1000: LR=1.68e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:08:13,166 | INFO | Epoch 1895 Train Time 22.93458604812622s

2025-10-19 04:08:33,906 | INFO | Training epoch 1896, Batch 1000/1000: LR=1.67e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 04:08:33,975 | INFO | Epoch 1896 Train Time 20.807849884033203s

2025-10-19 04:08:56,739 | INFO | Training epoch 1897, Batch 1000/1000: LR=1.66e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:08:56,817 | INFO | Epoch 1897 Train Time 22.8416965007782s

2025-10-19 04:09:20,211 | INFO | Training epoch 1898, Batch 1000/1000: LR=1.65e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:09:20,280 | INFO | Epoch 1898 Train Time 23.46111488342285s

2025-10-19 04:09:43,832 | INFO | Training epoch 1899, Batch 1000/1000: LR=1.63e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:09:43,925 | INFO | Epoch 1899 Train Time 23.643633604049683s

2025-10-19 04:10:07,351 | INFO | Training epoch 1900, Batch 1000/1000: LR=1.62e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:10:07,420 | INFO | Epoch 1900 Train Time 23.49450659751892s

2025-10-19 04:10:30,795 | INFO | Training epoch 1901, Batch 1000/1000: LR=1.61e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.30e-01
2025-10-19 04:10:30,873 | INFO | Epoch 1901 Train Time 23.451390981674194s

2025-10-19 04:10:53,740 | INFO | Training epoch 1902, Batch 1000/1000: LR=1.60e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:10:53,815 | INFO | Epoch 1902 Train Time 22.940231800079346s

2025-10-19 04:11:17,554 | INFO | Training epoch 1903, Batch 1000/1000: LR=1.59e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:11:17,629 | INFO | Epoch 1903 Train Time 23.813425302505493s

2025-10-19 04:11:40,537 | INFO | Training epoch 1904, Batch 1000/1000: LR=1.57e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:11:40,617 | INFO | Epoch 1904 Train Time 22.986697912216187s

2025-10-19 04:12:01,840 | INFO | Training epoch 1905, Batch 1000/1000: LR=1.56e-06, Loss=2.69e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:12:01,922 | INFO | Epoch 1905 Train Time 21.302661895751953s

2025-10-19 04:12:25,037 | INFO | Training epoch 1906, Batch 1000/1000: LR=1.55e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-19 04:12:25,121 | INFO | Epoch 1906 Train Time 23.198620557785034s

2025-10-19 04:12:49,030 | INFO | Training epoch 1907, Batch 1000/1000: LR=1.54e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:12:49,118 | INFO | Epoch 1907 Train Time 23.99450993537903s

2025-10-19 04:13:11,555 | INFO | Training epoch 1908, Batch 1000/1000: LR=1.53e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:13:11,643 | INFO | Epoch 1908 Train Time 22.524123191833496s

2025-10-19 04:13:34,837 | INFO | Training epoch 1909, Batch 1000/1000: LR=1.52e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:13:34,912 | INFO | Epoch 1909 Train Time 23.2684063911438s

2025-10-19 04:13:58,147 | INFO | Training epoch 1910, Batch 1000/1000: LR=1.50e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:13:58,222 | INFO | Epoch 1910 Train Time 23.308002948760986s

2025-10-19 04:14:21,509 | INFO | Training epoch 1911, Batch 1000/1000: LR=1.49e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.29e-01
2025-10-19 04:14:21,590 | INFO | Epoch 1911 Train Time 23.3676118850708s

2025-10-19 04:14:45,135 | INFO | Training epoch 1912, Batch 1000/1000: LR=1.48e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:14:45,215 | INFO | Epoch 1912 Train Time 23.623554706573486s

2025-10-19 04:15:10,242 | INFO | Training epoch 1913, Batch 1000/1000: LR=1.47e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:15:10,336 | INFO | Epoch 1913 Train Time 25.11937403678894s

2025-10-19 04:15:32,929 | INFO | Training epoch 1914, Batch 1000/1000: LR=1.46e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 04:15:33,019 | INFO | Epoch 1914 Train Time 22.68202495574951s

2025-10-19 04:15:56,234 | INFO | Training epoch 1915, Batch 1000/1000: LR=1.45e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:15:56,313 | INFO | Epoch 1915 Train Time 23.293577671051025s

2025-10-19 04:16:19,324 | INFO | Training epoch 1916, Batch 1000/1000: LR=1.44e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:16:19,394 | INFO | Epoch 1916 Train Time 23.08006453514099s

2025-10-19 04:16:42,531 | INFO | Training epoch 1917, Batch 1000/1000: LR=1.43e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 04:16:42,604 | INFO | Epoch 1917 Train Time 23.209062337875366s

2025-10-19 04:17:05,808 | INFO | Training epoch 1918, Batch 1000/1000: LR=1.42e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.32e-01
2025-10-19 04:17:05,878 | INFO | Epoch 1918 Train Time 23.272610664367676s

2025-10-19 04:17:28,649 | INFO | Training epoch 1919, Batch 1000/1000: LR=1.41e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:17:28,737 | INFO | Epoch 1919 Train Time 22.858089447021484s

2025-10-19 04:17:52,165 | INFO | Training epoch 1920, Batch 1000/1000: LR=1.40e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 04:17:52,261 | INFO | Epoch 1920 Train Time 23.522189617156982s

2025-10-19 04:18:14,559 | INFO | Training epoch 1921, Batch 1000/1000: LR=1.39e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:18:14,635 | INFO | Epoch 1921 Train Time 22.37253165245056s

2025-10-19 04:18:38,157 | INFO | Training epoch 1922, Batch 1000/1000: LR=1.38e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.31e-01
2025-10-19 04:18:38,224 | INFO | Epoch 1922 Train Time 23.588175773620605s

2025-10-19 04:19:01,336 | INFO | Training epoch 1923, Batch 1000/1000: LR=1.37e-06, Loss=2.68e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:19:01,426 | INFO | Epoch 1923 Train Time 23.199363708496094s

2025-10-19 04:19:24,825 | INFO | Training epoch 1924, Batch 1000/1000: LR=1.36e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:19:24,918 | INFO | Epoch 1924 Train Time 23.491867542266846s

2025-10-19 04:19:47,423 | INFO | Training epoch 1925, Batch 1000/1000: LR=1.35e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:19:47,491 | INFO | Epoch 1925 Train Time 22.572245836257935s

2025-10-19 04:20:11,538 | INFO | Training epoch 1926, Batch 1000/1000: LR=1.34e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:20:11,622 | INFO | Epoch 1926 Train Time 24.12954068183899s

2025-10-19 04:20:34,237 | INFO | Training epoch 1927, Batch 1000/1000: LR=1.33e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:20:34,327 | INFO | Epoch 1927 Train Time 22.704442262649536s

2025-10-19 04:20:57,760 | INFO | Training epoch 1928, Batch 1000/1000: LR=1.33e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:20:57,864 | INFO | Epoch 1928 Train Time 23.535778760910034s

2025-10-19 04:21:20,236 | INFO | Training epoch 1929, Batch 1000/1000: LR=1.32e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:21:20,310 | INFO | Epoch 1929 Train Time 22.443562269210815s

2025-10-19 04:21:43,836 | INFO | Training epoch 1930, Batch 1000/1000: LR=1.31e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:21:43,924 | INFO | Epoch 1930 Train Time 23.61240005493164s

2025-10-19 04:22:07,547 | INFO | Training epoch 1931, Batch 1000/1000: LR=1.30e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:22:07,633 | INFO | Epoch 1931 Train Time 23.70773482322693s

2025-10-19 04:22:30,413 | INFO | Training epoch 1932, Batch 1000/1000: LR=1.29e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:22:30,499 | INFO | Epoch 1932 Train Time 22.865588665008545s

2025-10-19 04:22:51,522 | INFO | Training epoch 1933, Batch 1000/1000: LR=1.28e-06, Loss=2.66e-02 BER=9.99e-03 FER=1.29e-01
2025-10-19 04:22:51,607 | INFO | Epoch 1933 Train Time 21.104950428009033s

2025-10-19 04:23:14,646 | INFO | Training epoch 1934, Batch 1000/1000: LR=1.27e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:23:14,712 | INFO | Epoch 1934 Train Time 23.104410886764526s

2025-10-19 04:23:38,105 | INFO | Training epoch 1935, Batch 1000/1000: LR=1.27e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:23:38,194 | INFO | Epoch 1935 Train Time 23.48058533668518s

2025-10-19 04:24:01,527 | INFO | Training epoch 1936, Batch 1000/1000: LR=1.26e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:24:01,638 | INFO | Epoch 1936 Train Time 23.442646503448486s

2025-10-19 04:24:24,818 | INFO | Training epoch 1937, Batch 1000/1000: LR=1.25e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:24:24,904 | INFO | Epoch 1937 Train Time 23.264877557754517s

2025-10-19 04:24:46,044 | INFO | Training epoch 1938, Batch 1000/1000: LR=1.24e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:24:46,137 | INFO | Epoch 1938 Train Time 21.231889963150024s

2025-10-19 04:25:09,508 | INFO | Training epoch 1939, Batch 1000/1000: LR=1.23e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:25:09,594 | INFO | Epoch 1939 Train Time 23.45579433441162s

2025-10-19 04:25:33,062 | INFO | Training epoch 1940, Batch 1000/1000: LR=1.23e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:25:33,160 | INFO | Epoch 1940 Train Time 23.563061475753784s

2025-10-19 04:25:55,832 | INFO | Training epoch 1941, Batch 1000/1000: LR=1.22e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:25:55,915 | INFO | Epoch 1941 Train Time 22.752803802490234s

2025-10-19 04:26:18,604 | INFO | Training epoch 1942, Batch 1000/1000: LR=1.21e-06, Loss=2.72e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 04:26:18,681 | INFO | Epoch 1942 Train Time 22.765255212783813s

2025-10-19 04:26:41,028 | INFO | Training epoch 1943, Batch 1000/1000: LR=1.21e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:26:41,102 | INFO | Epoch 1943 Train Time 22.419196367263794s

2025-10-19 04:27:04,358 | INFO | Training epoch 1944, Batch 1000/1000: LR=1.20e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:27:04,431 | INFO | Epoch 1944 Train Time 23.328424215316772s

2025-10-19 04:27:27,398 | INFO | Training epoch 1945, Batch 1000/1000: LR=1.19e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:27:27,491 | INFO | Epoch 1945 Train Time 23.058468103408813s

2025-10-19 04:27:50,821 | INFO | Training epoch 1946, Batch 1000/1000: LR=1.18e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:27:50,893 | INFO | Epoch 1946 Train Time 23.40118098258972s

2025-10-19 04:28:13,209 | INFO | Training epoch 1947, Batch 1000/1000: LR=1.18e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:28:13,278 | INFO | Epoch 1947 Train Time 22.383466720581055s

2025-10-19 04:28:35,855 | INFO | Training epoch 1948, Batch 1000/1000: LR=1.17e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:28:35,929 | INFO | Epoch 1948 Train Time 22.64940857887268s

2025-10-19 04:28:58,754 | INFO | Training epoch 1949, Batch 1000/1000: LR=1.17e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:28:58,825 | INFO | Epoch 1949 Train Time 22.895569562911987s

2025-10-19 04:29:21,250 | INFO | Training epoch 1950, Batch 1000/1000: LR=1.16e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:29:21,326 | INFO | Epoch 1950 Train Time 22.499599933624268s

2025-10-19 04:29:43,622 | INFO | Training epoch 1951, Batch 1000/1000: LR=1.15e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:29:43,702 | INFO | Epoch 1951 Train Time 22.37493324279785s

2025-10-19 04:30:06,040 | INFO | Training epoch 1952, Batch 1000/1000: LR=1.15e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:30:06,116 | INFO | Epoch 1952 Train Time 22.412834644317627s

2025-10-19 04:30:29,217 | INFO | Training epoch 1953, Batch 1000/1000: LR=1.14e-06, Loss=2.67e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:30:29,310 | INFO | Epoch 1953 Train Time 23.19265127182007s

2025-10-19 04:30:52,432 | INFO | Training epoch 1954, Batch 1000/1000: LR=1.13e-06, Loss=2.66e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:30:52,510 | INFO | Epoch 1954 Train Time 23.199549436569214s

2025-10-19 04:31:15,225 | INFO | Training epoch 1955, Batch 1000/1000: LR=1.13e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:31:15,307 | INFO | Epoch 1955 Train Time 22.796377182006836s

2025-10-19 04:31:37,340 | INFO | Training epoch 1956, Batch 1000/1000: LR=1.12e-06, Loss=2.66e-02 BER=9.95e-03 FER=1.29e-01
2025-10-19 04:31:37,415 | INFO | Epoch 1956 Train Time 22.107072591781616s

2025-10-19 04:32:00,431 | INFO | Training epoch 1957, Batch 1000/1000: LR=1.12e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 04:32:00,505 | INFO | Epoch 1957 Train Time 23.088384866714478s

2025-10-19 04:32:23,826 | INFO | Training epoch 1958, Batch 1000/1000: LR=1.11e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:32:23,906 | INFO | Epoch 1958 Train Time 23.400155067443848s

2025-10-19 04:32:47,744 | INFO | Training epoch 1959, Batch 1000/1000: LR=1.11e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:32:47,828 | INFO | Epoch 1959 Train Time 23.91980266571045s

2025-10-19 04:33:11,949 | INFO | Training epoch 1960, Batch 1000/1000: LR=1.10e-06, Loss=2.69e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:33:12,028 | INFO | Epoch 1960 Train Time 24.199138402938843s

2025-10-19 04:33:35,330 | INFO | Training epoch 1961, Batch 1000/1000: LR=1.10e-06, Loss=2.67e-02 BER=1.00e-02 FER=1.29e-01
2025-10-19 04:33:35,408 | INFO | Epoch 1961 Train Time 23.37838363647461s

2025-10-19 04:33:59,439 | INFO | Training epoch 1962, Batch 1000/1000: LR=1.09e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:33:59,524 | INFO | Epoch 1962 Train Time 24.11497473716736s

2025-10-19 04:34:22,744 | INFO | Training epoch 1963, Batch 1000/1000: LR=1.09e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:34:22,824 | INFO | Epoch 1963 Train Time 23.29969811439514s

2025-10-19 04:34:45,845 | INFO | Training epoch 1964, Batch 1000/1000: LR=1.08e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:34:45,945 | INFO | Epoch 1964 Train Time 23.11931014060974s

2025-10-19 04:35:09,832 | INFO | Training epoch 1965, Batch 1000/1000: LR=1.08e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:35:09,913 | INFO | Epoch 1965 Train Time 23.966275691986084s

2025-10-19 04:35:33,046 | INFO | Training epoch 1966, Batch 1000/1000: LR=1.07e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.30e-01
2025-10-19 04:35:33,113 | INFO | Epoch 1966 Train Time 23.19845175743103s

2025-10-19 04:35:55,306 | INFO | Training epoch 1967, Batch 1000/1000: LR=1.07e-06, Loss=2.70e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 04:35:55,381 | INFO | Epoch 1967 Train Time 22.266722679138184s

2025-10-19 04:36:18,055 | INFO | Training epoch 1968, Batch 1000/1000: LR=1.07e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:36:18,151 | INFO | Epoch 1968 Train Time 22.768845796585083s

2025-10-19 04:36:41,039 | INFO | Training epoch 1969, Batch 1000/1000: LR=1.06e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:36:41,122 | INFO | Epoch 1969 Train Time 22.96930432319641s

2025-10-19 04:37:02,463 | INFO | Training epoch 1970, Batch 1000/1000: LR=1.06e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:37:02,552 | INFO | Epoch 1970 Train Time 21.42862367630005s

2025-10-19 04:37:23,748 | INFO | Training epoch 1971, Batch 1000/1000: LR=1.05e-06, Loss=2.67e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:37:23,838 | INFO | Epoch 1971 Train Time 21.28477907180786s

2025-10-19 04:37:46,905 | INFO | Training epoch 1972, Batch 1000/1000: LR=1.05e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:37:46,991 | INFO | Epoch 1972 Train Time 23.151164293289185s

2025-10-19 04:38:10,140 | INFO | Training epoch 1973, Batch 1000/1000: LR=1.05e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:38:10,212 | INFO | Epoch 1973 Train Time 23.2194504737854s

2025-10-19 04:38:32,528 | INFO | Training epoch 1974, Batch 1000/1000: LR=1.04e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:38:32,612 | INFO | Epoch 1974 Train Time 22.399651288986206s

2025-10-19 04:38:55,326 | INFO | Training epoch 1975, Batch 1000/1000: LR=1.04e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:38:55,411 | INFO | Epoch 1975 Train Time 22.79810905456543s

2025-10-19 04:39:19,222 | INFO | Training epoch 1976, Batch 1000/1000: LR=1.04e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:39:19,293 | INFO | Epoch 1976 Train Time 23.880447149276733s

2025-10-19 04:39:40,552 | INFO | Training epoch 1977, Batch 1000/1000: LR=1.04e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:39:40,644 | INFO | Epoch 1977 Train Time 21.347839832305908s

2025-10-19 04:40:03,329 | INFO | Training epoch 1978, Batch 1000/1000: LR=1.03e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:40:03,399 | INFO | Epoch 1978 Train Time 22.755175590515137s

2025-10-19 04:40:26,370 | INFO | Training epoch 1979, Batch 1000/1000: LR=1.03e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:40:26,446 | INFO | Epoch 1979 Train Time 23.044875621795654s

2025-10-19 04:40:49,043 | INFO | Training epoch 1980, Batch 1000/1000: LR=1.03e-06, Loss=2.66e-02 BER=9.97e-03 FER=1.29e-01
2025-10-19 04:40:49,134 | INFO | Epoch 1980 Train Time 22.68763780593872s

2025-10-19 04:41:13,614 | INFO | Training epoch 1981, Batch 1000/1000: LR=1.02e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-19 04:41:13,716 | INFO | Epoch 1981 Train Time 24.579826831817627s

2025-10-19 04:41:36,542 | INFO | Training epoch 1982, Batch 1000/1000: LR=1.02e-06, Loss=2.68e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:41:36,631 | INFO | Epoch 1982 Train Time 22.91391134262085s

2025-10-19 04:41:58,746 | INFO | Training epoch 1983, Batch 1000/1000: LR=1.02e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:41:58,823 | INFO | Epoch 1983 Train Time 22.19087243080139s

2025-10-19 04:42:22,161 | INFO | Training epoch 1984, Batch 1000/1000: LR=1.02e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:42:22,240 | INFO | Epoch 1984 Train Time 23.416145086288452s

2025-10-19 04:42:44,424 | INFO | Training epoch 1985, Batch 1000/1000: LR=1.02e-06, Loss=2.69e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:42:44,494 | INFO | Epoch 1985 Train Time 22.253127098083496s

2025-10-19 04:43:08,420 | INFO | Training epoch 1986, Batch 1000/1000: LR=1.01e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:43:08,497 | INFO | Epoch 1986 Train Time 24.001144647598267s

2025-10-19 04:43:31,834 | INFO | Training epoch 1987, Batch 1000/1000: LR=1.01e-06, Loss=2.71e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:43:31,915 | INFO | Epoch 1987 Train Time 23.416435480117798s

2025-10-19 04:43:55,136 | INFO | Training epoch 1988, Batch 1000/1000: LR=1.01e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:43:55,232 | INFO | Epoch 1988 Train Time 23.31404948234558s

2025-10-19 04:44:18,451 | INFO | Training epoch 1989, Batch 1000/1000: LR=1.01e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 04:44:18,526 | INFO | Epoch 1989 Train Time 23.292787551879883s

2025-10-19 04:44:42,034 | INFO | Training epoch 1990, Batch 1000/1000: LR=1.01e-06, Loss=2.65e-02 BER=1.00e-02 FER=1.29e-01
2025-10-19 04:44:42,118 | INFO | Epoch 1990 Train Time 23.590994834899902s

2025-10-19 04:45:04,927 | INFO | Training epoch 1991, Batch 1000/1000: LR=1.01e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:45:05,008 | INFO | Epoch 1991 Train Time 22.888697624206543s

2025-10-19 04:45:27,906 | INFO | Training epoch 1992, Batch 1000/1000: LR=1.00e-06, Loss=2.69e-02 BER=1.00e-02 FER=1.29e-01
2025-10-19 04:45:27,980 | INFO | Epoch 1992 Train Time 22.97086477279663s

2025-10-19 04:45:48,426 | INFO | Training epoch 1993, Batch 1000/1000: LR=1.00e-06, Loss=2.71e-02 BER=1.03e-02 FER=1.32e-01
2025-10-19 04:45:48,523 | INFO | Epoch 1993 Train Time 20.542069673538208s

2025-10-19 04:46:08,129 | INFO | Training epoch 1994, Batch 1000/1000: LR=1.00e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:46:08,218 | INFO | Epoch 1994 Train Time 19.694177865982056s

2025-10-19 04:46:29,417 | INFO | Training epoch 1995, Batch 1000/1000: LR=1.00e-06, Loss=2.71e-02 BER=1.02e-02 FER=1.31e-01
2025-10-19 04:46:29,492 | INFO | Epoch 1995 Train Time 21.272910356521606s

2025-10-19 04:46:52,335 | INFO | Training epoch 1996, Batch 1000/1000: LR=1.00e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.30e-01
2025-10-19 04:46:52,409 | INFO | Epoch 1996 Train Time 22.91616940498352s

2025-10-19 04:47:15,859 | INFO | Training epoch 1997, Batch 1000/1000: LR=1.00e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-19 04:47:15,947 | INFO | Epoch 1997 Train Time 23.53602170944214s

2025-10-19 04:47:39,338 | INFO | Training epoch 1998, Batch 1000/1000: LR=1.00e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-19 04:47:39,421 | INFO | Epoch 1998 Train Time 23.473045110702515s

2025-10-19 04:48:03,328 | INFO | Training epoch 1999, Batch 1000/1000: LR=1.00e-06, Loss=2.69e-02 BER=1.01e-02 FER=1.31e-01
2025-10-19 04:48:03,408 | INFO | Epoch 1999 Train Time 23.986184120178223s

2025-10-19 04:48:27,361 | INFO | Training epoch 2000, Batch 1000/1000: LR=1.00e-06, Loss=2.69e-02 BER=1.00e-02 FER=1.30e-01
2025-10-19 04:48:27,432 | INFO | Epoch 2000 Train Time 24.021695852279663s

2025-10-19 04:48:27,439 | INFO | Checkpoint saved: runs/20251018_155726/stage1_fp32__BCH_n31_k16__Ndec2_d16_h8.pth
2025-10-19 04:48:27,443 | INFO | Checkpoint saved: runs/20251018_155726/stage1_fp32__BCH_n31_k16__Ndec2_d16_h8__e2000_loss0.026891.pth
2025-10-19 04:48:33,978 | INFO | FER count threshold reached for EbN0:4
2025-10-19 04:48:34,075 | INFO | Test EbN0=4, BER=1.52e-02
2025-10-19 04:48:40,081 | INFO | FER count threshold reached for EbN0:5
2025-10-19 04:48:40,189 | INFO | Test EbN0=5, BER=4.60e-03
2025-10-19 04:48:47,013 | INFO | FER count threshold reached for EbN0:6
2025-10-19 04:48:47,109 | INFO | Test EbN0=6, BER=9.60e-04
2025-10-19 04:48:47,110 | INFO | 
Test Loss 4: 4.0175e-02 5: 1.3075e-02 6: 3.1618e-03
2025-10-19 04:48:47,110 | INFO | Test FER 4: 2.0593e-01 5: 7.5305e-02 6: 1.8834e-02
2025-10-19 04:48:47,110 | INFO | Test BER 4: 1.5246e-02 5: 4.6025e-03 6: 9.6049e-04
2025-10-19 04:48:47,110 | INFO | Test -ln(BER) 4: 4.1834e+00 5: 5.3812e+00 6: 6.9481e+00
2025-10-19 04:48:47,110 | INFO | # of testing samples: [100352.0, 100352.0, 100352.0]
 Test Time 19.667028427124023 s

2025-10-19 04:48:47,193 | INFO | Loaded checkpoint: runs/20251018_155726/stage1_fp32__BCH_n31_k16__Ndec2_d16_h8.pth (strict=False)
2025-10-19 04:49:23,229 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=6.58e-02 BER=2.34e-02 FER=3.36e-01
2025-10-19 04:49:23,291 | INFO | Epoch 1 Train Time 36.09675931930542s

2025-10-19 04:49:23,293 | INFO | [P2] saving best_model (QAT) with loss 0.065764 at epoch 1
2025-10-19 04:49:59,275 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=4.65e-02 BER=1.70e-02 FER=2.47e-01
2025-10-19 04:49:59,367 | INFO | Epoch 2 Train Time 36.05923795700073s

2025-10-19 04:49:59,368 | INFO | [P2] saving best_model (QAT) with loss 0.046545 at epoch 2
2025-10-19 04:50:35,065 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=4.26e-02 BER=1.57e-02 FER=2.18e-01
2025-10-19 04:50:35,164 | INFO | Epoch 3 Train Time 35.77683186531067s

2025-10-19 04:50:35,164 | INFO | [P2] saving best_model (QAT) with loss 0.042640 at epoch 3
2025-10-19 04:51:11,121 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=4.16e-02 BER=1.53e-02 FER=2.08e-01
2025-10-19 04:51:11,206 | INFO | Epoch 4 Train Time 36.024587869644165s

2025-10-19 04:51:11,207 | INFO | [P2] saving best_model (QAT) with loss 0.041569 at epoch 4
2025-10-19 04:51:47,024 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=4.04e-02 BER=1.49e-02 FER=2.01e-01
2025-10-19 04:51:47,113 | INFO | Epoch 5 Train Time 35.89101219177246s

2025-10-19 04:51:47,113 | INFO | [P2] saving best_model (QAT) with loss 0.040366 at epoch 5
2025-10-19 04:52:23,227 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=4.00e-02 BER=1.47e-02 FER=1.97e-01
2025-10-19 04:52:23,325 | INFO | Epoch 6 Train Time 36.1883819103241s

2025-10-19 04:52:23,325 | INFO | [P2] saving best_model (QAT) with loss 0.040014 at epoch 6
2025-10-19 04:52:58,850 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=3.96e-02 BER=1.46e-02 FER=1.96e-01
2025-10-19 04:52:58,921 | INFO | Epoch 7 Train Time 35.565510272979736s

2025-10-19 04:52:58,922 | INFO | [P2] saving best_model (QAT) with loss 0.039601 at epoch 7
2025-10-19 04:53:34,629 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=3.88e-02 BER=1.43e-02 FER=1.91e-01
2025-10-19 04:53:34,722 | INFO | Epoch 8 Train Time 35.77767491340637s

2025-10-19 04:53:34,723 | INFO | [P2] saving best_model (QAT) with loss 0.038830 at epoch 8
2025-10-19 04:54:10,726 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=3.82e-02 BER=1.42e-02 FER=1.90e-01
2025-10-19 04:54:10,808 | INFO | Epoch 9 Train Time 36.067851066589355s

2025-10-19 04:54:10,808 | INFO | [P2] saving best_model (QAT) with loss 0.038182 at epoch 9
2025-10-19 04:54:46,231 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=3.77e-02 BER=1.40e-02 FER=1.87e-01
2025-10-19 04:54:46,344 | INFO | Epoch 10 Train Time 35.506542682647705s

2025-10-19 04:54:46,344 | INFO | [P2] saving best_model (QAT) with loss 0.037728 at epoch 10
2025-10-19 04:55:22,424 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=3.69e-02 BER=1.37e-02 FER=1.84e-01
2025-10-19 04:55:22,509 | INFO | Epoch 11 Train Time 36.14842915534973s

2025-10-19 04:55:22,509 | INFO | [P2] saving best_model (QAT) with loss 0.036851 at epoch 11
2025-10-19 04:55:57,948 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=3.78e-02 BER=1.41e-02 FER=1.88e-01
2025-10-19 04:55:58,045 | INFO | Epoch 12 Train Time 35.515869140625s

2025-10-19 04:56:33,951 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=3.72e-02 BER=1.38e-02 FER=1.84e-01
2025-10-19 04:56:34,061 | INFO | Epoch 13 Train Time 36.014347553253174s

2025-10-19 04:57:09,843 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=3.66e-02 BER=1.36e-02 FER=1.83e-01
2025-10-19 04:57:09,960 | INFO | Epoch 14 Train Time 35.89845824241638s

2025-10-19 04:57:09,961 | INFO | [P2] saving best_model (QAT) with loss 0.036638 at epoch 14
2025-10-19 04:57:46,122 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=3.62e-02 BER=1.34e-02 FER=1.83e-01
2025-10-19 04:57:46,224 | INFO | Epoch 15 Train Time 36.242281913757324s

2025-10-19 04:57:46,225 | INFO | [P2] saving best_model (QAT) with loss 0.036186 at epoch 15
2025-10-19 04:58:22,123 | INFO | Training epoch 16, Batch 1000/1000: LR=1.00e-04, Loss=3.60e-02 BER=1.34e-02 FER=1.80e-01
2025-10-19 04:58:22,209 | INFO | Epoch 16 Train Time 35.96780490875244s

2025-10-19 04:58:22,210 | INFO | [P2] saving best_model (QAT) with loss 0.035983 at epoch 16
2025-10-19 04:58:58,322 | INFO | Training epoch 17, Batch 1000/1000: LR=1.00e-04, Loss=3.57e-02 BER=1.33e-02 FER=1.79e-01
2025-10-19 04:58:58,400 | INFO | Epoch 17 Train Time 36.17367482185364s

2025-10-19 04:58:58,401 | INFO | [P2] saving best_model (QAT) with loss 0.035655 at epoch 17
2025-10-19 04:59:35,354 | INFO | Training epoch 18, Batch 1000/1000: LR=1.00e-04, Loss=3.53e-02 BER=1.31e-02 FER=1.77e-01
2025-10-19 04:59:35,451 | INFO | Epoch 18 Train Time 37.024287939071655s

2025-10-19 04:59:35,452 | INFO | [P2] saving best_model (QAT) with loss 0.035284 at epoch 18
2025-10-19 05:00:11,339 | INFO | Training epoch 19, Batch 1000/1000: LR=1.00e-04, Loss=3.52e-02 BER=1.31e-02 FER=1.75e-01
2025-10-19 05:00:11,423 | INFO | Epoch 19 Train Time 35.955209732055664s

2025-10-19 05:00:11,424 | INFO | [P2] saving best_model (QAT) with loss 0.035232 at epoch 19
2025-10-19 05:00:47,655 | INFO | Training epoch 20, Batch 1000/1000: LR=1.00e-04, Loss=3.51e-02 BER=1.31e-02 FER=1.75e-01
2025-10-19 05:00:47,736 | INFO | Epoch 20 Train Time 36.2968635559082s

2025-10-19 05:00:47,737 | INFO | [P2] saving best_model (QAT) with loss 0.035105 at epoch 20
2025-10-19 05:01:22,428 | INFO | Training epoch 21, Batch 1000/1000: LR=1.00e-04, Loss=3.50e-02 BER=1.29e-02 FER=1.74e-01
2025-10-19 05:01:22,498 | INFO | Epoch 21 Train Time 34.74451780319214s

2025-10-19 05:01:22,499 | INFO | [P2] saving best_model (QAT) with loss 0.034971 at epoch 21
2025-10-19 05:01:58,118 | INFO | Training epoch 22, Batch 1000/1000: LR=1.00e-04, Loss=3.49e-02 BER=1.30e-02 FER=1.74e-01
2025-10-19 05:01:58,201 | INFO | Epoch 22 Train Time 35.685142278671265s

2025-10-19 05:01:58,202 | INFO | [P2] saving best_model (QAT) with loss 0.034883 at epoch 22
2025-10-19 05:02:34,224 | INFO | Training epoch 23, Batch 1000/1000: LR=1.00e-04, Loss=3.52e-02 BER=1.31e-02 FER=1.76e-01
2025-10-19 05:02:34,324 | INFO | Epoch 23 Train Time 36.10677099227905s

2025-10-19 05:03:10,268 | INFO | Training epoch 24, Batch 1000/1000: LR=1.00e-04, Loss=3.41e-02 BER=1.28e-02 FER=1.72e-01
2025-10-19 05:03:10,363 | INFO | Epoch 24 Train Time 36.038819789886475s

2025-10-19 05:03:10,364 | INFO | [P2] saving best_model (QAT) with loss 0.034076 at epoch 24
2025-10-19 05:03:46,671 | INFO | Training epoch 25, Batch 1000/1000: LR=1.00e-04, Loss=3.42e-02 BER=1.28e-02 FER=1.72e-01
2025-10-19 05:03:46,764 | INFO | Epoch 25 Train Time 36.38203501701355s

2025-10-19 05:04:25,275 | INFO | Training epoch 26, Batch 1000/1000: LR=1.00e-04, Loss=3.46e-02 BER=1.29e-02 FER=1.73e-01
2025-10-19 05:04:25,354 | INFO | Epoch 26 Train Time 38.58915996551514s

2025-10-19 05:05:02,967 | INFO | Training epoch 27, Batch 1000/1000: LR=1.00e-04, Loss=3.45e-02 BER=1.28e-02 FER=1.71e-01
2025-10-19 05:05:03,057 | INFO | Epoch 27 Train Time 37.70127820968628s

2025-10-19 05:05:40,237 | INFO | Training epoch 28, Batch 1000/1000: LR=1.00e-04, Loss=3.40e-02 BER=1.26e-02 FER=1.70e-01
2025-10-19 05:05:40,342 | INFO | Epoch 28 Train Time 37.283180236816406s

2025-10-19 05:05:40,342 | INFO | [P2] saving best_model (QAT) with loss 0.033968 at epoch 28
2025-10-19 05:06:17,751 | INFO | Training epoch 29, Batch 1000/1000: LR=1.00e-04, Loss=3.43e-02 BER=1.28e-02 FER=1.71e-01
2025-10-19 05:06:17,851 | INFO | Epoch 29 Train Time 37.49215006828308s

2025-10-19 05:06:54,542 | INFO | Training epoch 30, Batch 1000/1000: LR=9.99e-05, Loss=3.37e-02 BER=1.26e-02 FER=1.70e-01
2025-10-19 05:06:54,623 | INFO | Epoch 30 Train Time 36.77035450935364s

2025-10-19 05:06:54,625 | INFO | [P2] saving best_model (QAT) with loss 0.033651 at epoch 30
2025-10-19 05:07:32,035 | INFO | Training epoch 31, Batch 1000/1000: LR=9.99e-05, Loss=3.34e-02 BER=1.24e-02 FER=1.68e-01
2025-10-19 05:07:32,140 | INFO | Epoch 31 Train Time 37.47675395011902s

2025-10-19 05:07:32,140 | INFO | [P2] saving best_model (QAT) with loss 0.033419 at epoch 31
2025-10-19 05:08:09,442 | INFO | Training epoch 32, Batch 1000/1000: LR=9.99e-05, Loss=3.42e-02 BER=1.27e-02 FER=1.70e-01
2025-10-19 05:08:09,543 | INFO | Epoch 32 Train Time 37.38225531578064s

2025-10-19 05:08:46,780 | INFO | Training epoch 33, Batch 1000/1000: LR=9.99e-05, Loss=3.37e-02 BER=1.26e-02 FER=1.69e-01
2025-10-19 05:08:46,865 | INFO | Epoch 33 Train Time 37.32063817977905s

2025-10-19 05:09:24,458 | INFO | Training epoch 34, Batch 1000/1000: LR=9.99e-05, Loss=3.37e-02 BER=1.26e-02 FER=1.69e-01
2025-10-19 05:09:24,562 | INFO | Epoch 34 Train Time 37.69583797454834s

2025-10-19 05:10:02,425 | INFO | Training epoch 35, Batch 1000/1000: LR=9.99e-05, Loss=3.36e-02 BER=1.26e-02 FER=1.68e-01
2025-10-19 05:10:02,531 | INFO | Epoch 35 Train Time 37.96783256530762s

2025-10-19 05:10:38,528 | INFO | Training epoch 36, Batch 1000/1000: LR=9.99e-05, Loss=3.37e-02 BER=1.26e-02 FER=1.68e-01
2025-10-19 05:10:38,607 | INFO | Epoch 36 Train Time 36.074054479599s

2025-10-19 05:11:14,733 | INFO | Training epoch 37, Batch 1000/1000: LR=9.99e-05, Loss=3.36e-02 BER=1.26e-02 FER=1.68e-01
2025-10-19 05:11:14,819 | INFO | Epoch 37 Train Time 36.21058487892151s

2025-10-19 05:11:50,045 | INFO | Training epoch 38, Batch 1000/1000: LR=9.99e-05, Loss=3.36e-02 BER=1.25e-02 FER=1.68e-01
2025-10-19 05:11:50,122 | INFO | Epoch 38 Train Time 35.3014349937439s

2025-10-19 05:12:26,022 | INFO | Training epoch 39, Batch 1000/1000: LR=9.99e-05, Loss=3.33e-02 BER=1.25e-02 FER=1.67e-01
2025-10-19 05:12:26,116 | INFO | Epoch 39 Train Time 35.99387001991272s

2025-10-19 05:12:26,118 | INFO | [P2] saving best_model (QAT) with loss 0.033263 at epoch 39
2025-10-19 05:13:00,815 | INFO | Training epoch 40, Batch 1000/1000: LR=9.99e-05, Loss=3.34e-02 BER=1.25e-02 FER=1.68e-01
2025-10-19 05:13:00,890 | INFO | Epoch 40 Train Time 34.7560601234436s

2025-10-19 05:13:37,074 | INFO | Training epoch 41, Batch 1000/1000: LR=9.99e-05, Loss=3.37e-02 BER=1.26e-02 FER=1.68e-01
2025-10-19 05:13:37,164 | INFO | Epoch 41 Train Time 36.27289962768555s

2025-10-19 05:14:13,164 | INFO | Training epoch 42, Batch 1000/1000: LR=9.99e-05, Loss=3.31e-02 BER=1.23e-02 FER=1.66e-01
2025-10-19 05:14:13,249 | INFO | Epoch 42 Train Time 36.08369588851929s

2025-10-19 05:14:13,251 | INFO | [P2] saving best_model (QAT) with loss 0.033058 at epoch 42
2025-10-19 05:14:49,165 | INFO | Training epoch 43, Batch 1000/1000: LR=9.99e-05, Loss=3.38e-02 BER=1.26e-02 FER=1.68e-01
2025-10-19 05:14:49,258 | INFO | Epoch 43 Train Time 35.99246621131897s

2025-10-19 05:15:25,424 | INFO | Training epoch 44, Batch 1000/1000: LR=9.99e-05, Loss=3.34e-02 BER=1.24e-02 FER=1.66e-01
2025-10-19 05:15:25,520 | INFO | Epoch 44 Train Time 36.26064419746399s

2025-10-19 05:16:01,527 | INFO | Training epoch 45, Batch 1000/1000: LR=9.99e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.65e-01
2025-10-19 05:16:01,611 | INFO | Epoch 45 Train Time 36.08871388435364s

2025-10-19 05:16:01,612 | INFO | [P2] saving best_model (QAT) with loss 0.032789 at epoch 45
2025-10-19 05:16:37,563 | INFO | Training epoch 46, Batch 1000/1000: LR=9.99e-05, Loss=3.28e-02 BER=1.22e-02 FER=1.64e-01
2025-10-19 05:16:37,635 | INFO | Epoch 46 Train Time 36.00058937072754s

2025-10-19 05:16:37,636 | INFO | [P2] saving best_model (QAT) with loss 0.032758 at epoch 46
2025-10-19 05:17:13,825 | INFO | Training epoch 47, Batch 1000/1000: LR=9.99e-05, Loss=3.37e-02 BER=1.26e-02 FER=1.68e-01
2025-10-19 05:17:13,930 | INFO | Epoch 47 Train Time 36.27417826652527s

2025-10-19 05:17:49,058 | INFO | Training epoch 48, Batch 1000/1000: LR=9.99e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.64e-01
2025-10-19 05:17:49,145 | INFO | Epoch 48 Train Time 35.213531494140625s

2025-10-19 05:18:24,726 | INFO | Training epoch 49, Batch 1000/1000: LR=9.99e-05, Loss=3.30e-02 BER=1.24e-02 FER=1.65e-01
2025-10-19 05:18:24,820 | INFO | Epoch 49 Train Time 35.67446994781494s

2025-10-19 05:19:00,215 | INFO | Training epoch 50, Batch 1000/1000: LR=9.99e-05, Loss=3.33e-02 BER=1.25e-02 FER=1.66e-01
2025-10-19 05:19:00,282 | INFO | Epoch 50 Train Time 35.460328817367554s

2025-10-19 05:19:36,649 | INFO | Training epoch 51, Batch 1000/1000: LR=9.98e-05, Loss=3.31e-02 BER=1.24e-02 FER=1.66e-01
2025-10-19 05:19:36,718 | INFO | Epoch 51 Train Time 36.434218883514404s

2025-10-19 05:20:12,345 | INFO | Training epoch 52, Batch 1000/1000: LR=9.98e-05, Loss=3.26e-02 BER=1.22e-02 FER=1.65e-01
2025-10-19 05:20:12,439 | INFO | Epoch 52 Train Time 35.720043659210205s

2025-10-19 05:20:12,440 | INFO | [P2] saving best_model (QAT) with loss 0.032609 at epoch 52
2025-10-19 05:20:48,035 | INFO | Training epoch 53, Batch 1000/1000: LR=9.98e-05, Loss=3.32e-02 BER=1.24e-02 FER=1.65e-01
2025-10-19 05:20:48,131 | INFO | Epoch 53 Train Time 35.676804065704346s

2025-10-19 05:21:24,865 | INFO | Training epoch 54, Batch 1000/1000: LR=9.98e-05, Loss=3.34e-02 BER=1.25e-02 FER=1.66e-01
2025-10-19 05:21:24,938 | INFO | Epoch 54 Train Time 36.80632758140564s

2025-10-19 05:22:00,591 | INFO | Training epoch 55, Batch 1000/1000: LR=9.98e-05, Loss=3.29e-02 BER=1.23e-02 FER=1.64e-01
2025-10-19 05:22:00,660 | INFO | Epoch 55 Train Time 35.719064474105835s

2025-10-19 05:22:36,634 | INFO | Training epoch 56, Batch 1000/1000: LR=9.98e-05, Loss=3.32e-02 BER=1.24e-02 FER=1.65e-01
2025-10-19 05:22:36,728 | INFO | Epoch 56 Train Time 36.0676109790802s

2025-10-19 05:23:12,754 | INFO | Training epoch 57, Batch 1000/1000: LR=9.98e-05, Loss=3.31e-02 BER=1.24e-02 FER=1.65e-01
2025-10-19 05:23:12,857 | INFO | Epoch 57 Train Time 36.127835750579834s

2025-10-19 05:23:48,665 | INFO | Training epoch 58, Batch 1000/1000: LR=9.98e-05, Loss=3.25e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:23:48,751 | INFO | Epoch 58 Train Time 35.892101526260376s

2025-10-19 05:23:48,752 | INFO | [P2] saving best_model (QAT) with loss 0.032539 at epoch 58
2025-10-19 05:24:24,421 | INFO | Training epoch 59, Batch 1000/1000: LR=9.98e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:24:24,528 | INFO | Epoch 59 Train Time 35.76094388961792s

2025-10-19 05:25:00,231 | INFO | Training epoch 60, Batch 1000/1000: LR=9.98e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:25:00,317 | INFO | Epoch 60 Train Time 35.7880277633667s

2025-10-19 05:25:36,719 | INFO | Training epoch 61, Batch 1000/1000: LR=9.98e-05, Loss=3.26e-02 BER=1.22e-02 FER=1.62e-01
2025-10-19 05:25:36,797 | INFO | Epoch 61 Train Time 36.47936701774597s

2025-10-19 05:26:12,327 | INFO | Training epoch 62, Batch 1000/1000: LR=9.98e-05, Loss=3.30e-02 BER=1.24e-02 FER=1.66e-01
2025-10-19 05:26:12,406 | INFO | Epoch 62 Train Time 35.60650563240051s

2025-10-19 05:26:47,727 | INFO | Training epoch 63, Batch 1000/1000: LR=9.98e-05, Loss=3.26e-02 BER=1.23e-02 FER=1.64e-01
2025-10-19 05:26:47,801 | INFO | Epoch 63 Train Time 35.39383387565613s

2025-10-19 05:27:23,420 | INFO | Training epoch 64, Batch 1000/1000: LR=9.98e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.63e-01
2025-10-19 05:27:23,519 | INFO | Epoch 64 Train Time 35.71660542488098s

2025-10-19 05:27:23,520 | INFO | [P2] saving best_model (QAT) with loss 0.032242 at epoch 64
2025-10-19 05:27:59,831 | INFO | Training epoch 65, Batch 1000/1000: LR=9.98e-05, Loss=3.25e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:27:59,921 | INFO | Epoch 65 Train Time 36.37716460227966s

2025-10-19 05:28:35,750 | INFO | Training epoch 66, Batch 1000/1000: LR=9.97e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.65e-01
2025-10-19 05:28:35,842 | INFO | Epoch 66 Train Time 35.918962478637695s

2025-10-19 05:29:11,315 | INFO | Training epoch 67, Batch 1000/1000: LR=9.97e-05, Loss=3.26e-02 BER=1.21e-02 FER=1.62e-01
2025-10-19 05:29:11,408 | INFO | Epoch 67 Train Time 35.56510877609253s

2025-10-19 05:29:47,255 | INFO | Training epoch 68, Batch 1000/1000: LR=9.97e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.63e-01
2025-10-19 05:29:47,350 | INFO | Epoch 68 Train Time 35.94014358520508s

2025-10-19 05:30:23,226 | INFO | Training epoch 69, Batch 1000/1000: LR=9.97e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:30:23,293 | INFO | Epoch 69 Train Time 35.94282126426697s

2025-10-19 05:30:59,035 | INFO | Training epoch 70, Batch 1000/1000: LR=9.97e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:30:59,105 | INFO | Epoch 70 Train Time 35.810447216033936s

2025-10-19 05:31:35,423 | INFO | Training epoch 71, Batch 1000/1000: LR=9.97e-05, Loss=3.31e-02 BER=1.24e-02 FER=1.65e-01
2025-10-19 05:31:35,497 | INFO | Epoch 71 Train Time 36.3918137550354s

2025-10-19 05:32:11,014 | INFO | Training epoch 72, Batch 1000/1000: LR=9.97e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.64e-01
2025-10-19 05:32:11,101 | INFO | Epoch 72 Train Time 35.602761030197144s

2025-10-19 05:32:46,627 | INFO | Training epoch 73, Batch 1000/1000: LR=9.97e-05, Loss=3.25e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:32:46,715 | INFO | Epoch 73 Train Time 35.61359477043152s

2025-10-19 05:33:22,325 | INFO | Training epoch 74, Batch 1000/1000: LR=9.97e-05, Loss=3.25e-02 BER=1.22e-02 FER=1.61e-01
2025-10-19 05:33:22,415 | INFO | Epoch 74 Train Time 35.698808431625366s

2025-10-19 05:33:57,528 | INFO | Training epoch 75, Batch 1000/1000: LR=9.97e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.62e-01
2025-10-19 05:33:57,605 | INFO | Epoch 75 Train Time 35.18890357017517s

2025-10-19 05:34:33,437 | INFO | Training epoch 76, Batch 1000/1000: LR=9.97e-05, Loss=3.25e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:34:33,532 | INFO | Epoch 76 Train Time 35.92578482627869s

2025-10-19 05:35:09,632 | INFO | Training epoch 77, Batch 1000/1000: LR=9.96e-05, Loss=3.26e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:35:09,706 | INFO | Epoch 77 Train Time 36.171316146850586s

2025-10-19 05:35:45,731 | INFO | Training epoch 78, Batch 1000/1000: LR=9.96e-05, Loss=3.24e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:35:45,806 | INFO | Epoch 78 Train Time 36.09916663169861s

2025-10-19 05:36:21,810 | INFO | Training epoch 79, Batch 1000/1000: LR=9.96e-05, Loss=3.28e-02 BER=1.23e-02 FER=1.63e-01
2025-10-19 05:36:21,890 | INFO | Epoch 79 Train Time 36.08281946182251s

2025-10-19 05:36:57,724 | INFO | Training epoch 80, Batch 1000/1000: LR=9.96e-05, Loss=3.26e-02 BER=1.22e-02 FER=1.63e-01
2025-10-19 05:36:57,817 | INFO | Epoch 80 Train Time 35.925095319747925s

2025-10-19 05:37:33,828 | INFO | Training epoch 81, Batch 1000/1000: LR=9.96e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:37:33,932 | INFO | Epoch 81 Train Time 36.114054679870605s

2025-10-19 05:38:09,821 | INFO | Training epoch 82, Batch 1000/1000: LR=9.96e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.62e-01
2025-10-19 05:38:09,906 | INFO | Epoch 82 Train Time 35.973628282547s

2025-10-19 05:38:45,326 | INFO | Training epoch 83, Batch 1000/1000: LR=9.96e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 05:38:45,428 | INFO | Epoch 83 Train Time 35.520482540130615s

2025-10-19 05:38:45,430 | INFO | [P2] saving best_model (QAT) with loss 0.032124 at epoch 83
2025-10-19 05:39:21,426 | INFO | Training epoch 84, Batch 1000/1000: LR=9.96e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:39:21,519 | INFO | Epoch 84 Train Time 36.068564653396606s

2025-10-19 05:39:56,623 | INFO | Training epoch 85, Batch 1000/1000: LR=9.96e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 05:39:56,701 | INFO | Epoch 85 Train Time 35.18093729019165s

2025-10-19 05:39:56,702 | INFO | [P2] saving best_model (QAT) with loss 0.032066 at epoch 85
2025-10-19 05:40:32,524 | INFO | Training epoch 86, Batch 1000/1000: LR=9.96e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 05:40:32,605 | INFO | Epoch 86 Train Time 35.886303663253784s

2025-10-19 05:40:32,606 | INFO | [P2] saving best_model (QAT) with loss 0.031992 at epoch 86
2025-10-19 05:41:08,732 | INFO | Training epoch 87, Batch 1000/1000: LR=9.95e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 05:41:08,824 | INFO | Epoch 87 Train Time 36.198508739471436s

2025-10-19 05:41:08,826 | INFO | [P2] saving best_model (QAT) with loss 0.031939 at epoch 87
2025-10-19 05:41:44,629 | INFO | Training epoch 88, Batch 1000/1000: LR=9.95e-05, Loss=3.25e-02 BER=1.21e-02 FER=1.62e-01
2025-10-19 05:41:44,724 | INFO | Epoch 88 Train Time 35.879169940948486s

2025-10-19 05:42:20,360 | INFO | Training epoch 89, Batch 1000/1000: LR=9.95e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:42:20,428 | INFO | Epoch 89 Train Time 35.70172929763794s

2025-10-19 05:42:56,238 | INFO | Training epoch 90, Batch 1000/1000: LR=9.95e-05, Loss=3.27e-02 BER=1.22e-02 FER=1.62e-01
2025-10-19 05:42:56,334 | INFO | Epoch 90 Train Time 35.90448832511902s

2025-10-19 05:43:32,177 | INFO | Training epoch 91, Batch 1000/1000: LR=9.95e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 05:43:32,268 | INFO | Epoch 91 Train Time 35.93286848068237s

2025-10-19 05:43:32,270 | INFO | [P2] saving best_model (QAT) with loss 0.031876 at epoch 91
2025-10-19 05:44:08,127 | INFO | Training epoch 92, Batch 1000/1000: LR=9.95e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:44:08,199 | INFO | Epoch 92 Train Time 35.914535999298096s

2025-10-19 05:44:43,728 | INFO | Training epoch 93, Batch 1000/1000: LR=9.95e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:44:43,821 | INFO | Epoch 93 Train Time 35.62079429626465s

2025-10-19 05:45:19,340 | INFO | Training epoch 94, Batch 1000/1000: LR=9.95e-05, Loss=3.20e-02 BER=1.21e-02 FER=1.62e-01
2025-10-19 05:45:19,408 | INFO | Epoch 94 Train Time 35.58718538284302s

2025-10-19 05:45:55,222 | INFO | Training epoch 95, Batch 1000/1000: LR=9.95e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:45:55,294 | INFO | Epoch 95 Train Time 35.88426470756531s

2025-10-19 05:46:30,128 | INFO | Training epoch 96, Batch 1000/1000: LR=9.94e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 05:46:30,200 | INFO | Epoch 96 Train Time 34.905067920684814s

2025-10-19 05:47:05,936 | INFO | Training epoch 97, Batch 1000/1000: LR=9.94e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 05:47:06,013 | INFO | Epoch 97 Train Time 35.81131029129028s

2025-10-19 05:47:41,832 | INFO | Training epoch 98, Batch 1000/1000: LR=9.94e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.59e-01
2025-10-19 05:47:41,923 | INFO | Epoch 98 Train Time 35.9082396030426s

2025-10-19 05:47:41,924 | INFO | [P2] saving best_model (QAT) with loss 0.031585 at epoch 98
2025-10-19 05:48:17,646 | INFO | Training epoch 99, Batch 1000/1000: LR=9.94e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 05:48:17,725 | INFO | Epoch 99 Train Time 35.77531313896179s

2025-10-19 05:48:53,325 | INFO | Training epoch 100, Batch 1000/1000: LR=9.94e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:48:53,396 | INFO | Epoch 100 Train Time 35.6694393157959s

2025-10-19 05:49:29,272 | INFO | Training epoch 101, Batch 1000/1000: LR=9.94e-05, Loss=3.21e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 05:49:29,339 | INFO | Epoch 101 Train Time 35.941402435302734s

2025-10-19 05:50:04,617 | INFO | Training epoch 102, Batch 1000/1000: LR=9.94e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 05:50:04,700 | INFO | Epoch 102 Train Time 35.36037087440491s

2025-10-19 05:50:40,542 | INFO | Training epoch 103, Batch 1000/1000: LR=9.94e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:50:40,623 | INFO | Epoch 103 Train Time 35.92121338844299s

2025-10-19 05:51:15,137 | INFO | Training epoch 104, Batch 1000/1000: LR=9.94e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 05:51:15,207 | INFO | Epoch 104 Train Time 34.58358144760132s

2025-10-19 05:51:50,732 | INFO | Training epoch 105, Batch 1000/1000: LR=9.93e-05, Loss=3.18e-02 BER=1.20e-02 FER=1.58e-01
2025-10-19 05:51:50,818 | INFO | Epoch 105 Train Time 35.609325647354126s

2025-10-19 05:52:25,649 | INFO | Training epoch 106, Batch 1000/1000: LR=9.93e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 05:52:25,732 | INFO | Epoch 106 Train Time 34.912585496902466s

2025-10-19 05:53:02,026 | INFO | Training epoch 107, Batch 1000/1000: LR=9.93e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 05:53:02,106 | INFO | Epoch 107 Train Time 36.37322235107422s

2025-10-19 05:53:37,730 | INFO | Training epoch 108, Batch 1000/1000: LR=9.93e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 05:53:37,821 | INFO | Epoch 108 Train Time 35.713088512420654s

2025-10-19 05:54:12,741 | INFO | Training epoch 109, Batch 1000/1000: LR=9.93e-05, Loss=3.20e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 05:54:12,828 | INFO | Epoch 109 Train Time 35.00640106201172s

2025-10-19 05:54:49,513 | INFO | Training epoch 110, Batch 1000/1000: LR=9.93e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 05:54:49,614 | INFO | Epoch 110 Train Time 36.7838659286499s

2025-10-19 05:55:25,557 | INFO | Training epoch 111, Batch 1000/1000: LR=9.93e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 05:55:25,630 | INFO | Epoch 111 Train Time 36.015506744384766s

2025-10-19 05:56:01,427 | INFO | Training epoch 112, Batch 1000/1000: LR=9.92e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 05:56:01,523 | INFO | Epoch 112 Train Time 35.8911497592926s

2025-10-19 05:56:36,849 | INFO | Training epoch 113, Batch 1000/1000: LR=9.92e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 05:56:36,940 | INFO | Epoch 113 Train Time 35.415223360061646s

2025-10-19 05:57:12,934 | INFO | Training epoch 114, Batch 1000/1000: LR=9.92e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 05:57:13,012 | INFO | Epoch 114 Train Time 36.070467948913574s

2025-10-19 05:57:49,324 | INFO | Training epoch 115, Batch 1000/1000: LR=9.92e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.61e-01
2025-10-19 05:57:49,394 | INFO | Epoch 115 Train Time 36.38211798667908s

2025-10-19 05:58:25,637 | INFO | Training epoch 116, Batch 1000/1000: LR=9.92e-05, Loss=3.18e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 05:58:25,704 | INFO | Epoch 116 Train Time 36.30892539024353s

2025-10-19 05:59:00,848 | INFO | Training epoch 117, Batch 1000/1000: LR=9.92e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 05:59:00,939 | INFO | Epoch 117 Train Time 35.23357582092285s

2025-10-19 05:59:36,424 | INFO | Training epoch 118, Batch 1000/1000: LR=9.92e-05, Loss=3.18e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 05:59:36,510 | INFO | Epoch 118 Train Time 35.56958985328674s

2025-10-19 06:00:12,318 | INFO | Training epoch 119, Batch 1000/1000: LR=9.92e-05, Loss=3.23e-02 BER=1.22e-02 FER=1.62e-01
2025-10-19 06:00:12,403 | INFO | Epoch 119 Train Time 35.89074182510376s

2025-10-19 06:00:48,626 | INFO | Training epoch 120, Batch 1000/1000: LR=9.91e-05, Loss=3.21e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 06:00:48,700 | INFO | Epoch 120 Train Time 36.29603362083435s

2025-10-19 06:01:24,577 | INFO | Training epoch 121, Batch 1000/1000: LR=9.91e-05, Loss=3.20e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 06:01:24,653 | INFO | Epoch 121 Train Time 35.95126914978027s

2025-10-19 06:02:01,215 | INFO | Training epoch 122, Batch 1000/1000: LR=9.91e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.59e-01
2025-10-19 06:02:01,290 | INFO | Epoch 122 Train Time 36.63577699661255s

2025-10-19 06:02:35,929 | INFO | Training epoch 123, Batch 1000/1000: LR=9.91e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:02:36,011 | INFO | Epoch 123 Train Time 34.71968722343445s

2025-10-19 06:03:11,882 | INFO | Training epoch 124, Batch 1000/1000: LR=9.91e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:03:11,956 | INFO | Epoch 124 Train Time 35.94486689567566s

2025-10-19 06:03:47,634 | INFO | Training epoch 125, Batch 1000/1000: LR=9.91e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:03:47,722 | INFO | Epoch 125 Train Time 35.765291690826416s

2025-10-19 06:04:23,525 | INFO | Training epoch 126, Batch 1000/1000: LR=9.90e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:04:23,604 | INFO | Epoch 126 Train Time 35.88073182106018s

2025-10-19 06:04:59,234 | INFO | Training epoch 127, Batch 1000/1000: LR=9.90e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.59e-01
2025-10-19 06:04:59,311 | INFO | Epoch 127 Train Time 35.70594787597656s

2025-10-19 06:05:34,230 | INFO | Training epoch 128, Batch 1000/1000: LR=9.90e-05, Loss=3.22e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:05:34,309 | INFO | Epoch 128 Train Time 34.99646782875061s

2025-10-19 06:06:09,974 | INFO | Training epoch 129, Batch 1000/1000: LR=9.90e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:06:10,046 | INFO | Epoch 129 Train Time 35.7357976436615s

2025-10-19 06:06:46,236 | INFO | Training epoch 130, Batch 1000/1000: LR=9.90e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:06:46,300 | INFO | Epoch 130 Train Time 36.25243782997131s

2025-10-19 06:07:22,661 | INFO | Training epoch 131, Batch 1000/1000: LR=9.90e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:07:22,767 | INFO | Epoch 131 Train Time 36.46546936035156s

2025-10-19 06:07:22,769 | INFO | [P2] saving best_model (QAT) with loss 0.031553 at epoch 131
2025-10-19 06:07:58,434 | INFO | Training epoch 132, Batch 1000/1000: LR=9.90e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 06:07:58,518 | INFO | Epoch 132 Train Time 35.72495174407959s

2025-10-19 06:08:33,830 | INFO | Training epoch 133, Batch 1000/1000: LR=9.89e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:08:33,924 | INFO | Epoch 133 Train Time 35.40429353713989s

2025-10-19 06:09:09,815 | INFO | Training epoch 134, Batch 1000/1000: LR=9.89e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.61e-01
2025-10-19 06:09:09,892 | INFO | Epoch 134 Train Time 35.965513467788696s

2025-10-19 06:09:46,043 | INFO | Training epoch 135, Batch 1000/1000: LR=9.89e-05, Loss=3.15e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 06:09:46,111 | INFO | Epoch 135 Train Time 36.21801209449768s

2025-10-19 06:09:46,111 | INFO | [P2] saving best_model (QAT) with loss 0.031532 at epoch 135
2025-10-19 06:10:21,532 | INFO | Training epoch 136, Batch 1000/1000: LR=9.89e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:10:21,623 | INFO | Epoch 136 Train Time 35.49506497383118s

2025-10-19 06:10:58,219 | INFO | Training epoch 137, Batch 1000/1000: LR=9.89e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:10:58,307 | INFO | Epoch 137 Train Time 36.68300127983093s

2025-10-19 06:11:33,828 | INFO | Training epoch 138, Batch 1000/1000: LR=9.89e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 06:11:33,916 | INFO | Epoch 138 Train Time 35.60740923881531s

2025-10-19 06:12:09,532 | INFO | Training epoch 139, Batch 1000/1000: LR=9.88e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.61e-01
2025-10-19 06:12:09,621 | INFO | Epoch 139 Train Time 35.70321297645569s

2025-10-19 06:12:45,240 | INFO | Training epoch 140, Batch 1000/1000: LR=9.88e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 06:12:45,310 | INFO | Epoch 140 Train Time 35.68763208389282s

2025-10-19 06:13:21,209 | INFO | Training epoch 141, Batch 1000/1000: LR=9.88e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:13:21,280 | INFO | Epoch 141 Train Time 35.968263149261475s

2025-10-19 06:13:57,505 | INFO | Training epoch 142, Batch 1000/1000: LR=9.88e-05, Loss=3.18e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:13:57,602 | INFO | Epoch 142 Train Time 36.32159376144409s

2025-10-19 06:14:33,923 | INFO | Training epoch 143, Batch 1000/1000: LR=9.88e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:14:34,016 | INFO | Epoch 143 Train Time 36.41059994697571s

2025-10-19 06:15:09,811 | INFO | Training epoch 144, Batch 1000/1000: LR=9.88e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 06:15:09,902 | INFO | Epoch 144 Train Time 35.88536262512207s

2025-10-19 06:15:45,609 | INFO | Training epoch 145, Batch 1000/1000: LR=9.87e-05, Loss=3.20e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:15:45,694 | INFO | Epoch 145 Train Time 35.7904486656189s

2025-10-19 06:16:20,945 | INFO | Training epoch 146, Batch 1000/1000: LR=9.87e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:16:21,034 | INFO | Epoch 146 Train Time 35.33806324005127s

2025-10-19 06:16:56,742 | INFO | Training epoch 147, Batch 1000/1000: LR=9.87e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 06:16:56,810 | INFO | Epoch 147 Train Time 35.77447557449341s

2025-10-19 06:17:33,073 | INFO | Training epoch 148, Batch 1000/1000: LR=9.87e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.60e-01
2025-10-19 06:17:33,171 | INFO | Epoch 148 Train Time 36.35945463180542s

2025-10-19 06:18:09,119 | INFO | Training epoch 149, Batch 1000/1000: LR=9.87e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 06:18:09,198 | INFO | Epoch 149 Train Time 36.02635312080383s

2025-10-19 06:18:44,520 | INFO | Training epoch 150, Batch 1000/1000: LR=9.87e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:18:44,614 | INFO | Epoch 150 Train Time 35.41496825218201s

2025-10-19 06:19:20,436 | INFO | Training epoch 151, Batch 1000/1000: LR=9.86e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:19:20,527 | INFO | Epoch 151 Train Time 35.91184139251709s

2025-10-19 06:19:56,333 | INFO | Training epoch 152, Batch 1000/1000: LR=9.86e-05, Loss=3.22e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 06:19:56,400 | INFO | Epoch 152 Train Time 35.87108588218689s

2025-10-19 06:20:32,421 | INFO | Training epoch 153, Batch 1000/1000: LR=9.86e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:20:32,495 | INFO | Epoch 153 Train Time 36.09513211250305s

2025-10-19 06:21:08,634 | INFO | Training epoch 154, Batch 1000/1000: LR=9.86e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:21:08,747 | INFO | Epoch 154 Train Time 36.25024604797363s

2025-10-19 06:21:44,110 | INFO | Training epoch 155, Batch 1000/1000: LR=9.86e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 06:21:44,193 | INFO | Epoch 155 Train Time 35.44552707672119s

2025-10-19 06:22:19,842 | INFO | Training epoch 156, Batch 1000/1000: LR=9.85e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:22:19,922 | INFO | Epoch 156 Train Time 35.727017641067505s

2025-10-19 06:22:55,520 | INFO | Training epoch 157, Batch 1000/1000: LR=9.85e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:22:55,596 | INFO | Epoch 157 Train Time 35.67318034172058s

2025-10-19 06:23:31,229 | INFO | Training epoch 158, Batch 1000/1000: LR=9.85e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:23:31,333 | INFO | Epoch 158 Train Time 35.735857009887695s

2025-10-19 06:24:06,532 | INFO | Training epoch 159, Batch 1000/1000: LR=9.85e-05, Loss=3.18e-02 BER=1.20e-02 FER=1.58e-01
2025-10-19 06:24:06,602 | INFO | Epoch 159 Train Time 35.266680002212524s

2025-10-19 06:24:42,861 | INFO | Training epoch 160, Batch 1000/1000: LR=9.85e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:24:42,932 | INFO | Epoch 160 Train Time 36.32941389083862s

2025-10-19 06:25:18,315 | INFO | Training epoch 161, Batch 1000/1000: LR=9.84e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:25:18,399 | INFO | Epoch 161 Train Time 35.466208696365356s

2025-10-19 06:25:54,131 | INFO | Training epoch 162, Batch 1000/1000: LR=9.84e-05, Loss=3.18e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:25:54,213 | INFO | Epoch 162 Train Time 35.813347578048706s

2025-10-19 06:26:30,210 | INFO | Training epoch 163, Batch 1000/1000: LR=9.84e-05, Loss=3.18e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:26:30,309 | INFO | Epoch 163 Train Time 36.09429621696472s

2025-10-19 06:27:06,334 | INFO | Training epoch 164, Batch 1000/1000: LR=9.84e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:27:06,409 | INFO | Epoch 164 Train Time 36.09877848625183s

2025-10-19 06:27:06,410 | INFO | [P2] saving best_model (QAT) with loss 0.031512 at epoch 164
2025-10-19 06:27:42,424 | INFO | Training epoch 165, Batch 1000/1000: LR=9.84e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:27:42,532 | INFO | Epoch 165 Train Time 36.09635043144226s

2025-10-19 06:28:18,137 | INFO | Training epoch 166, Batch 1000/1000: LR=9.83e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:28:18,221 | INFO | Epoch 166 Train Time 35.68666195869446s

2025-10-19 06:28:53,831 | INFO | Training epoch 167, Batch 1000/1000: LR=9.83e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:28:53,907 | INFO | Epoch 167 Train Time 35.6837215423584s

2025-10-19 06:29:30,024 | INFO | Training epoch 168, Batch 1000/1000: LR=9.83e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:29:30,117 | INFO | Epoch 168 Train Time 36.20921802520752s

2025-10-19 06:30:06,334 | INFO | Training epoch 169, Batch 1000/1000: LR=9.83e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:30:06,406 | INFO | Epoch 169 Train Time 36.286683320999146s

2025-10-19 06:30:42,417 | INFO | Training epoch 170, Batch 1000/1000: LR=9.83e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 06:30:42,503 | INFO | Epoch 170 Train Time 36.09638237953186s

2025-10-19 06:31:18,523 | INFO | Training epoch 171, Batch 1000/1000: LR=9.82e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 06:31:18,598 | INFO | Epoch 171 Train Time 36.09300470352173s

2025-10-19 06:31:54,831 | INFO | Training epoch 172, Batch 1000/1000: LR=9.82e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:31:54,918 | INFO | Epoch 172 Train Time 36.31874084472656s

2025-10-19 06:32:30,829 | INFO | Training epoch 173, Batch 1000/1000: LR=9.82e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.59e-01
2025-10-19 06:32:30,913 | INFO | Epoch 173 Train Time 35.99347114562988s

2025-10-19 06:33:08,562 | INFO | Training epoch 174, Batch 1000/1000: LR=9.82e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:33:08,638 | INFO | Epoch 174 Train Time 37.724140644073486s

2025-10-19 06:33:45,129 | INFO | Training epoch 175, Batch 1000/1000: LR=9.82e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:33:45,229 | INFO | Epoch 175 Train Time 36.58981966972351s

2025-10-19 06:34:21,125 | INFO | Training epoch 176, Batch 1000/1000: LR=9.81e-05, Loss=3.20e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:34:21,216 | INFO | Epoch 176 Train Time 35.98566508293152s

2025-10-19 06:34:57,033 | INFO | Training epoch 177, Batch 1000/1000: LR=9.81e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 06:34:57,127 | INFO | Epoch 177 Train Time 35.90985035896301s

2025-10-19 06:34:57,128 | INFO | [P2] saving best_model (QAT) with loss 0.031336 at epoch 177
2025-10-19 06:35:33,035 | INFO | Training epoch 178, Batch 1000/1000: LR=9.81e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 06:35:33,118 | INFO | Epoch 178 Train Time 35.973973989486694s

2025-10-19 06:36:08,951 | INFO | Training epoch 179, Batch 1000/1000: LR=9.81e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:36:09,072 | INFO | Epoch 179 Train Time 35.95319581031799s

2025-10-19 06:36:45,033 | INFO | Training epoch 180, Batch 1000/1000: LR=9.81e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:36:45,108 | INFO | Epoch 180 Train Time 36.03406071662903s

2025-10-19 06:37:21,167 | INFO | Training epoch 181, Batch 1000/1000: LR=9.80e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:37:21,247 | INFO | Epoch 181 Train Time 36.137736558914185s

2025-10-19 06:37:57,016 | INFO | Training epoch 182, Batch 1000/1000: LR=9.80e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:37:57,088 | INFO | Epoch 182 Train Time 35.8400764465332s

2025-10-19 06:38:32,936 | INFO | Training epoch 183, Batch 1000/1000: LR=9.80e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 06:38:33,022 | INFO | Epoch 183 Train Time 35.932856798172s

2025-10-19 06:38:33,023 | INFO | [P2] saving best_model (QAT) with loss 0.031157 at epoch 183
2025-10-19 06:39:09,922 | INFO | Training epoch 184, Batch 1000/1000: LR=9.80e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:39:10,018 | INFO | Epoch 184 Train Time 36.98076796531677s

2025-10-19 06:39:46,449 | INFO | Training epoch 185, Batch 1000/1000: LR=9.79e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:39:46,525 | INFO | Epoch 185 Train Time 36.50497221946716s

2025-10-19 06:40:22,127 | INFO | Training epoch 186, Batch 1000/1000: LR=9.79e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:40:22,234 | INFO | Epoch 186 Train Time 35.70788931846619s

2025-10-19 06:40:58,025 | INFO | Training epoch 187, Batch 1000/1000: LR=9.79e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:40:58,105 | INFO | Epoch 187 Train Time 35.87030005455017s

2025-10-19 06:41:34,022 | INFO | Training epoch 188, Batch 1000/1000: LR=9.79e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 06:41:34,113 | INFO | Epoch 188 Train Time 36.006598711013794s

2025-10-19 06:42:10,015 | INFO | Training epoch 189, Batch 1000/1000: LR=9.79e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:42:10,102 | INFO | Epoch 189 Train Time 35.98645997047424s

2025-10-19 06:42:45,719 | INFO | Training epoch 190, Batch 1000/1000: LR=9.78e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:42:45,814 | INFO | Epoch 190 Train Time 35.71104145050049s

2025-10-19 06:43:21,630 | INFO | Training epoch 191, Batch 1000/1000: LR=9.78e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:43:21,724 | INFO | Epoch 191 Train Time 35.90952777862549s

2025-10-19 06:43:57,634 | INFO | Training epoch 192, Batch 1000/1000: LR=9.78e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 06:43:57,706 | INFO | Epoch 192 Train Time 35.980488300323486s

2025-10-19 06:44:33,427 | INFO | Training epoch 193, Batch 1000/1000: LR=9.78e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:44:33,501 | INFO | Epoch 193 Train Time 35.793320178985596s

2025-10-19 06:45:09,139 | INFO | Training epoch 194, Batch 1000/1000: LR=9.77e-05, Loss=3.20e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:45:09,239 | INFO | Epoch 194 Train Time 35.73779582977295s

2025-10-19 06:45:44,469 | INFO | Training epoch 195, Batch 1000/1000: LR=9.77e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:45:44,561 | INFO | Epoch 195 Train Time 35.32114768028259s

2025-10-19 06:46:20,550 | INFO | Training epoch 196, Batch 1000/1000: LR=9.77e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:46:20,644 | INFO | Epoch 196 Train Time 36.080819845199585s

2025-10-19 06:46:55,226 | INFO | Training epoch 197, Batch 1000/1000: LR=9.77e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 06:46:55,293 | INFO | Epoch 197 Train Time 34.647475719451904s

2025-10-19 06:47:30,853 | INFO | Training epoch 198, Batch 1000/1000: LR=9.76e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:47:30,923 | INFO | Epoch 198 Train Time 35.62859630584717s

2025-10-19 06:48:06,682 | INFO | Training epoch 199, Batch 1000/1000: LR=9.76e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:48:06,755 | INFO | Epoch 199 Train Time 35.83106303215027s

2025-10-19 06:48:42,315 | INFO | Training epoch 200, Batch 1000/1000: LR=9.76e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:48:42,379 | INFO | Epoch 200 Train Time 35.622177839279175s

2025-10-19 06:49:17,548 | INFO | Training epoch 201, Batch 1000/1000: LR=9.76e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:49:17,641 | INFO | Epoch 201 Train Time 35.26085567474365s

2025-10-19 06:49:54,226 | INFO | Training epoch 202, Batch 1000/1000: LR=9.76e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:49:54,308 | INFO | Epoch 202 Train Time 36.665302753448486s

2025-10-19 06:50:30,730 | INFO | Training epoch 203, Batch 1000/1000: LR=9.75e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 06:50:30,807 | INFO | Epoch 203 Train Time 36.49817156791687s

2025-10-19 06:51:06,322 | INFO | Training epoch 204, Batch 1000/1000: LR=9.75e-05, Loss=3.23e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 06:51:06,396 | INFO | Epoch 204 Train Time 35.58691668510437s

2025-10-19 06:51:41,615 | INFO | Training epoch 205, Batch 1000/1000: LR=9.75e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 06:51:41,685 | INFO | Epoch 205 Train Time 35.28676438331604s

2025-10-19 06:52:17,427 | INFO | Training epoch 206, Batch 1000/1000: LR=9.75e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:52:17,503 | INFO | Epoch 206 Train Time 35.81772565841675s

2025-10-19 06:52:53,234 | INFO | Training epoch 207, Batch 1000/1000: LR=9.74e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.60e-01
2025-10-19 06:52:53,305 | INFO | Epoch 207 Train Time 35.80124831199646s

2025-10-19 06:53:29,414 | INFO | Training epoch 208, Batch 1000/1000: LR=9.74e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.60e-01
2025-10-19 06:53:29,490 | INFO | Epoch 208 Train Time 36.18331551551819s

2025-10-19 06:54:05,154 | INFO | Training epoch 209, Batch 1000/1000: LR=9.74e-05, Loss=3.16e-02 BER=1.17e-02 FER=1.58e-01
2025-10-19 06:54:05,237 | INFO | Epoch 209 Train Time 35.74679899215698s

2025-10-19 06:54:41,134 | INFO | Training epoch 210, Batch 1000/1000: LR=9.74e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:54:41,236 | INFO | Epoch 210 Train Time 35.996586084365845s

2025-10-19 06:55:17,753 | INFO | Training epoch 211, Batch 1000/1000: LR=9.73e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.58e-01
2025-10-19 06:55:17,841 | INFO | Epoch 211 Train Time 36.60434031486511s

2025-10-19 06:55:54,126 | INFO | Training epoch 212, Batch 1000/1000: LR=9.73e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 06:55:54,212 | INFO | Epoch 212 Train Time 36.36956167221069s

2025-10-19 06:56:30,029 | INFO | Training epoch 213, Batch 1000/1000: LR=9.73e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:56:30,104 | INFO | Epoch 213 Train Time 35.891637325286865s

2025-10-19 06:57:06,030 | INFO | Training epoch 214, Batch 1000/1000: LR=9.73e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 06:57:06,118 | INFO | Epoch 214 Train Time 36.01162362098694s

2025-10-19 06:57:41,973 | INFO | Training epoch 215, Batch 1000/1000: LR=9.72e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 06:57:42,060 | INFO | Epoch 215 Train Time 35.94052028656006s

2025-10-19 06:58:19,667 | INFO | Training epoch 216, Batch 1000/1000: LR=9.72e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 06:58:19,746 | INFO | Epoch 216 Train Time 37.684889793395996s

2025-10-19 06:58:55,024 | INFO | Training epoch 217, Batch 1000/1000: LR=9.72e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 06:58:55,103 | INFO | Epoch 217 Train Time 35.355488777160645s

2025-10-19 06:59:30,527 | INFO | Training epoch 218, Batch 1000/1000: LR=9.72e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.58e-01
2025-10-19 06:59:30,603 | INFO | Epoch 218 Train Time 35.498284339904785s

2025-10-19 07:00:06,427 | INFO | Training epoch 219, Batch 1000/1000: LR=9.71e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:00:06,524 | INFO | Epoch 219 Train Time 35.91958737373352s

2025-10-19 07:00:42,257 | INFO | Training epoch 220, Batch 1000/1000: LR=9.71e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:00:42,368 | INFO | Epoch 220 Train Time 35.843271255493164s

2025-10-19 07:01:17,933 | INFO | Training epoch 221, Batch 1000/1000: LR=9.71e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:01:18,030 | INFO | Epoch 221 Train Time 35.66070747375488s

2025-10-19 07:01:53,558 | INFO | Training epoch 222, Batch 1000/1000: LR=9.70e-05, Loss=3.21e-02 BER=1.20e-02 FER=1.58e-01
2025-10-19 07:01:53,635 | INFO | Epoch 222 Train Time 35.60442042350769s

2025-10-19 07:02:28,738 | INFO | Training epoch 223, Batch 1000/1000: LR=9.70e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:02:28,818 | INFO | Epoch 223 Train Time 35.180800437927246s

2025-10-19 07:03:04,989 | INFO | Training epoch 224, Batch 1000/1000: LR=9.70e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.59e-01
2025-10-19 07:03:05,067 | INFO | Epoch 224 Train Time 36.246681928634644s

2025-10-19 07:03:40,914 | INFO | Training epoch 225, Batch 1000/1000: LR=9.70e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 07:03:40,984 | INFO | Epoch 225 Train Time 35.91649079322815s

2025-10-19 07:04:16,623 | INFO | Training epoch 226, Batch 1000/1000: LR=9.69e-05, Loss=3.20e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 07:04:16,705 | INFO | Epoch 226 Train Time 35.71949100494385s

2025-10-19 07:04:52,429 | INFO | Training epoch 227, Batch 1000/1000: LR=9.69e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:04:52,525 | INFO | Epoch 227 Train Time 35.81927800178528s

2025-10-19 07:05:28,739 | INFO | Training epoch 228, Batch 1000/1000: LR=9.69e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:05:28,828 | INFO | Epoch 228 Train Time 36.301387310028076s

2025-10-19 07:06:05,547 | INFO | Training epoch 229, Batch 1000/1000: LR=9.69e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:06:05,640 | INFO | Epoch 229 Train Time 36.81044626235962s

2025-10-19 07:06:40,633 | INFO | Training epoch 230, Batch 1000/1000: LR=9.68e-05, Loss=3.15e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:06:40,716 | INFO | Epoch 230 Train Time 35.07537651062012s

2025-10-19 07:07:16,429 | INFO | Training epoch 231, Batch 1000/1000: LR=9.68e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:07:16,522 | INFO | Epoch 231 Train Time 35.805503368377686s

2025-10-19 07:07:52,725 | INFO | Training epoch 232, Batch 1000/1000: LR=9.68e-05, Loss=3.23e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 07:07:52,808 | INFO | Epoch 232 Train Time 36.28482389450073s

2025-10-19 07:08:28,532 | INFO | Training epoch 233, Batch 1000/1000: LR=9.67e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 07:08:28,624 | INFO | Epoch 233 Train Time 35.815327644348145s

2025-10-19 07:09:04,538 | INFO | Training epoch 234, Batch 1000/1000: LR=9.67e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:09:04,637 | INFO | Epoch 234 Train Time 36.011090993881226s

2025-10-19 07:09:40,758 | INFO | Training epoch 235, Batch 1000/1000: LR=9.67e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 07:09:40,844 | INFO | Epoch 235 Train Time 36.20628833770752s

2025-10-19 07:10:17,168 | INFO | Training epoch 236, Batch 1000/1000: LR=9.67e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:10:17,243 | INFO | Epoch 236 Train Time 36.397430658340454s

2025-10-19 07:10:55,633 | INFO | Training epoch 237, Batch 1000/1000: LR=9.66e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:10:55,721 | INFO | Epoch 237 Train Time 38.47681713104248s

2025-10-19 07:11:33,143 | INFO | Training epoch 238, Batch 1000/1000: LR=9.66e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 07:11:33,235 | INFO | Epoch 238 Train Time 37.511733055114746s

2025-10-19 07:12:10,038 | INFO | Training epoch 239, Batch 1000/1000: LR=9.66e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:12:10,136 | INFO | Epoch 239 Train Time 36.89945602416992s

2025-10-19 07:12:47,539 | INFO | Training epoch 240, Batch 1000/1000: LR=9.66e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:12:47,632 | INFO | Epoch 240 Train Time 37.49493384361267s

2025-10-19 07:13:24,996 | INFO | Training epoch 241, Batch 1000/1000: LR=9.65e-05, Loss=3.15e-02 BER=1.17e-02 FER=1.57e-01
2025-10-19 07:13:25,089 | INFO | Epoch 241 Train Time 37.45591592788696s

2025-10-19 07:14:02,171 | INFO | Training epoch 242, Batch 1000/1000: LR=9.65e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:14:02,253 | INFO | Epoch 242 Train Time 37.16229557991028s

2025-10-19 07:14:39,339 | INFO | Training epoch 243, Batch 1000/1000: LR=9.65e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:14:39,421 | INFO | Epoch 243 Train Time 37.16720128059387s

2025-10-19 07:15:16,349 | INFO | Training epoch 244, Batch 1000/1000: LR=9.64e-05, Loss=3.18e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:15:16,443 | INFO | Epoch 244 Train Time 37.02096390724182s

2025-10-19 07:15:53,839 | INFO | Training epoch 245, Batch 1000/1000: LR=9.64e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:15:53,931 | INFO | Epoch 245 Train Time 37.48641061782837s

2025-10-19 07:16:31,047 | INFO | Training epoch 246, Batch 1000/1000: LR=9.64e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:16:31,150 | INFO | Epoch 246 Train Time 37.217902183532715s

2025-10-19 07:17:08,878 | INFO | Training epoch 247, Batch 1000/1000: LR=9.64e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:17:08,967 | INFO | Epoch 247 Train Time 37.81580853462219s

2025-10-19 07:17:46,060 | INFO | Training epoch 248, Batch 1000/1000: LR=9.63e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:17:46,142 | INFO | Epoch 248 Train Time 37.17482256889343s

2025-10-19 07:18:22,846 | INFO | Training epoch 249, Batch 1000/1000: LR=9.63e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:18:22,945 | INFO | Epoch 249 Train Time 36.801093101501465s

2025-10-19 07:18:59,937 | INFO | Training epoch 250, Batch 1000/1000: LR=9.63e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:19:00,048 | INFO | Epoch 250 Train Time 37.10210943222046s

2025-10-19 07:19:37,233 | INFO | Training epoch 251, Batch 1000/1000: LR=9.62e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.57e-01
2025-10-19 07:19:37,333 | INFO | Epoch 251 Train Time 37.28288769721985s

2025-10-19 07:20:14,651 | INFO | Training epoch 252, Batch 1000/1000: LR=9.62e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.56e-01
2025-10-19 07:20:14,749 | INFO | Epoch 252 Train Time 37.41608738899231s

2025-10-19 07:20:14,750 | INFO | [P2] saving best_model (QAT) with loss 0.030993 at epoch 252
2025-10-19 07:20:52,248 | INFO | Training epoch 253, Batch 1000/1000: LR=9.62e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.57e-01
2025-10-19 07:20:52,358 | INFO | Epoch 253 Train Time 37.58865571022034s

2025-10-19 07:21:28,651 | INFO | Training epoch 254, Batch 1000/1000: LR=9.61e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 07:21:28,768 | INFO | Epoch 254 Train Time 36.408923864364624s

2025-10-19 07:22:06,146 | INFO | Training epoch 255, Batch 1000/1000: LR=9.61e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 07:22:06,249 | INFO | Epoch 255 Train Time 37.47995042800903s

2025-10-19 07:22:43,245 | INFO | Training epoch 256, Batch 1000/1000: LR=9.61e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 07:22:43,336 | INFO | Epoch 256 Train Time 37.08600640296936s

2025-10-19 07:23:20,235 | INFO | Training epoch 257, Batch 1000/1000: LR=9.61e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:23:20,327 | INFO | Epoch 257 Train Time 36.989545583724976s

2025-10-19 07:23:57,192 | INFO | Training epoch 258, Batch 1000/1000: LR=9.60e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:23:57,278 | INFO | Epoch 258 Train Time 36.950459241867065s

2025-10-19 07:24:34,961 | INFO | Training epoch 259, Batch 1000/1000: LR=9.60e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:24:35,045 | INFO | Epoch 259 Train Time 37.764159202575684s

2025-10-19 07:25:12,447 | INFO | Training epoch 260, Batch 1000/1000: LR=9.60e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 07:25:12,530 | INFO | Epoch 260 Train Time 37.48336935043335s

2025-10-19 07:25:49,327 | INFO | Training epoch 261, Batch 1000/1000: LR=9.59e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:25:49,415 | INFO | Epoch 261 Train Time 36.884337186813354s

2025-10-19 07:26:25,481 | INFO | Training epoch 262, Batch 1000/1000: LR=9.59e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:26:25,564 | INFO | Epoch 262 Train Time 36.148258686065674s

2025-10-19 07:27:03,778 | INFO | Training epoch 263, Batch 1000/1000: LR=9.59e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:27:03,858 | INFO | Epoch 263 Train Time 38.29129600524902s

2025-10-19 07:27:41,254 | INFO | Training epoch 264, Batch 1000/1000: LR=9.58e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.59e-01
2025-10-19 07:27:41,337 | INFO | Epoch 264 Train Time 37.477604150772095s

2025-10-19 07:28:18,465 | INFO | Training epoch 265, Batch 1000/1000: LR=9.58e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:28:18,560 | INFO | Epoch 265 Train Time 37.222049951553345s

2025-10-19 07:28:55,635 | INFO | Training epoch 266, Batch 1000/1000: LR=9.58e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 07:28:55,731 | INFO | Epoch 266 Train Time 37.169312953948975s

2025-10-19 07:29:33,346 | INFO | Training epoch 267, Batch 1000/1000: LR=9.57e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.56e-01
2025-10-19 07:29:33,442 | INFO | Epoch 267 Train Time 37.70824694633484s

2025-10-19 07:30:10,934 | INFO | Training epoch 268, Batch 1000/1000: LR=9.57e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:30:11,024 | INFO | Epoch 268 Train Time 37.5810968875885s

2025-10-19 07:30:47,948 | INFO | Training epoch 269, Batch 1000/1000: LR=9.57e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:30:48,032 | INFO | Epoch 269 Train Time 37.0061252117157s

2025-10-19 07:31:25,337 | INFO | Training epoch 270, Batch 1000/1000: LR=9.56e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:31:25,436 | INFO | Epoch 270 Train Time 37.402348279953s

2025-10-19 07:32:02,764 | INFO | Training epoch 271, Batch 1000/1000: LR=9.56e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:32:02,844 | INFO | Epoch 271 Train Time 37.40674901008606s

2025-10-19 07:32:40,151 | INFO | Training epoch 272, Batch 1000/1000: LR=9.56e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:32:40,248 | INFO | Epoch 272 Train Time 37.40201163291931s

2025-10-19 07:33:17,834 | INFO | Training epoch 273, Batch 1000/1000: LR=9.56e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:33:17,933 | INFO | Epoch 273 Train Time 37.683303356170654s

2025-10-19 07:33:55,044 | INFO | Training epoch 274, Batch 1000/1000: LR=9.55e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 07:33:55,133 | INFO | Epoch 274 Train Time 37.19819641113281s

2025-10-19 07:33:55,134 | INFO | [P2] saving best_model (QAT) with loss 0.030943 at epoch 274
2025-10-19 07:34:32,644 | INFO | Training epoch 275, Batch 1000/1000: LR=9.55e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:34:32,751 | INFO | Epoch 275 Train Time 37.591145038604736s

2025-10-19 07:35:09,640 | INFO | Training epoch 276, Batch 1000/1000: LR=9.55e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:35:09,747 | INFO | Epoch 276 Train Time 36.99456429481506s

2025-10-19 07:35:47,370 | INFO | Training epoch 277, Batch 1000/1000: LR=9.54e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 07:35:47,460 | INFO | Epoch 277 Train Time 37.71274948120117s

2025-10-19 07:35:47,461 | INFO | [P2] saving best_model (QAT) with loss 0.030868 at epoch 277
2025-10-19 07:36:24,801 | INFO | Training epoch 278, Batch 1000/1000: LR=9.54e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:36:24,877 | INFO | Epoch 278 Train Time 37.40086054801941s

2025-10-19 07:37:01,865 | INFO | Training epoch 279, Batch 1000/1000: LR=9.54e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:37:01,976 | INFO | Epoch 279 Train Time 37.096632957458496s

2025-10-19 07:37:39,237 | INFO | Training epoch 280, Batch 1000/1000: LR=9.53e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 07:37:39,328 | INFO | Epoch 280 Train Time 37.35034441947937s

2025-10-19 07:38:15,940 | INFO | Training epoch 281, Batch 1000/1000: LR=9.53e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:38:16,031 | INFO | Epoch 281 Train Time 36.70058608055115s

2025-10-19 07:38:52,938 | INFO | Training epoch 282, Batch 1000/1000: LR=9.53e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:38:53,034 | INFO | Epoch 282 Train Time 36.99972701072693s

2025-10-19 07:39:30,150 | INFO | Training epoch 283, Batch 1000/1000: LR=9.52e-05, Loss=3.18e-02 BER=1.20e-02 FER=1.57e-01
2025-10-19 07:39:30,242 | INFO | Epoch 283 Train Time 37.206117153167725s

2025-10-19 07:40:07,160 | INFO | Training epoch 284, Batch 1000/1000: LR=9.52e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:40:07,259 | INFO | Epoch 284 Train Time 37.016711473464966s

2025-10-19 07:40:44,556 | INFO | Training epoch 285, Batch 1000/1000: LR=9.52e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:40:44,650 | INFO | Epoch 285 Train Time 37.389861822128296s

2025-10-19 07:41:21,654 | INFO | Training epoch 286, Batch 1000/1000: LR=9.51e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:41:21,749 | INFO | Epoch 286 Train Time 37.097970485687256s

2025-10-19 07:41:59,139 | INFO | Training epoch 287, Batch 1000/1000: LR=9.51e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:41:59,232 | INFO | Epoch 287 Train Time 37.48136496543884s

2025-10-19 07:42:36,775 | INFO | Training epoch 288, Batch 1000/1000: LR=9.51e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:42:36,855 | INFO | Epoch 288 Train Time 37.62099242210388s

2025-10-19 07:43:13,340 | INFO | Training epoch 289, Batch 1000/1000: LR=9.50e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 07:43:13,423 | INFO | Epoch 289 Train Time 36.566444396972656s

2025-10-19 07:43:50,382 | INFO | Training epoch 290, Batch 1000/1000: LR=9.50e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:43:50,468 | INFO | Epoch 290 Train Time 37.04340839385986s

2025-10-19 07:44:27,241 | INFO | Training epoch 291, Batch 1000/1000: LR=9.50e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:44:27,322 | INFO | Epoch 291 Train Time 36.8514506816864s

2025-10-19 07:45:04,380 | INFO | Training epoch 292, Batch 1000/1000: LR=9.49e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:45:04,465 | INFO | Epoch 292 Train Time 37.14178800582886s

2025-10-19 07:45:41,361 | INFO | Training epoch 293, Batch 1000/1000: LR=9.49e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.58e-01
2025-10-19 07:45:41,440 | INFO | Epoch 293 Train Time 36.97404670715332s

2025-10-19 07:46:18,169 | INFO | Training epoch 294, Batch 1000/1000: LR=9.48e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:46:18,253 | INFO | Epoch 294 Train Time 36.811516523361206s

2025-10-19 07:46:55,252 | INFO | Training epoch 295, Batch 1000/1000: LR=9.48e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:46:55,346 | INFO | Epoch 295 Train Time 37.091604471206665s

2025-10-19 07:47:32,445 | INFO | Training epoch 296, Batch 1000/1000: LR=9.48e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:47:32,540 | INFO | Epoch 296 Train Time 37.193201780319214s

2025-10-19 07:48:09,645 | INFO | Training epoch 297, Batch 1000/1000: LR=9.47e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:48:09,736 | INFO | Epoch 297 Train Time 37.194698095321655s

2025-10-19 07:48:47,072 | INFO | Training epoch 298, Batch 1000/1000: LR=9.47e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 07:48:47,157 | INFO | Epoch 298 Train Time 37.42047142982483s

2025-10-19 07:49:24,464 | INFO | Training epoch 299, Batch 1000/1000: LR=9.47e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:49:24,564 | INFO | Epoch 299 Train Time 37.40562319755554s

2025-10-19 07:50:01,647 | INFO | Training epoch 300, Batch 1000/1000: LR=9.46e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:50:01,751 | INFO | Epoch 300 Train Time 37.18565487861633s

2025-10-19 07:50:38,771 | INFO | Training epoch 301, Batch 1000/1000: LR=9.46e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:50:38,850 | INFO | Epoch 301 Train Time 37.097957372665405s

2025-10-19 07:51:16,169 | INFO | Training epoch 302, Batch 1000/1000: LR=9.46e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:51:16,252 | INFO | Epoch 302 Train Time 37.400264501571655s

2025-10-19 07:51:53,650 | INFO | Training epoch 303, Batch 1000/1000: LR=9.45e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:51:53,737 | INFO | Epoch 303 Train Time 37.48358654975891s

2025-10-19 07:52:31,287 | INFO | Training epoch 304, Batch 1000/1000: LR=9.45e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:52:31,382 | INFO | Epoch 304 Train Time 37.64378809928894s

2025-10-19 07:53:08,962 | INFO | Training epoch 305, Batch 1000/1000: LR=9.45e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 07:53:09,051 | INFO | Epoch 305 Train Time 37.66802191734314s

2025-10-19 07:53:46,046 | INFO | Training epoch 306, Batch 1000/1000: LR=9.44e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:53:46,133 | INFO | Epoch 306 Train Time 37.08087468147278s

2025-10-19 07:54:23,234 | INFO | Training epoch 307, Batch 1000/1000: LR=9.44e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:54:23,312 | INFO | Epoch 307 Train Time 37.17801594734192s

2025-10-19 07:55:00,549 | INFO | Training epoch 308, Batch 1000/1000: LR=9.44e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 07:55:00,642 | INFO | Epoch 308 Train Time 37.32841658592224s

2025-10-19 07:55:37,738 | INFO | Training epoch 309, Batch 1000/1000: LR=9.43e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 07:55:37,838 | INFO | Epoch 309 Train Time 37.194897413253784s

2025-10-19 07:56:15,115 | INFO | Training epoch 310, Batch 1000/1000: LR=9.43e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 07:56:15,207 | INFO | Epoch 310 Train Time 37.368573904037476s

2025-10-19 07:56:52,643 | INFO | Training epoch 311, Batch 1000/1000: LR=9.42e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 07:56:52,752 | INFO | Epoch 311 Train Time 37.543901443481445s

2025-10-19 07:57:29,838 | INFO | Training epoch 312, Batch 1000/1000: LR=9.42e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 07:57:29,927 | INFO | Epoch 312 Train Time 37.17418026924133s

2025-10-19 07:58:07,235 | INFO | Training epoch 313, Batch 1000/1000: LR=9.42e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 07:58:07,341 | INFO | Epoch 313 Train Time 37.41257977485657s

2025-10-19 07:58:44,063 | INFO | Training epoch 314, Batch 1000/1000: LR=9.41e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:58:44,147 | INFO | Epoch 314 Train Time 36.8037588596344s

2025-10-19 07:59:21,357 | INFO | Training epoch 315, Batch 1000/1000: LR=9.41e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 07:59:21,455 | INFO | Epoch 315 Train Time 37.30697774887085s

2025-10-19 07:59:58,541 | INFO | Training epoch 316, Batch 1000/1000: LR=9.41e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 07:59:58,638 | INFO | Epoch 316 Train Time 37.181671142578125s

2025-10-19 08:00:35,652 | INFO | Training epoch 317, Batch 1000/1000: LR=9.40e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.54e-01
2025-10-19 08:00:35,744 | INFO | Epoch 317 Train Time 37.10426473617554s

2025-10-19 08:00:35,744 | INFO | [P2] saving best_model (QAT) with loss 0.030689 at epoch 317
2025-10-19 08:01:12,649 | INFO | Training epoch 318, Batch 1000/1000: LR=9.40e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 08:01:12,738 | INFO | Epoch 318 Train Time 36.97701144218445s

2025-10-19 08:01:49,862 | INFO | Training epoch 319, Batch 1000/1000: LR=9.40e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:01:49,948 | INFO | Epoch 319 Train Time 37.20948338508606s

2025-10-19 08:02:27,569 | INFO | Training epoch 320, Batch 1000/1000: LR=9.39e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:02:27,681 | INFO | Epoch 320 Train Time 37.731931924819946s

2025-10-19 08:03:03,938 | INFO | Training epoch 321, Batch 1000/1000: LR=9.39e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:03:04,028 | INFO | Epoch 321 Train Time 36.34531807899475s

2025-10-19 08:03:40,736 | INFO | Training epoch 322, Batch 1000/1000: LR=9.38e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:03:40,832 | INFO | Epoch 322 Train Time 36.80315828323364s

2025-10-19 08:04:18,173 | INFO | Training epoch 323, Batch 1000/1000: LR=9.38e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:04:18,271 | INFO | Epoch 323 Train Time 37.43745541572571s

2025-10-19 08:04:55,541 | INFO | Training epoch 324, Batch 1000/1000: LR=9.38e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 08:04:55,646 | INFO | Epoch 324 Train Time 37.37422275543213s

2025-10-19 08:05:33,252 | INFO | Training epoch 325, Batch 1000/1000: LR=9.37e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:05:33,344 | INFO | Epoch 325 Train Time 37.69682431221008s

2025-10-19 08:06:10,888 | INFO | Training epoch 326, Batch 1000/1000: LR=9.37e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 08:06:10,966 | INFO | Epoch 326 Train Time 37.620508432388306s

2025-10-19 08:06:48,044 | INFO | Training epoch 327, Batch 1000/1000: LR=9.37e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:06:48,135 | INFO | Epoch 327 Train Time 37.16824722290039s

2025-10-19 08:07:25,246 | INFO | Training epoch 328, Batch 1000/1000: LR=9.36e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:07:25,343 | INFO | Epoch 328 Train Time 37.20571231842041s

2025-10-19 08:08:02,656 | INFO | Training epoch 329, Batch 1000/1000: LR=9.36e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:08:02,732 | INFO | Epoch 329 Train Time 37.38757514953613s

2025-10-19 08:08:39,078 | INFO | Training epoch 330, Batch 1000/1000: LR=9.35e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:08:39,161 | INFO | Epoch 330 Train Time 36.42655920982361s

2025-10-19 08:09:16,183 | INFO | Training epoch 331, Batch 1000/1000: LR=9.35e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:09:16,280 | INFO | Epoch 331 Train Time 37.11721205711365s

2025-10-19 08:09:53,438 | INFO | Training epoch 332, Batch 1000/1000: LR=9.35e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 08:09:53,521 | INFO | Epoch 332 Train Time 37.23981785774231s

2025-10-19 08:10:30,754 | INFO | Training epoch 333, Batch 1000/1000: LR=9.34e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:10:30,846 | INFO | Epoch 333 Train Time 37.323445320129395s

2025-10-19 08:11:07,849 | INFO | Training epoch 334, Batch 1000/1000: LR=9.34e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:11:07,959 | INFO | Epoch 334 Train Time 37.11123013496399s

2025-10-19 08:11:44,248 | INFO | Training epoch 335, Batch 1000/1000: LR=9.33e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:11:44,330 | INFO | Epoch 335 Train Time 36.37008452415466s

2025-10-19 08:12:21,650 | INFO | Training epoch 336, Batch 1000/1000: LR=9.33e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 08:12:21,738 | INFO | Epoch 336 Train Time 37.40618681907654s

2025-10-19 08:12:58,958 | INFO | Training epoch 337, Batch 1000/1000: LR=9.33e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:12:59,040 | INFO | Epoch 337 Train Time 37.30098295211792s

2025-10-19 08:13:35,989 | INFO | Training epoch 338, Batch 1000/1000: LR=9.32e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 08:13:36,074 | INFO | Epoch 338 Train Time 37.03205490112305s

2025-10-19 08:14:12,893 | INFO | Training epoch 339, Batch 1000/1000: LR=9.32e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 08:14:13,001 | INFO | Epoch 339 Train Time 36.926522731781006s

2025-10-19 08:14:50,060 | INFO | Training epoch 340, Batch 1000/1000: LR=9.31e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.56e-01
2025-10-19 08:14:50,141 | INFO | Epoch 340 Train Time 37.13853907585144s

2025-10-19 08:15:27,937 | INFO | Training epoch 341, Batch 1000/1000: LR=9.31e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:15:28,030 | INFO | Epoch 341 Train Time 37.88728189468384s

2025-10-19 08:16:05,241 | INFO | Training epoch 342, Batch 1000/1000: LR=9.31e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:16:05,347 | INFO | Epoch 342 Train Time 37.31627130508423s

2025-10-19 08:16:42,555 | INFO | Training epoch 343, Batch 1000/1000: LR=9.30e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:16:42,635 | INFO | Epoch 343 Train Time 37.287720918655396s

2025-10-19 08:17:19,948 | INFO | Training epoch 344, Batch 1000/1000: LR=9.30e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:17:20,038 | INFO | Epoch 344 Train Time 37.40010905265808s

2025-10-19 08:17:57,366 | INFO | Training epoch 345, Batch 1000/1000: LR=9.29e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:17:57,459 | INFO | Epoch 345 Train Time 37.4201500415802s

2025-10-19 08:18:34,360 | INFO | Training epoch 346, Batch 1000/1000: LR=9.29e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 08:18:34,465 | INFO | Epoch 346 Train Time 37.00510096549988s

2025-10-19 08:19:11,838 | INFO | Training epoch 347, Batch 1000/1000: LR=9.29e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:19:11,932 | INFO | Epoch 347 Train Time 37.46540546417236s

2025-10-19 08:19:49,143 | INFO | Training epoch 348, Batch 1000/1000: LR=9.28e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 08:19:49,243 | INFO | Epoch 348 Train Time 37.309226512908936s

2025-10-19 08:20:26,546 | INFO | Training epoch 349, Batch 1000/1000: LR=9.28e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 08:20:26,636 | INFO | Epoch 349 Train Time 37.39234137535095s

2025-10-19 08:21:03,564 | INFO | Training epoch 350, Batch 1000/1000: LR=9.27e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 08:21:03,650 | INFO | Epoch 350 Train Time 37.012216567993164s

2025-10-19 08:21:40,678 | INFO | Training epoch 351, Batch 1000/1000: LR=9.27e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:21:40,768 | INFO | Epoch 351 Train Time 37.11620378494263s

2025-10-19 08:22:17,740 | INFO | Training epoch 352, Batch 1000/1000: LR=9.27e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:22:17,817 | INFO | Epoch 352 Train Time 37.047964334487915s

2025-10-19 08:22:55,274 | INFO | Training epoch 353, Batch 1000/1000: LR=9.26e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:22:55,354 | INFO | Epoch 353 Train Time 37.535794734954834s

2025-10-19 08:23:32,241 | INFO | Training epoch 354, Batch 1000/1000: LR=9.26e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 08:23:32,326 | INFO | Epoch 354 Train Time 36.971091508865356s

2025-10-19 08:24:09,435 | INFO | Training epoch 355, Batch 1000/1000: LR=9.25e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 08:24:09,516 | INFO | Epoch 355 Train Time 37.18953585624695s

2025-10-19 08:24:46,344 | INFO | Training epoch 356, Batch 1000/1000: LR=9.25e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:24:46,436 | INFO | Epoch 356 Train Time 36.918694734573364s

2025-10-19 08:25:23,636 | INFO | Training epoch 357, Batch 1000/1000: LR=9.25e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:25:23,718 | INFO | Epoch 357 Train Time 37.28045296669006s

2025-10-19 08:26:00,643 | INFO | Training epoch 358, Batch 1000/1000: LR=9.24e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:26:00,731 | INFO | Epoch 358 Train Time 37.01192569732666s

2025-10-19 08:26:38,083 | INFO | Training epoch 359, Batch 1000/1000: LR=9.24e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 08:26:38,172 | INFO | Epoch 359 Train Time 37.440895318984985s

2025-10-19 08:27:15,365 | INFO | Training epoch 360, Batch 1000/1000: LR=9.23e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:27:15,462 | INFO | Epoch 360 Train Time 37.288703203201294s

2025-10-19 08:27:53,028 | INFO | Training epoch 361, Batch 1000/1000: LR=9.23e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 08:27:53,127 | INFO | Epoch 361 Train Time 37.664098262786865s

2025-10-19 08:28:30,072 | INFO | Training epoch 362, Batch 1000/1000: LR=9.23e-05, Loss=3.15e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:28:30,164 | INFO | Epoch 362 Train Time 37.03568077087402s

2025-10-19 08:29:07,250 | INFO | Training epoch 363, Batch 1000/1000: LR=9.22e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 08:29:07,347 | INFO | Epoch 363 Train Time 37.180983543395996s

2025-10-19 08:29:44,658 | INFO | Training epoch 364, Batch 1000/1000: LR=9.22e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:29:44,743 | INFO | Epoch 364 Train Time 37.394538164138794s

2025-10-19 08:30:22,039 | INFO | Training epoch 365, Batch 1000/1000: LR=9.21e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:30:22,125 | INFO | Epoch 365 Train Time 37.38128972053528s

2025-10-19 08:31:00,049 | INFO | Training epoch 366, Batch 1000/1000: LR=9.21e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:31:00,132 | INFO | Epoch 366 Train Time 38.00516390800476s

2025-10-19 08:31:37,461 | INFO | Training epoch 367, Batch 1000/1000: LR=9.20e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:31:37,565 | INFO | Epoch 367 Train Time 37.43041753768921s

2025-10-19 08:32:14,432 | INFO | Training epoch 368, Batch 1000/1000: LR=9.20e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 08:32:14,532 | INFO | Epoch 368 Train Time 36.96681785583496s

2025-10-19 08:32:51,738 | INFO | Training epoch 369, Batch 1000/1000: LR=9.20e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:32:51,835 | INFO | Epoch 369 Train Time 37.30166578292847s

2025-10-19 08:33:29,250 | INFO | Training epoch 370, Batch 1000/1000: LR=9.19e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:33:29,342 | INFO | Epoch 370 Train Time 37.5050892829895s

2025-10-19 08:34:06,145 | INFO | Training epoch 371, Batch 1000/1000: LR=9.19e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:34:06,222 | INFO | Epoch 371 Train Time 36.877861738204956s

2025-10-19 08:34:43,345 | INFO | Training epoch 372, Batch 1000/1000: LR=9.18e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:34:43,432 | INFO | Epoch 372 Train Time 37.208373069763184s

2025-10-19 08:35:20,440 | INFO | Training epoch 373, Batch 1000/1000: LR=9.18e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:35:20,526 | INFO | Epoch 373 Train Time 37.09306836128235s

2025-10-19 08:35:57,445 | INFO | Training epoch 374, Batch 1000/1000: LR=9.17e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 08:35:57,542 | INFO | Epoch 374 Train Time 37.014800786972046s

2025-10-19 08:36:34,644 | INFO | Training epoch 375, Batch 1000/1000: LR=9.17e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:36:34,743 | INFO | Epoch 375 Train Time 37.19892883300781s

2025-10-19 08:37:11,949 | INFO | Training epoch 376, Batch 1000/1000: LR=9.17e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 08:37:12,044 | INFO | Epoch 376 Train Time 37.29846787452698s

2025-10-19 08:37:49,951 | INFO | Training epoch 377, Batch 1000/1000: LR=9.16e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.55e-01
2025-10-19 08:37:50,046 | INFO | Epoch 377 Train Time 38.00125169754028s

2025-10-19 08:38:27,582 | INFO | Training epoch 378, Batch 1000/1000: LR=9.16e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 08:38:27,678 | INFO | Epoch 378 Train Time 37.63030815124512s

2025-10-19 08:39:05,451 | INFO | Training epoch 379, Batch 1000/1000: LR=9.15e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:39:05,562 | INFO | Epoch 379 Train Time 37.88280916213989s

2025-10-19 08:39:42,435 | INFO | Training epoch 380, Batch 1000/1000: LR=9.15e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:39:42,520 | INFO | Epoch 380 Train Time 36.95746469497681s

2025-10-19 08:40:20,038 | INFO | Training epoch 381, Batch 1000/1000: LR=9.14e-05, Loss=3.15e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:40:20,119 | INFO | Epoch 381 Train Time 37.59605073928833s

2025-10-19 08:40:57,269 | INFO | Training epoch 382, Batch 1000/1000: LR=9.14e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 08:40:57,348 | INFO | Epoch 382 Train Time 37.22863292694092s

2025-10-19 08:41:34,641 | INFO | Training epoch 383, Batch 1000/1000: LR=9.14e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:41:34,732 | INFO | Epoch 383 Train Time 37.38267779350281s

2025-10-19 08:42:12,253 | INFO | Training epoch 384, Batch 1000/1000: LR=9.13e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:42:12,340 | INFO | Epoch 384 Train Time 37.6052622795105s

2025-10-19 08:42:49,834 | INFO | Training epoch 385, Batch 1000/1000: LR=9.13e-05, Loss=3.18e-02 BER=1.19e-02 FER=1.56e-01
2025-10-19 08:42:49,913 | INFO | Epoch 385 Train Time 37.57201313972473s

2025-10-19 08:43:27,245 | INFO | Training epoch 386, Batch 1000/1000: LR=9.12e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:43:27,330 | INFO | Epoch 386 Train Time 37.414108753204346s

2025-10-19 08:44:04,557 | INFO | Training epoch 387, Batch 1000/1000: LR=9.12e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:44:04,653 | INFO | Epoch 387 Train Time 37.3204619884491s

2025-10-19 08:44:41,744 | INFO | Training epoch 388, Batch 1000/1000: LR=9.11e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 08:44:41,846 | INFO | Epoch 388 Train Time 37.1925094127655s

2025-10-19 08:45:19,439 | INFO | Training epoch 389, Batch 1000/1000: LR=9.11e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 08:45:19,516 | INFO | Epoch 389 Train Time 37.66860795021057s

2025-10-19 08:45:56,797 | INFO | Training epoch 390, Batch 1000/1000: LR=9.10e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 08:45:56,884 | INFO | Epoch 390 Train Time 37.367106914520264s

2025-10-19 08:46:32,442 | INFO | Training epoch 391, Batch 1000/1000: LR=9.10e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:46:32,520 | INFO | Epoch 391 Train Time 35.63384246826172s

2025-10-19 08:47:09,344 | INFO | Training epoch 392, Batch 1000/1000: LR=9.10e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 08:47:09,451 | INFO | Epoch 392 Train Time 36.92980098724365s

2025-10-19 08:47:46,383 | INFO | Training epoch 393, Batch 1000/1000: LR=9.09e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:47:46,481 | INFO | Epoch 393 Train Time 37.028406381607056s

2025-10-19 08:48:23,490 | INFO | Training epoch 394, Batch 1000/1000: LR=9.09e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:48:23,567 | INFO | Epoch 394 Train Time 37.0848925113678s

2025-10-19 08:49:00,160 | INFO | Training epoch 395, Batch 1000/1000: LR=9.08e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 08:49:00,240 | INFO | Epoch 395 Train Time 36.671666622161865s

2025-10-19 08:49:38,040 | INFO | Training epoch 396, Batch 1000/1000: LR=9.08e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:49:38,129 | INFO | Epoch 396 Train Time 37.88731360435486s

2025-10-19 08:50:15,351 | INFO | Training epoch 397, Batch 1000/1000: LR=9.07e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:50:15,437 | INFO | Epoch 397 Train Time 37.306628704071045s

2025-10-19 08:50:52,462 | INFO | Training epoch 398, Batch 1000/1000: LR=9.07e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 08:50:52,560 | INFO | Epoch 398 Train Time 37.12136769294739s

2025-10-19 08:51:29,538 | INFO | Training epoch 399, Batch 1000/1000: LR=9.06e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:51:29,634 | INFO | Epoch 399 Train Time 37.07373833656311s

2025-10-19 08:52:06,961 | INFO | Training epoch 400, Batch 1000/1000: LR=9.06e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 08:52:07,046 | INFO | Epoch 400 Train Time 37.4104425907135s

2025-10-19 08:52:44,342 | INFO | Training epoch 401, Batch 1000/1000: LR=9.05e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 08:52:44,443 | INFO | Epoch 401 Train Time 37.39517331123352s

2025-10-19 08:53:21,570 | INFO | Training epoch 402, Batch 1000/1000: LR=9.05e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:53:21,653 | INFO | Epoch 402 Train Time 37.20928120613098s

2025-10-19 08:53:57,687 | INFO | Training epoch 403, Batch 1000/1000: LR=9.05e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 08:53:57,765 | INFO | Epoch 403 Train Time 36.110639333724976s

2025-10-19 08:54:35,247 | INFO | Training epoch 404, Batch 1000/1000: LR=9.04e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 08:54:35,330 | INFO | Epoch 404 Train Time 37.56385135650635s

2025-10-19 08:55:12,360 | INFO | Training epoch 405, Batch 1000/1000: LR=9.04e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 08:55:12,466 | INFO | Epoch 405 Train Time 37.13307738304138s

2025-10-19 08:55:49,167 | INFO | Training epoch 406, Batch 1000/1000: LR=9.03e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 08:55:49,263 | INFO | Epoch 406 Train Time 36.795183181762695s

2025-10-19 08:56:26,541 | INFO | Training epoch 407, Batch 1000/1000: LR=9.03e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:56:26,621 | INFO | Epoch 407 Train Time 37.356528520584106s

2025-10-19 08:57:04,174 | INFO | Training epoch 408, Batch 1000/1000: LR=9.02e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 08:57:04,252 | INFO | Epoch 408 Train Time 37.630505323410034s

2025-10-19 08:57:41,347 | INFO | Training epoch 409, Batch 1000/1000: LR=9.02e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 08:57:41,422 | INFO | Epoch 409 Train Time 37.16774082183838s

2025-10-19 08:58:18,162 | INFO | Training epoch 410, Batch 1000/1000: LR=9.01e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 08:58:18,246 | INFO | Epoch 410 Train Time 36.823001861572266s

2025-10-19 08:58:55,647 | INFO | Training epoch 411, Batch 1000/1000: LR=9.01e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 08:58:55,729 | INFO | Epoch 411 Train Time 37.48123216629028s

2025-10-19 08:59:32,985 | INFO | Training epoch 412, Batch 1000/1000: LR=9.00e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 08:59:33,069 | INFO | Epoch 412 Train Time 37.33871793746948s

2025-10-19 09:00:09,878 | INFO | Training epoch 413, Batch 1000/1000: LR=9.00e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:00:09,960 | INFO | Epoch 413 Train Time 36.888871908187866s

2025-10-19 09:00:47,043 | INFO | Training epoch 414, Batch 1000/1000: LR=8.99e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:00:47,143 | INFO | Epoch 414 Train Time 37.18098568916321s

2025-10-19 09:01:24,740 | INFO | Training epoch 415, Batch 1000/1000: LR=8.99e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 09:01:24,822 | INFO | Epoch 415 Train Time 37.67794871330261s

2025-10-19 09:02:01,948 | INFO | Training epoch 416, Batch 1000/1000: LR=8.98e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:02:02,057 | INFO | Epoch 416 Train Time 37.23400044441223s

2025-10-19 09:02:39,445 | INFO | Training epoch 417, Batch 1000/1000: LR=8.98e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:02:39,544 | INFO | Epoch 417 Train Time 37.48575305938721s

2025-10-19 09:03:16,346 | INFO | Training epoch 418, Batch 1000/1000: LR=8.98e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:03:16,425 | INFO | Epoch 418 Train Time 36.879825830459595s

2025-10-19 09:03:53,947 | INFO | Training epoch 419, Batch 1000/1000: LR=8.97e-05, Loss=3.16e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 09:03:54,033 | INFO | Epoch 419 Train Time 37.60564565658569s

2025-10-19 09:04:31,046 | INFO | Training epoch 420, Batch 1000/1000: LR=8.97e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:04:31,152 | INFO | Epoch 420 Train Time 37.117799282073975s

2025-10-19 09:05:08,800 | INFO | Training epoch 421, Batch 1000/1000: LR=8.96e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 09:05:08,885 | INFO | Epoch 421 Train Time 37.732173442840576s

2025-10-19 09:05:45,739 | INFO | Training epoch 422, Batch 1000/1000: LR=8.96e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:05:45,831 | INFO | Epoch 422 Train Time 36.94463396072388s

2025-10-19 09:06:23,246 | INFO | Training epoch 423, Batch 1000/1000: LR=8.95e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:06:23,348 | INFO | Epoch 423 Train Time 37.516170263290405s

2025-10-19 09:07:00,981 | INFO | Training epoch 424, Batch 1000/1000: LR=8.95e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:07:01,078 | INFO | Epoch 424 Train Time 37.72810435295105s

2025-10-19 09:07:37,935 | INFO | Training epoch 425, Batch 1000/1000: LR=8.94e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:07:38,027 | INFO | Epoch 425 Train Time 36.946847677230835s

2025-10-19 09:08:14,890 | INFO | Training epoch 426, Batch 1000/1000: LR=8.94e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:08:14,996 | INFO | Epoch 426 Train Time 36.96833086013794s

2025-10-19 09:08:52,162 | INFO | Training epoch 427, Batch 1000/1000: LR=8.93e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 09:08:52,255 | INFO | Epoch 427 Train Time 37.258362770080566s

2025-10-19 09:09:29,145 | INFO | Training epoch 428, Batch 1000/1000: LR=8.93e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:09:29,245 | INFO | Epoch 428 Train Time 36.98840689659119s

2025-10-19 09:10:06,046 | INFO | Training epoch 429, Batch 1000/1000: LR=8.92e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 09:10:06,137 | INFO | Epoch 429 Train Time 36.89016103744507s

2025-10-19 09:10:43,650 | INFO | Training epoch 430, Batch 1000/1000: LR=8.92e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:10:43,756 | INFO | Epoch 430 Train Time 37.61733841896057s

2025-10-19 09:11:20,864 | INFO | Training epoch 431, Batch 1000/1000: LR=8.91e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:11:20,977 | INFO | Epoch 431 Train Time 37.220277070999146s

2025-10-19 09:11:57,840 | INFO | Training epoch 432, Batch 1000/1000: LR=8.91e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 09:11:57,927 | INFO | Epoch 432 Train Time 36.94851064682007s

2025-10-19 09:12:35,462 | INFO | Training epoch 433, Batch 1000/1000: LR=8.90e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:12:35,553 | INFO | Epoch 433 Train Time 37.62422823905945s

2025-10-19 09:13:12,856 | INFO | Training epoch 434, Batch 1000/1000: LR=8.90e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 09:13:12,938 | INFO | Epoch 434 Train Time 37.38355588912964s

2025-10-19 09:13:49,947 | INFO | Training epoch 435, Batch 1000/1000: LR=8.89e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:13:50,038 | INFO | Epoch 435 Train Time 37.09869408607483s

2025-10-19 09:14:27,344 | INFO | Training epoch 436, Batch 1000/1000: LR=8.89e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:14:27,443 | INFO | Epoch 436 Train Time 37.40314817428589s

2025-10-19 09:15:05,842 | INFO | Training epoch 437, Batch 1000/1000: LR=8.88e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:15:05,921 | INFO | Epoch 437 Train Time 38.47766733169556s

2025-10-19 09:15:43,543 | INFO | Training epoch 438, Batch 1000/1000: LR=8.88e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:15:43,628 | INFO | Epoch 438 Train Time 37.70616030693054s

2025-10-19 09:16:19,959 | INFO | Training epoch 439, Batch 1000/1000: LR=8.87e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:16:20,049 | INFO | Epoch 439 Train Time 36.41998314857483s

2025-10-19 09:16:56,864 | INFO | Training epoch 440, Batch 1000/1000: LR=8.87e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:16:56,941 | INFO | Epoch 440 Train Time 36.890307903289795s

2025-10-19 09:17:34,345 | INFO | Training epoch 441, Batch 1000/1000: LR=8.86e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:17:34,441 | INFO | Epoch 441 Train Time 37.4987735748291s

2025-10-19 09:18:11,361 | INFO | Training epoch 442, Batch 1000/1000: LR=8.86e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:18:11,458 | INFO | Epoch 442 Train Time 37.016149044036865s

2025-10-19 09:18:48,136 | INFO | Training epoch 443, Batch 1000/1000: LR=8.85e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:18:48,219 | INFO | Epoch 443 Train Time 36.75938415527344s

2025-10-19 09:19:25,534 | INFO | Training epoch 444, Batch 1000/1000: LR=8.85e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:19:25,646 | INFO | Epoch 444 Train Time 37.42603898048401s

2025-10-19 09:20:02,940 | INFO | Training epoch 445, Batch 1000/1000: LR=8.84e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:20:03,032 | INFO | Epoch 445 Train Time 37.3838312625885s

2025-10-19 09:20:40,434 | INFO | Training epoch 446, Batch 1000/1000: LR=8.84e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:20:40,527 | INFO | Epoch 446 Train Time 37.4946722984314s

2025-10-19 09:21:17,251 | INFO | Training epoch 447, Batch 1000/1000: LR=8.83e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:21:17,344 | INFO | Epoch 447 Train Time 36.81496620178223s

2025-10-19 09:21:54,444 | INFO | Training epoch 448, Batch 1000/1000: LR=8.83e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:21:54,552 | INFO | Epoch 448 Train Time 37.20648145675659s

2025-10-19 09:22:31,532 | INFO | Training epoch 449, Batch 1000/1000: LR=8.82e-05, Loss=3.19e-02 BER=1.20e-02 FER=1.59e-01
2025-10-19 09:22:31,622 | INFO | Epoch 449 Train Time 37.06800603866577s

2025-10-19 09:23:09,150 | INFO | Training epoch 450, Batch 1000/1000: LR=8.82e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:23:09,245 | INFO | Epoch 450 Train Time 37.621612548828125s

2025-10-19 09:23:46,154 | INFO | Training epoch 451, Batch 1000/1000: LR=8.81e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:23:46,235 | INFO | Epoch 451 Train Time 36.98957633972168s

2025-10-19 09:24:23,531 | INFO | Training epoch 452, Batch 1000/1000: LR=8.81e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:24:23,612 | INFO | Epoch 452 Train Time 37.374983072280884s

2025-10-19 09:25:00,343 | INFO | Training epoch 453, Batch 1000/1000: LR=8.80e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:25:00,432 | INFO | Epoch 453 Train Time 36.81996440887451s

2025-10-19 09:25:37,841 | INFO | Training epoch 454, Batch 1000/1000: LR=8.80e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.58e-01
2025-10-19 09:25:37,929 | INFO | Epoch 454 Train Time 37.49401640892029s

2025-10-19 09:26:15,461 | INFO | Training epoch 455, Batch 1000/1000: LR=8.79e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:26:15,545 | INFO | Epoch 455 Train Time 37.615262269973755s

2025-10-19 09:26:52,859 | INFO | Training epoch 456, Batch 1000/1000: LR=8.79e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:26:52,954 | INFO | Epoch 456 Train Time 37.4076874256134s

2025-10-19 09:27:29,958 | INFO | Training epoch 457, Batch 1000/1000: LR=8.78e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:27:30,043 | INFO | Epoch 457 Train Time 37.08651041984558s

2025-10-19 09:28:07,442 | INFO | Training epoch 458, Batch 1000/1000: LR=8.78e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 09:28:07,525 | INFO | Epoch 458 Train Time 37.480021476745605s

2025-10-19 09:28:45,038 | INFO | Training epoch 459, Batch 1000/1000: LR=8.77e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:28:45,135 | INFO | Epoch 459 Train Time 37.6082558631897s

2025-10-19 09:29:22,255 | INFO | Training epoch 460, Batch 1000/1000: LR=8.77e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:29:22,353 | INFO | Epoch 460 Train Time 37.21720480918884s

2025-10-19 09:29:59,454 | INFO | Training epoch 461, Batch 1000/1000: LR=8.76e-05, Loss=3.17e-02 BER=1.19e-02 FER=1.57e-01
2025-10-19 09:29:59,569 | INFO | Epoch 461 Train Time 37.2151620388031s

2025-10-19 09:30:36,041 | INFO | Training epoch 462, Batch 1000/1000: LR=8.76e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:30:36,125 | INFO | Epoch 462 Train Time 36.5549156665802s

2025-10-19 09:31:12,896 | INFO | Training epoch 463, Batch 1000/1000: LR=8.75e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:31:12,983 | INFO | Epoch 463 Train Time 36.85746192932129s

2025-10-19 09:31:50,543 | INFO | Training epoch 464, Batch 1000/1000: LR=8.75e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:31:50,622 | INFO | Epoch 464 Train Time 37.63746523857117s

2025-10-19 09:32:28,052 | INFO | Training epoch 465, Batch 1000/1000: LR=8.74e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 09:32:28,137 | INFO | Epoch 465 Train Time 37.51300239562988s

2025-10-19 09:33:05,128 | INFO | Training epoch 466, Batch 1000/1000: LR=8.74e-05, Loss=3.08e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 09:33:05,217 | INFO | Epoch 466 Train Time 37.07884192466736s

2025-10-19 09:33:42,447 | INFO | Training epoch 467, Batch 1000/1000: LR=8.73e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:33:42,533 | INFO | Epoch 467 Train Time 37.31539607048035s

2025-10-19 09:34:19,347 | INFO | Training epoch 468, Batch 1000/1000: LR=8.73e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:34:19,426 | INFO | Epoch 468 Train Time 36.89159607887268s

2025-10-19 09:34:56,558 | INFO | Training epoch 469, Batch 1000/1000: LR=8.72e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:34:56,663 | INFO | Epoch 469 Train Time 37.23593235015869s

2025-10-19 09:35:34,050 | INFO | Training epoch 470, Batch 1000/1000: LR=8.72e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 09:35:34,147 | INFO | Epoch 470 Train Time 37.48309254646301s

2025-10-19 09:36:11,433 | INFO | Training epoch 471, Batch 1000/1000: LR=8.71e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:36:11,518 | INFO | Epoch 471 Train Time 37.3693265914917s

2025-10-19 09:36:48,547 | INFO | Training epoch 472, Batch 1000/1000: LR=8.71e-05, Loss=3.15e-02 BER=1.19e-02 FER=1.56e-01
2025-10-19 09:36:48,635 | INFO | Epoch 472 Train Time 37.11613893508911s

2025-10-19 09:37:26,052 | INFO | Training epoch 473, Batch 1000/1000: LR=8.70e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 09:37:26,143 | INFO | Epoch 473 Train Time 37.506776094436646s

2025-10-19 09:38:03,552 | INFO | Training epoch 474, Batch 1000/1000: LR=8.70e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:38:03,657 | INFO | Epoch 474 Train Time 37.512253761291504s

2025-10-19 09:38:40,459 | INFO | Training epoch 475, Batch 1000/1000: LR=8.69e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:38:40,559 | INFO | Epoch 475 Train Time 36.902002811431885s

2025-10-19 09:39:16,849 | INFO | Training epoch 476, Batch 1000/1000: LR=8.68e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:39:16,952 | INFO | Epoch 476 Train Time 36.39075016975403s

2025-10-19 09:39:54,228 | INFO | Training epoch 477, Batch 1000/1000: LR=8.68e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 09:39:54,322 | INFO | Epoch 477 Train Time 37.36958646774292s

2025-10-19 09:40:31,583 | INFO | Training epoch 478, Batch 1000/1000: LR=8.67e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:40:31,667 | INFO | Epoch 478 Train Time 37.343937397003174s

2025-10-19 09:41:08,745 | INFO | Training epoch 479, Batch 1000/1000: LR=8.67e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:41:08,839 | INFO | Epoch 479 Train Time 37.170893907547s

2025-10-19 09:41:46,141 | INFO | Training epoch 480, Batch 1000/1000: LR=8.66e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:41:46,241 | INFO | Epoch 480 Train Time 37.40073800086975s

2025-10-19 09:42:23,118 | INFO | Training epoch 481, Batch 1000/1000: LR=8.66e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:42:23,207 | INFO | Epoch 481 Train Time 36.964319944381714s

2025-10-19 09:43:00,461 | INFO | Training epoch 482, Batch 1000/1000: LR=8.65e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 09:43:00,538 | INFO | Epoch 482 Train Time 37.32949113845825s

2025-10-19 09:43:38,040 | INFO | Training epoch 483, Batch 1000/1000: LR=8.65e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:43:38,128 | INFO | Epoch 483 Train Time 37.58917832374573s

2025-10-19 09:44:15,039 | INFO | Training epoch 484, Batch 1000/1000: LR=8.64e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:44:15,134 | INFO | Epoch 484 Train Time 37.00295090675354s

2025-10-19 09:44:52,345 | INFO | Training epoch 485, Batch 1000/1000: LR=8.64e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:44:52,434 | INFO | Epoch 485 Train Time 37.299142360687256s

2025-10-19 09:45:30,137 | INFO | Training epoch 486, Batch 1000/1000: LR=8.63e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:45:30,234 | INFO | Epoch 486 Train Time 37.79825735092163s

2025-10-19 09:46:07,240 | INFO | Training epoch 487, Batch 1000/1000: LR=8.63e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 09:46:07,334 | INFO | Epoch 487 Train Time 37.098894357681274s

2025-10-19 09:46:44,347 | INFO | Training epoch 488, Batch 1000/1000: LR=8.62e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:46:44,431 | INFO | Epoch 488 Train Time 37.09552335739136s

2025-10-19 09:47:21,512 | INFO | Training epoch 489, Batch 1000/1000: LR=8.62e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:47:21,609 | INFO | Epoch 489 Train Time 37.17608165740967s

2025-10-19 09:47:58,452 | INFO | Training epoch 490, Batch 1000/1000: LR=8.61e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:47:58,536 | INFO | Epoch 490 Train Time 36.92607879638672s

2025-10-19 09:48:35,438 | INFO | Training epoch 491, Batch 1000/1000: LR=8.60e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:48:35,525 | INFO | Epoch 491 Train Time 36.98817157745361s

2025-10-19 09:49:13,228 | INFO | Training epoch 492, Batch 1000/1000: LR=8.60e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:49:13,321 | INFO | Epoch 492 Train Time 37.79373908042908s

2025-10-19 09:49:50,545 | INFO | Training epoch 493, Batch 1000/1000: LR=8.59e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:49:50,632 | INFO | Epoch 493 Train Time 37.31030344963074s

2025-10-19 09:50:28,141 | INFO | Training epoch 494, Batch 1000/1000: LR=8.59e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 09:50:28,236 | INFO | Epoch 494 Train Time 37.60146951675415s

2025-10-19 09:51:05,229 | INFO | Training epoch 495, Batch 1000/1000: LR=8.58e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:51:05,333 | INFO | Epoch 495 Train Time 37.09364724159241s

2025-10-19 09:51:42,542 | INFO | Training epoch 496, Batch 1000/1000: LR=8.58e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:51:42,634 | INFO | Epoch 496 Train Time 37.30006241798401s

2025-10-19 09:52:19,830 | INFO | Training epoch 497, Batch 1000/1000: LR=8.57e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 09:52:19,922 | INFO | Epoch 497 Train Time 37.28732204437256s

2025-10-19 09:52:57,171 | INFO | Training epoch 498, Batch 1000/1000: LR=8.57e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:52:57,255 | INFO | Epoch 498 Train Time 37.33198881149292s

2025-10-19 09:53:34,539 | INFO | Training epoch 499, Batch 1000/1000: LR=8.56e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:53:34,627 | INFO | Epoch 499 Train Time 37.371100425720215s

2025-10-19 09:54:11,619 | INFO | Training epoch 500, Batch 1000/1000: LR=8.56e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 09:54:11,715 | INFO | Epoch 500 Train Time 37.08634090423584s

2025-10-19 09:54:48,936 | INFO | Training epoch 501, Batch 1000/1000: LR=8.55e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:54:49,043 | INFO | Epoch 501 Train Time 37.328033208847046s

2025-10-19 09:55:25,948 | INFO | Training epoch 502, Batch 1000/1000: LR=8.54e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 09:55:26,027 | INFO | Epoch 502 Train Time 36.98266315460205s

2025-10-19 09:56:03,277 | INFO | Training epoch 503, Batch 1000/1000: LR=8.54e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:56:03,362 | INFO | Epoch 503 Train Time 37.33307361602783s

2025-10-19 09:56:40,749 | INFO | Training epoch 504, Batch 1000/1000: LR=8.53e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 09:56:40,834 | INFO | Epoch 504 Train Time 37.469927072525024s

2025-10-19 09:57:17,381 | INFO | Training epoch 505, Batch 1000/1000: LR=8.53e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:57:17,463 | INFO | Epoch 505 Train Time 36.627110958099365s

2025-10-19 09:57:54,651 | INFO | Training epoch 506, Batch 1000/1000: LR=8.52e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 09:57:54,739 | INFO | Epoch 506 Train Time 37.27492165565491s

2025-10-19 09:57:54,739 | INFO | [P2] saving best_model (QAT) with loss 0.030688 at epoch 506
2025-10-19 09:58:31,770 | INFO | Training epoch 507, Batch 1000/1000: LR=8.52e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 09:58:31,868 | INFO | Epoch 507 Train Time 37.109641790390015s

2025-10-19 09:59:08,441 | INFO | Training epoch 508, Batch 1000/1000: LR=8.51e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 09:59:08,527 | INFO | Epoch 508 Train Time 36.65750217437744s

2025-10-19 09:59:45,229 | INFO | Training epoch 509, Batch 1000/1000: LR=8.51e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 09:59:45,317 | INFO | Epoch 509 Train Time 36.78852367401123s

2025-10-19 10:00:22,291 | INFO | Training epoch 510, Batch 1000/1000: LR=8.50e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:00:22,372 | INFO | Epoch 510 Train Time 37.05270528793335s

2025-10-19 10:00:59,200 | INFO | Training epoch 511, Batch 1000/1000: LR=8.49e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:00:59,286 | INFO | Epoch 511 Train Time 36.911216020584106s

2025-10-19 10:01:36,243 | INFO | Training epoch 512, Batch 1000/1000: LR=8.49e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 10:01:36,339 | INFO | Epoch 512 Train Time 37.05199646949768s

2025-10-19 10:02:12,980 | INFO | Training epoch 513, Batch 1000/1000: LR=8.48e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 10:02:13,070 | INFO | Epoch 513 Train Time 36.72983407974243s

2025-10-19 10:02:49,837 | INFO | Training epoch 514, Batch 1000/1000: LR=8.48e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:02:49,922 | INFO | Epoch 514 Train Time 36.85119080543518s

2025-10-19 10:03:27,642 | INFO | Training epoch 515, Batch 1000/1000: LR=8.47e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:03:27,725 | INFO | Epoch 515 Train Time 37.801658630371094s

2025-10-19 10:04:04,949 | INFO | Training epoch 516, Batch 1000/1000: LR=8.47e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:04:05,043 | INFO | Epoch 516 Train Time 37.316553831100464s

2025-10-19 10:04:42,035 | INFO | Training epoch 517, Batch 1000/1000: LR=8.46e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:04:42,126 | INFO | Epoch 517 Train Time 37.08213686943054s

2025-10-19 10:05:19,353 | INFO | Training epoch 518, Batch 1000/1000: LR=8.46e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:05:19,452 | INFO | Epoch 518 Train Time 37.32382345199585s

2025-10-19 10:05:56,454 | INFO | Training epoch 519, Batch 1000/1000: LR=8.45e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:05:56,570 | INFO | Epoch 519 Train Time 37.1173996925354s

2025-10-19 10:06:34,072 | INFO | Training epoch 520, Batch 1000/1000: LR=8.44e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:06:34,176 | INFO | Epoch 520 Train Time 37.603503465652466s

2025-10-19 10:07:11,494 | INFO | Training epoch 521, Batch 1000/1000: LR=8.44e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:07:11,592 | INFO | Epoch 521 Train Time 37.414998054504395s

2025-10-19 10:07:48,868 | INFO | Training epoch 522, Batch 1000/1000: LR=8.43e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 10:07:48,982 | INFO | Epoch 522 Train Time 37.38925838470459s

2025-10-19 10:07:48,984 | INFO | [P2] saving best_model (QAT) with loss 0.030687 at epoch 522
2025-10-19 10:08:26,231 | INFO | Training epoch 523, Batch 1000/1000: LR=8.43e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:08:26,336 | INFO | Epoch 523 Train Time 37.336520195007324s

2025-10-19 10:09:03,242 | INFO | Training epoch 524, Batch 1000/1000: LR=8.42e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:09:03,349 | INFO | Epoch 524 Train Time 37.0120165348053s

2025-10-19 10:09:40,845 | INFO | Training epoch 525, Batch 1000/1000: LR=8.42e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:09:40,934 | INFO | Epoch 525 Train Time 37.583632946014404s

2025-10-19 10:10:18,040 | INFO | Training epoch 526, Batch 1000/1000: LR=8.41e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:10:18,142 | INFO | Epoch 526 Train Time 37.206368923187256s

2025-10-19 10:10:55,658 | INFO | Training epoch 527, Batch 1000/1000: LR=8.40e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:10:55,769 | INFO | Epoch 527 Train Time 37.62583327293396s

2025-10-19 10:11:32,846 | INFO | Training epoch 528, Batch 1000/1000: LR=8.40e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:11:32,945 | INFO | Epoch 528 Train Time 37.17560887336731s

2025-10-19 10:12:10,086 | INFO | Training epoch 529, Batch 1000/1000: LR=8.39e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:12:10,187 | INFO | Epoch 529 Train Time 37.23966670036316s

2025-10-19 10:12:47,482 | INFO | Training epoch 530, Batch 1000/1000: LR=8.39e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:12:47,569 | INFO | Epoch 530 Train Time 37.38117480278015s

2025-10-19 10:13:24,787 | INFO | Training epoch 531, Batch 1000/1000: LR=8.38e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.53e-01
2025-10-19 10:13:24,871 | INFO | Epoch 531 Train Time 37.30001974105835s

2025-10-19 10:13:24,873 | INFO | [P2] saving best_model (QAT) with loss 0.030469 at epoch 531
2025-10-19 10:14:02,042 | INFO | Training epoch 532, Batch 1000/1000: LR=8.38e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:14:02,144 | INFO | Epoch 532 Train Time 37.24197578430176s

2025-10-19 10:14:39,238 | INFO | Training epoch 533, Batch 1000/1000: LR=8.37e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:14:39,337 | INFO | Epoch 533 Train Time 37.19051742553711s

2025-10-19 10:15:16,866 | INFO | Training epoch 534, Batch 1000/1000: LR=8.36e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:15:16,964 | INFO | Epoch 534 Train Time 37.625004291534424s

2025-10-19 10:15:54,035 | INFO | Training epoch 535, Batch 1000/1000: LR=8.36e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.57e-01
2025-10-19 10:15:54,123 | INFO | Epoch 535 Train Time 37.157416343688965s

2025-10-19 10:16:31,243 | INFO | Training epoch 536, Batch 1000/1000: LR=8.35e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:16:31,333 | INFO | Epoch 536 Train Time 37.20931553840637s

2025-10-19 10:17:08,389 | INFO | Training epoch 537, Batch 1000/1000: LR=8.35e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:17:08,482 | INFO | Epoch 537 Train Time 37.14762759208679s

2025-10-19 10:17:45,665 | INFO | Training epoch 538, Batch 1000/1000: LR=8.34e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:17:45,752 | INFO | Epoch 538 Train Time 37.2680242061615s

2025-10-19 10:18:22,728 | INFO | Training epoch 539, Batch 1000/1000: LR=8.34e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:18:22,826 | INFO | Epoch 539 Train Time 37.0724196434021s

2025-10-19 10:19:00,043 | INFO | Training epoch 540, Batch 1000/1000: LR=8.33e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.54e-01
2025-10-19 10:19:00,130 | INFO | Epoch 540 Train Time 37.30304026603699s

2025-10-19 10:19:36,944 | INFO | Training epoch 541, Batch 1000/1000: LR=8.32e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:19:37,046 | INFO | Epoch 541 Train Time 36.914785385131836s

2025-10-19 10:20:14,552 | INFO | Training epoch 542, Batch 1000/1000: LR=8.32e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:20:14,651 | INFO | Epoch 542 Train Time 37.60158586502075s

2025-10-19 10:20:51,439 | INFO | Training epoch 543, Batch 1000/1000: LR=8.31e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:20:51,543 | INFO | Epoch 543 Train Time 36.890589475631714s

2025-10-19 10:21:29,129 | INFO | Training epoch 544, Batch 1000/1000: LR=8.31e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:21:29,220 | INFO | Epoch 544 Train Time 37.67532730102539s

2025-10-19 10:22:05,148 | INFO | Training epoch 545, Batch 1000/1000: LR=8.30e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:22:05,241 | INFO | Epoch 545 Train Time 36.0194149017334s

2025-10-19 10:22:42,067 | INFO | Training epoch 546, Batch 1000/1000: LR=8.29e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:22:42,156 | INFO | Epoch 546 Train Time 36.913479804992676s

2025-10-19 10:23:19,033 | INFO | Training epoch 547, Batch 1000/1000: LR=8.29e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:23:19,132 | INFO | Epoch 547 Train Time 36.97486209869385s

2025-10-19 10:23:56,447 | INFO | Training epoch 548, Batch 1000/1000: LR=8.28e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 10:23:56,561 | INFO | Epoch 548 Train Time 37.42853045463562s

2025-10-19 10:24:33,735 | INFO | Training epoch 549, Batch 1000/1000: LR=8.28e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 10:24:33,833 | INFO | Epoch 549 Train Time 37.271034479141235s

2025-10-19 10:25:11,230 | INFO | Training epoch 550, Batch 1000/1000: LR=8.27e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:25:11,340 | INFO | Epoch 550 Train Time 37.50512886047363s

2025-10-19 10:25:48,661 | INFO | Training epoch 551, Batch 1000/1000: LR=8.26e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:25:48,765 | INFO | Epoch 551 Train Time 37.42415714263916s

2025-10-19 10:26:26,056 | INFO | Training epoch 552, Batch 1000/1000: LR=8.26e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:26:26,142 | INFO | Epoch 552 Train Time 37.37629723548889s

2025-10-19 10:27:03,469 | INFO | Training epoch 553, Batch 1000/1000: LR=8.25e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:27:03,559 | INFO | Epoch 553 Train Time 37.41497325897217s

2025-10-19 10:27:40,933 | INFO | Training epoch 554, Batch 1000/1000: LR=8.25e-05, Loss=3.15e-02 BER=1.19e-02 FER=1.56e-01
2025-10-19 10:27:41,045 | INFO | Epoch 554 Train Time 37.48471450805664s

2025-10-19 10:28:18,154 | INFO | Training epoch 555, Batch 1000/1000: LR=8.24e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:28:18,268 | INFO | Epoch 555 Train Time 37.221136808395386s

2025-10-19 10:28:55,481 | INFO | Training epoch 556, Batch 1000/1000: LR=8.24e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 10:28:55,579 | INFO | Epoch 556 Train Time 37.31039047241211s

2025-10-19 10:29:32,643 | INFO | Training epoch 557, Batch 1000/1000: LR=8.23e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.54e-01
2025-10-19 10:29:32,733 | INFO | Epoch 557 Train Time 37.153090715408325s

2025-10-19 10:30:10,145 | INFO | Training epoch 558, Batch 1000/1000: LR=8.22e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:30:10,234 | INFO | Epoch 558 Train Time 37.499470949172974s

2025-10-19 10:30:47,637 | INFO | Training epoch 559, Batch 1000/1000: LR=8.22e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:30:47,740 | INFO | Epoch 559 Train Time 37.504342794418335s

2025-10-19 10:31:25,150 | INFO | Training epoch 560, Batch 1000/1000: LR=8.21e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:31:25,243 | INFO | Epoch 560 Train Time 37.502126932144165s

2025-10-19 10:32:02,542 | INFO | Training epoch 561, Batch 1000/1000: LR=8.21e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.56e-01
2025-10-19 10:32:02,642 | INFO | Epoch 561 Train Time 37.3974814414978s

2025-10-19 10:32:39,645 | INFO | Training epoch 562, Batch 1000/1000: LR=8.20e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:32:39,742 | INFO | Epoch 562 Train Time 37.09874939918518s

2025-10-19 10:33:17,062 | INFO | Training epoch 563, Batch 1000/1000: LR=8.19e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:33:17,163 | INFO | Epoch 563 Train Time 37.420093059539795s

2025-10-19 10:33:54,537 | INFO | Training epoch 564, Batch 1000/1000: LR=8.19e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:33:54,657 | INFO | Epoch 564 Train Time 37.49343276023865s

2025-10-19 10:34:30,642 | INFO | Training epoch 565, Batch 1000/1000: LR=8.18e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:34:30,725 | INFO | Epoch 565 Train Time 36.06595826148987s

2025-10-19 10:35:07,548 | INFO | Training epoch 566, Batch 1000/1000: LR=8.18e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 10:35:07,634 | INFO | Epoch 566 Train Time 36.90837097167969s

2025-10-19 10:35:44,944 | INFO | Training epoch 567, Batch 1000/1000: LR=8.17e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:35:45,051 | INFO | Epoch 567 Train Time 37.41415596008301s

2025-10-19 10:36:22,566 | INFO | Training epoch 568, Batch 1000/1000: LR=8.16e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 10:36:22,680 | INFO | Epoch 568 Train Time 37.62828350067139s

2025-10-19 10:37:00,268 | INFO | Training epoch 569, Batch 1000/1000: LR=8.16e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:37:00,364 | INFO | Epoch 569 Train Time 37.68354868888855s

2025-10-19 10:37:37,742 | INFO | Training epoch 570, Batch 1000/1000: LR=8.15e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:37:37,834 | INFO | Epoch 570 Train Time 37.468422651290894s

2025-10-19 10:38:14,785 | INFO | Training epoch 571, Batch 1000/1000: LR=8.14e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:38:14,871 | INFO | Epoch 571 Train Time 37.03552317619324s

2025-10-19 10:38:52,044 | INFO | Training epoch 572, Batch 1000/1000: LR=8.14e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:38:52,143 | INFO | Epoch 572 Train Time 37.27116799354553s

2025-10-19 10:39:29,242 | INFO | Training epoch 573, Batch 1000/1000: LR=8.13e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:39:29,329 | INFO | Epoch 573 Train Time 37.18452429771423s

2025-10-19 10:40:06,149 | INFO | Training epoch 574, Batch 1000/1000: LR=8.13e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:40:06,244 | INFO | Epoch 574 Train Time 36.91395926475525s

2025-10-19 10:40:43,346 | INFO | Training epoch 575, Batch 1000/1000: LR=8.12e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:40:43,430 | INFO | Epoch 575 Train Time 37.184926986694336s

2025-10-19 10:41:21,050 | INFO | Training epoch 576, Batch 1000/1000: LR=8.11e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:41:21,152 | INFO | Epoch 576 Train Time 37.72133278846741s

2025-10-19 10:41:58,666 | INFO | Training epoch 577, Batch 1000/1000: LR=8.11e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:41:58,752 | INFO | Epoch 577 Train Time 37.598251819610596s

2025-10-19 10:42:35,744 | INFO | Training epoch 578, Batch 1000/1000: LR=8.10e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.55e-01
2025-10-19 10:42:35,830 | INFO | Epoch 578 Train Time 37.07618045806885s

2025-10-19 10:43:12,549 | INFO | Training epoch 579, Batch 1000/1000: LR=8.10e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:43:12,651 | INFO | Epoch 579 Train Time 36.819292068481445s

2025-10-19 10:43:49,953 | INFO | Training epoch 580, Batch 1000/1000: LR=8.09e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:43:50,052 | INFO | Epoch 580 Train Time 37.39969301223755s

2025-10-19 10:44:27,742 | INFO | Training epoch 581, Batch 1000/1000: LR=8.08e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 10:44:27,851 | INFO | Epoch 581 Train Time 37.7979416847229s

2025-10-19 10:45:05,634 | INFO | Training epoch 582, Batch 1000/1000: LR=8.08e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:45:05,747 | INFO | Epoch 582 Train Time 37.89405012130737s

2025-10-19 10:45:42,764 | INFO | Training epoch 583, Batch 1000/1000: LR=8.07e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:45:42,852 | INFO | Epoch 583 Train Time 37.10271883010864s

2025-10-19 10:46:20,146 | INFO | Training epoch 584, Batch 1000/1000: LR=8.07e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 10:46:20,246 | INFO | Epoch 584 Train Time 37.39162087440491s

2025-10-19 10:46:57,747 | INFO | Training epoch 585, Batch 1000/1000: LR=8.06e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 10:46:57,867 | INFO | Epoch 585 Train Time 37.62020254135132s

2025-10-19 10:47:35,051 | INFO | Training epoch 586, Batch 1000/1000: LR=8.05e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 10:47:35,151 | INFO | Epoch 586 Train Time 37.28230619430542s

2025-10-19 10:48:12,740 | INFO | Training epoch 587, Batch 1000/1000: LR=8.05e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 10:48:12,841 | INFO | Epoch 587 Train Time 37.68914747238159s

2025-10-19 10:48:50,265 | INFO | Training epoch 588, Batch 1000/1000: LR=8.04e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 10:48:50,371 | INFO | Epoch 588 Train Time 37.52877855300903s

2025-10-19 10:49:27,846 | INFO | Training epoch 589, Batch 1000/1000: LR=8.03e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:49:27,955 | INFO | Epoch 589 Train Time 37.58268904685974s

2025-10-19 10:50:05,241 | INFO | Training epoch 590, Batch 1000/1000: LR=8.03e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:50:05,333 | INFO | Epoch 590 Train Time 37.37717008590698s

2025-10-19 10:50:42,531 | INFO | Training epoch 591, Batch 1000/1000: LR=8.02e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:50:42,620 | INFO | Epoch 591 Train Time 37.28582048416138s

2025-10-19 10:51:19,704 | INFO | Training epoch 592, Batch 1000/1000: LR=8.02e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:51:19,783 | INFO | Epoch 592 Train Time 37.1618390083313s

2025-10-19 10:51:57,624 | INFO | Training epoch 593, Batch 1000/1000: LR=8.01e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 10:51:57,713 | INFO | Epoch 593 Train Time 37.929166316986084s

2025-10-19 10:52:34,631 | INFO | Training epoch 594, Batch 1000/1000: LR=8.00e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:52:34,734 | INFO | Epoch 594 Train Time 37.01763153076172s

2025-10-19 10:53:11,446 | INFO | Training epoch 595, Batch 1000/1000: LR=8.00e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 10:53:11,540 | INFO | Epoch 595 Train Time 36.80498123168945s

2025-10-19 10:53:48,966 | INFO | Training epoch 596, Batch 1000/1000: LR=7.99e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:53:49,049 | INFO | Epoch 596 Train Time 37.507601261138916s

2025-10-19 10:54:26,538 | INFO | Training epoch 597, Batch 1000/1000: LR=7.98e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:54:26,637 | INFO | Epoch 597 Train Time 37.58689832687378s

2025-10-19 10:55:04,143 | INFO | Training epoch 598, Batch 1000/1000: LR=7.98e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 10:55:04,234 | INFO | Epoch 598 Train Time 37.59454083442688s

2025-10-19 10:55:41,237 | INFO | Training epoch 599, Batch 1000/1000: LR=7.97e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:55:41,328 | INFO | Epoch 599 Train Time 37.092456102371216s

2025-10-19 10:56:18,085 | INFO | Training epoch 600, Batch 1000/1000: LR=7.97e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 10:56:18,176 | INFO | Epoch 600 Train Time 36.84598898887634s

2025-10-19 10:56:55,251 | INFO | Training epoch 601, Batch 1000/1000: LR=7.96e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 10:56:55,378 | INFO | Epoch 601 Train Time 37.200621604919434s

2025-10-19 10:57:32,652 | INFO | Training epoch 602, Batch 1000/1000: LR=7.95e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 10:57:32,746 | INFO | Epoch 602 Train Time 37.365697622299194s

2025-10-19 10:58:10,090 | INFO | Training epoch 603, Batch 1000/1000: LR=7.95e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 10:58:10,180 | INFO | Epoch 603 Train Time 37.43184947967529s

2025-10-19 10:58:47,985 | INFO | Training epoch 604, Batch 1000/1000: LR=7.94e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 10:58:48,088 | INFO | Epoch 604 Train Time 37.90578651428223s

2025-10-19 10:59:25,342 | INFO | Training epoch 605, Batch 1000/1000: LR=7.93e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.54e-01
2025-10-19 10:59:25,429 | INFO | Epoch 605 Train Time 37.33951783180237s

2025-10-19 11:00:02,749 | INFO | Training epoch 606, Batch 1000/1000: LR=7.93e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 11:00:02,843 | INFO | Epoch 606 Train Time 37.412707805633545s

2025-10-19 11:00:39,984 | INFO | Training epoch 607, Batch 1000/1000: LR=7.92e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.54e-01
2025-10-19 11:00:40,066 | INFO | Epoch 607 Train Time 37.22000575065613s

2025-10-19 11:01:17,062 | INFO | Training epoch 608, Batch 1000/1000: LR=7.92e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 11:01:17,168 | INFO | Epoch 608 Train Time 37.100496768951416s

2025-10-19 11:01:54,545 | INFO | Training epoch 609, Batch 1000/1000: LR=7.91e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:01:54,632 | INFO | Epoch 609 Train Time 37.46309447288513s

2025-10-19 11:02:32,071 | INFO | Training epoch 610, Batch 1000/1000: LR=7.90e-05, Loss=3.12e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 11:02:32,177 | INFO | Epoch 610 Train Time 37.544092655181885s

2025-10-19 11:03:09,705 | INFO | Training epoch 611, Batch 1000/1000: LR=7.90e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:03:09,794 | INFO | Epoch 611 Train Time 37.615639209747314s

2025-10-19 11:03:46,367 | INFO | Training epoch 612, Batch 1000/1000: LR=7.89e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:03:46,466 | INFO | Epoch 612 Train Time 36.671123027801514s

2025-10-19 11:04:23,546 | INFO | Training epoch 613, Batch 1000/1000: LR=7.88e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:04:23,640 | INFO | Epoch 613 Train Time 37.17243576049805s

2025-10-19 11:05:00,887 | INFO | Training epoch 614, Batch 1000/1000: LR=7.88e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:05:00,993 | INFO | Epoch 614 Train Time 37.3507354259491s

2025-10-19 11:05:38,259 | INFO | Training epoch 615, Batch 1000/1000: LR=7.87e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:05:38,351 | INFO | Epoch 615 Train Time 37.35649657249451s

2025-10-19 11:06:15,246 | INFO | Training epoch 616, Batch 1000/1000: LR=7.86e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:06:15,350 | INFO | Epoch 616 Train Time 36.99661588668823s

2025-10-19 11:06:52,643 | INFO | Training epoch 617, Batch 1000/1000: LR=7.86e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 11:06:52,746 | INFO | Epoch 617 Train Time 37.39547324180603s

2025-10-19 11:07:30,045 | INFO | Training epoch 618, Batch 1000/1000: LR=7.85e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:07:30,156 | INFO | Epoch 618 Train Time 37.408308029174805s

2025-10-19 11:08:07,384 | INFO | Training epoch 619, Batch 1000/1000: LR=7.85e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 11:08:07,469 | INFO | Epoch 619 Train Time 37.31188893318176s

2025-10-19 11:08:44,962 | INFO | Training epoch 620, Batch 1000/1000: LR=7.84e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:08:45,053 | INFO | Epoch 620 Train Time 37.58208990097046s

2025-10-19 11:09:22,046 | INFO | Training epoch 621, Batch 1000/1000: LR=7.83e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:09:22,142 | INFO | Epoch 621 Train Time 37.08820605278015s

2025-10-19 11:09:59,629 | INFO | Training epoch 622, Batch 1000/1000: LR=7.83e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:09:59,723 | INFO | Epoch 622 Train Time 37.57952356338501s

2025-10-19 11:10:36,642 | INFO | Training epoch 623, Batch 1000/1000: LR=7.82e-05, Loss=3.16e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 11:10:36,748 | INFO | Epoch 623 Train Time 37.02430605888367s

2025-10-19 11:11:13,440 | INFO | Training epoch 624, Batch 1000/1000: LR=7.81e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:11:13,531 | INFO | Epoch 624 Train Time 36.78187942504883s

2025-10-19 11:11:50,680 | INFO | Training epoch 625, Batch 1000/1000: LR=7.81e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 11:11:50,779 | INFO | Epoch 625 Train Time 37.246676445007324s

2025-10-19 11:12:27,751 | INFO | Training epoch 626, Batch 1000/1000: LR=7.80e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:12:27,847 | INFO | Epoch 626 Train Time 37.066150426864624s

2025-10-19 11:13:05,045 | INFO | Training epoch 627, Batch 1000/1000: LR=7.79e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 11:13:05,137 | INFO | Epoch 627 Train Time 37.28949785232544s

2025-10-19 11:13:42,039 | INFO | Training epoch 628, Batch 1000/1000: LR=7.79e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 11:13:42,141 | INFO | Epoch 628 Train Time 37.00168180465698s

2025-10-19 11:14:19,461 | INFO | Training epoch 629, Batch 1000/1000: LR=7.78e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 11:14:19,549 | INFO | Epoch 629 Train Time 37.40738344192505s

2025-10-19 11:14:57,073 | INFO | Training epoch 630, Batch 1000/1000: LR=7.77e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:14:57,180 | INFO | Epoch 630 Train Time 37.62975025177002s

2025-10-19 11:15:34,391 | INFO | Training epoch 631, Batch 1000/1000: LR=7.77e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:15:34,490 | INFO | Epoch 631 Train Time 37.30958414077759s

2025-10-19 11:16:11,539 | INFO | Training epoch 632, Batch 1000/1000: LR=7.76e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:16:11,632 | INFO | Epoch 632 Train Time 37.140414237976074s

2025-10-19 11:16:48,684 | INFO | Training epoch 633, Batch 1000/1000: LR=7.75e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 11:16:48,790 | INFO | Epoch 633 Train Time 37.157429456710815s

2025-10-19 11:17:26,052 | INFO | Training epoch 634, Batch 1000/1000: LR=7.75e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:17:26,160 | INFO | Epoch 634 Train Time 37.36831617355347s

2025-10-19 11:18:02,938 | INFO | Training epoch 635, Batch 1000/1000: LR=7.74e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:18:03,037 | INFO | Epoch 635 Train Time 36.87586212158203s

2025-10-19 11:18:40,043 | INFO | Training epoch 636, Batch 1000/1000: LR=7.74e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:18:40,164 | INFO | Epoch 636 Train Time 37.12482142448425s

2025-10-19 11:19:16,435 | INFO | Training epoch 637, Batch 1000/1000: LR=7.73e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:19:16,525 | INFO | Epoch 637 Train Time 36.360151052474976s

2025-10-19 11:19:53,730 | INFO | Training epoch 638, Batch 1000/1000: LR=7.72e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:19:53,825 | INFO | Epoch 638 Train Time 37.298898696899414s

2025-10-19 11:20:30,977 | INFO | Training epoch 639, Batch 1000/1000: LR=7.72e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:20:31,065 | INFO | Epoch 639 Train Time 37.23875403404236s

2025-10-19 11:21:08,042 | INFO | Training epoch 640, Batch 1000/1000: LR=7.71e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:21:08,160 | INFO | Epoch 640 Train Time 37.09311056137085s

2025-10-19 11:21:45,432 | INFO | Training epoch 641, Batch 1000/1000: LR=7.70e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 11:21:45,526 | INFO | Epoch 641 Train Time 37.36438703536987s

2025-10-19 11:22:22,441 | INFO | Training epoch 642, Batch 1000/1000: LR=7.70e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 11:22:22,535 | INFO | Epoch 642 Train Time 37.00839114189148s

2025-10-19 11:22:22,536 | INFO | [P2] saving best_model (QAT) with loss 0.030420 at epoch 642
2025-10-19 11:22:59,545 | INFO | Training epoch 643, Batch 1000/1000: LR=7.69e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:22:59,645 | INFO | Epoch 643 Train Time 37.091885805130005s

2025-10-19 11:23:36,031 | INFO | Training epoch 644, Batch 1000/1000: LR=7.68e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.55e-01
2025-10-19 11:23:36,126 | INFO | Epoch 644 Train Time 36.480326890945435s

2025-10-19 11:24:12,944 | INFO | Training epoch 645, Batch 1000/1000: LR=7.68e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 11:24:13,038 | INFO | Epoch 645 Train Time 36.90940976142883s

2025-10-19 11:24:49,753 | INFO | Training epoch 646, Batch 1000/1000: LR=7.67e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:24:49,838 | INFO | Epoch 646 Train Time 36.79899263381958s

2025-10-19 11:25:26,145 | INFO | Training epoch 647, Batch 1000/1000: LR=7.66e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:25:26,235 | INFO | Epoch 647 Train Time 36.396592140197754s

2025-10-19 11:26:03,051 | INFO | Training epoch 648, Batch 1000/1000: LR=7.66e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 11:26:03,162 | INFO | Epoch 648 Train Time 36.92533326148987s

2025-10-19 11:26:40,340 | INFO | Training epoch 649, Batch 1000/1000: LR=7.65e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 11:26:40,447 | INFO | Epoch 649 Train Time 37.2839560508728s

2025-10-19 11:27:17,839 | INFO | Training epoch 650, Batch 1000/1000: LR=7.64e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 11:27:17,943 | INFO | Epoch 650 Train Time 37.495304346084595s

2025-10-19 11:27:55,344 | INFO | Training epoch 651, Batch 1000/1000: LR=7.64e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:27:55,437 | INFO | Epoch 651 Train Time 37.49354863166809s

2025-10-19 11:28:32,415 | INFO | Training epoch 652, Batch 1000/1000: LR=7.63e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:28:32,513 | INFO | Epoch 652 Train Time 37.07516145706177s

2025-10-19 11:29:09,845 | INFO | Training epoch 653, Batch 1000/1000: LR=7.62e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 11:29:09,940 | INFO | Epoch 653 Train Time 37.42472052574158s

2025-10-19 11:29:47,157 | INFO | Training epoch 654, Batch 1000/1000: LR=7.62e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:29:47,249 | INFO | Epoch 654 Train Time 37.30867052078247s

2025-10-19 11:30:24,293 | INFO | Training epoch 655, Batch 1000/1000: LR=7.61e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:30:24,374 | INFO | Epoch 655 Train Time 37.1232225894928s

2025-10-19 11:31:01,837 | INFO | Training epoch 656, Batch 1000/1000: LR=7.60e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:31:01,929 | INFO | Epoch 656 Train Time 37.553494930267334s

2025-10-19 11:31:39,460 | INFO | Training epoch 657, Batch 1000/1000: LR=7.60e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:31:39,560 | INFO | Epoch 657 Train Time 37.62928891181946s

2025-10-19 11:32:16,541 | INFO | Training epoch 658, Batch 1000/1000: LR=7.59e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:32:16,627 | INFO | Epoch 658 Train Time 37.06617712974548s

2025-10-19 11:32:53,535 | INFO | Training epoch 659, Batch 1000/1000: LR=7.58e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:32:53,619 | INFO | Epoch 659 Train Time 36.9889931678772s

2025-10-19 11:33:30,650 | INFO | Training epoch 660, Batch 1000/1000: LR=7.58e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:33:30,753 | INFO | Epoch 660 Train Time 37.13376307487488s

2025-10-19 11:34:07,868 | INFO | Training epoch 661, Batch 1000/1000: LR=7.57e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:34:07,979 | INFO | Epoch 661 Train Time 37.22452139854431s

2025-10-19 11:34:45,437 | INFO | Training epoch 662, Batch 1000/1000: LR=7.56e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 11:34:45,551 | INFO | Epoch 662 Train Time 37.57077598571777s

2025-10-19 11:35:22,939 | INFO | Training epoch 663, Batch 1000/1000: LR=7.56e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:35:23,040 | INFO | Epoch 663 Train Time 37.48653030395508s

2025-10-19 11:36:00,356 | INFO | Training epoch 664, Batch 1000/1000: LR=7.55e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:36:00,455 | INFO | Epoch 664 Train Time 37.413175106048584s

2025-10-19 11:36:37,142 | INFO | Training epoch 665, Batch 1000/1000: LR=7.54e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:36:37,256 | INFO | Epoch 665 Train Time 36.8002724647522s

2025-10-19 11:37:14,145 | INFO | Training epoch 666, Batch 1000/1000: LR=7.54e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 11:37:14,238 | INFO | Epoch 666 Train Time 36.98085379600525s

2025-10-19 11:37:51,756 | INFO | Training epoch 667, Batch 1000/1000: LR=7.53e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:37:51,864 | INFO | Epoch 667 Train Time 37.624831199645996s

2025-10-19 11:38:28,456 | INFO | Training epoch 668, Batch 1000/1000: LR=7.52e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:38:28,541 | INFO | Epoch 668 Train Time 36.675873041152954s

2025-10-19 11:39:05,778 | INFO | Training epoch 669, Batch 1000/1000: LR=7.52e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 11:39:05,879 | INFO | Epoch 669 Train Time 37.336952209472656s

2025-10-19 11:39:43,068 | INFO | Training epoch 670, Batch 1000/1000: LR=7.51e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:39:43,169 | INFO | Epoch 670 Train Time 37.288394927978516s

2025-10-19 11:40:20,349 | INFO | Training epoch 671, Batch 1000/1000: LR=7.50e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:40:20,454 | INFO | Epoch 671 Train Time 37.283499240875244s

2025-10-19 11:40:57,542 | INFO | Training epoch 672, Batch 1000/1000: LR=7.50e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:40:57,637 | INFO | Epoch 672 Train Time 37.178091287612915s

2025-10-19 11:41:34,748 | INFO | Training epoch 673, Batch 1000/1000: LR=7.49e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:41:34,861 | INFO | Epoch 673 Train Time 37.22291421890259s

2025-10-19 11:42:12,275 | INFO | Training epoch 674, Batch 1000/1000: LR=7.48e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:42:12,370 | INFO | Epoch 674 Train Time 37.50732660293579s

2025-10-19 11:42:49,730 | INFO | Training epoch 675, Batch 1000/1000: LR=7.48e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:42:49,818 | INFO | Epoch 675 Train Time 37.4471378326416s

2025-10-19 11:43:26,953 | INFO | Training epoch 676, Batch 1000/1000: LR=7.47e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:43:27,049 | INFO | Epoch 676 Train Time 37.22961688041687s

2025-10-19 11:44:04,145 | INFO | Training epoch 677, Batch 1000/1000: LR=7.46e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 11:44:04,234 | INFO | Epoch 677 Train Time 37.18407392501831s

2025-10-19 11:44:41,339 | INFO | Training epoch 678, Batch 1000/1000: LR=7.46e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 11:44:41,422 | INFO | Epoch 678 Train Time 37.185999631881714s

2025-10-19 11:45:18,634 | INFO | Training epoch 679, Batch 1000/1000: LR=7.45e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 11:45:18,722 | INFO | Epoch 679 Train Time 37.298362255096436s

2025-10-19 11:45:55,653 | INFO | Training epoch 680, Batch 1000/1000: LR=7.44e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:45:55,751 | INFO | Epoch 680 Train Time 37.02801322937012s

2025-10-19 11:46:32,958 | INFO | Training epoch 681, Batch 1000/1000: LR=7.43e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:46:33,062 | INFO | Epoch 681 Train Time 37.310017347335815s

2025-10-19 11:47:10,042 | INFO | Training epoch 682, Batch 1000/1000: LR=7.43e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:47:10,146 | INFO | Epoch 682 Train Time 37.0826313495636s

2025-10-19 11:47:47,656 | INFO | Training epoch 683, Batch 1000/1000: LR=7.42e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 11:47:47,746 | INFO | Epoch 683 Train Time 37.59913492202759s

2025-10-19 11:48:25,306 | INFO | Training epoch 684, Batch 1000/1000: LR=7.41e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:48:25,413 | INFO | Epoch 684 Train Time 37.666101694107056s

2025-10-19 11:49:02,864 | INFO | Training epoch 685, Batch 1000/1000: LR=7.41e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:49:02,963 | INFO | Epoch 685 Train Time 37.5484778881073s

2025-10-19 11:49:39,953 | INFO | Training epoch 686, Batch 1000/1000: LR=7.40e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:49:40,050 | INFO | Epoch 686 Train Time 37.08580923080444s

2025-10-19 11:50:17,119 | INFO | Training epoch 687, Batch 1000/1000: LR=7.39e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 11:50:17,220 | INFO | Epoch 687 Train Time 37.16848301887512s

2025-10-19 11:50:53,941 | INFO | Training epoch 688, Batch 1000/1000: LR=7.39e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:50:54,036 | INFO | Epoch 688 Train Time 36.81550598144531s

2025-10-19 11:51:30,913 | INFO | Training epoch 689, Batch 1000/1000: LR=7.38e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 11:51:31,005 | INFO | Epoch 689 Train Time 36.9673707485199s

2025-10-19 11:52:07,739 | INFO | Training epoch 690, Batch 1000/1000: LR=7.37e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:52:07,850 | INFO | Epoch 690 Train Time 36.844505071640015s

2025-10-19 11:52:44,893 | INFO | Training epoch 691, Batch 1000/1000: LR=7.37e-05, Loss=3.16e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 11:52:44,986 | INFO | Epoch 691 Train Time 37.13403391838074s

2025-10-19 11:53:22,250 | INFO | Training epoch 692, Batch 1000/1000: LR=7.36e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:53:22,353 | INFO | Epoch 692 Train Time 37.36482763290405s

2025-10-19 11:53:59,439 | INFO | Training epoch 693, Batch 1000/1000: LR=7.35e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 11:53:59,524 | INFO | Epoch 693 Train Time 37.169939041137695s

2025-10-19 11:54:36,550 | INFO | Training epoch 694, Batch 1000/1000: LR=7.35e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 11:54:36,663 | INFO | Epoch 694 Train Time 37.13583040237427s

2025-10-19 11:55:13,864 | INFO | Training epoch 695, Batch 1000/1000: LR=7.34e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:55:13,947 | INFO | Epoch 695 Train Time 37.28270101547241s

2025-10-19 11:55:51,551 | INFO | Training epoch 696, Batch 1000/1000: LR=7.33e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 11:55:51,639 | INFO | Epoch 696 Train Time 37.690680742263794s

2025-10-19 11:55:51,639 | INFO | [P2] saving best_model (QAT) with loss 0.030366 at epoch 696
2025-10-19 11:56:28,772 | INFO | Training epoch 697, Batch 1000/1000: LR=7.32e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 11:56:28,858 | INFO | Epoch 697 Train Time 37.20220994949341s

2025-10-19 11:57:05,779 | INFO | Training epoch 698, Batch 1000/1000: LR=7.32e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 11:57:05,889 | INFO | Epoch 698 Train Time 37.029202938079834s

2025-10-19 11:57:42,845 | INFO | Training epoch 699, Batch 1000/1000: LR=7.31e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 11:57:42,936 | INFO | Epoch 699 Train Time 37.046282052993774s

2025-10-19 11:58:20,250 | INFO | Training epoch 700, Batch 1000/1000: LR=7.30e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 11:58:20,342 | INFO | Epoch 700 Train Time 37.404576539993286s

2025-10-19 11:58:57,243 | INFO | Training epoch 701, Batch 1000/1000: LR=7.30e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 11:58:57,344 | INFO | Epoch 701 Train Time 37.00002312660217s

2025-10-19 11:59:34,377 | INFO | Training epoch 702, Batch 1000/1000: LR=7.29e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 11:59:34,511 | INFO | Epoch 702 Train Time 37.16513419151306s

2025-10-19 12:00:11,737 | INFO | Training epoch 703, Batch 1000/1000: LR=7.28e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:00:11,844 | INFO | Epoch 703 Train Time 37.33260464668274s

2025-10-19 12:00:48,876 | INFO | Training epoch 704, Batch 1000/1000: LR=7.28e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:00:48,976 | INFO | Epoch 704 Train Time 37.1298394203186s

2025-10-19 12:01:26,135 | INFO | Training epoch 705, Batch 1000/1000: LR=7.27e-05, Loss=3.09e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:01:26,236 | INFO | Epoch 705 Train Time 37.2585289478302s

2025-10-19 12:02:03,546 | INFO | Training epoch 706, Batch 1000/1000: LR=7.26e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:02:03,653 | INFO | Epoch 706 Train Time 37.415984869003296s

2025-10-19 12:02:40,682 | INFO | Training epoch 707, Batch 1000/1000: LR=7.26e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:02:40,791 | INFO | Epoch 707 Train Time 37.13585591316223s

2025-10-19 12:03:17,324 | INFO | Training epoch 708, Batch 1000/1000: LR=7.25e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:03:17,413 | INFO | Epoch 708 Train Time 36.61939573287964s

2025-10-19 12:03:54,770 | INFO | Training epoch 709, Batch 1000/1000: LR=7.24e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:03:54,858 | INFO | Epoch 709 Train Time 37.44398355484009s

2025-10-19 12:04:31,943 | INFO | Training epoch 710, Batch 1000/1000: LR=7.23e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:04:32,037 | INFO | Epoch 710 Train Time 37.17706918716431s

2025-10-19 12:05:09,348 | INFO | Training epoch 711, Batch 1000/1000: LR=7.23e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:05:09,448 | INFO | Epoch 711 Train Time 37.40838313102722s

2025-10-19 12:05:46,564 | INFO | Training epoch 712, Batch 1000/1000: LR=7.22e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 12:05:46,665 | INFO | Epoch 712 Train Time 37.216148138046265s

2025-10-19 12:06:22,845 | INFO | Training epoch 713, Batch 1000/1000: LR=7.21e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:06:22,941 | INFO | Epoch 713 Train Time 36.27432680130005s

2025-10-19 12:07:00,043 | INFO | Training epoch 714, Batch 1000/1000: LR=7.21e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:07:00,125 | INFO | Epoch 714 Train Time 37.18308448791504s

2025-10-19 12:07:37,540 | INFO | Training epoch 715, Batch 1000/1000: LR=7.20e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:07:37,634 | INFO | Epoch 715 Train Time 37.50668382644653s

2025-10-19 12:08:14,938 | INFO | Training epoch 716, Batch 1000/1000: LR=7.19e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:08:15,045 | INFO | Epoch 716 Train Time 37.40977215766907s

2025-10-19 12:08:52,040 | INFO | Training epoch 717, Batch 1000/1000: LR=7.19e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:08:52,126 | INFO | Epoch 717 Train Time 37.080037117004395s

2025-10-19 12:09:29,341 | INFO | Training epoch 718, Batch 1000/1000: LR=7.18e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:09:29,438 | INFO | Epoch 718 Train Time 37.309569120407104s

2025-10-19 12:10:06,648 | INFO | Training epoch 719, Batch 1000/1000: LR=7.17e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:10:06,748 | INFO | Epoch 719 Train Time 37.30835938453674s

2025-10-19 12:10:43,939 | INFO | Training epoch 720, Batch 1000/1000: LR=7.16e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:10:44,027 | INFO | Epoch 720 Train Time 37.276244163513184s

2025-10-19 12:11:21,172 | INFO | Training epoch 721, Batch 1000/1000: LR=7.16e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 12:11:21,271 | INFO | Epoch 721 Train Time 37.24387001991272s

2025-10-19 12:11:58,546 | INFO | Training epoch 722, Batch 1000/1000: LR=7.15e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 12:11:58,661 | INFO | Epoch 722 Train Time 37.388885736465454s

2025-10-19 12:12:35,941 | INFO | Training epoch 723, Batch 1000/1000: LR=7.14e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:12:36,034 | INFO | Epoch 723 Train Time 37.37178421020508s

2025-10-19 12:13:13,448 | INFO | Training epoch 724, Batch 1000/1000: LR=7.14e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:13:13,560 | INFO | Epoch 724 Train Time 37.522573947906494s

2025-10-19 12:13:13,560 | INFO | [P2] saving best_model (QAT) with loss 0.030329 at epoch 724
2025-10-19 12:13:50,977 | INFO | Training epoch 725, Batch 1000/1000: LR=7.13e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:13:51,059 | INFO | Epoch 725 Train Time 37.47856545448303s

2025-10-19 12:14:28,188 | INFO | Training epoch 726, Batch 1000/1000: LR=7.12e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:14:28,285 | INFO | Epoch 726 Train Time 37.223609924316406s

2025-10-19 12:15:05,139 | INFO | Training epoch 727, Batch 1000/1000: LR=7.12e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:15:05,246 | INFO | Epoch 727 Train Time 36.959349155426025s

2025-10-19 12:15:42,245 | INFO | Training epoch 728, Batch 1000/1000: LR=7.11e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:15:42,338 | INFO | Epoch 728 Train Time 37.09152889251709s

2025-10-19 12:16:19,957 | INFO | Training epoch 729, Batch 1000/1000: LR=7.10e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:16:20,054 | INFO | Epoch 729 Train Time 37.714762926101685s

2025-10-19 12:16:57,067 | INFO | Training epoch 730, Batch 1000/1000: LR=7.09e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:16:57,169 | INFO | Epoch 730 Train Time 37.113239765167236s

2025-10-19 12:17:34,037 | INFO | Training epoch 731, Batch 1000/1000: LR=7.09e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:17:34,134 | INFO | Epoch 731 Train Time 36.96436357498169s

2025-10-19 12:18:11,138 | INFO | Training epoch 732, Batch 1000/1000: LR=7.08e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:18:11,242 | INFO | Epoch 732 Train Time 37.10649585723877s

2025-10-19 12:18:48,186 | INFO | Training epoch 733, Batch 1000/1000: LR=7.07e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:18:48,279 | INFO | Epoch 733 Train Time 37.03566765785217s

2025-10-19 12:19:24,742 | INFO | Training epoch 734, Batch 1000/1000: LR=7.07e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:19:24,833 | INFO | Epoch 734 Train Time 36.55266046524048s

2025-10-19 12:20:02,855 | INFO | Training epoch 735, Batch 1000/1000: LR=7.06e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:20:02,950 | INFO | Epoch 735 Train Time 38.1164186000824s

2025-10-19 12:20:40,048 | INFO | Training epoch 736, Batch 1000/1000: LR=7.05e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:20:40,158 | INFO | Epoch 736 Train Time 37.2063672542572s

2025-10-19 12:21:17,151 | INFO | Training epoch 737, Batch 1000/1000: LR=7.04e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 12:21:17,257 | INFO | Epoch 737 Train Time 37.09782934188843s

2025-10-19 12:21:54,448 | INFO | Training epoch 738, Batch 1000/1000: LR=7.04e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:21:54,544 | INFO | Epoch 738 Train Time 37.28587460517883s

2025-10-19 12:22:31,945 | INFO | Training epoch 739, Batch 1000/1000: LR=7.03e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:22:32,032 | INFO | Epoch 739 Train Time 37.48653173446655s

2025-10-19 12:23:09,034 | INFO | Training epoch 740, Batch 1000/1000: LR=7.02e-05, Loss=3.14e-02 BER=1.18e-02 FER=1.54e-01
2025-10-19 12:23:09,129 | INFO | Epoch 740 Train Time 37.09665870666504s

2025-10-19 12:23:45,954 | INFO | Training epoch 741, Batch 1000/1000: LR=7.02e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:23:46,040 | INFO | Epoch 741 Train Time 36.90966749191284s

2025-10-19 12:24:23,275 | INFO | Training epoch 742, Batch 1000/1000: LR=7.01e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:24:23,367 | INFO | Epoch 742 Train Time 37.32425808906555s

2025-10-19 12:25:00,029 | INFO | Training epoch 743, Batch 1000/1000: LR=7.00e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:25:00,135 | INFO | Epoch 743 Train Time 36.76703453063965s

2025-10-19 12:25:37,446 | INFO | Training epoch 744, Batch 1000/1000: LR=6.99e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:25:37,542 | INFO | Epoch 744 Train Time 37.40607142448425s

2025-10-19 12:26:14,954 | INFO | Training epoch 745, Batch 1000/1000: LR=6.99e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:26:15,042 | INFO | Epoch 745 Train Time 37.49927258491516s

2025-10-19 12:26:52,376 | INFO | Training epoch 746, Batch 1000/1000: LR=6.98e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:26:52,463 | INFO | Epoch 746 Train Time 37.41971778869629s

2025-10-19 12:27:29,841 | INFO | Training epoch 747, Batch 1000/1000: LR=6.97e-05, Loss=3.15e-02 BER=1.18e-02 FER=1.56e-01
2025-10-19 12:27:29,945 | INFO | Epoch 747 Train Time 37.4806342124939s

2025-10-19 12:28:07,547 | INFO | Training epoch 748, Batch 1000/1000: LR=6.97e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:28:07,657 | INFO | Epoch 748 Train Time 37.71182346343994s

2025-10-19 12:28:44,745 | INFO | Training epoch 749, Batch 1000/1000: LR=6.96e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:28:44,836 | INFO | Epoch 749 Train Time 37.177008390426636s

2025-10-19 12:29:22,052 | INFO | Training epoch 750, Batch 1000/1000: LR=6.95e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:29:22,165 | INFO | Epoch 750 Train Time 37.328349351882935s

2025-10-19 12:29:59,088 | INFO | Training epoch 751, Batch 1000/1000: LR=6.94e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:29:59,187 | INFO | Epoch 751 Train Time 37.02088189125061s

2025-10-19 12:30:35,162 | INFO | Training epoch 752, Batch 1000/1000: LR=6.94e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:30:35,265 | INFO | Epoch 752 Train Time 36.0763885974884s

2025-10-19 12:31:12,570 | INFO | Training epoch 753, Batch 1000/1000: LR=6.93e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:31:12,673 | INFO | Epoch 753 Train Time 37.406280279159546s

2025-10-19 12:31:50,589 | INFO | Training epoch 754, Batch 1000/1000: LR=6.92e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 12:31:50,686 | INFO | Epoch 754 Train Time 38.01143670082092s

2025-10-19 12:31:50,687 | INFO | [P2] saving best_model (QAT) with loss 0.030120 at epoch 754
2025-10-19 12:32:28,341 | INFO | Training epoch 755, Batch 1000/1000: LR=6.92e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:32:28,420 | INFO | Epoch 755 Train Time 37.71470832824707s

2025-10-19 12:33:05,873 | INFO | Training epoch 756, Batch 1000/1000: LR=6.91e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:33:05,957 | INFO | Epoch 756 Train Time 37.535980224609375s

2025-10-19 12:33:42,971 | INFO | Training epoch 757, Batch 1000/1000: LR=6.90e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 12:33:43,080 | INFO | Epoch 757 Train Time 37.120985984802246s

2025-10-19 12:34:20,045 | INFO | Training epoch 758, Batch 1000/1000: LR=6.89e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 12:34:20,150 | INFO | Epoch 758 Train Time 37.06958723068237s

2025-10-19 12:34:56,447 | INFO | Training epoch 759, Batch 1000/1000: LR=6.89e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:34:56,524 | INFO | Epoch 759 Train Time 36.37244749069214s

2025-10-19 12:35:33,894 | INFO | Training epoch 760, Batch 1000/1000: LR=6.88e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:35:33,975 | INFO | Epoch 760 Train Time 37.449674129486084s

2025-10-19 12:36:11,346 | INFO | Training epoch 761, Batch 1000/1000: LR=6.87e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:36:11,441 | INFO | Epoch 761 Train Time 37.465287923812866s

2025-10-19 12:36:48,843 | INFO | Training epoch 762, Batch 1000/1000: LR=6.86e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:36:48,949 | INFO | Epoch 762 Train Time 37.5068416595459s

2025-10-19 12:37:26,250 | INFO | Training epoch 763, Batch 1000/1000: LR=6.86e-05, Loss=3.08e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 12:37:26,335 | INFO | Epoch 763 Train Time 37.38520956039429s

2025-10-19 12:38:03,457 | INFO | Training epoch 764, Batch 1000/1000: LR=6.85e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:38:03,549 | INFO | Epoch 764 Train Time 37.21235418319702s

2025-10-19 12:38:40,644 | INFO | Training epoch 765, Batch 1000/1000: LR=6.84e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:38:40,733 | INFO | Epoch 765 Train Time 37.183388233184814s

2025-10-19 12:39:17,942 | INFO | Training epoch 766, Batch 1000/1000: LR=6.84e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:39:18,031 | INFO | Epoch 766 Train Time 37.29728698730469s

2025-10-19 12:39:54,749 | INFO | Training epoch 767, Batch 1000/1000: LR=6.83e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:39:54,853 | INFO | Epoch 767 Train Time 36.82082915306091s

2025-10-19 12:40:32,061 | INFO | Training epoch 768, Batch 1000/1000: LR=6.82e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:40:32,148 | INFO | Epoch 768 Train Time 37.293572664260864s

2025-10-19 12:41:09,345 | INFO | Training epoch 769, Batch 1000/1000: LR=6.81e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:41:09,422 | INFO | Epoch 769 Train Time 37.272343158721924s

2025-10-19 12:41:46,430 | INFO | Training epoch 770, Batch 1000/1000: LR=6.81e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:41:46,508 | INFO | Epoch 770 Train Time 37.08459401130676s

2025-10-19 12:42:23,647 | INFO | Training epoch 771, Batch 1000/1000: LR=6.80e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:42:23,731 | INFO | Epoch 771 Train Time 37.22211241722107s

2025-10-19 12:43:00,764 | INFO | Training epoch 772, Batch 1000/1000: LR=6.79e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:43:00,846 | INFO | Epoch 772 Train Time 37.11376237869263s

2025-10-19 12:43:38,855 | INFO | Training epoch 773, Batch 1000/1000: LR=6.79e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 12:43:38,951 | INFO | Epoch 773 Train Time 38.1035521030426s

2025-10-19 12:44:16,838 | INFO | Training epoch 774, Batch 1000/1000: LR=6.78e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:44:16,937 | INFO | Epoch 774 Train Time 37.98511075973511s

2025-10-19 12:44:54,437 | INFO | Training epoch 775, Batch 1000/1000: LR=6.77e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:44:54,540 | INFO | Epoch 775 Train Time 37.60105895996094s

2025-10-19 12:45:31,532 | INFO | Training epoch 776, Batch 1000/1000: LR=6.76e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:45:31,622 | INFO | Epoch 776 Train Time 37.079901456832886s

2025-10-19 12:46:08,973 | INFO | Training epoch 777, Batch 1000/1000: LR=6.76e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 12:46:09,059 | INFO | Epoch 777 Train Time 37.43594455718994s

2025-10-19 12:46:45,840 | INFO | Training epoch 778, Batch 1000/1000: LR=6.75e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 12:46:45,938 | INFO | Epoch 778 Train Time 36.87684750556946s

2025-10-19 12:47:23,249 | INFO | Training epoch 779, Batch 1000/1000: LR=6.74e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 12:47:23,339 | INFO | Epoch 779 Train Time 37.39931869506836s

2025-10-19 12:48:00,640 | INFO | Training epoch 780, Batch 1000/1000: LR=6.73e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 12:48:00,739 | INFO | Epoch 780 Train Time 37.39693880081177s

2025-10-19 12:48:37,947 | INFO | Training epoch 781, Batch 1000/1000: LR=6.73e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 12:48:38,067 | INFO | Epoch 781 Train Time 37.32743763923645s

2025-10-19 12:49:14,142 | INFO | Training epoch 782, Batch 1000/1000: LR=6.72e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 12:49:14,238 | INFO | Epoch 782 Train Time 36.16962766647339s

2025-10-19 12:49:51,345 | INFO | Training epoch 783, Batch 1000/1000: LR=6.71e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:49:51,429 | INFO | Epoch 783 Train Time 37.18935751914978s

2025-10-19 12:50:28,237 | INFO | Training epoch 784, Batch 1000/1000: LR=6.70e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:50:28,327 | INFO | Epoch 784 Train Time 36.89707136154175s

2025-10-19 12:51:05,445 | INFO | Training epoch 785, Batch 1000/1000: LR=6.70e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:51:05,557 | INFO | Epoch 785 Train Time 37.2284460067749s

2025-10-19 12:51:42,845 | INFO | Training epoch 786, Batch 1000/1000: LR=6.69e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:51:42,936 | INFO | Epoch 786 Train Time 37.37796115875244s

2025-10-19 12:52:20,133 | INFO | Training epoch 787, Batch 1000/1000: LR=6.68e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:52:20,221 | INFO | Epoch 787 Train Time 37.28350377082825s

2025-10-19 12:52:57,540 | INFO | Training epoch 788, Batch 1000/1000: LR=6.68e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 12:52:57,639 | INFO | Epoch 788 Train Time 37.41736054420471s

2025-10-19 12:53:34,860 | INFO | Training epoch 789, Batch 1000/1000: LR=6.67e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:53:34,958 | INFO | Epoch 789 Train Time 37.318443298339844s

2025-10-19 12:54:12,046 | INFO | Training epoch 790, Batch 1000/1000: LR=6.66e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:54:12,150 | INFO | Epoch 790 Train Time 37.190593957901s

2025-10-19 12:54:49,846 | INFO | Training epoch 791, Batch 1000/1000: LR=6.65e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:54:49,944 | INFO | Epoch 791 Train Time 37.79337692260742s

2025-10-19 12:55:27,246 | INFO | Training epoch 792, Batch 1000/1000: LR=6.65e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 12:55:27,336 | INFO | Epoch 792 Train Time 37.388163805007935s

2025-10-19 12:56:04,068 | INFO | Training epoch 793, Batch 1000/1000: LR=6.64e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 12:56:04,168 | INFO | Epoch 793 Train Time 36.83140754699707s

2025-10-19 12:56:41,742 | INFO | Training epoch 794, Batch 1000/1000: LR=6.63e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:56:41,838 | INFO | Epoch 794 Train Time 37.66858386993408s

2025-10-19 12:57:19,270 | INFO | Training epoch 795, Batch 1000/1000: LR=6.62e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:57:19,367 | INFO | Epoch 795 Train Time 37.5281343460083s

2025-10-19 12:57:56,432 | INFO | Training epoch 796, Batch 1000/1000: LR=6.62e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:57:56,535 | INFO | Epoch 796 Train Time 37.166423082351685s

2025-10-19 12:58:33,959 | INFO | Training epoch 797, Batch 1000/1000: LR=6.61e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 12:58:34,053 | INFO | Epoch 797 Train Time 37.516876459121704s

2025-10-19 12:59:11,163 | INFO | Training epoch 798, Batch 1000/1000: LR=6.60e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 12:59:11,250 | INFO | Epoch 798 Train Time 37.19612455368042s

2025-10-19 12:59:48,370 | INFO | Training epoch 799, Batch 1000/1000: LR=6.59e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 12:59:48,456 | INFO | Epoch 799 Train Time 37.20432257652283s

2025-10-19 13:00:25,185 | INFO | Training epoch 800, Batch 1000/1000: LR=6.59e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:00:25,273 | INFO | Epoch 800 Train Time 36.815860986709595s

2025-10-19 13:01:02,511 | INFO | Training epoch 801, Batch 1000/1000: LR=6.58e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:01:02,605 | INFO | Epoch 801 Train Time 37.3297381401062s

2025-10-19 13:01:39,839 | INFO | Training epoch 802, Batch 1000/1000: LR=6.57e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:01:39,927 | INFO | Epoch 802 Train Time 37.32044076919556s

2025-10-19 13:02:17,480 | INFO | Training epoch 803, Batch 1000/1000: LR=6.56e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:02:17,566 | INFO | Epoch 803 Train Time 37.638572454452515s

2025-10-19 13:02:54,966 | INFO | Training epoch 804, Batch 1000/1000: LR=6.56e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:02:55,060 | INFO | Epoch 804 Train Time 37.491921186447144s

2025-10-19 13:03:32,148 | INFO | Training epoch 805, Batch 1000/1000: LR=6.55e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:03:32,238 | INFO | Epoch 805 Train Time 37.17689085006714s

2025-10-19 13:04:09,129 | INFO | Training epoch 806, Batch 1000/1000: LR=6.54e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 13:04:09,212 | INFO | Epoch 806 Train Time 36.97327136993408s

2025-10-19 13:04:46,566 | INFO | Training epoch 807, Batch 1000/1000: LR=6.54e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:04:46,654 | INFO | Epoch 807 Train Time 37.441858768463135s

2025-10-19 13:05:24,181 | INFO | Training epoch 808, Batch 1000/1000: LR=6.53e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:05:24,293 | INFO | Epoch 808 Train Time 37.63725662231445s

2025-10-19 13:06:01,440 | INFO | Training epoch 809, Batch 1000/1000: LR=6.52e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:06:01,527 | INFO | Epoch 809 Train Time 37.23257637023926s

2025-10-19 13:06:38,649 | INFO | Training epoch 810, Batch 1000/1000: LR=6.51e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:06:38,735 | INFO | Epoch 810 Train Time 37.20613503456116s

2025-10-19 13:07:16,155 | INFO | Training epoch 811, Batch 1000/1000: LR=6.51e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:07:16,241 | INFO | Epoch 811 Train Time 37.5043683052063s

2025-10-19 13:07:53,641 | INFO | Training epoch 812, Batch 1000/1000: LR=6.50e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 13:07:53,725 | INFO | Epoch 812 Train Time 37.48348951339722s

2025-10-19 13:08:30,954 | INFO | Training epoch 813, Batch 1000/1000: LR=6.49e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:08:31,055 | INFO | Epoch 813 Train Time 37.32808303833008s

2025-10-19 13:09:07,874 | INFO | Training epoch 814, Batch 1000/1000: LR=6.48e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:09:07,968 | INFO | Epoch 814 Train Time 36.91165590286255s

2025-10-19 13:09:45,367 | INFO | Training epoch 815, Batch 1000/1000: LR=6.48e-05, Loss=3.13e-02 BER=1.18e-02 FER=1.54e-01
2025-10-19 13:09:45,465 | INFO | Epoch 815 Train Time 37.496222496032715s

2025-10-19 13:10:22,655 | INFO | Training epoch 816, Batch 1000/1000: LR=6.47e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 13:10:22,768 | INFO | Epoch 816 Train Time 37.30186724662781s

2025-10-19 13:10:59,840 | INFO | Training epoch 817, Batch 1000/1000: LR=6.46e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:10:59,934 | INFO | Epoch 817 Train Time 37.16274166107178s

2025-10-19 13:11:37,144 | INFO | Training epoch 818, Batch 1000/1000: LR=6.45e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:11:37,231 | INFO | Epoch 818 Train Time 37.29645085334778s

2025-10-19 13:12:14,543 | INFO | Training epoch 819, Batch 1000/1000: LR=6.45e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:12:14,635 | INFO | Epoch 819 Train Time 37.403008460998535s

2025-10-19 13:12:50,978 | INFO | Training epoch 820, Batch 1000/1000: LR=6.44e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:12:51,069 | INFO | Epoch 820 Train Time 36.43267488479614s

2025-10-19 13:13:28,261 | INFO | Training epoch 821, Batch 1000/1000: LR=6.43e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:13:28,342 | INFO | Epoch 821 Train Time 37.27192950248718s

2025-10-19 13:14:05,942 | INFO | Training epoch 822, Batch 1000/1000: LR=6.42e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:14:06,037 | INFO | Epoch 822 Train Time 37.69337606430054s

2025-10-19 13:14:43,545 | INFO | Training epoch 823, Batch 1000/1000: LR=6.42e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.53e-01
2025-10-19 13:14:43,637 | INFO | Epoch 823 Train Time 37.59818196296692s

2025-10-19 13:15:20,777 | INFO | Training epoch 824, Batch 1000/1000: LR=6.41e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:15:20,870 | INFO | Epoch 824 Train Time 37.232749462127686s

2025-10-19 13:15:58,641 | INFO | Training epoch 825, Batch 1000/1000: LR=6.40e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:15:58,722 | INFO | Epoch 825 Train Time 37.84952116012573s

2025-10-19 13:16:35,747 | INFO | Training epoch 826, Batch 1000/1000: LR=6.39e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:16:35,863 | INFO | Epoch 826 Train Time 37.140037059783936s

2025-10-19 13:17:13,068 | INFO | Training epoch 827, Batch 1000/1000: LR=6.39e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:17:13,161 | INFO | Epoch 827 Train Time 37.297054290771484s

2025-10-19 13:17:50,780 | INFO | Training epoch 828, Batch 1000/1000: LR=6.38e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 13:17:50,858 | INFO | Epoch 828 Train Time 37.69602680206299s

2025-10-19 13:18:27,850 | INFO | Training epoch 829, Batch 1000/1000: LR=6.37e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:18:27,926 | INFO | Epoch 829 Train Time 37.066869497299194s

2025-10-19 13:19:05,494 | INFO | Training epoch 830, Batch 1000/1000: LR=6.36e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:19:05,578 | INFO | Epoch 830 Train Time 37.650956869125366s

2025-10-19 13:19:42,649 | INFO | Training epoch 831, Batch 1000/1000: LR=6.36e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:19:42,749 | INFO | Epoch 831 Train Time 37.170411348342896s

2025-10-19 13:20:19,659 | INFO | Training epoch 832, Batch 1000/1000: LR=6.35e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:20:19,744 | INFO | Epoch 832 Train Time 36.99346327781677s

2025-10-19 13:20:57,350 | INFO | Training epoch 833, Batch 1000/1000: LR=6.34e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:20:57,452 | INFO | Epoch 833 Train Time 37.706058979034424s

2025-10-19 13:21:34,940 | INFO | Training epoch 834, Batch 1000/1000: LR=6.33e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:21:35,049 | INFO | Epoch 834 Train Time 37.59467935562134s

2025-10-19 13:22:12,150 | INFO | Training epoch 835, Batch 1000/1000: LR=6.33e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:22:12,245 | INFO | Epoch 835 Train Time 37.19591283798218s

2025-10-19 13:22:49,499 | INFO | Training epoch 836, Batch 1000/1000: LR=6.32e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 13:22:49,589 | INFO | Epoch 836 Train Time 37.34274125099182s

2025-10-19 13:23:26,331 | INFO | Training epoch 837, Batch 1000/1000: LR=6.31e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:23:26,424 | INFO | Epoch 837 Train Time 36.83405423164368s

2025-10-19 13:24:03,541 | INFO | Training epoch 838, Batch 1000/1000: LR=6.30e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:24:03,638 | INFO | Epoch 838 Train Time 37.212868452072144s

2025-10-19 13:24:40,886 | INFO | Training epoch 839, Batch 1000/1000: LR=6.30e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:24:40,982 | INFO | Epoch 839 Train Time 37.34326195716858s

2025-10-19 13:25:18,027 | INFO | Training epoch 840, Batch 1000/1000: LR=6.29e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:25:18,121 | INFO | Epoch 840 Train Time 37.13700318336487s

2025-10-19 13:25:55,443 | INFO | Training epoch 841, Batch 1000/1000: LR=6.28e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:25:55,536 | INFO | Epoch 841 Train Time 37.414491176605225s

2025-10-19 13:26:33,136 | INFO | Training epoch 842, Batch 1000/1000: LR=6.27e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:26:33,234 | INFO | Epoch 842 Train Time 37.69624590873718s

2025-10-19 13:27:10,542 | INFO | Training epoch 843, Batch 1000/1000: LR=6.27e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 13:27:10,620 | INFO | Epoch 843 Train Time 37.384742736816406s

2025-10-19 13:27:47,840 | INFO | Training epoch 844, Batch 1000/1000: LR=6.26e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:27:47,949 | INFO | Epoch 844 Train Time 37.32861042022705s

2025-10-19 13:28:25,294 | INFO | Training epoch 845, Batch 1000/1000: LR=6.25e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:28:25,384 | INFO | Epoch 845 Train Time 37.43416118621826s

2025-10-19 13:29:03,083 | INFO | Training epoch 846, Batch 1000/1000: LR=6.24e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 13:29:03,176 | INFO | Epoch 846 Train Time 37.79120063781738s

2025-10-19 13:29:39,633 | INFO | Training epoch 847, Batch 1000/1000: LR=6.24e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:29:39,716 | INFO | Epoch 847 Train Time 36.53873920440674s

2025-10-19 13:30:16,951 | INFO | Training epoch 848, Batch 1000/1000: LR=6.23e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:30:17,034 | INFO | Epoch 848 Train Time 37.31742548942566s

2025-10-19 13:30:54,269 | INFO | Training epoch 849, Batch 1000/1000: LR=6.22e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:30:54,355 | INFO | Epoch 849 Train Time 37.318522214889526s

2025-10-19 13:31:31,729 | INFO | Training epoch 850, Batch 1000/1000: LR=6.21e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:31:31,820 | INFO | Epoch 850 Train Time 37.46451950073242s

2025-10-19 13:32:08,451 | INFO | Training epoch 851, Batch 1000/1000: LR=6.21e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:32:08,543 | INFO | Epoch 851 Train Time 36.72232484817505s

2025-10-19 13:32:46,079 | INFO | Training epoch 852, Batch 1000/1000: LR=6.20e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:32:46,170 | INFO | Epoch 852 Train Time 37.62559795379639s

2025-10-19 13:33:23,060 | INFO | Training epoch 853, Batch 1000/1000: LR=6.19e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:33:23,151 | INFO | Epoch 853 Train Time 36.97903394699097s

2025-10-19 13:34:00,744 | INFO | Training epoch 854, Batch 1000/1000: LR=6.18e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:34:00,818 | INFO | Epoch 854 Train Time 37.664345026016235s

2025-10-19 13:34:37,936 | INFO | Training epoch 855, Batch 1000/1000: LR=6.18e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:34:38,032 | INFO | Epoch 855 Train Time 37.213156938552856s

2025-10-19 13:35:15,362 | INFO | Training epoch 856, Batch 1000/1000: LR=6.17e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:35:15,454 | INFO | Epoch 856 Train Time 37.42017364501953s

2025-10-19 13:35:52,665 | INFO | Training epoch 857, Batch 1000/1000: LR=6.16e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:35:52,757 | INFO | Epoch 857 Train Time 37.301491260528564s

2025-10-19 13:36:31,348 | INFO | Training epoch 858, Batch 1000/1000: LR=6.15e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:36:31,426 | INFO | Epoch 858 Train Time 38.667564392089844s

2025-10-19 13:37:08,197 | INFO | Training epoch 859, Batch 1000/1000: LR=6.14e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 13:37:08,248 | INFO | Epoch 859 Train Time 36.82128930091858s

2025-10-19 13:37:44,856 | INFO | Training epoch 860, Batch 1000/1000: LR=6.14e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:37:44,950 | INFO | Epoch 860 Train Time 36.701563596725464s

2025-10-19 13:38:22,544 | INFO | Training epoch 861, Batch 1000/1000: LR=6.13e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:38:22,626 | INFO | Epoch 861 Train Time 37.67402911186218s

2025-10-19 13:38:59,855 | INFO | Training epoch 862, Batch 1000/1000: LR=6.12e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:38:59,945 | INFO | Epoch 862 Train Time 37.31745433807373s

2025-10-19 13:39:36,880 | INFO | Training epoch 863, Batch 1000/1000: LR=6.11e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:39:36,963 | INFO | Epoch 863 Train Time 37.0171537399292s

2025-10-19 13:40:13,945 | INFO | Training epoch 864, Batch 1000/1000: LR=6.11e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 13:40:14,043 | INFO | Epoch 864 Train Time 37.077815771102905s

2025-10-19 13:40:51,243 | INFO | Training epoch 865, Batch 1000/1000: LR=6.10e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:40:51,331 | INFO | Epoch 865 Train Time 37.28367233276367s

2025-10-19 13:41:28,746 | INFO | Training epoch 866, Batch 1000/1000: LR=6.09e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 13:41:28,830 | INFO | Epoch 866 Train Time 37.497904539108276s

2025-10-19 13:41:28,831 | INFO | [P2] saving best_model (QAT) with loss 0.029949 at epoch 866
2025-10-19 13:42:06,037 | INFO | Training epoch 867, Batch 1000/1000: LR=6.08e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:42:06,134 | INFO | Epoch 867 Train Time 37.27729916572571s

2025-10-19 13:42:43,031 | INFO | Training epoch 868, Batch 1000/1000: LR=6.08e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:42:43,131 | INFO | Epoch 868 Train Time 36.995434522628784s

2025-10-19 13:43:20,274 | INFO | Training epoch 869, Batch 1000/1000: LR=6.07e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:43:20,355 | INFO | Epoch 869 Train Time 37.223185539245605s

2025-10-19 13:43:57,247 | INFO | Training epoch 870, Batch 1000/1000: LR=6.06e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:43:57,341 | INFO | Epoch 870 Train Time 36.98480725288391s

2025-10-19 13:44:34,247 | INFO | Training epoch 871, Batch 1000/1000: LR=6.05e-05, Loss=3.08e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:44:34,341 | INFO | Epoch 871 Train Time 36.99853539466858s

2025-10-19 13:45:11,541 | INFO | Training epoch 872, Batch 1000/1000: LR=6.05e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 13:45:11,649 | INFO | Epoch 872 Train Time 37.30666780471802s

2025-10-19 13:45:49,040 | INFO | Training epoch 873, Batch 1000/1000: LR=6.04e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:45:49,135 | INFO | Epoch 873 Train Time 37.485334157943726s

2025-10-19 13:46:25,486 | INFO | Training epoch 874, Batch 1000/1000: LR=6.03e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:46:25,587 | INFO | Epoch 874 Train Time 36.450613021850586s

2025-10-19 13:47:02,848 | INFO | Training epoch 875, Batch 1000/1000: LR=6.02e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:47:02,941 | INFO | Epoch 875 Train Time 37.35314345359802s

2025-10-19 13:47:40,241 | INFO | Training epoch 876, Batch 1000/1000: LR=6.02e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 13:47:40,348 | INFO | Epoch 876 Train Time 37.405696630477905s

2025-10-19 13:48:17,335 | INFO | Training epoch 877, Batch 1000/1000: LR=6.01e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:48:17,416 | INFO | Epoch 877 Train Time 37.066662549972534s

2025-10-19 13:48:54,841 | INFO | Training epoch 878, Batch 1000/1000: LR=6.00e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 13:48:54,922 | INFO | Epoch 878 Train Time 37.504059076309204s

2025-10-19 13:49:32,347 | INFO | Training epoch 879, Batch 1000/1000: LR=5.99e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:49:32,448 | INFO | Epoch 879 Train Time 37.525461196899414s

2025-10-19 13:50:09,736 | INFO | Training epoch 880, Batch 1000/1000: LR=5.99e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:50:09,827 | INFO | Epoch 880 Train Time 37.37792110443115s

2025-10-19 13:50:47,069 | INFO | Training epoch 881, Batch 1000/1000: LR=5.98e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:50:47,151 | INFO | Epoch 881 Train Time 37.32207536697388s

2025-10-19 13:51:24,438 | INFO | Training epoch 882, Batch 1000/1000: LR=5.97e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:51:24,518 | INFO | Epoch 882 Train Time 37.365615367889404s

2025-10-19 13:52:01,858 | INFO | Training epoch 883, Batch 1000/1000: LR=5.96e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 13:52:01,937 | INFO | Epoch 883 Train Time 37.41804552078247s

2025-10-19 13:52:39,264 | INFO | Training epoch 884, Batch 1000/1000: LR=5.95e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:52:39,353 | INFO | Epoch 884 Train Time 37.41443872451782s

2025-10-19 13:53:15,943 | INFO | Training epoch 885, Batch 1000/1000: LR=5.95e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:53:16,025 | INFO | Epoch 885 Train Time 36.67113399505615s

2025-10-19 13:53:52,446 | INFO | Training epoch 886, Batch 1000/1000: LR=5.94e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.54e-01
2025-10-19 13:53:52,522 | INFO | Epoch 886 Train Time 36.49596190452576s

2025-10-19 13:54:29,680 | INFO | Training epoch 887, Batch 1000/1000: LR=5.93e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:54:29,759 | INFO | Epoch 887 Train Time 37.23557114601135s

2025-10-19 13:55:06,962 | INFO | Training epoch 888, Batch 1000/1000: LR=5.92e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:55:07,052 | INFO | Epoch 888 Train Time 37.292036056518555s

2025-10-19 13:55:44,063 | INFO | Training epoch 889, Batch 1000/1000: LR=5.92e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 13:55:44,157 | INFO | Epoch 889 Train Time 37.10234618186951s

2025-10-19 13:56:22,346 | INFO | Training epoch 890, Batch 1000/1000: LR=5.91e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.52e-01
2025-10-19 13:56:22,442 | INFO | Epoch 890 Train Time 38.284226179122925s

2025-10-19 13:56:59,546 | INFO | Training epoch 891, Batch 1000/1000: LR=5.90e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 13:56:59,636 | INFO | Epoch 891 Train Time 37.19364047050476s

2025-10-19 13:57:37,142 | INFO | Training epoch 892, Batch 1000/1000: LR=5.89e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 13:57:37,233 | INFO | Epoch 892 Train Time 37.59652400016785s

2025-10-19 13:58:14,347 | INFO | Training epoch 893, Batch 1000/1000: LR=5.89e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 13:58:14,430 | INFO | Epoch 893 Train Time 37.19404339790344s

2025-10-19 13:58:51,140 | INFO | Training epoch 894, Batch 1000/1000: LR=5.88e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 13:58:51,234 | INFO | Epoch 894 Train Time 36.80322885513306s

2025-10-19 13:59:28,641 | INFO | Training epoch 895, Batch 1000/1000: LR=5.87e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 13:59:28,738 | INFO | Epoch 895 Train Time 37.5034863948822s

2025-10-19 14:00:05,844 | INFO | Training epoch 896, Batch 1000/1000: LR=5.86e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 14:00:05,930 | INFO | Epoch 896 Train Time 37.19069766998291s

2025-10-19 14:00:43,052 | INFO | Training epoch 897, Batch 1000/1000: LR=5.86e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:00:43,137 | INFO | Epoch 897 Train Time 37.206355810165405s

2025-10-19 14:01:20,148 | INFO | Training epoch 898, Batch 1000/1000: LR=5.85e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:01:20,257 | INFO | Epoch 898 Train Time 37.117777824401855s

2025-10-19 14:01:57,248 | INFO | Training epoch 899, Batch 1000/1000: LR=5.84e-05, Loss=3.12e-02 BER=1.18e-02 FER=1.55e-01
2025-10-19 14:01:57,346 | INFO | Epoch 899 Train Time 37.086366415023804s

2025-10-19 14:02:35,049 | INFO | Training epoch 900, Batch 1000/1000: LR=5.83e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 14:02:35,140 | INFO | Epoch 900 Train Time 37.79300284385681s

2025-10-19 14:03:12,464 | INFO | Training epoch 901, Batch 1000/1000: LR=5.82e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:03:12,546 | INFO | Epoch 901 Train Time 37.40396690368652s

2025-10-19 14:03:50,235 | INFO | Training epoch 902, Batch 1000/1000: LR=5.82e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:03:50,328 | INFO | Epoch 902 Train Time 37.781434535980225s

2025-10-19 14:04:27,992 | INFO | Training epoch 903, Batch 1000/1000: LR=5.81e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:04:28,077 | INFO | Epoch 903 Train Time 37.74735426902771s

2025-10-19 14:05:04,842 | INFO | Training epoch 904, Batch 1000/1000: LR=5.80e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 14:05:04,945 | INFO | Epoch 904 Train Time 36.86638307571411s

2025-10-19 14:05:41,464 | INFO | Training epoch 905, Batch 1000/1000: LR=5.79e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:05:41,561 | INFO | Epoch 905 Train Time 36.613882541656494s

2025-10-19 14:06:19,243 | INFO | Training epoch 906, Batch 1000/1000: LR=5.79e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:06:19,325 | INFO | Epoch 906 Train Time 37.76269841194153s

2025-10-19 14:06:56,637 | INFO | Training epoch 907, Batch 1000/1000: LR=5.78e-05, Loss=3.09e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 14:06:56,720 | INFO | Epoch 907 Train Time 37.39334034919739s

2025-10-19 14:07:33,536 | INFO | Training epoch 908, Batch 1000/1000: LR=5.77e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:07:33,619 | INFO | Epoch 908 Train Time 36.897315979003906s

2025-10-19 14:08:10,755 | INFO | Training epoch 909, Batch 1000/1000: LR=5.76e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:08:10,844 | INFO | Epoch 909 Train Time 37.22296380996704s

2025-10-19 14:08:47,623 | INFO | Training epoch 910, Batch 1000/1000: LR=5.76e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:08:47,718 | INFO | Epoch 910 Train Time 36.871476888656616s

2025-10-19 14:09:24,760 | INFO | Training epoch 911, Batch 1000/1000: LR=5.75e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 14:09:24,849 | INFO | Epoch 911 Train Time 37.129775285720825s

2025-10-19 14:10:02,032 | INFO | Training epoch 912, Batch 1000/1000: LR=5.74e-05, Loss=3.09e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 14:10:02,123 | INFO | Epoch 912 Train Time 37.273818492889404s

2025-10-19 14:10:39,546 | INFO | Training epoch 913, Batch 1000/1000: LR=5.73e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:10:39,652 | INFO | Epoch 913 Train Time 37.52800393104553s

2025-10-19 14:11:17,381 | INFO | Training epoch 914, Batch 1000/1000: LR=5.72e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:11:17,467 | INFO | Epoch 914 Train Time 37.814122915267944s

2025-10-19 14:11:54,547 | INFO | Training epoch 915, Batch 1000/1000: LR=5.72e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 14:11:54,634 | INFO | Epoch 915 Train Time 37.165223598480225s

2025-10-19 14:12:32,147 | INFO | Training epoch 916, Batch 1000/1000: LR=5.71e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:12:32,234 | INFO | Epoch 916 Train Time 37.59958839416504s

2025-10-19 14:13:09,632 | INFO | Training epoch 917, Batch 1000/1000: LR=5.70e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 14:13:09,732 | INFO | Epoch 917 Train Time 37.49598550796509s

2025-10-19 14:13:47,442 | INFO | Training epoch 918, Batch 1000/1000: LR=5.69e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:13:47,529 | INFO | Epoch 918 Train Time 37.79641628265381s

2025-10-19 14:14:24,747 | INFO | Training epoch 919, Batch 1000/1000: LR=5.69e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:14:24,868 | INFO | Epoch 919 Train Time 37.336764097213745s

2025-10-19 14:15:02,408 | INFO | Training epoch 920, Batch 1000/1000: LR=5.68e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:15:02,500 | INFO | Epoch 920 Train Time 37.63097357749939s

2025-10-19 14:15:39,762 | INFO | Training epoch 921, Batch 1000/1000: LR=5.67e-05, Loss=3.12e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 14:15:39,858 | INFO | Epoch 921 Train Time 37.35722613334656s

2025-10-19 14:16:17,146 | INFO | Training epoch 922, Batch 1000/1000: LR=5.66e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 14:16:17,237 | INFO | Epoch 922 Train Time 37.37713932991028s

2025-10-19 14:16:54,984 | INFO | Training epoch 923, Batch 1000/1000: LR=5.65e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:16:55,069 | INFO | Epoch 923 Train Time 37.830273151397705s

2025-10-19 14:17:32,562 | INFO | Training epoch 924, Batch 1000/1000: LR=5.65e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 14:17:32,658 | INFO | Epoch 924 Train Time 37.58844614028931s

2025-10-19 14:18:09,926 | INFO | Training epoch 925, Batch 1000/1000: LR=5.64e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:18:10,026 | INFO | Epoch 925 Train Time 37.36730217933655s

2025-10-19 14:18:46,980 | INFO | Training epoch 926, Batch 1000/1000: LR=5.63e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:18:47,074 | INFO | Epoch 926 Train Time 37.04727602005005s

2025-10-19 14:19:23,543 | INFO | Training epoch 927, Batch 1000/1000: LR=5.62e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:19:23,647 | INFO | Epoch 927 Train Time 36.57111310958862s

2025-10-19 14:20:01,148 | INFO | Training epoch 928, Batch 1000/1000: LR=5.62e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 14:20:01,241 | INFO | Epoch 928 Train Time 37.59290432929993s

2025-10-19 14:20:38,950 | INFO | Training epoch 929, Batch 1000/1000: LR=5.61e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:20:39,038 | INFO | Epoch 929 Train Time 37.79600977897644s

2025-10-19 14:21:15,947 | INFO | Training epoch 930, Batch 1000/1000: LR=5.60e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:21:16,028 | INFO | Epoch 930 Train Time 36.98877549171448s

2025-10-19 14:21:53,370 | INFO | Training epoch 931, Batch 1000/1000: LR=5.59e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:21:53,454 | INFO | Epoch 931 Train Time 37.42427849769592s

2025-10-19 14:22:31,340 | INFO | Training epoch 932, Batch 1000/1000: LR=5.59e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:22:31,426 | INFO | Epoch 932 Train Time 37.970704317092896s

2025-10-19 14:23:08,770 | INFO | Training epoch 933, Batch 1000/1000: LR=5.58e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:23:08,849 | INFO | Epoch 933 Train Time 37.42277002334595s

2025-10-19 14:23:46,160 | INFO | Training epoch 934, Batch 1000/1000: LR=5.57e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:23:46,241 | INFO | Epoch 934 Train Time 37.39053916931152s

2025-10-19 14:24:22,955 | INFO | Training epoch 935, Batch 1000/1000: LR=5.56e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:24:23,058 | INFO | Epoch 935 Train Time 36.814878940582275s

2025-10-19 14:25:00,141 | INFO | Training epoch 936, Batch 1000/1000: LR=5.55e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:25:00,250 | INFO | Epoch 936 Train Time 37.19066023826599s

2025-10-19 14:25:37,242 | INFO | Training epoch 937, Batch 1000/1000: LR=5.55e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 14:25:37,324 | INFO | Epoch 937 Train Time 37.0715606212616s

2025-10-19 14:25:37,325 | INFO | [P2] saving best_model (QAT) with loss 0.029945 at epoch 937
2025-10-19 14:26:14,863 | INFO | Training epoch 938, Batch 1000/1000: LR=5.54e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 14:26:14,945 | INFO | Epoch 938 Train Time 37.60370492935181s

2025-10-19 14:26:51,956 | INFO | Training epoch 939, Batch 1000/1000: LR=5.53e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:26:52,058 | INFO | Epoch 939 Train Time 37.111002683639526s

2025-10-19 14:27:29,541 | INFO | Training epoch 940, Batch 1000/1000: LR=5.52e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:27:29,623 | INFO | Epoch 940 Train Time 37.563122272491455s

2025-10-19 14:28:07,234 | INFO | Training epoch 941, Batch 1000/1000: LR=5.52e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:28:07,317 | INFO | Epoch 941 Train Time 37.693358421325684s

2025-10-19 14:28:44,641 | INFO | Training epoch 942, Batch 1000/1000: LR=5.51e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:28:44,735 | INFO | Epoch 942 Train Time 37.415743350982666s

2025-10-19 14:29:21,844 | INFO | Training epoch 943, Batch 1000/1000: LR=5.50e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:29:21,931 | INFO | Epoch 943 Train Time 37.19488787651062s

2025-10-19 14:29:58,975 | INFO | Training epoch 944, Batch 1000/1000: LR=5.49e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:29:59,061 | INFO | Epoch 944 Train Time 37.128493785858154s

2025-10-19 14:30:35,351 | INFO | Training epoch 945, Batch 1000/1000: LR=5.48e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 14:30:35,431 | INFO | Epoch 945 Train Time 36.36852765083313s

2025-10-19 14:31:12,733 | INFO | Training epoch 946, Batch 1000/1000: LR=5.48e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:31:12,817 | INFO | Epoch 946 Train Time 37.38563084602356s

2025-10-19 14:31:49,844 | INFO | Training epoch 947, Batch 1000/1000: LR=5.47e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:31:49,938 | INFO | Epoch 947 Train Time 37.118855476379395s

2025-10-19 14:32:27,333 | INFO | Training epoch 948, Batch 1000/1000: LR=5.46e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:32:27,442 | INFO | Epoch 948 Train Time 37.50366020202637s

2025-10-19 14:33:04,639 | INFO | Training epoch 949, Batch 1000/1000: LR=5.45e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 14:33:04,735 | INFO | Epoch 949 Train Time 37.291250467300415s

2025-10-19 14:33:42,043 | INFO | Training epoch 950, Batch 1000/1000: LR=5.45e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:33:42,132 | INFO | Epoch 950 Train Time 37.39519786834717s

2025-10-19 14:34:19,559 | INFO | Training epoch 951, Batch 1000/1000: LR=5.44e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:34:19,661 | INFO | Epoch 951 Train Time 37.527913093566895s

2025-10-19 14:34:56,944 | INFO | Training epoch 952, Batch 1000/1000: LR=5.43e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 14:34:57,039 | INFO | Epoch 952 Train Time 37.37762689590454s

2025-10-19 14:35:34,052 | INFO | Training epoch 953, Batch 1000/1000: LR=5.42e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 14:35:34,147 | INFO | Epoch 953 Train Time 37.106725215911865s

2025-10-19 14:36:11,331 | INFO | Training epoch 954, Batch 1000/1000: LR=5.42e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:36:11,423 | INFO | Epoch 954 Train Time 37.27510738372803s

2025-10-19 14:36:48,566 | INFO | Training epoch 955, Batch 1000/1000: LR=5.41e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:36:48,680 | INFO | Epoch 955 Train Time 37.25561881065369s

2025-10-19 14:37:26,341 | INFO | Training epoch 956, Batch 1000/1000: LR=5.40e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 14:37:26,428 | INFO | Epoch 956 Train Time 37.74727964401245s

2025-10-19 14:38:03,646 | INFO | Training epoch 957, Batch 1000/1000: LR=5.39e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:38:03,735 | INFO | Epoch 957 Train Time 37.305901288986206s

2025-10-19 14:38:40,941 | INFO | Training epoch 958, Batch 1000/1000: LR=5.38e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:38:41,026 | INFO | Epoch 958 Train Time 37.289527893066406s

2025-10-19 14:39:18,038 | INFO | Training epoch 959, Batch 1000/1000: LR=5.38e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:39:18,138 | INFO | Epoch 959 Train Time 37.110090494155884s

2025-10-19 14:39:55,379 | INFO | Training epoch 960, Batch 1000/1000: LR=5.37e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:39:55,470 | INFO | Epoch 960 Train Time 37.3316867351532s

2025-10-19 14:40:32,647 | INFO | Training epoch 961, Batch 1000/1000: LR=5.36e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 14:40:32,748 | INFO | Epoch 961 Train Time 37.276273012161255s

2025-10-19 14:41:09,347 | INFO | Training epoch 962, Batch 1000/1000: LR=5.35e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 14:41:09,429 | INFO | Epoch 962 Train Time 36.679911851882935s

2025-10-19 14:41:45,544 | INFO | Training epoch 963, Batch 1000/1000: LR=5.35e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:41:45,622 | INFO | Epoch 963 Train Time 36.191086292266846s

2025-10-19 14:42:23,449 | INFO | Training epoch 964, Batch 1000/1000: LR=5.34e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:42:23,538 | INFO | Epoch 964 Train Time 37.91567778587341s

2025-10-19 14:43:00,861 | INFO | Training epoch 965, Batch 1000/1000: LR=5.33e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:43:00,952 | INFO | Epoch 965 Train Time 37.41181707382202s

2025-10-19 14:43:37,667 | INFO | Training epoch 966, Batch 1000/1000: LR=5.32e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:43:37,757 | INFO | Epoch 966 Train Time 36.80327844619751s

2025-10-19 14:44:15,056 | INFO | Training epoch 967, Batch 1000/1000: LR=5.31e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 14:44:15,158 | INFO | Epoch 967 Train Time 37.40051031112671s

2025-10-19 14:44:52,936 | INFO | Training epoch 968, Batch 1000/1000: LR=5.31e-05, Loss=3.10e-02 BER=1.17e-02 FER=1.52e-01
2025-10-19 14:44:53,031 | INFO | Epoch 968 Train Time 37.87072682380676s

2025-10-19 14:45:30,848 | INFO | Training epoch 969, Batch 1000/1000: LR=5.30e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:45:30,948 | INFO | Epoch 969 Train Time 37.915776014328s

2025-10-19 14:46:08,444 | INFO | Training epoch 970, Batch 1000/1000: LR=5.29e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 14:46:08,538 | INFO | Epoch 970 Train Time 37.58966398239136s

2025-10-19 14:46:45,439 | INFO | Training epoch 971, Batch 1000/1000: LR=5.28e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 14:46:45,533 | INFO | Epoch 971 Train Time 36.99343943595886s

2025-10-19 14:47:22,441 | INFO | Training epoch 972, Batch 1000/1000: LR=5.28e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:47:22,526 | INFO | Epoch 972 Train Time 36.99216628074646s

2025-10-19 14:48:00,064 | INFO | Training epoch 973, Batch 1000/1000: LR=5.27e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:48:00,172 | INFO | Epoch 973 Train Time 37.64504671096802s

2025-10-19 14:48:37,345 | INFO | Training epoch 974, Batch 1000/1000: LR=5.26e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 14:48:37,437 | INFO | Epoch 974 Train Time 37.26284694671631s

2025-10-19 14:49:14,893 | INFO | Training epoch 975, Batch 1000/1000: LR=5.25e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:49:14,975 | INFO | Epoch 975 Train Time 37.53626370429993s

2025-10-19 14:49:51,638 | INFO | Training epoch 976, Batch 1000/1000: LR=5.24e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 14:49:51,726 | INFO | Epoch 976 Train Time 36.75045084953308s

2025-10-19 14:50:28,643 | INFO | Training epoch 977, Batch 1000/1000: LR=5.24e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:50:28,733 | INFO | Epoch 977 Train Time 37.006314277648926s

2025-10-19 14:51:05,639 | INFO | Training epoch 978, Batch 1000/1000: LR=5.23e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:51:05,736 | INFO | Epoch 978 Train Time 37.00108742713928s

2025-10-19 14:51:42,942 | INFO | Training epoch 979, Batch 1000/1000: LR=5.22e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:51:43,050 | INFO | Epoch 979 Train Time 37.312798738479614s

2025-10-19 14:52:19,118 | INFO | Training epoch 980, Batch 1000/1000: LR=5.21e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:52:19,198 | INFO | Epoch 980 Train Time 36.14692497253418s

2025-10-19 14:52:56,067 | INFO | Training epoch 981, Batch 1000/1000: LR=5.21e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:52:56,174 | INFO | Epoch 981 Train Time 36.974586963653564s

2025-10-19 14:53:33,739 | INFO | Training epoch 982, Batch 1000/1000: LR=5.20e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:53:33,828 | INFO | Epoch 982 Train Time 37.65172100067139s

2025-10-19 14:54:11,261 | INFO | Training epoch 983, Batch 1000/1000: LR=5.19e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.54e-01
2025-10-19 14:54:11,347 | INFO | Epoch 983 Train Time 37.51827836036682s

2025-10-19 14:54:48,193 | INFO | Training epoch 984, Batch 1000/1000: LR=5.18e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 14:54:48,276 | INFO | Epoch 984 Train Time 36.92847990989685s

2025-10-19 14:55:25,140 | INFO | Training epoch 985, Batch 1000/1000: LR=5.17e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 14:55:25,223 | INFO | Epoch 985 Train Time 36.94573187828064s

2025-10-19 14:56:02,049 | INFO | Training epoch 986, Batch 1000/1000: LR=5.17e-05, Loss=3.06e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 14:56:02,132 | INFO | Epoch 986 Train Time 36.90750813484192s

2025-10-19 14:56:39,430 | INFO | Training epoch 987, Batch 1000/1000: LR=5.16e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 14:56:39,513 | INFO | Epoch 987 Train Time 37.3796706199646s

2025-10-19 14:57:16,341 | INFO | Training epoch 988, Batch 1000/1000: LR=5.15e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:57:16,423 | INFO | Epoch 988 Train Time 36.90869688987732s

2025-10-19 14:57:53,631 | INFO | Training epoch 989, Batch 1000/1000: LR=5.14e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:57:53,724 | INFO | Epoch 989 Train Time 37.300049781799316s

2025-10-19 14:58:30,345 | INFO | Training epoch 990, Batch 1000/1000: LR=5.14e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 14:58:30,440 | INFO | Epoch 990 Train Time 36.715100049972534s

2025-10-19 14:59:07,572 | INFO | Training epoch 991, Batch 1000/1000: LR=5.13e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 14:59:07,656 | INFO | Epoch 991 Train Time 37.215139627456665s

2025-10-19 14:59:44,877 | INFO | Training epoch 992, Batch 1000/1000: LR=5.12e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 14:59:44,973 | INFO | Epoch 992 Train Time 37.314972162246704s

2025-10-19 15:00:22,451 | INFO | Training epoch 993, Batch 1000/1000: LR=5.11e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:00:22,557 | INFO | Epoch 993 Train Time 37.58339047431946s

2025-10-19 15:00:59,449 | INFO | Training epoch 994, Batch 1000/1000: LR=5.10e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:00:59,539 | INFO | Epoch 994 Train Time 36.98022770881653s

2025-10-19 15:01:36,239 | INFO | Training epoch 995, Batch 1000/1000: LR=5.10e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 15:01:36,321 | INFO | Epoch 995 Train Time 36.78073978424072s

2025-10-19 15:02:13,729 | INFO | Training epoch 996, Batch 1000/1000: LR=5.09e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:02:13,827 | INFO | Epoch 996 Train Time 37.50453448295593s

2025-10-19 15:02:50,646 | INFO | Training epoch 997, Batch 1000/1000: LR=5.08e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:02:50,735 | INFO | Epoch 997 Train Time 36.9069709777832s

2025-10-19 15:03:27,555 | INFO | Training epoch 998, Batch 1000/1000: LR=5.07e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 15:03:27,643 | INFO | Epoch 998 Train Time 36.90769410133362s

2025-10-19 15:04:04,145 | INFO | Training epoch 999, Batch 1000/1000: LR=5.07e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:04:04,239 | INFO | Epoch 999 Train Time 36.59490609169006s

2025-10-19 15:04:41,743 | INFO | Training epoch 1000, Batch 1000/1000: LR=5.06e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 15:04:41,836 | INFO | Epoch 1000 Train Time 37.59474062919617s

2025-10-19 15:05:19,140 | INFO | Training epoch 1001, Batch 1000/1000: LR=5.05e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:05:19,226 | INFO | Epoch 1001 Train Time 37.3879280090332s

2025-10-19 15:05:56,641 | INFO | Training epoch 1002, Batch 1000/1000: LR=5.04e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:05:56,735 | INFO | Epoch 1002 Train Time 37.50814366340637s

2025-10-19 15:06:33,742 | INFO | Training epoch 1003, Batch 1000/1000: LR=5.03e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 15:06:33,825 | INFO | Epoch 1003 Train Time 37.08897614479065s

2025-10-19 15:07:11,299 | INFO | Training epoch 1004, Batch 1000/1000: LR=5.03e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:07:11,387 | INFO | Epoch 1004 Train Time 37.560298681259155s

2025-10-19 15:07:48,765 | INFO | Training epoch 1005, Batch 1000/1000: LR=5.02e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 15:07:48,860 | INFO | Epoch 1005 Train Time 37.47230315208435s

2025-10-19 15:08:26,042 | INFO | Training epoch 1006, Batch 1000/1000: LR=5.01e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:08:26,147 | INFO | Epoch 1006 Train Time 37.28614687919617s

2025-10-19 15:09:03,431 | INFO | Training epoch 1007, Batch 1000/1000: LR=5.00e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:09:03,520 | INFO | Epoch 1007 Train Time 37.37187314033508s

2025-10-19 15:09:40,269 | INFO | Training epoch 1008, Batch 1000/1000: LR=5.00e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:09:40,353 | INFO | Epoch 1008 Train Time 36.82992219924927s

2025-10-19 15:10:17,845 | INFO | Training epoch 1009, Batch 1000/1000: LR=4.99e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 15:10:17,932 | INFO | Epoch 1009 Train Time 37.578619718551636s

2025-10-19 15:10:54,144 | INFO | Training epoch 1010, Batch 1000/1000: LR=4.98e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 15:10:54,223 | INFO | Epoch 1010 Train Time 36.28833293914795s

2025-10-19 15:11:31,677 | INFO | Training epoch 1011, Batch 1000/1000: LR=4.97e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:11:31,757 | INFO | Epoch 1011 Train Time 37.53338170051575s

2025-10-19 15:12:09,145 | INFO | Training epoch 1012, Batch 1000/1000: LR=4.96e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:12:09,252 | INFO | Epoch 1012 Train Time 37.493316411972046s

2025-10-19 15:12:46,048 | INFO | Training epoch 1013, Batch 1000/1000: LR=4.96e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:12:46,124 | INFO | Epoch 1013 Train Time 36.87088203430176s

2025-10-19 15:13:23,543 | INFO | Training epoch 1014, Batch 1000/1000: LR=4.95e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:13:23,638 | INFO | Epoch 1014 Train Time 37.51209831237793s

2025-10-19 15:14:01,038 | INFO | Training epoch 1015, Batch 1000/1000: LR=4.94e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:14:01,133 | INFO | Epoch 1015 Train Time 37.4944224357605s

2025-10-19 15:14:38,641 | INFO | Training epoch 1016, Batch 1000/1000: LR=4.93e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:14:38,729 | INFO | Epoch 1016 Train Time 37.594621896743774s

2025-10-19 15:15:16,139 | INFO | Training epoch 1017, Batch 1000/1000: LR=4.93e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:15:16,230 | INFO | Epoch 1017 Train Time 37.49958252906799s

2025-10-19 15:15:53,164 | INFO | Training epoch 1018, Batch 1000/1000: LR=4.92e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:15:53,254 | INFO | Epoch 1018 Train Time 37.02194356918335s

2025-10-19 15:16:30,965 | INFO | Training epoch 1019, Batch 1000/1000: LR=4.91e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:16:31,047 | INFO | Epoch 1019 Train Time 37.78983974456787s

2025-10-19 15:17:08,582 | INFO | Training epoch 1020, Batch 1000/1000: LR=4.90e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:17:08,677 | INFO | Epoch 1020 Train Time 37.628538608551025s

2025-10-19 15:17:45,790 | INFO | Training epoch 1021, Batch 1000/1000: LR=4.89e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:17:45,887 | INFO | Epoch 1021 Train Time 37.20875263214111s

2025-10-19 15:18:23,655 | INFO | Training epoch 1022, Batch 1000/1000: LR=4.89e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:18:23,740 | INFO | Epoch 1022 Train Time 37.85226893424988s

2025-10-19 15:19:00,763 | INFO | Training epoch 1023, Batch 1000/1000: LR=4.88e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:19:00,856 | INFO | Epoch 1023 Train Time 37.11344003677368s

2025-10-19 15:19:38,345 | INFO | Training epoch 1024, Batch 1000/1000: LR=4.87e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:19:38,428 | INFO | Epoch 1024 Train Time 37.57095289230347s

2025-10-19 15:20:15,842 | INFO | Training epoch 1025, Batch 1000/1000: LR=4.86e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:20:15,919 | INFO | Epoch 1025 Train Time 37.490068435668945s

2025-10-19 15:20:53,152 | INFO | Training epoch 1026, Batch 1000/1000: LR=4.86e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:20:53,251 | INFO | Epoch 1026 Train Time 37.329381227493286s

2025-10-19 15:21:30,144 | INFO | Training epoch 1027, Batch 1000/1000: LR=4.85e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:21:30,227 | INFO | Epoch 1027 Train Time 36.97601580619812s

2025-10-19 15:22:06,985 | INFO | Training epoch 1028, Batch 1000/1000: LR=4.84e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 15:22:07,077 | INFO | Epoch 1028 Train Time 36.84920263290405s

2025-10-19 15:22:44,037 | INFO | Training epoch 1029, Batch 1000/1000: LR=4.83e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:22:44,139 | INFO | Epoch 1029 Train Time 37.06057834625244s

2025-10-19 15:23:21,171 | INFO | Training epoch 1030, Batch 1000/1000: LR=4.82e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:23:21,266 | INFO | Epoch 1030 Train Time 37.12502598762512s

2025-10-19 15:23:58,447 | INFO | Training epoch 1031, Batch 1000/1000: LR=4.82e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:23:58,549 | INFO | Epoch 1031 Train Time 37.28135633468628s

2025-10-19 15:24:35,751 | INFO | Training epoch 1032, Batch 1000/1000: LR=4.81e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:24:35,846 | INFO | Epoch 1032 Train Time 37.29529428482056s

2025-10-19 15:25:12,943 | INFO | Training epoch 1033, Batch 1000/1000: LR=4.80e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:25:13,037 | INFO | Epoch 1033 Train Time 37.19037652015686s

2025-10-19 15:25:50,359 | INFO | Training epoch 1034, Batch 1000/1000: LR=4.79e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:25:50,447 | INFO | Epoch 1034 Train Time 37.409271001815796s

2025-10-19 15:26:27,965 | INFO | Training epoch 1035, Batch 1000/1000: LR=4.79e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 15:26:28,047 | INFO | Epoch 1035 Train Time 37.599146366119385s

2025-10-19 15:27:04,746 | INFO | Training epoch 1036, Batch 1000/1000: LR=4.78e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:27:04,848 | INFO | Epoch 1036 Train Time 36.80009198188782s

2025-10-19 15:27:42,052 | INFO | Training epoch 1037, Batch 1000/1000: LR=4.77e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:27:42,138 | INFO | Epoch 1037 Train Time 37.288074254989624s

2025-10-19 15:28:19,537 | INFO | Training epoch 1038, Batch 1000/1000: LR=4.76e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:28:19,626 | INFO | Epoch 1038 Train Time 37.486512184143066s

2025-10-19 15:28:57,067 | INFO | Training epoch 1039, Batch 1000/1000: LR=4.75e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:28:57,171 | INFO | Epoch 1039 Train Time 37.5444974899292s

2025-10-19 15:29:34,340 | INFO | Training epoch 1040, Batch 1000/1000: LR=4.75e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:29:34,434 | INFO | Epoch 1040 Train Time 37.261470317840576s

2025-10-19 15:30:11,080 | INFO | Training epoch 1041, Batch 1000/1000: LR=4.74e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:30:11,165 | INFO | Epoch 1041 Train Time 36.72600436210632s

2025-10-19 15:30:48,745 | INFO | Training epoch 1042, Batch 1000/1000: LR=4.73e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:30:48,837 | INFO | Epoch 1042 Train Time 37.67105436325073s

2025-10-19 15:31:25,865 | INFO | Training epoch 1043, Batch 1000/1000: LR=4.72e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 15:31:25,944 | INFO | Epoch 1043 Train Time 37.10640907287598s

2025-10-19 15:32:03,068 | INFO | Training epoch 1044, Batch 1000/1000: LR=4.72e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:32:03,161 | INFO | Epoch 1044 Train Time 37.214887857437134s

2025-10-19 15:32:39,688 | INFO | Training epoch 1045, Batch 1000/1000: LR=4.71e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:32:39,771 | INFO | Epoch 1045 Train Time 36.60914349555969s

2025-10-19 15:33:16,734 | INFO | Training epoch 1046, Batch 1000/1000: LR=4.70e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:33:16,824 | INFO | Epoch 1046 Train Time 37.05213904380798s

2025-10-19 15:33:53,762 | INFO | Training epoch 1047, Batch 1000/1000: LR=4.69e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:33:53,854 | INFO | Epoch 1047 Train Time 37.0277304649353s

2025-10-19 15:34:31,042 | INFO | Training epoch 1048, Batch 1000/1000: LR=4.68e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 15:34:31,144 | INFO | Epoch 1048 Train Time 37.28838872909546s

2025-10-19 15:35:08,142 | INFO | Training epoch 1049, Batch 1000/1000: LR=4.68e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:35:08,238 | INFO | Epoch 1049 Train Time 37.092790365219116s

2025-10-19 15:35:45,436 | INFO | Training epoch 1050, Batch 1000/1000: LR=4.67e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:35:45,518 | INFO | Epoch 1050 Train Time 37.27913737297058s

2025-10-19 15:36:23,254 | INFO | Training epoch 1051, Batch 1000/1000: LR=4.66e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:36:23,339 | INFO | Epoch 1051 Train Time 37.81991767883301s

2025-10-19 15:36:59,366 | INFO | Training epoch 1052, Batch 1000/1000: LR=4.65e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:36:59,447 | INFO | Epoch 1052 Train Time 36.105719566345215s

2025-10-19 15:37:36,351 | INFO | Training epoch 1053, Batch 1000/1000: LR=4.65e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:37:36,447 | INFO | Epoch 1053 Train Time 36.99926018714905s

2025-10-19 15:38:13,745 | INFO | Training epoch 1054, Batch 1000/1000: LR=4.64e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 15:38:13,832 | INFO | Epoch 1054 Train Time 37.38432455062866s

2025-10-19 15:38:51,250 | INFO | Training epoch 1055, Batch 1000/1000: LR=4.63e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 15:38:51,353 | INFO | Epoch 1055 Train Time 37.51931667327881s

2025-10-19 15:39:28,738 | INFO | Training epoch 1056, Batch 1000/1000: LR=4.62e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:39:28,824 | INFO | Epoch 1056 Train Time 37.46924376487732s

2025-10-19 15:40:05,536 | INFO | Training epoch 1057, Batch 1000/1000: LR=4.62e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:40:05,620 | INFO | Epoch 1057 Train Time 36.79491209983826s

2025-10-19 15:40:42,447 | INFO | Training epoch 1058, Batch 1000/1000: LR=4.61e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:40:42,531 | INFO | Epoch 1058 Train Time 36.909806966781616s

2025-10-19 15:40:42,531 | INFO | [P2] saving best_model (QAT) with loss 0.029943 at epoch 1058
2025-10-19 15:41:19,658 | INFO | Training epoch 1059, Batch 1000/1000: LR=4.60e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:41:19,743 | INFO | Epoch 1059 Train Time 37.195379972457886s

2025-10-19 15:41:56,641 | INFO | Training epoch 1060, Batch 1000/1000: LR=4.59e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:41:56,719 | INFO | Epoch 1060 Train Time 36.97483992576599s

2025-10-19 15:42:33,132 | INFO | Training epoch 1061, Batch 1000/1000: LR=4.58e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 15:42:33,227 | INFO | Epoch 1061 Train Time 36.50706100463867s

2025-10-19 15:43:10,871 | INFO | Training epoch 1062, Batch 1000/1000: LR=4.58e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:43:10,965 | INFO | Epoch 1062 Train Time 37.736849784851074s

2025-10-19 15:43:48,163 | INFO | Training epoch 1063, Batch 1000/1000: LR=4.57e-05, Loss=3.03e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 15:43:48,252 | INFO | Epoch 1063 Train Time 37.28589630126953s

2025-10-19 15:44:25,848 | INFO | Training epoch 1064, Batch 1000/1000: LR=4.56e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 15:44:25,933 | INFO | Epoch 1064 Train Time 37.679460287094116s

2025-10-19 15:45:03,061 | INFO | Training epoch 1065, Batch 1000/1000: LR=4.55e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 15:45:03,163 | INFO | Epoch 1065 Train Time 37.22931504249573s

2025-10-19 15:45:40,647 | INFO | Training epoch 1066, Batch 1000/1000: LR=4.55e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 15:45:40,746 | INFO | Epoch 1066 Train Time 37.58099722862244s

2025-10-19 15:45:40,747 | INFO | [P2] saving best_model (QAT) with loss 0.029851 at epoch 1066
2025-10-19 15:46:17,837 | INFO | Training epoch 1067, Batch 1000/1000: LR=4.54e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:46:17,930 | INFO | Epoch 1067 Train Time 37.13521075248718s

2025-10-19 15:46:55,363 | INFO | Training epoch 1068, Batch 1000/1000: LR=4.53e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:46:55,471 | INFO | Epoch 1068 Train Time 37.539225578308105s

2025-10-19 15:47:32,569 | INFO | Training epoch 1069, Batch 1000/1000: LR=4.52e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:47:32,663 | INFO | Epoch 1069 Train Time 37.191203594207764s

2025-10-19 15:48:10,275 | INFO | Training epoch 1070, Batch 1000/1000: LR=4.51e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:48:10,351 | INFO | Epoch 1070 Train Time 37.68711447715759s

2025-10-19 15:48:47,242 | INFO | Training epoch 1071, Batch 1000/1000: LR=4.51e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 15:48:47,339 | INFO | Epoch 1071 Train Time 36.98733639717102s

2025-10-19 15:49:23,940 | INFO | Training epoch 1072, Batch 1000/1000: LR=4.50e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 15:49:24,032 | INFO | Epoch 1072 Train Time 36.691701889038086s

2025-10-19 15:50:00,785 | INFO | Training epoch 1073, Batch 1000/1000: LR=4.49e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 15:50:00,877 | INFO | Epoch 1073 Train Time 36.84402799606323s

2025-10-19 15:50:38,042 | INFO | Training epoch 1074, Batch 1000/1000: LR=4.48e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 15:50:38,134 | INFO | Epoch 1074 Train Time 37.25564527511597s

2025-10-19 15:51:15,446 | INFO | Training epoch 1075, Batch 1000/1000: LR=4.48e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 15:51:15,549 | INFO | Epoch 1075 Train Time 37.41414403915405s

2025-10-19 15:51:52,439 | INFO | Training epoch 1076, Batch 1000/1000: LR=4.47e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:51:52,526 | INFO | Epoch 1076 Train Time 36.976308822631836s

2025-10-19 15:52:29,965 | INFO | Training epoch 1077, Batch 1000/1000: LR=4.46e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 15:52:30,065 | INFO | Epoch 1077 Train Time 37.53709554672241s

2025-10-19 15:53:07,553 | INFO | Training epoch 1078, Batch 1000/1000: LR=4.45e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 15:53:07,647 | INFO | Epoch 1078 Train Time 37.58045959472656s

2025-10-19 15:53:44,856 | INFO | Training epoch 1079, Batch 1000/1000: LR=4.45e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:53:44,949 | INFO | Epoch 1079 Train Time 37.30047345161438s

2025-10-19 15:54:22,447 | INFO | Training epoch 1080, Batch 1000/1000: LR=4.44e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:54:22,536 | INFO | Epoch 1080 Train Time 37.58705377578735s

2025-10-19 15:54:59,990 | INFO | Training epoch 1081, Batch 1000/1000: LR=4.43e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:55:00,074 | INFO | Epoch 1081 Train Time 37.536200284957886s

2025-10-19 15:55:36,978 | INFO | Training epoch 1082, Batch 1000/1000: LR=4.42e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 15:55:37,058 | INFO | Epoch 1082 Train Time 36.98313760757446s

2025-10-19 15:56:14,487 | INFO | Training epoch 1083, Batch 1000/1000: LR=4.41e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:56:14,578 | INFO | Epoch 1083 Train Time 37.51823306083679s

2025-10-19 15:56:51,857 | INFO | Training epoch 1084, Batch 1000/1000: LR=4.41e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:56:51,954 | INFO | Epoch 1084 Train Time 37.37481737136841s

2025-10-19 15:57:29,344 | INFO | Training epoch 1085, Batch 1000/1000: LR=4.40e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 15:57:29,432 | INFO | Epoch 1085 Train Time 37.476526498794556s

2025-10-19 15:58:07,142 | INFO | Training epoch 1086, Batch 1000/1000: LR=4.39e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:58:07,250 | INFO | Epoch 1086 Train Time 37.81603026390076s

2025-10-19 15:58:44,349 | INFO | Training epoch 1087, Batch 1000/1000: LR=4.38e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:58:44,432 | INFO | Epoch 1087 Train Time 37.18130564689636s

2025-10-19 15:59:21,645 | INFO | Training epoch 1088, Batch 1000/1000: LR=4.38e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 15:59:21,740 | INFO | Epoch 1088 Train Time 37.30452108383179s

2025-10-19 15:59:59,142 | INFO | Training epoch 1089, Batch 1000/1000: LR=4.37e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 15:59:59,220 | INFO | Epoch 1089 Train Time 37.47735238075256s

2025-10-19 16:00:36,049 | INFO | Training epoch 1090, Batch 1000/1000: LR=4.36e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:00:36,133 | INFO | Epoch 1090 Train Time 36.911760330200195s

2025-10-19 16:01:13,492 | INFO | Training epoch 1091, Batch 1000/1000: LR=4.35e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 16:01:13,575 | INFO | Epoch 1091 Train Time 37.44101881980896s

2025-10-19 16:01:50,638 | INFO | Training epoch 1092, Batch 1000/1000: LR=4.34e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:01:50,719 | INFO | Epoch 1092 Train Time 37.143051624298096s

2025-10-19 16:02:28,145 | INFO | Training epoch 1093, Batch 1000/1000: LR=4.34e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 16:02:28,228 | INFO | Epoch 1093 Train Time 37.5082688331604s

2025-10-19 16:03:05,237 | INFO | Training epoch 1094, Batch 1000/1000: LR=4.33e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:03:05,325 | INFO | Epoch 1094 Train Time 37.09542441368103s

2025-10-19 16:03:42,885 | INFO | Training epoch 1095, Batch 1000/1000: LR=4.32e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 16:03:42,990 | INFO | Epoch 1095 Train Time 37.665058851242065s

2025-10-19 16:04:20,242 | INFO | Training epoch 1096, Batch 1000/1000: LR=4.31e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:04:20,325 | INFO | Epoch 1096 Train Time 37.33332633972168s

2025-10-19 16:04:57,948 | INFO | Training epoch 1097, Batch 1000/1000: LR=4.31e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:04:58,035 | INFO | Epoch 1097 Train Time 37.7086718082428s

2025-10-19 16:05:35,439 | INFO | Training epoch 1098, Batch 1000/1000: LR=4.30e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:05:35,524 | INFO | Epoch 1098 Train Time 37.48811411857605s

2025-10-19 16:06:12,964 | INFO | Training epoch 1099, Batch 1000/1000: LR=4.29e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:06:13,059 | INFO | Epoch 1099 Train Time 37.532674074172974s

2025-10-19 16:06:49,949 | INFO | Training epoch 1100, Batch 1000/1000: LR=4.28e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:06:50,054 | INFO | Epoch 1100 Train Time 36.99338722229004s

2025-10-19 16:07:27,372 | INFO | Training epoch 1101, Batch 1000/1000: LR=4.28e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:07:27,455 | INFO | Epoch 1101 Train Time 37.39876174926758s

2025-10-19 16:08:03,847 | INFO | Training epoch 1102, Batch 1000/1000: LR=4.27e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:08:03,945 | INFO | Epoch 1102 Train Time 36.48833227157593s

2025-10-19 16:08:41,060 | INFO | Training epoch 1103, Batch 1000/1000: LR=4.26e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:08:41,171 | INFO | Epoch 1103 Train Time 37.22537851333618s

2025-10-19 16:09:18,443 | INFO | Training epoch 1104, Batch 1000/1000: LR=4.25e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.53e-01
2025-10-19 16:09:18,531 | INFO | Epoch 1104 Train Time 37.35803771018982s

2025-10-19 16:09:55,444 | INFO | Training epoch 1105, Batch 1000/1000: LR=4.24e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:09:55,534 | INFO | Epoch 1105 Train Time 37.002593755722046s

2025-10-19 16:10:32,436 | INFO | Training epoch 1106, Batch 1000/1000: LR=4.24e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:10:32,530 | INFO | Epoch 1106 Train Time 36.994470834732056s

2025-10-19 16:11:10,040 | INFO | Training epoch 1107, Batch 1000/1000: LR=4.23e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:11:10,144 | INFO | Epoch 1107 Train Time 37.612661838531494s

2025-10-19 16:11:47,636 | INFO | Training epoch 1108, Batch 1000/1000: LR=4.22e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:11:47,743 | INFO | Epoch 1108 Train Time 37.59834885597229s

2025-10-19 16:12:25,041 | INFO | Training epoch 1109, Batch 1000/1000: LR=4.21e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:12:25,130 | INFO | Epoch 1109 Train Time 37.38474369049072s

2025-10-19 16:13:02,045 | INFO | Training epoch 1110, Batch 1000/1000: LR=4.21e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:13:02,145 | INFO | Epoch 1110 Train Time 37.01306986808777s

2025-10-19 16:13:39,242 | INFO | Training epoch 1111, Batch 1000/1000: LR=4.20e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.47e-01
2025-10-19 16:13:39,352 | INFO | Epoch 1111 Train Time 37.20612812042236s

2025-10-19 16:13:39,353 | INFO | [P2] saving best_model (QAT) with loss 0.029676 at epoch 1111
2025-10-19 16:14:17,262 | INFO | Training epoch 1112, Batch 1000/1000: LR=4.19e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:14:17,352 | INFO | Epoch 1112 Train Time 37.98053431510925s

2025-10-19 16:14:56,134 | INFO | Training epoch 1113, Batch 1000/1000: LR=4.18e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 16:14:56,235 | INFO | Epoch 1113 Train Time 38.88178777694702s

2025-10-19 16:15:32,232 | INFO | Training epoch 1114, Batch 1000/1000: LR=4.18e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:15:32,333 | INFO | Epoch 1114 Train Time 36.09567999839783s

2025-10-19 16:16:08,613 | INFO | Training epoch 1115, Batch 1000/1000: LR=4.17e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:16:08,708 | INFO | Epoch 1115 Train Time 36.37387180328369s

2025-10-19 16:16:44,523 | INFO | Training epoch 1116, Batch 1000/1000: LR=4.16e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:16:44,621 | INFO | Epoch 1116 Train Time 35.91195011138916s

2025-10-19 16:17:20,733 | INFO | Training epoch 1117, Batch 1000/1000: LR=4.15e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:17:20,824 | INFO | Epoch 1117 Train Time 36.20131850242615s

2025-10-19 16:17:56,722 | INFO | Training epoch 1118, Batch 1000/1000: LR=4.15e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:17:56,792 | INFO | Epoch 1118 Train Time 35.96722078323364s

2025-10-19 16:18:33,123 | INFO | Training epoch 1119, Batch 1000/1000: LR=4.14e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:18:33,210 | INFO | Epoch 1119 Train Time 36.41667938232422s

2025-10-19 16:19:09,112 | INFO | Training epoch 1120, Batch 1000/1000: LR=4.13e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:19:09,196 | INFO | Epoch 1120 Train Time 35.98453211784363s

2025-10-19 16:19:45,134 | INFO | Training epoch 1121, Batch 1000/1000: LR=4.12e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:19:45,216 | INFO | Epoch 1121 Train Time 36.019819259643555s

2025-10-19 16:20:21,240 | INFO | Training epoch 1122, Batch 1000/1000: LR=4.11e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:20:21,318 | INFO | Epoch 1122 Train Time 36.10034394264221s

2025-10-19 16:20:57,343 | INFO | Training epoch 1123, Batch 1000/1000: LR=4.11e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:20:57,424 | INFO | Epoch 1123 Train Time 36.105841875076294s

2025-10-19 16:21:33,321 | INFO | Training epoch 1124, Batch 1000/1000: LR=4.10e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.51e-01
2025-10-19 16:21:33,392 | INFO | Epoch 1124 Train Time 35.96658515930176s

2025-10-19 16:22:09,213 | INFO | Training epoch 1125, Batch 1000/1000: LR=4.09e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:22:09,292 | INFO | Epoch 1125 Train Time 35.89817714691162s

2025-10-19 16:22:45,335 | INFO | Training epoch 1126, Batch 1000/1000: LR=4.08e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:22:45,429 | INFO | Epoch 1126 Train Time 36.13536977767944s

2025-10-19 16:23:21,723 | INFO | Training epoch 1127, Batch 1000/1000: LR=4.08e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:23:21,837 | INFO | Epoch 1127 Train Time 36.40655589103699s

2025-10-19 16:23:58,230 | INFO | Training epoch 1128, Batch 1000/1000: LR=4.07e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 16:23:58,313 | INFO | Epoch 1128 Train Time 36.47447371482849s

2025-10-19 16:24:34,529 | INFO | Training epoch 1129, Batch 1000/1000: LR=4.06e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:24:34,603 | INFO | Epoch 1129 Train Time 36.28872275352478s

2025-10-19 16:25:10,530 | INFO | Training epoch 1130, Batch 1000/1000: LR=4.05e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:25:10,613 | INFO | Epoch 1130 Train Time 36.00905108451843s

2025-10-19 16:25:46,215 | INFO | Training epoch 1131, Batch 1000/1000: LR=4.05e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:25:46,309 | INFO | Epoch 1131 Train Time 35.69459533691406s

2025-10-19 16:26:21,838 | INFO | Training epoch 1132, Batch 1000/1000: LR=4.04e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:26:21,934 | INFO | Epoch 1132 Train Time 35.624645948410034s

2025-10-19 16:26:57,644 | INFO | Training epoch 1133, Batch 1000/1000: LR=4.03e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:26:57,721 | INFO | Epoch 1133 Train Time 35.786121129989624s

2025-10-19 16:27:33,737 | INFO | Training epoch 1134, Batch 1000/1000: LR=4.02e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 16:27:33,809 | INFO | Epoch 1134 Train Time 36.08700966835022s

2025-10-19 16:28:09,539 | INFO | Training epoch 1135, Batch 1000/1000: LR=4.02e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 16:28:09,625 | INFO | Epoch 1135 Train Time 35.81415390968323s

2025-10-19 16:28:45,922 | INFO | Training epoch 1136, Batch 1000/1000: LR=4.01e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:28:45,995 | INFO | Epoch 1136 Train Time 36.368969202041626s

2025-10-19 16:29:21,821 | INFO | Training epoch 1137, Batch 1000/1000: LR=4.00e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 16:29:21,891 | INFO | Epoch 1137 Train Time 35.893726110458374s

2025-10-19 16:29:57,527 | INFO | Training epoch 1138, Batch 1000/1000: LR=3.99e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:29:57,617 | INFO | Epoch 1138 Train Time 35.723857164382935s

2025-10-19 16:30:34,049 | INFO | Training epoch 1139, Batch 1000/1000: LR=3.99e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:30:34,138 | INFO | Epoch 1139 Train Time 36.51957035064697s

2025-10-19 16:31:09,923 | INFO | Training epoch 1140, Batch 1000/1000: LR=3.98e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 16:31:10,019 | INFO | Epoch 1140 Train Time 35.87999224662781s

2025-10-19 16:31:46,058 | INFO | Training epoch 1141, Batch 1000/1000: LR=3.97e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:31:46,144 | INFO | Epoch 1141 Train Time 36.124329805374146s

2025-10-19 16:32:22,430 | INFO | Training epoch 1142, Batch 1000/1000: LR=3.96e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:32:22,514 | INFO | Epoch 1142 Train Time 36.369213819503784s

2025-10-19 16:32:58,144 | INFO | Training epoch 1143, Batch 1000/1000: LR=3.96e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:32:58,222 | INFO | Epoch 1143 Train Time 35.70639705657959s

2025-10-19 16:33:34,224 | INFO | Training epoch 1144, Batch 1000/1000: LR=3.95e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:33:34,306 | INFO | Epoch 1144 Train Time 36.08224868774414s

2025-10-19 16:34:10,126 | INFO | Training epoch 1145, Batch 1000/1000: LR=3.94e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 16:34:10,222 | INFO | Epoch 1145 Train Time 35.91323804855347s

2025-10-19 16:34:46,472 | INFO | Training epoch 1146, Batch 1000/1000: LR=3.93e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:34:46,554 | INFO | Epoch 1146 Train Time 36.33119082450867s

2025-10-19 16:35:22,533 | INFO | Training epoch 1147, Batch 1000/1000: LR=3.92e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:35:22,648 | INFO | Epoch 1147 Train Time 36.09264516830444s

2025-10-19 16:35:57,814 | INFO | Training epoch 1148, Batch 1000/1000: LR=3.92e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:35:57,902 | INFO | Epoch 1148 Train Time 35.25289583206177s

2025-10-19 16:36:33,932 | INFO | Training epoch 1149, Batch 1000/1000: LR=3.91e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:36:34,008 | INFO | Epoch 1149 Train Time 36.103830099105835s

2025-10-19 16:37:10,028 | INFO | Training epoch 1150, Batch 1000/1000: LR=3.90e-05, Loss=3.04e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:37:10,105 | INFO | Epoch 1150 Train Time 36.09469509124756s

2025-10-19 16:37:45,620 | INFO | Training epoch 1151, Batch 1000/1000: LR=3.89e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 16:37:45,709 | INFO | Epoch 1151 Train Time 35.602514028549194s

2025-10-19 16:38:21,624 | INFO | Training epoch 1152, Batch 1000/1000: LR=3.89e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:38:21,711 | INFO | Epoch 1152 Train Time 36.000433921813965s

2025-10-19 16:38:58,227 | INFO | Training epoch 1153, Batch 1000/1000: LR=3.88e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:38:58,300 | INFO | Epoch 1153 Train Time 36.58855605125427s

2025-10-19 16:39:34,578 | INFO | Training epoch 1154, Batch 1000/1000: LR=3.87e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:39:34,671 | INFO | Epoch 1154 Train Time 36.369943380355835s

2025-10-19 16:40:10,379 | INFO | Training epoch 1155, Batch 1000/1000: LR=3.86e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:40:10,463 | INFO | Epoch 1155 Train Time 35.791029930114746s

2025-10-19 16:40:46,125 | INFO | Training epoch 1156, Batch 1000/1000: LR=3.86e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:40:46,220 | INFO | Epoch 1156 Train Time 35.75557327270508s

2025-10-19 16:41:22,035 | INFO | Training epoch 1157, Batch 1000/1000: LR=3.85e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 16:41:22,117 | INFO | Epoch 1157 Train Time 35.89523434638977s

2025-10-19 16:41:58,224 | INFO | Training epoch 1158, Batch 1000/1000: LR=3.84e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:41:58,310 | INFO | Epoch 1158 Train Time 36.19219470024109s

2025-10-19 16:42:34,353 | INFO | Training epoch 1159, Batch 1000/1000: LR=3.83e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:42:34,446 | INFO | Epoch 1159 Train Time 36.1336829662323s

2025-10-19 16:43:09,830 | INFO | Training epoch 1160, Batch 1000/1000: LR=3.83e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 16:43:09,923 | INFO | Epoch 1160 Train Time 35.47494626045227s

2025-10-19 16:43:45,734 | INFO | Training epoch 1161, Batch 1000/1000: LR=3.82e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:43:45,820 | INFO | Epoch 1161 Train Time 35.89520072937012s

2025-10-19 16:44:21,825 | INFO | Training epoch 1162, Batch 1000/1000: LR=3.81e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:44:21,922 | INFO | Epoch 1162 Train Time 36.10133624076843s

2025-10-19 16:44:57,610 | INFO | Training epoch 1163, Batch 1000/1000: LR=3.80e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:44:57,691 | INFO | Epoch 1163 Train Time 35.76837730407715s

2025-10-19 16:45:33,622 | INFO | Training epoch 1164, Batch 1000/1000: LR=3.80e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:45:33,704 | INFO | Epoch 1164 Train Time 36.01204228401184s

2025-10-19 16:46:10,037 | INFO | Training epoch 1165, Batch 1000/1000: LR=3.79e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 16:46:10,113 | INFO | Epoch 1165 Train Time 36.406883239746094s

2025-10-19 16:46:46,178 | INFO | Training epoch 1166, Batch 1000/1000: LR=3.78e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 16:46:46,248 | INFO | Epoch 1166 Train Time 36.13404440879822s

2025-10-19 16:47:22,826 | INFO | Training epoch 1167, Batch 1000/1000: LR=3.77e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:47:22,916 | INFO | Epoch 1167 Train Time 36.666823387145996s

2025-10-19 16:47:58,537 | INFO | Training epoch 1168, Batch 1000/1000: LR=3.77e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:47:58,631 | INFO | Epoch 1168 Train Time 35.713343143463135s

2025-10-19 16:48:34,530 | INFO | Training epoch 1169, Batch 1000/1000: LR=3.76e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.48e-01
2025-10-19 16:48:34,611 | INFO | Epoch 1169 Train Time 35.97966265678406s

2025-10-19 16:48:34,612 | INFO | [P2] saving best_model (QAT) with loss 0.029627 at epoch 1169
2025-10-19 16:49:10,325 | INFO | Training epoch 1170, Batch 1000/1000: LR=3.75e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:49:10,419 | INFO | Epoch 1170 Train Time 35.79103660583496s

2025-10-19 16:49:45,827 | INFO | Training epoch 1171, Batch 1000/1000: LR=3.74e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:49:45,922 | INFO | Epoch 1171 Train Time 35.50256967544556s

2025-10-19 16:50:21,624 | INFO | Training epoch 1172, Batch 1000/1000: LR=3.74e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:50:21,718 | INFO | Epoch 1172 Train Time 35.7950553894043s

2025-10-19 16:50:57,127 | INFO | Training epoch 1173, Batch 1000/1000: LR=3.73e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:50:57,230 | INFO | Epoch 1173 Train Time 35.51020073890686s

2025-10-19 16:51:33,480 | INFO | Training epoch 1174, Batch 1000/1000: LR=3.72e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 16:51:33,579 | INFO | Epoch 1174 Train Time 36.348082065582275s

2025-10-19 16:52:09,715 | INFO | Training epoch 1175, Batch 1000/1000: LR=3.71e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:52:09,806 | INFO | Epoch 1175 Train Time 36.22548699378967s

2025-10-19 16:52:45,247 | INFO | Training epoch 1176, Batch 1000/1000: LR=3.71e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 16:52:45,311 | INFO | Epoch 1176 Train Time 35.50358867645264s

2025-10-19 16:53:21,325 | INFO | Training epoch 1177, Batch 1000/1000: LR=3.70e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 16:53:21,419 | INFO | Epoch 1177 Train Time 36.1063232421875s

2025-10-19 16:53:57,212 | INFO | Training epoch 1178, Batch 1000/1000: LR=3.69e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:53:57,287 | INFO | Epoch 1178 Train Time 35.86740326881409s

2025-10-19 16:54:33,190 | INFO | Training epoch 1179, Batch 1000/1000: LR=3.68e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:54:33,263 | INFO | Epoch 1179 Train Time 35.974055767059326s

2025-10-19 16:55:09,070 | INFO | Training epoch 1180, Batch 1000/1000: LR=3.68e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 16:55:09,147 | INFO | Epoch 1180 Train Time 35.88313841819763s

2025-10-19 16:55:44,618 | INFO | Training epoch 1181, Batch 1000/1000: LR=3.67e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 16:55:44,703 | INFO | Epoch 1181 Train Time 35.55447244644165s

2025-10-19 16:56:21,524 | INFO | Training epoch 1182, Batch 1000/1000: LR=3.66e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 16:56:21,613 | INFO | Epoch 1182 Train Time 36.90873408317566s

2025-10-19 16:56:56,825 | INFO | Training epoch 1183, Batch 1000/1000: LR=3.65e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:56:56,909 | INFO | Epoch 1183 Train Time 35.29551148414612s

2025-10-19 16:57:32,417 | INFO | Training epoch 1184, Batch 1000/1000: LR=3.65e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 16:57:32,522 | INFO | Epoch 1184 Train Time 35.61022996902466s

2025-10-19 16:58:08,224 | INFO | Training epoch 1185, Batch 1000/1000: LR=3.64e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:58:08,341 | INFO | Epoch 1185 Train Time 35.81834697723389s

2025-10-19 16:58:44,433 | INFO | Training epoch 1186, Batch 1000/1000: LR=3.63e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 16:58:44,532 | INFO | Epoch 1186 Train Time 36.19027400016785s

2025-10-19 16:59:21,106 | INFO | Training epoch 1187, Batch 1000/1000: LR=3.62e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 16:59:21,184 | INFO | Epoch 1187 Train Time 36.65019464492798s

2025-10-19 16:59:57,539 | INFO | Training epoch 1188, Batch 1000/1000: LR=3.62e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 16:59:57,618 | INFO | Epoch 1188 Train Time 36.43227505683899s

2025-10-19 17:00:33,824 | INFO | Training epoch 1189, Batch 1000/1000: LR=3.61e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:00:33,913 | INFO | Epoch 1189 Train Time 36.29386782646179s

2025-10-19 17:01:10,124 | INFO | Training epoch 1190, Batch 1000/1000: LR=3.60e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:01:10,218 | INFO | Epoch 1190 Train Time 36.30290603637695s

2025-10-19 17:01:46,111 | INFO | Training epoch 1191, Batch 1000/1000: LR=3.59e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:01:46,177 | INFO | Epoch 1191 Train Time 35.95864248275757s

2025-10-19 17:02:22,553 | INFO | Training epoch 1192, Batch 1000/1000: LR=3.59e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:02:22,631 | INFO | Epoch 1192 Train Time 36.45227599143982s

2025-10-19 17:02:58,945 | INFO | Training epoch 1193, Batch 1000/1000: LR=3.58e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:02:59,037 | INFO | Epoch 1193 Train Time 36.40575361251831s

2025-10-19 17:03:34,223 | INFO | Training epoch 1194, Batch 1000/1000: LR=3.57e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 17:03:34,308 | INFO | Epoch 1194 Train Time 35.269122838974s

2025-10-19 17:04:10,535 | INFO | Training epoch 1195, Batch 1000/1000: LR=3.56e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:04:10,626 | INFO | Epoch 1195 Train Time 36.31745481491089s

2025-10-19 17:04:46,510 | INFO | Training epoch 1196, Batch 1000/1000: LR=3.56e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:04:46,591 | INFO | Epoch 1196 Train Time 35.96395015716553s

2025-10-19 17:05:22,787 | INFO | Training epoch 1197, Batch 1000/1000: LR=3.55e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:05:22,881 | INFO | Epoch 1197 Train Time 36.28926396369934s

2025-10-19 17:05:59,131 | INFO | Training epoch 1198, Batch 1000/1000: LR=3.54e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:05:59,214 | INFO | Epoch 1198 Train Time 36.33069944381714s

2025-10-19 17:06:34,885 | INFO | Training epoch 1199, Batch 1000/1000: LR=3.54e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:06:34,966 | INFO | Epoch 1199 Train Time 35.75168490409851s

2025-10-19 17:07:10,749 | INFO | Training epoch 1200, Batch 1000/1000: LR=3.53e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:07:10,842 | INFO | Epoch 1200 Train Time 35.874590158462524s

2025-10-19 17:07:47,130 | INFO | Training epoch 1201, Batch 1000/1000: LR=3.52e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:07:47,225 | INFO | Epoch 1201 Train Time 36.38214659690857s

2025-10-19 17:08:22,631 | INFO | Training epoch 1202, Batch 1000/1000: LR=3.51e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:08:22,731 | INFO | Epoch 1202 Train Time 35.503947496414185s

2025-10-19 17:08:58,327 | INFO | Training epoch 1203, Batch 1000/1000: LR=3.51e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:08:58,418 | INFO | Epoch 1203 Train Time 35.685569763183594s

2025-10-19 17:09:34,419 | INFO | Training epoch 1204, Batch 1000/1000: LR=3.50e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:09:34,504 | INFO | Epoch 1204 Train Time 36.08511257171631s

2025-10-19 17:10:10,531 | INFO | Training epoch 1205, Batch 1000/1000: LR=3.49e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:10:10,611 | INFO | Epoch 1205 Train Time 36.10599684715271s

2025-10-19 17:10:45,928 | INFO | Training epoch 1206, Batch 1000/1000: LR=3.48e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:10:46,001 | INFO | Epoch 1206 Train Time 35.3884060382843s

2025-10-19 17:11:21,921 | INFO | Training epoch 1207, Batch 1000/1000: LR=3.48e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 17:11:21,996 | INFO | Epoch 1207 Train Time 35.99343419075012s

2025-10-19 17:11:57,624 | INFO | Training epoch 1208, Batch 1000/1000: LR=3.47e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:11:57,718 | INFO | Epoch 1208 Train Time 35.720797300338745s

2025-10-19 17:12:33,726 | INFO | Training epoch 1209, Batch 1000/1000: LR=3.46e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:12:33,794 | INFO | Epoch 1209 Train Time 36.07570838928223s

2025-10-19 17:13:09,225 | INFO | Training epoch 1210, Batch 1000/1000: LR=3.45e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:13:09,305 | INFO | Epoch 1210 Train Time 35.509140968322754s

2025-10-19 17:13:45,135 | INFO | Training epoch 1211, Batch 1000/1000: LR=3.45e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:13:45,211 | INFO | Epoch 1211 Train Time 35.90492272377014s

2025-10-19 17:14:21,432 | INFO | Training epoch 1212, Batch 1000/1000: LR=3.44e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:14:21,528 | INFO | Epoch 1212 Train Time 36.31500744819641s

2025-10-19 17:14:57,118 | INFO | Training epoch 1213, Batch 1000/1000: LR=3.43e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:14:57,202 | INFO | Epoch 1213 Train Time 35.67176675796509s

2025-10-19 17:15:33,412 | INFO | Training epoch 1214, Batch 1000/1000: LR=3.42e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:15:33,526 | INFO | Epoch 1214 Train Time 36.32238698005676s

2025-10-19 17:16:09,613 | INFO | Training epoch 1215, Batch 1000/1000: LR=3.42e-05, Loss=3.04e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:16:09,692 | INFO | Epoch 1215 Train Time 36.16469860076904s

2025-10-19 17:16:45,753 | INFO | Training epoch 1216, Batch 1000/1000: LR=3.41e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:16:45,855 | INFO | Epoch 1216 Train Time 36.16242218017578s

2025-10-19 17:17:21,819 | INFO | Training epoch 1217, Batch 1000/1000: LR=3.40e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:17:21,894 | INFO | Epoch 1217 Train Time 36.03823685646057s

2025-10-19 17:17:58,234 | INFO | Training epoch 1218, Batch 1000/1000: LR=3.40e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:17:58,312 | INFO | Epoch 1218 Train Time 36.416590213775635s

2025-10-19 17:18:33,628 | INFO | Training epoch 1219, Batch 1000/1000: LR=3.39e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:18:33,721 | INFO | Epoch 1219 Train Time 35.40761089324951s

2025-10-19 17:19:09,564 | INFO | Training epoch 1220, Batch 1000/1000: LR=3.38e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:19:09,655 | INFO | Epoch 1220 Train Time 35.93225979804993s

2025-10-19 17:19:45,916 | INFO | Training epoch 1221, Batch 1000/1000: LR=3.37e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:19:46,012 | INFO | Epoch 1221 Train Time 36.35609722137451s

2025-10-19 17:20:21,955 | INFO | Training epoch 1222, Batch 1000/1000: LR=3.37e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:20:22,035 | INFO | Epoch 1222 Train Time 36.02156639099121s

2025-10-19 17:20:58,233 | INFO | Training epoch 1223, Batch 1000/1000: LR=3.36e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:20:58,314 | INFO | Epoch 1223 Train Time 36.278918504714966s

2025-10-19 17:21:33,725 | INFO | Training epoch 1224, Batch 1000/1000: LR=3.35e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:21:33,803 | INFO | Epoch 1224 Train Time 35.48734974861145s

2025-10-19 17:22:09,227 | INFO | Training epoch 1225, Batch 1000/1000: LR=3.34e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 17:22:09,304 | INFO | Epoch 1225 Train Time 35.49986815452576s

2025-10-19 17:22:45,157 | INFO | Training epoch 1226, Batch 1000/1000: LR=3.34e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 17:22:45,263 | INFO | Epoch 1226 Train Time 35.95757484436035s

2025-10-19 17:23:21,094 | INFO | Training epoch 1227, Batch 1000/1000: LR=3.33e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:23:21,200 | INFO | Epoch 1227 Train Time 35.93424344062805s

2025-10-19 17:23:57,149 | INFO | Training epoch 1228, Batch 1000/1000: LR=3.32e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 17:23:57,241 | INFO | Epoch 1228 Train Time 36.03871560096741s

2025-10-19 17:24:32,829 | INFO | Training epoch 1229, Batch 1000/1000: LR=3.31e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:24:32,929 | INFO | Epoch 1229 Train Time 35.68692350387573s

2025-10-19 17:25:09,301 | INFO | Training epoch 1230, Batch 1000/1000: LR=3.31e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:25:09,371 | INFO | Epoch 1230 Train Time 36.44080996513367s

2025-10-19 17:25:45,326 | INFO | Training epoch 1231, Batch 1000/1000: LR=3.30e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 17:25:45,396 | INFO | Epoch 1231 Train Time 36.02429723739624s

2025-10-19 17:26:21,420 | INFO | Training epoch 1232, Batch 1000/1000: LR=3.29e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:26:21,485 | INFO | Epoch 1232 Train Time 36.08805251121521s

2025-10-19 17:26:57,728 | INFO | Training epoch 1233, Batch 1000/1000: LR=3.29e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:26:57,822 | INFO | Epoch 1233 Train Time 36.33475685119629s

2025-10-19 17:27:34,051 | INFO | Training epoch 1234, Batch 1000/1000: LR=3.28e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:27:34,146 | INFO | Epoch 1234 Train Time 36.322415828704834s

2025-10-19 17:28:10,431 | INFO | Training epoch 1235, Batch 1000/1000: LR=3.27e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:28:10,509 | INFO | Epoch 1235 Train Time 36.361467361450195s

2025-10-19 17:28:46,733 | INFO | Training epoch 1236, Batch 1000/1000: LR=3.26e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:28:46,815 | INFO | Epoch 1236 Train Time 36.30474328994751s

2025-10-19 17:29:23,183 | INFO | Training epoch 1237, Batch 1000/1000: LR=3.26e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:29:23,273 | INFO | Epoch 1237 Train Time 36.45661783218384s

2025-10-19 17:29:59,331 | INFO | Training epoch 1238, Batch 1000/1000: LR=3.25e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:29:59,412 | INFO | Epoch 1238 Train Time 36.13803219795227s

2025-10-19 17:30:35,635 | INFO | Training epoch 1239, Batch 1000/1000: LR=3.24e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:30:35,709 | INFO | Epoch 1239 Train Time 36.29555559158325s

2025-10-19 17:31:11,604 | INFO | Training epoch 1240, Batch 1000/1000: LR=3.24e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 17:31:11,728 | INFO | Epoch 1240 Train Time 36.01793646812439s

2025-10-19 17:31:47,818 | INFO | Training epoch 1241, Batch 1000/1000: LR=3.23e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 17:31:47,893 | INFO | Epoch 1241 Train Time 36.164132833480835s

2025-10-19 17:32:24,025 | INFO | Training epoch 1242, Batch 1000/1000: LR=3.22e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:32:24,107 | INFO | Epoch 1242 Train Time 36.21307349205017s

2025-10-19 17:33:00,048 | INFO | Training epoch 1243, Batch 1000/1000: LR=3.21e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:33:00,122 | INFO | Epoch 1243 Train Time 36.0139274597168s

2025-10-19 17:33:35,823 | INFO | Training epoch 1244, Batch 1000/1000: LR=3.21e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:33:35,923 | INFO | Epoch 1244 Train Time 35.80024337768555s

2025-10-19 17:34:11,827 | INFO | Training epoch 1245, Batch 1000/1000: LR=3.20e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 17:34:11,941 | INFO | Epoch 1245 Train Time 36.01686644554138s

2025-10-19 17:34:47,924 | INFO | Training epoch 1246, Batch 1000/1000: LR=3.19e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:34:48,033 | INFO | Epoch 1246 Train Time 36.09139084815979s

2025-10-19 17:35:23,080 | INFO | Training epoch 1247, Batch 1000/1000: LR=3.18e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:35:23,173 | INFO | Epoch 1247 Train Time 35.13721680641174s

2025-10-19 17:35:58,828 | INFO | Training epoch 1248, Batch 1000/1000: LR=3.18e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:35:58,947 | INFO | Epoch 1248 Train Time 35.77293515205383s

2025-10-19 17:36:34,913 | INFO | Training epoch 1249, Batch 1000/1000: LR=3.17e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:36:35,008 | INFO | Epoch 1249 Train Time 36.059362173080444s

2025-10-19 17:37:10,636 | INFO | Training epoch 1250, Batch 1000/1000: LR=3.16e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:37:10,742 | INFO | Epoch 1250 Train Time 35.732256174087524s

2025-10-19 17:37:47,148 | INFO | Training epoch 1251, Batch 1000/1000: LR=3.16e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:37:47,239 | INFO | Epoch 1251 Train Time 36.494914293289185s

2025-10-19 17:38:22,924 | INFO | Training epoch 1252, Batch 1000/1000: LR=3.15e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:38:23,010 | INFO | Epoch 1252 Train Time 35.770041942596436s

2025-10-19 17:38:59,036 | INFO | Training epoch 1253, Batch 1000/1000: LR=3.14e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 17:38:59,129 | INFO | Epoch 1253 Train Time 36.11771202087402s

2025-10-19 17:39:35,227 | INFO | Training epoch 1254, Batch 1000/1000: LR=3.13e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:39:35,304 | INFO | Epoch 1254 Train Time 36.17341995239258s

2025-10-19 17:40:10,030 | INFO | Training epoch 1255, Batch 1000/1000: LR=3.13e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:40:10,116 | INFO | Epoch 1255 Train Time 34.81114220619202s

2025-10-19 17:40:45,352 | INFO | Training epoch 1256, Batch 1000/1000: LR=3.12e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:40:45,441 | INFO | Epoch 1256 Train Time 35.322885513305664s

2025-10-19 17:41:21,125 | INFO | Training epoch 1257, Batch 1000/1000: LR=3.11e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:41:21,222 | INFO | Epoch 1257 Train Time 35.779829025268555s

2025-10-19 17:41:57,330 | INFO | Training epoch 1258, Batch 1000/1000: LR=3.11e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 17:41:57,424 | INFO | Epoch 1258 Train Time 36.200603723526s

2025-10-19 17:42:33,754 | INFO | Training epoch 1259, Batch 1000/1000: LR=3.10e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:42:33,857 | INFO | Epoch 1259 Train Time 36.4303834438324s

2025-10-19 17:43:09,418 | INFO | Training epoch 1260, Batch 1000/1000: LR=3.09e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:43:09,501 | INFO | Epoch 1260 Train Time 35.64260530471802s

2025-10-19 17:43:45,310 | INFO | Training epoch 1261, Batch 1000/1000: LR=3.08e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 17:43:45,405 | INFO | Epoch 1261 Train Time 35.90292716026306s

2025-10-19 17:44:20,514 | INFO | Training epoch 1262, Batch 1000/1000: LR=3.08e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 17:44:20,615 | INFO | Epoch 1262 Train Time 35.209474325180054s

2025-10-19 17:44:56,161 | INFO | Training epoch 1263, Batch 1000/1000: LR=3.07e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:44:56,251 | INFO | Epoch 1263 Train Time 35.63455772399902s

2025-10-19 17:45:31,738 | INFO | Training epoch 1264, Batch 1000/1000: LR=3.06e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 17:45:31,817 | INFO | Epoch 1264 Train Time 35.56476092338562s

2025-10-19 17:46:07,731 | INFO | Training epoch 1265, Batch 1000/1000: LR=3.06e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:46:07,821 | INFO | Epoch 1265 Train Time 36.0031156539917s

2025-10-19 17:46:43,930 | INFO | Training epoch 1266, Batch 1000/1000: LR=3.05e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:46:44,009 | INFO | Epoch 1266 Train Time 36.18648290634155s

2025-10-19 17:47:19,951 | INFO | Training epoch 1267, Batch 1000/1000: LR=3.04e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:47:20,031 | INFO | Epoch 1267 Train Time 36.020586252212524s

2025-10-19 17:47:56,119 | INFO | Training epoch 1268, Batch 1000/1000: LR=3.03e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 17:47:56,207 | INFO | Epoch 1268 Train Time 36.174890756607056s

2025-10-19 17:48:31,636 | INFO | Training epoch 1269, Batch 1000/1000: LR=3.03e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:48:31,731 | INFO | Epoch 1269 Train Time 35.523199796676636s

2025-10-19 17:49:07,432 | INFO | Training epoch 1270, Batch 1000/1000: LR=3.02e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 17:49:07,524 | INFO | Epoch 1270 Train Time 35.792288064956665s

2025-10-19 17:49:43,017 | INFO | Training epoch 1271, Batch 1000/1000: LR=3.01e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 17:49:43,097 | INFO | Epoch 1271 Train Time 35.57007837295532s

2025-10-19 17:50:18,446 | INFO | Training epoch 1272, Batch 1000/1000: LR=3.01e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:50:18,556 | INFO | Epoch 1272 Train Time 35.45796990394592s

2025-10-19 17:50:54,422 | INFO | Training epoch 1273, Batch 1000/1000: LR=3.00e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:50:54,510 | INFO | Epoch 1273 Train Time 35.95242381095886s

2025-10-19 17:51:29,624 | INFO | Training epoch 1274, Batch 1000/1000: LR=2.99e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 17:51:29,709 | INFO | Epoch 1274 Train Time 35.197429895401s

2025-10-19 17:52:05,621 | INFO | Training epoch 1275, Batch 1000/1000: LR=2.98e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:52:05,719 | INFO | Epoch 1275 Train Time 36.008137464523315s

2025-10-19 17:52:41,960 | INFO | Training epoch 1276, Batch 1000/1000: LR=2.98e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 17:52:42,042 | INFO | Epoch 1276 Train Time 36.322691202163696s

2025-10-19 17:53:17,129 | INFO | Training epoch 1277, Batch 1000/1000: LR=2.97e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:53:17,205 | INFO | Epoch 1277 Train Time 35.161301136016846s

2025-10-19 17:53:53,632 | INFO | Training epoch 1278, Batch 1000/1000: LR=2.96e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 17:53:53,729 | INFO | Epoch 1278 Train Time 36.52243089675903s

2025-10-19 17:54:29,817 | INFO | Training epoch 1279, Batch 1000/1000: LR=2.96e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:54:29,897 | INFO | Epoch 1279 Train Time 36.16646194458008s

2025-10-19 17:55:05,415 | INFO | Training epoch 1280, Batch 1000/1000: LR=2.95e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:55:05,490 | INFO | Epoch 1280 Train Time 35.592283487319946s

2025-10-19 17:55:41,531 | INFO | Training epoch 1281, Batch 1000/1000: LR=2.94e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:55:41,633 | INFO | Epoch 1281 Train Time 36.139931201934814s

2025-10-19 17:56:17,425 | INFO | Training epoch 1282, Batch 1000/1000: LR=2.94e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 17:56:17,527 | INFO | Epoch 1282 Train Time 35.892210483551025s

2025-10-19 17:56:51,399 | INFO | Training epoch 1283, Batch 1000/1000: LR=2.93e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 17:56:51,462 | INFO | Epoch 1283 Train Time 33.93413519859314s

2025-10-19 17:57:26,129 | INFO | Training epoch 1284, Batch 1000/1000: LR=2.92e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 17:57:26,217 | INFO | Epoch 1284 Train Time 34.75355362892151s

2025-10-19 17:58:01,057 | INFO | Training epoch 1285, Batch 1000/1000: LR=2.91e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 17:58:01,136 | INFO | Epoch 1285 Train Time 34.917423725128174s

2025-10-19 17:58:36,308 | INFO | Training epoch 1286, Batch 1000/1000: LR=2.91e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 17:58:36,391 | INFO | Epoch 1286 Train Time 35.25471305847168s

2025-10-19 17:59:12,533 | INFO | Training epoch 1287, Batch 1000/1000: LR=2.90e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 17:59:12,616 | INFO | Epoch 1287 Train Time 36.224018573760986s

2025-10-19 17:59:46,123 | INFO | Training epoch 1288, Batch 1000/1000: LR=2.89e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 17:59:46,230 | INFO | Epoch 1288 Train Time 33.6120285987854s

2025-10-19 18:00:21,515 | INFO | Training epoch 1289, Batch 1000/1000: LR=2.89e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:00:21,617 | INFO | Epoch 1289 Train Time 35.38584041595459s

2025-10-19 18:00:57,428 | INFO | Training epoch 1290, Batch 1000/1000: LR=2.88e-05, Loss=3.01e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 18:00:57,528 | INFO | Epoch 1290 Train Time 35.91052293777466s

2025-10-19 18:01:29,713 | INFO | Training epoch 1291, Batch 1000/1000: LR=2.87e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:01:29,792 | INFO | Epoch 1291 Train Time 32.262941122055054s

2025-10-19 18:02:05,928 | INFO | Training epoch 1292, Batch 1000/1000: LR=2.87e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:02:06,026 | INFO | Epoch 1292 Train Time 36.23257112503052s

2025-10-19 18:02:41,623 | INFO | Training epoch 1293, Batch 1000/1000: LR=2.86e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:02:41,725 | INFO | Epoch 1293 Train Time 35.698214054107666s

2025-10-19 18:03:17,630 | INFO | Training epoch 1294, Batch 1000/1000: LR=2.85e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:03:17,714 | INFO | Epoch 1294 Train Time 35.987295389175415s

2025-10-19 18:03:53,129 | INFO | Training epoch 1295, Batch 1000/1000: LR=2.84e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:03:53,242 | INFO | Epoch 1295 Train Time 35.52539896965027s

2025-10-19 18:04:27,682 | INFO | Training epoch 1296, Batch 1000/1000: LR=2.84e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:04:27,737 | INFO | Epoch 1296 Train Time 34.4943265914917s

2025-10-19 18:05:01,402 | INFO | Training epoch 1297, Batch 1000/1000: LR=2.83e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:05:01,480 | INFO | Epoch 1297 Train Time 33.74150586128235s

2025-10-19 18:05:35,227 | INFO | Training epoch 1298, Batch 1000/1000: LR=2.82e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:05:35,327 | INFO | Epoch 1298 Train Time 33.8467652797699s

2025-10-19 18:06:11,048 | INFO | Training epoch 1299, Batch 1000/1000: LR=2.82e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:06:11,144 | INFO | Epoch 1299 Train Time 35.8153076171875s

2025-10-19 18:06:47,346 | INFO | Training epoch 1300, Batch 1000/1000: LR=2.81e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:06:47,429 | INFO | Epoch 1300 Train Time 36.283015727996826s

2025-10-19 18:07:22,589 | INFO | Training epoch 1301, Batch 1000/1000: LR=2.80e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:07:22,672 | INFO | Epoch 1301 Train Time 35.24179553985596s

2025-10-19 18:07:57,234 | INFO | Training epoch 1302, Batch 1000/1000: LR=2.80e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:07:57,337 | INFO | Epoch 1302 Train Time 34.66393041610718s

2025-10-19 18:08:32,779 | INFO | Training epoch 1303, Batch 1000/1000: LR=2.79e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:08:32,860 | INFO | Epoch 1303 Train Time 35.52232098579407s

2025-10-19 18:09:08,539 | INFO | Training epoch 1304, Batch 1000/1000: LR=2.78e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:09:08,623 | INFO | Epoch 1304 Train Time 35.76130294799805s

2025-10-19 18:09:44,327 | INFO | Training epoch 1305, Batch 1000/1000: LR=2.78e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:09:44,442 | INFO | Epoch 1305 Train Time 35.8187301158905s

2025-10-19 18:10:19,812 | INFO | Training epoch 1306, Batch 1000/1000: LR=2.77e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 18:10:19,906 | INFO | Epoch 1306 Train Time 35.461365699768066s

2025-10-19 18:10:55,930 | INFO | Training epoch 1307, Batch 1000/1000: LR=2.76e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:10:56,017 | INFO | Epoch 1307 Train Time 36.110339403152466s

2025-10-19 18:11:32,030 | INFO | Training epoch 1308, Batch 1000/1000: LR=2.75e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:11:32,129 | INFO | Epoch 1308 Train Time 36.110238790512085s

2025-10-19 18:12:08,951 | INFO | Training epoch 1309, Batch 1000/1000: LR=2.75e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:12:09,052 | INFO | Epoch 1309 Train Time 36.921712160110474s

2025-10-19 18:12:45,428 | INFO | Training epoch 1310, Batch 1000/1000: LR=2.74e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:12:45,525 | INFO | Epoch 1310 Train Time 36.4721143245697s

2025-10-19 18:13:20,044 | INFO | Training epoch 1311, Batch 1000/1000: LR=2.73e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 18:13:20,117 | INFO | Epoch 1311 Train Time 34.590752601623535s

2025-10-19 18:13:56,025 | INFO | Training epoch 1312, Batch 1000/1000: LR=2.73e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 18:13:56,109 | INFO | Epoch 1312 Train Time 35.99000382423401s

2025-10-19 18:14:31,620 | INFO | Training epoch 1313, Batch 1000/1000: LR=2.72e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:14:31,699 | INFO | Epoch 1313 Train Time 35.58883881568909s

2025-10-19 18:15:07,527 | INFO | Training epoch 1314, Batch 1000/1000: LR=2.71e-05, Loss=3.07e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 18:15:07,611 | INFO | Epoch 1314 Train Time 35.91109561920166s

2025-10-19 18:15:44,232 | INFO | Training epoch 1315, Batch 1000/1000: LR=2.71e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:15:44,317 | INFO | Epoch 1315 Train Time 36.705159425735474s

2025-10-19 18:16:20,113 | INFO | Training epoch 1316, Batch 1000/1000: LR=2.70e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:16:20,219 | INFO | Epoch 1316 Train Time 35.9015417098999s

2025-10-19 18:16:56,124 | INFO | Training epoch 1317, Batch 1000/1000: LR=2.69e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:16:56,226 | INFO | Epoch 1317 Train Time 36.00378704071045s

2025-10-19 18:17:31,937 | INFO | Training epoch 1318, Batch 1000/1000: LR=2.69e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:17:32,040 | INFO | Epoch 1318 Train Time 35.8140025138855s

2025-10-19 18:18:07,942 | INFO | Training epoch 1319, Batch 1000/1000: LR=2.68e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:18:08,034 | INFO | Epoch 1319 Train Time 35.991907835006714s

2025-10-19 18:18:43,666 | INFO | Training epoch 1320, Batch 1000/1000: LR=2.67e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:18:43,757 | INFO | Epoch 1320 Train Time 35.7227783203125s

2025-10-19 18:19:19,533 | INFO | Training epoch 1321, Batch 1000/1000: LR=2.67e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:19:19,610 | INFO | Epoch 1321 Train Time 35.850839376449585s

2025-10-19 18:19:55,015 | INFO | Training epoch 1322, Batch 1000/1000: LR=2.66e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 18:19:55,097 | INFO | Epoch 1322 Train Time 35.48506546020508s

2025-10-19 18:20:30,943 | INFO | Training epoch 1323, Batch 1000/1000: LR=2.65e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:20:31,028 | INFO | Epoch 1323 Train Time 35.930792808532715s

2025-10-19 18:21:06,829 | INFO | Training epoch 1324, Batch 1000/1000: LR=2.64e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:21:06,909 | INFO | Epoch 1324 Train Time 35.87757468223572s

2025-10-19 18:21:42,630 | INFO | Training epoch 1325, Batch 1000/1000: LR=2.64e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:21:42,730 | INFO | Epoch 1325 Train Time 35.819674253463745s

2025-10-19 18:22:18,319 | INFO | Training epoch 1326, Batch 1000/1000: LR=2.63e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:22:18,426 | INFO | Epoch 1326 Train Time 35.69471025466919s

2025-10-19 18:22:54,170 | INFO | Training epoch 1327, Batch 1000/1000: LR=2.62e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:22:54,264 | INFO | Epoch 1327 Train Time 35.83624005317688s

2025-10-19 18:23:30,030 | INFO | Training epoch 1328, Batch 1000/1000: LR=2.62e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 18:23:30,130 | INFO | Epoch 1328 Train Time 35.86381983757019s

2025-10-19 18:24:06,137 | INFO | Training epoch 1329, Batch 1000/1000: LR=2.61e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 18:24:06,250 | INFO | Epoch 1329 Train Time 36.118338108062744s

2025-10-19 18:24:41,722 | INFO | Training epoch 1330, Batch 1000/1000: LR=2.60e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.52e-01
2025-10-19 18:24:41,828 | INFO | Epoch 1330 Train Time 35.57582449913025s

2025-10-19 18:25:18,147 | INFO | Training epoch 1331, Batch 1000/1000: LR=2.60e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:25:18,243 | INFO | Epoch 1331 Train Time 36.4128954410553s

2025-10-19 18:25:54,234 | INFO | Training epoch 1332, Batch 1000/1000: LR=2.59e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 18:25:54,334 | INFO | Epoch 1332 Train Time 36.089810371398926s

2025-10-19 18:26:30,891 | INFO | Training epoch 1333, Batch 1000/1000: LR=2.58e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 18:26:30,983 | INFO | Epoch 1333 Train Time 36.6487500667572s

2025-10-19 18:27:07,732 | INFO | Training epoch 1334, Batch 1000/1000: LR=2.58e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:27:07,823 | INFO | Epoch 1334 Train Time 36.837823152542114s

2025-10-19 18:27:43,627 | INFO | Training epoch 1335, Batch 1000/1000: LR=2.57e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:27:43,722 | INFO | Epoch 1335 Train Time 35.898465633392334s

2025-10-19 18:28:20,029 | INFO | Training epoch 1336, Batch 1000/1000: LR=2.56e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:28:20,132 | INFO | Epoch 1336 Train Time 36.40898084640503s

2025-10-19 18:28:55,634 | INFO | Training epoch 1337, Batch 1000/1000: LR=2.56e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:28:55,737 | INFO | Epoch 1337 Train Time 35.60356283187866s

2025-10-19 18:29:31,728 | INFO | Training epoch 1338, Batch 1000/1000: LR=2.55e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 18:29:31,836 | INFO | Epoch 1338 Train Time 36.09794855117798s

2025-10-19 18:30:07,746 | INFO | Training epoch 1339, Batch 1000/1000: LR=2.54e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 18:30:07,846 | INFO | Epoch 1339 Train Time 36.009896755218506s

2025-10-19 18:30:43,734 | INFO | Training epoch 1340, Batch 1000/1000: LR=2.54e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:30:43,813 | INFO | Epoch 1340 Train Time 35.964882373809814s

2025-10-19 18:31:19,328 | INFO | Training epoch 1341, Batch 1000/1000: LR=2.53e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:31:19,405 | INFO | Epoch 1341 Train Time 35.590418338775635s

2025-10-19 18:31:55,768 | INFO | Training epoch 1342, Batch 1000/1000: LR=2.52e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:31:55,846 | INFO | Epoch 1342 Train Time 36.43981623649597s

2025-10-19 18:32:32,027 | INFO | Training epoch 1343, Batch 1000/1000: LR=2.52e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:32:32,121 | INFO | Epoch 1343 Train Time 36.27474617958069s

2025-10-19 18:33:08,326 | INFO | Training epoch 1344, Batch 1000/1000: LR=2.51e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:33:08,433 | INFO | Epoch 1344 Train Time 36.31038284301758s

2025-10-19 18:33:44,559 | INFO | Training epoch 1345, Batch 1000/1000: LR=2.50e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:33:44,655 | INFO | Epoch 1345 Train Time 36.22085118293762s

2025-10-19 18:34:19,817 | INFO | Training epoch 1346, Batch 1000/1000: LR=2.50e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 18:34:19,894 | INFO | Epoch 1346 Train Time 35.237677812576294s

2025-10-19 18:34:55,830 | INFO | Training epoch 1347, Batch 1000/1000: LR=2.49e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:34:55,909 | INFO | Epoch 1347 Train Time 36.013203620910645s

2025-10-19 18:35:31,748 | INFO | Training epoch 1348, Batch 1000/1000: LR=2.48e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:35:31,853 | INFO | Epoch 1348 Train Time 35.94258952140808s

2025-10-19 18:36:07,533 | INFO | Training epoch 1349, Batch 1000/1000: LR=2.48e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:36:07,644 | INFO | Epoch 1349 Train Time 35.790038108825684s

2025-10-19 18:36:43,632 | INFO | Training epoch 1350, Batch 1000/1000: LR=2.47e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 18:36:43,730 | INFO | Epoch 1350 Train Time 36.08521032333374s

2025-10-19 18:37:19,527 | INFO | Training epoch 1351, Batch 1000/1000: LR=2.46e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:37:19,601 | INFO | Epoch 1351 Train Time 35.86980366706848s

2025-10-19 18:37:56,516 | INFO | Training epoch 1352, Batch 1000/1000: LR=2.46e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 18:37:56,615 | INFO | Epoch 1352 Train Time 37.01251530647278s

2025-10-19 18:38:32,255 | INFO | Training epoch 1353, Batch 1000/1000: LR=2.45e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:38:32,352 | INFO | Epoch 1353 Train Time 35.735212087631226s

2025-10-19 18:39:07,735 | INFO | Training epoch 1354, Batch 1000/1000: LR=2.44e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 18:39:07,835 | INFO | Epoch 1354 Train Time 35.48188662528992s

2025-10-19 18:39:44,035 | INFO | Training epoch 1355, Batch 1000/1000: LR=2.44e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.50e-01
2025-10-19 18:39:44,115 | INFO | Epoch 1355 Train Time 36.279340744018555s

2025-10-19 18:40:20,060 | INFO | Training epoch 1356, Batch 1000/1000: LR=2.43e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:40:20,174 | INFO | Epoch 1356 Train Time 36.05777597427368s

2025-10-19 18:40:56,431 | INFO | Training epoch 1357, Batch 1000/1000: LR=2.42e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:40:56,515 | INFO | Epoch 1357 Train Time 36.34008073806763s

2025-10-19 18:41:32,243 | INFO | Training epoch 1358, Batch 1000/1000: LR=2.42e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 18:41:32,328 | INFO | Epoch 1358 Train Time 35.81193542480469s

2025-10-19 18:42:08,626 | INFO | Training epoch 1359, Batch 1000/1000: LR=2.41e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 18:42:08,720 | INFO | Epoch 1359 Train Time 36.390732765197754s

2025-10-19 18:42:44,515 | INFO | Training epoch 1360, Batch 1000/1000: LR=2.40e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:42:44,613 | INFO | Epoch 1360 Train Time 35.890219926834106s

2025-10-19 18:43:20,116 | INFO | Training epoch 1361, Batch 1000/1000: LR=2.40e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:43:20,200 | INFO | Epoch 1361 Train Time 35.584794759750366s

2025-10-19 18:43:55,920 | INFO | Training epoch 1362, Batch 1000/1000: LR=2.39e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:43:56,006 | INFO | Epoch 1362 Train Time 35.805508613586426s

2025-10-19 18:44:31,825 | INFO | Training epoch 1363, Batch 1000/1000: LR=2.38e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:44:31,912 | INFO | Epoch 1363 Train Time 35.904327154159546s

2025-10-19 18:45:07,756 | INFO | Training epoch 1364, Batch 1000/1000: LR=2.38e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:45:07,847 | INFO | Epoch 1364 Train Time 35.93386721611023s

2025-10-19 18:45:43,307 | INFO | Training epoch 1365, Batch 1000/1000: LR=2.37e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:45:43,386 | INFO | Epoch 1365 Train Time 35.53779149055481s

2025-10-19 18:46:19,323 | INFO | Training epoch 1366, Batch 1000/1000: LR=2.36e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:46:19,414 | INFO | Epoch 1366 Train Time 36.02737593650818s

2025-10-19 18:46:55,123 | INFO | Training epoch 1367, Batch 1000/1000: LR=2.36e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:46:55,216 | INFO | Epoch 1367 Train Time 35.80158710479736s

2025-10-19 18:47:31,326 | INFO | Training epoch 1368, Batch 1000/1000: LR=2.35e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 18:47:31,428 | INFO | Epoch 1368 Train Time 36.209797859191895s

2025-10-19 18:48:07,421 | INFO | Training epoch 1369, Batch 1000/1000: LR=2.35e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 18:48:07,499 | INFO | Epoch 1369 Train Time 36.06939220428467s

2025-10-19 18:48:43,108 | INFO | Training epoch 1370, Batch 1000/1000: LR=2.34e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:48:43,200 | INFO | Epoch 1370 Train Time 35.69942545890808s

2025-10-19 18:49:18,880 | INFO | Training epoch 1371, Batch 1000/1000: LR=2.33e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 18:49:18,997 | INFO | Epoch 1371 Train Time 35.796979904174805s

2025-10-19 18:49:55,149 | INFO | Training epoch 1372, Batch 1000/1000: LR=2.33e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:49:55,229 | INFO | Epoch 1372 Train Time 36.230414390563965s

2025-10-19 18:50:30,930 | INFO | Training epoch 1373, Batch 1000/1000: LR=2.32e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 18:50:31,031 | INFO | Epoch 1373 Train Time 35.80073952674866s

2025-10-19 18:51:06,934 | INFO | Training epoch 1374, Batch 1000/1000: LR=2.31e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 18:51:07,034 | INFO | Epoch 1374 Train Time 36.0012903213501s

2025-10-19 18:51:42,733 | INFO | Training epoch 1375, Batch 1000/1000: LR=2.31e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 18:51:42,834 | INFO | Epoch 1375 Train Time 35.79931139945984s

2025-10-19 18:52:18,764 | INFO | Training epoch 1376, Batch 1000/1000: LR=2.30e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:52:18,854 | INFO | Epoch 1376 Train Time 36.01878786087036s

2025-10-19 18:52:54,745 | INFO | Training epoch 1377, Batch 1000/1000: LR=2.29e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 18:52:54,840 | INFO | Epoch 1377 Train Time 35.98582077026367s

2025-10-19 18:53:30,230 | INFO | Training epoch 1378, Batch 1000/1000: LR=2.29e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 18:53:30,313 | INFO | Epoch 1378 Train Time 35.469884395599365s

2025-10-19 18:54:06,105 | INFO | Training epoch 1379, Batch 1000/1000: LR=2.28e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 18:54:06,218 | INFO | Epoch 1379 Train Time 35.903944969177246s

2025-10-19 18:54:42,234 | INFO | Training epoch 1380, Batch 1000/1000: LR=2.27e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:54:42,334 | INFO | Epoch 1380 Train Time 36.11419892311096s

2025-10-19 18:55:18,022 | INFO | Training epoch 1381, Batch 1000/1000: LR=2.27e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 18:55:18,119 | INFO | Epoch 1381 Train Time 35.78393602371216s

2025-10-19 18:55:53,934 | INFO | Training epoch 1382, Batch 1000/1000: LR=2.26e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:55:54,011 | INFO | Epoch 1382 Train Time 35.891857385635376s

2025-10-19 18:56:29,815 | INFO | Training epoch 1383, Batch 1000/1000: LR=2.25e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.50e-01
2025-10-19 18:56:29,915 | INFO | Epoch 1383 Train Time 35.90272903442383s

2025-10-19 18:57:05,629 | INFO | Training epoch 1384, Batch 1000/1000: LR=2.25e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 18:57:05,729 | INFO | Epoch 1384 Train Time 35.81271004676819s

2025-10-19 18:57:41,622 | INFO | Training epoch 1385, Batch 1000/1000: LR=2.24e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 18:57:41,720 | INFO | Epoch 1385 Train Time 35.989173889160156s

2025-10-19 18:58:17,752 | INFO | Training epoch 1386, Batch 1000/1000: LR=2.24e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:58:17,831 | INFO | Epoch 1386 Train Time 36.110294580459595s

2025-10-19 18:58:53,197 | INFO | Training epoch 1387, Batch 1000/1000: LR=2.23e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 18:58:53,296 | INFO | Epoch 1387 Train Time 35.46442151069641s

2025-10-19 18:59:29,431 | INFO | Training epoch 1388, Batch 1000/1000: LR=2.22e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 18:59:29,523 | INFO | Epoch 1388 Train Time 36.226309299468994s

2025-10-19 19:00:05,064 | INFO | Training epoch 1389, Batch 1000/1000: LR=2.22e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:00:05,161 | INFO | Epoch 1389 Train Time 35.63727807998657s

2025-10-19 19:00:41,029 | INFO | Training epoch 1390, Batch 1000/1000: LR=2.21e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:00:41,130 | INFO | Epoch 1390 Train Time 35.968076944351196s

2025-10-19 19:01:16,947 | INFO | Training epoch 1391, Batch 1000/1000: LR=2.20e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:01:17,038 | INFO | Epoch 1391 Train Time 35.90693116188049s

2025-10-19 19:01:52,332 | INFO | Training epoch 1392, Batch 1000/1000: LR=2.20e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:01:52,412 | INFO | Epoch 1392 Train Time 35.37251949310303s

2025-10-19 19:02:27,927 | INFO | Training epoch 1393, Batch 1000/1000: LR=2.19e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 19:02:28,018 | INFO | Epoch 1393 Train Time 35.60578417778015s

2025-10-19 19:03:05,231 | INFO | Training epoch 1394, Batch 1000/1000: LR=2.18e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:03:05,337 | INFO | Epoch 1394 Train Time 37.317200899124146s

2025-10-19 19:03:40,915 | INFO | Training epoch 1395, Batch 1000/1000: LR=2.18e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:03:41,003 | INFO | Epoch 1395 Train Time 35.66457438468933s

2025-10-19 19:04:16,967 | INFO | Training epoch 1396, Batch 1000/1000: LR=2.17e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:04:17,049 | INFO | Epoch 1396 Train Time 36.04402232170105s

2025-10-19 19:04:53,228 | INFO | Training epoch 1397, Batch 1000/1000: LR=2.17e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 19:04:53,317 | INFO | Epoch 1397 Train Time 36.26702284812927s

2025-10-19 19:05:29,238 | INFO | Training epoch 1398, Batch 1000/1000: LR=2.16e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:05:29,343 | INFO | Epoch 1398 Train Time 36.024170875549316s

2025-10-19 19:06:05,063 | INFO | Training epoch 1399, Batch 1000/1000: LR=2.15e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 19:06:05,153 | INFO | Epoch 1399 Train Time 35.80967593193054s

2025-10-19 19:06:41,122 | INFO | Training epoch 1400, Batch 1000/1000: LR=2.15e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:06:41,204 | INFO | Epoch 1400 Train Time 36.04956865310669s

2025-10-19 19:07:17,433 | INFO | Training epoch 1401, Batch 1000/1000: LR=2.14e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:07:17,521 | INFO | Epoch 1401 Train Time 36.315338373184204s

2025-10-19 19:07:53,860 | INFO | Training epoch 1402, Batch 1000/1000: LR=2.13e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:07:53,940 | INFO | Epoch 1402 Train Time 36.41678500175476s

2025-10-19 19:08:29,849 | INFO | Training epoch 1403, Batch 1000/1000: LR=2.13e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:08:29,931 | INFO | Epoch 1403 Train Time 35.98957180976868s

2025-10-19 19:09:06,070 | INFO | Training epoch 1404, Batch 1000/1000: LR=2.12e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:09:06,150 | INFO | Epoch 1404 Train Time 36.21858739852905s

2025-10-19 19:09:41,618 | INFO | Training epoch 1405, Batch 1000/1000: LR=2.12e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:09:41,703 | INFO | Epoch 1405 Train Time 35.55198097229004s

2025-10-19 19:10:17,255 | INFO | Training epoch 1406, Batch 1000/1000: LR=2.11e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:10:17,348 | INFO | Epoch 1406 Train Time 35.64388823509216s

2025-10-19 19:10:53,425 | INFO | Training epoch 1407, Batch 1000/1000: LR=2.10e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:10:53,522 | INFO | Epoch 1407 Train Time 36.17305016517639s

2025-10-19 19:11:28,958 | INFO | Training epoch 1408, Batch 1000/1000: LR=2.10e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:11:29,044 | INFO | Epoch 1408 Train Time 35.520944595336914s

2025-10-19 19:12:05,374 | INFO | Training epoch 1409, Batch 1000/1000: LR=2.09e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:12:05,456 | INFO | Epoch 1409 Train Time 36.4097466468811s

2025-10-19 19:12:41,038 | INFO | Training epoch 1410, Batch 1000/1000: LR=2.08e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 19:12:41,130 | INFO | Epoch 1410 Train Time 35.672274589538574s

2025-10-19 19:13:16,636 | INFO | Training epoch 1411, Batch 1000/1000: LR=2.08e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:13:16,728 | INFO | Epoch 1411 Train Time 35.59716033935547s

2025-10-19 19:13:52,128 | INFO | Training epoch 1412, Batch 1000/1000: LR=2.07e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:13:52,230 | INFO | Epoch 1412 Train Time 35.50000548362732s

2025-10-19 19:14:28,039 | INFO | Training epoch 1413, Batch 1000/1000: LR=2.07e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 19:14:28,129 | INFO | Epoch 1413 Train Time 35.897982597351074s

2025-10-19 19:15:03,326 | INFO | Training epoch 1414, Batch 1000/1000: LR=2.06e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:15:03,421 | INFO | Epoch 1414 Train Time 35.29024839401245s

2025-10-19 19:15:39,425 | INFO | Training epoch 1415, Batch 1000/1000: LR=2.05e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:15:39,517 | INFO | Epoch 1415 Train Time 36.093000650405884s

2025-10-19 19:16:15,227 | INFO | Training epoch 1416, Batch 1000/1000: LR=2.05e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:16:15,315 | INFO | Epoch 1416 Train Time 35.797391176223755s

2025-10-19 19:16:51,721 | INFO | Training epoch 1417, Batch 1000/1000: LR=2.04e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:16:51,827 | INFO | Epoch 1417 Train Time 36.511168479919434s

2025-10-19 19:17:27,734 | INFO | Training epoch 1418, Batch 1000/1000: LR=2.03e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:17:27,815 | INFO | Epoch 1418 Train Time 35.9847457408905s

2025-10-19 19:18:03,729 | INFO | Training epoch 1419, Batch 1000/1000: LR=2.03e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:18:03,818 | INFO | Epoch 1419 Train Time 36.00243878364563s

2025-10-19 19:18:39,923 | INFO | Training epoch 1420, Batch 1000/1000: LR=2.02e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:18:40,018 | INFO | Epoch 1420 Train Time 36.19814682006836s

2025-10-19 19:19:15,841 | INFO | Training epoch 1421, Batch 1000/1000: LR=2.02e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:19:15,924 | INFO | Epoch 1421 Train Time 35.9041805267334s

2025-10-19 19:19:51,635 | INFO | Training epoch 1422, Batch 1000/1000: LR=2.01e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:19:51,723 | INFO | Epoch 1422 Train Time 35.79796743392944s

2025-10-19 19:20:28,130 | INFO | Training epoch 1423, Batch 1000/1000: LR=2.00e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 19:20:28,215 | INFO | Epoch 1423 Train Time 36.489906311035156s

2025-10-19 19:21:03,726 | INFO | Training epoch 1424, Batch 1000/1000: LR=2.00e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 19:21:03,807 | INFO | Epoch 1424 Train Time 35.59175086021423s

2025-10-19 19:21:41,175 | INFO | Training epoch 1425, Batch 1000/1000: LR=1.99e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:21:41,262 | INFO | Epoch 1425 Train Time 37.4535276889801s

2025-10-19 19:22:17,111 | INFO | Training epoch 1426, Batch 1000/1000: LR=1.99e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 19:22:17,196 | INFO | Epoch 1426 Train Time 35.93344473838806s

2025-10-19 19:22:52,725 | INFO | Training epoch 1427, Batch 1000/1000: LR=1.98e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 19:22:52,816 | INFO | Epoch 1427 Train Time 35.61806583404541s

2025-10-19 19:23:28,963 | INFO | Training epoch 1428, Batch 1000/1000: LR=1.97e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:23:29,056 | INFO | Epoch 1428 Train Time 36.23812556266785s

2025-10-19 19:24:04,835 | INFO | Training epoch 1429, Batch 1000/1000: LR=1.97e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:24:04,950 | INFO | Epoch 1429 Train Time 35.89259934425354s

2025-10-19 19:24:40,529 | INFO | Training epoch 1430, Batch 1000/1000: LR=1.96e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:24:40,604 | INFO | Epoch 1430 Train Time 35.652700662612915s

2025-10-19 19:25:16,330 | INFO | Training epoch 1431, Batch 1000/1000: LR=1.96e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:25:16,428 | INFO | Epoch 1431 Train Time 35.82386565208435s

2025-10-19 19:25:53,038 | INFO | Training epoch 1432, Batch 1000/1000: LR=1.95e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:25:53,142 | INFO | Epoch 1432 Train Time 36.71140146255493s

2025-10-19 19:26:28,825 | INFO | Training epoch 1433, Batch 1000/1000: LR=1.94e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:26:28,900 | INFO | Epoch 1433 Train Time 35.757718324661255s

2025-10-19 19:27:04,634 | INFO | Training epoch 1434, Batch 1000/1000: LR=1.94e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:27:04,735 | INFO | Epoch 1434 Train Time 35.833330392837524s

2025-10-19 19:27:40,232 | INFO | Training epoch 1435, Batch 1000/1000: LR=1.93e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:27:40,329 | INFO | Epoch 1435 Train Time 35.59122323989868s

2025-10-19 19:28:16,426 | INFO | Training epoch 1436, Batch 1000/1000: LR=1.92e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:28:16,533 | INFO | Epoch 1436 Train Time 36.20313572883606s

2025-10-19 19:28:52,828 | INFO | Training epoch 1437, Batch 1000/1000: LR=1.92e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 19:28:52,919 | INFO | Epoch 1437 Train Time 36.38465356826782s

2025-10-19 19:29:28,729 | INFO | Training epoch 1438, Batch 1000/1000: LR=1.91e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:29:28,827 | INFO | Epoch 1438 Train Time 35.90468502044678s

2025-10-19 19:30:04,418 | INFO | Training epoch 1439, Batch 1000/1000: LR=1.91e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:30:04,514 | INFO | Epoch 1439 Train Time 35.686058044433594s

2025-10-19 19:30:39,712 | INFO | Training epoch 1440, Batch 1000/1000: LR=1.90e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:30:39,802 | INFO | Epoch 1440 Train Time 35.28736853599548s

2025-10-19 19:31:16,023 | INFO | Training epoch 1441, Batch 1000/1000: LR=1.89e-05, Loss=3.11e-02 BER=1.17e-02 FER=1.53e-01
2025-10-19 19:31:16,107 | INFO | Epoch 1441 Train Time 36.30353617668152s

2025-10-19 19:31:52,424 | INFO | Training epoch 1442, Batch 1000/1000: LR=1.89e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 19:31:52,504 | INFO | Epoch 1442 Train Time 36.39645004272461s

2025-10-19 19:32:28,235 | INFO | Training epoch 1443, Batch 1000/1000: LR=1.88e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:32:28,316 | INFO | Epoch 1443 Train Time 35.81066060066223s

2025-10-19 19:33:04,026 | INFO | Training epoch 1444, Batch 1000/1000: LR=1.88e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 19:33:04,112 | INFO | Epoch 1444 Train Time 35.79390788078308s

2025-10-19 19:33:40,131 | INFO | Training epoch 1445, Batch 1000/1000: LR=1.87e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:33:40,232 | INFO | Epoch 1445 Train Time 36.118826389312744s

2025-10-19 19:34:17,020 | INFO | Training epoch 1446, Batch 1000/1000: LR=1.86e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:34:17,118 | INFO | Epoch 1446 Train Time 36.883840799331665s

2025-10-19 19:34:53,323 | INFO | Training epoch 1447, Batch 1000/1000: LR=1.86e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:34:53,433 | INFO | Epoch 1447 Train Time 36.31410837173462s

2025-10-19 19:35:30,137 | INFO | Training epoch 1448, Batch 1000/1000: LR=1.85e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:35:30,241 | INFO | Epoch 1448 Train Time 36.80757522583008s

2025-10-19 19:36:06,734 | INFO | Training epoch 1449, Batch 1000/1000: LR=1.85e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:36:06,826 | INFO | Epoch 1449 Train Time 36.58337354660034s

2025-10-19 19:36:42,350 | INFO | Training epoch 1450, Batch 1000/1000: LR=1.84e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:36:42,442 | INFO | Epoch 1450 Train Time 35.61337685585022s

2025-10-19 19:37:18,328 | INFO | Training epoch 1451, Batch 1000/1000: LR=1.84e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:37:18,426 | INFO | Epoch 1451 Train Time 35.98373484611511s

2025-10-19 19:37:54,925 | INFO | Training epoch 1452, Batch 1000/1000: LR=1.83e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:37:55,002 | INFO | Epoch 1452 Train Time 36.57468271255493s

2025-10-19 19:38:30,755 | INFO | Training epoch 1453, Batch 1000/1000: LR=1.82e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:38:30,835 | INFO | Epoch 1453 Train Time 35.8316867351532s

2025-10-19 19:39:06,125 | INFO | Training epoch 1454, Batch 1000/1000: LR=1.82e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:39:06,217 | INFO | Epoch 1454 Train Time 35.38072323799133s

2025-10-19 19:39:41,929 | INFO | Training epoch 1455, Batch 1000/1000: LR=1.81e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 19:39:42,025 | INFO | Epoch 1455 Train Time 35.80710172653198s

2025-10-19 19:40:18,226 | INFO | Training epoch 1456, Batch 1000/1000: LR=1.81e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:40:18,308 | INFO | Epoch 1456 Train Time 36.282530784606934s

2025-10-19 19:40:54,130 | INFO | Training epoch 1457, Batch 1000/1000: LR=1.80e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 19:40:54,228 | INFO | Epoch 1457 Train Time 35.91824245452881s

2025-10-19 19:41:30,141 | INFO | Training epoch 1458, Batch 1000/1000: LR=1.79e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:41:30,240 | INFO | Epoch 1458 Train Time 36.01061940193176s

2025-10-19 19:42:07,433 | INFO | Training epoch 1459, Batch 1000/1000: LR=1.79e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:42:07,525 | INFO | Epoch 1459 Train Time 37.28398942947388s

2025-10-19 19:42:44,448 | INFO | Training epoch 1460, Batch 1000/1000: LR=1.78e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:42:44,544 | INFO | Epoch 1460 Train Time 37.018662452697754s

2025-10-19 19:43:20,129 | INFO | Training epoch 1461, Batch 1000/1000: LR=1.78e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.50e-01
2025-10-19 19:43:20,225 | INFO | Epoch 1461 Train Time 35.67957520484924s

2025-10-19 19:43:56,126 | INFO | Training epoch 1462, Batch 1000/1000: LR=1.77e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:43:56,206 | INFO | Epoch 1462 Train Time 35.98020887374878s

2025-10-19 19:44:32,430 | INFO | Training epoch 1463, Batch 1000/1000: LR=1.76e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:44:32,525 | INFO | Epoch 1463 Train Time 36.31645464897156s

2025-10-19 19:45:07,927 | INFO | Training epoch 1464, Batch 1000/1000: LR=1.76e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:45:08,014 | INFO | Epoch 1464 Train Time 35.488361120224s

2025-10-19 19:45:43,773 | INFO | Training epoch 1465, Batch 1000/1000: LR=1.75e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 19:45:43,854 | INFO | Epoch 1465 Train Time 35.83826041221619s

2025-10-19 19:46:19,056 | INFO | Training epoch 1466, Batch 1000/1000: LR=1.75e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:46:19,137 | INFO | Epoch 1466 Train Time 35.28070378303528s

2025-10-19 19:46:54,237 | INFO | Training epoch 1467, Batch 1000/1000: LR=1.74e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 19:46:54,321 | INFO | Epoch 1467 Train Time 35.183287620544434s

2025-10-19 19:47:30,228 | INFO | Training epoch 1468, Batch 1000/1000: LR=1.74e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 19:47:30,311 | INFO | Epoch 1468 Train Time 35.98748421669006s

2025-10-19 19:48:06,228 | INFO | Training epoch 1469, Batch 1000/1000: LR=1.73e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:48:06,332 | INFO | Epoch 1469 Train Time 36.01947999000549s

2025-10-19 19:48:42,134 | INFO | Training epoch 1470, Batch 1000/1000: LR=1.72e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:48:42,213 | INFO | Epoch 1470 Train Time 35.88022327423096s

2025-10-19 19:49:17,530 | INFO | Training epoch 1471, Batch 1000/1000: LR=1.72e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:49:17,630 | INFO | Epoch 1471 Train Time 35.415815591812134s

2025-10-19 19:49:53,014 | INFO | Training epoch 1472, Batch 1000/1000: LR=1.71e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 19:49:53,125 | INFO | Epoch 1472 Train Time 35.49373769760132s

2025-10-19 19:50:27,422 | INFO | Training epoch 1473, Batch 1000/1000: LR=1.71e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:50:27,498 | INFO | Epoch 1473 Train Time 34.371776819229126s

2025-10-19 19:51:03,023 | INFO | Training epoch 1474, Batch 1000/1000: LR=1.70e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:51:03,098 | INFO | Epoch 1474 Train Time 35.59923028945923s

2025-10-19 19:51:38,417 | INFO | Training epoch 1475, Batch 1000/1000: LR=1.70e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:51:38,496 | INFO | Epoch 1475 Train Time 35.39748668670654s

2025-10-19 19:52:13,528 | INFO | Training epoch 1476, Batch 1000/1000: LR=1.69e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:52:13,600 | INFO | Epoch 1476 Train Time 35.103166341781616s

2025-10-19 19:52:49,428 | INFO | Training epoch 1477, Batch 1000/1000: LR=1.68e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:52:49,517 | INFO | Epoch 1477 Train Time 35.91511559486389s

2025-10-19 19:53:26,022 | INFO | Training epoch 1478, Batch 1000/1000: LR=1.68e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:53:26,134 | INFO | Epoch 1478 Train Time 36.61649560928345s

2025-10-19 19:54:02,241 | INFO | Training epoch 1479, Batch 1000/1000: LR=1.67e-05, Loss=3.10e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 19:54:02,342 | INFO | Epoch 1479 Train Time 36.2066924571991s

2025-10-19 19:54:37,763 | INFO | Training epoch 1480, Batch 1000/1000: LR=1.67e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 19:54:37,865 | INFO | Epoch 1480 Train Time 35.52130103111267s

2025-10-19 19:55:13,927 | INFO | Training epoch 1481, Batch 1000/1000: LR=1.66e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 19:55:14,024 | INFO | Epoch 1481 Train Time 36.1573646068573s

2025-10-19 19:55:49,831 | INFO | Training epoch 1482, Batch 1000/1000: LR=1.66e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 19:55:49,925 | INFO | Epoch 1482 Train Time 35.89861869812012s

2025-10-19 19:56:25,091 | INFO | Training epoch 1483, Batch 1000/1000: LR=1.65e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 19:56:25,168 | INFO | Epoch 1483 Train Time 35.24225354194641s

2025-10-19 19:57:01,038 | INFO | Training epoch 1484, Batch 1000/1000: LR=1.64e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:57:01,135 | INFO | Epoch 1484 Train Time 35.96504735946655s

2025-10-19 19:57:36,525 | INFO | Training epoch 1485, Batch 1000/1000: LR=1.64e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.47e-01
2025-10-19 19:57:36,637 | INFO | Epoch 1485 Train Time 35.500279664993286s

2025-10-19 19:58:12,621 | INFO | Training epoch 1486, Batch 1000/1000: LR=1.63e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 19:58:12,706 | INFO | Epoch 1486 Train Time 36.06867027282715s

2025-10-19 19:58:48,662 | INFO | Training epoch 1487, Batch 1000/1000: LR=1.63e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 19:58:48,742 | INFO | Epoch 1487 Train Time 36.034865617752075s

2025-10-19 19:59:24,752 | INFO | Training epoch 1488, Batch 1000/1000: LR=1.62e-05, Loss=3.04e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 19:59:24,852 | INFO | Epoch 1488 Train Time 36.10881805419922s

2025-10-19 20:00:00,934 | INFO | Training epoch 1489, Batch 1000/1000: LR=1.62e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:00:01,032 | INFO | Epoch 1489 Train Time 36.17869973182678s

2025-10-19 20:00:36,740 | INFO | Training epoch 1490, Batch 1000/1000: LR=1.61e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:00:36,860 | INFO | Epoch 1490 Train Time 35.825706481933594s

2025-10-19 20:01:12,739 | INFO | Training epoch 1491, Batch 1000/1000: LR=1.61e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:01:12,812 | INFO | Epoch 1491 Train Time 35.95136284828186s

2025-10-19 20:01:48,335 | INFO | Training epoch 1492, Batch 1000/1000: LR=1.60e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:01:48,403 | INFO | Epoch 1492 Train Time 35.590184926986694s

2025-10-19 20:02:24,524 | INFO | Training epoch 1493, Batch 1000/1000: LR=1.59e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 20:02:24,610 | INFO | Epoch 1493 Train Time 36.20619487762451s

2025-10-19 20:03:00,926 | INFO | Training epoch 1494, Batch 1000/1000: LR=1.59e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:03:01,011 | INFO | Epoch 1494 Train Time 36.398841857910156s

2025-10-19 20:03:36,905 | INFO | Training epoch 1495, Batch 1000/1000: LR=1.58e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 20:03:36,985 | INFO | Epoch 1495 Train Time 35.97360444068909s

2025-10-19 20:04:13,714 | INFO | Training epoch 1496, Batch 1000/1000: LR=1.58e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:04:13,806 | INFO | Epoch 1496 Train Time 36.82016181945801s

2025-10-19 20:04:49,241 | INFO | Training epoch 1497, Batch 1000/1000: LR=1.57e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:04:49,315 | INFO | Epoch 1497 Train Time 35.50770592689514s

2025-10-19 20:05:25,331 | INFO | Training epoch 1498, Batch 1000/1000: LR=1.57e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:05:25,408 | INFO | Epoch 1498 Train Time 36.092002391815186s

2025-10-19 20:06:01,537 | INFO | Training epoch 1499, Batch 1000/1000: LR=1.56e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:06:01,615 | INFO | Epoch 1499 Train Time 36.20459747314453s

2025-10-19 20:06:37,848 | INFO | Training epoch 1500, Batch 1000/1000: LR=1.56e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 20:06:37,919 | INFO | Epoch 1500 Train Time 36.303268909454346s

2025-10-19 20:07:13,836 | INFO | Training epoch 1501, Batch 1000/1000: LR=1.55e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:07:13,919 | INFO | Epoch 1501 Train Time 35.99915385246277s

2025-10-19 20:07:49,032 | INFO | Training epoch 1502, Batch 1000/1000: LR=1.54e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 20:07:49,130 | INFO | Epoch 1502 Train Time 35.210225105285645s

2025-10-19 20:08:24,972 | INFO | Training epoch 1503, Batch 1000/1000: LR=1.54e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:08:25,083 | INFO | Epoch 1503 Train Time 35.95076847076416s

2025-10-19 20:09:01,065 | INFO | Training epoch 1504, Batch 1000/1000: LR=1.53e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:09:01,149 | INFO | Epoch 1504 Train Time 36.064738512039185s

2025-10-19 20:09:36,227 | INFO | Training epoch 1505, Batch 1000/1000: LR=1.53e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:09:36,319 | INFO | Epoch 1505 Train Time 35.17014980316162s

2025-10-19 20:10:12,527 | INFO | Training epoch 1506, Batch 1000/1000: LR=1.52e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 20:10:12,624 | INFO | Epoch 1506 Train Time 36.30364274978638s

2025-10-19 20:10:49,016 | INFO | Training epoch 1507, Batch 1000/1000: LR=1.52e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:10:49,087 | INFO | Epoch 1507 Train Time 36.46240305900574s

2025-10-19 20:11:24,432 | INFO | Training epoch 1508, Batch 1000/1000: LR=1.51e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 20:11:24,509 | INFO | Epoch 1508 Train Time 35.42095994949341s

2025-10-19 20:11:59,721 | INFO | Training epoch 1509, Batch 1000/1000: LR=1.51e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:11:59,808 | INFO | Epoch 1509 Train Time 35.29609823226929s

2025-10-19 20:12:35,685 | INFO | Training epoch 1510, Batch 1000/1000: LR=1.50e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:12:35,772 | INFO | Epoch 1510 Train Time 35.96312665939331s

2025-10-19 20:13:11,547 | INFO | Training epoch 1511, Batch 1000/1000: LR=1.50e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:13:11,641 | INFO | Epoch 1511 Train Time 35.867740631103516s

2025-10-19 20:13:47,522 | INFO | Training epoch 1512, Batch 1000/1000: LR=1.49e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:13:47,610 | INFO | Epoch 1512 Train Time 35.968008518218994s

2025-10-19 20:14:23,326 | INFO | Training epoch 1513, Batch 1000/1000: LR=1.48e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:14:23,406 | INFO | Epoch 1513 Train Time 35.794921875s

2025-10-19 20:14:59,545 | INFO | Training epoch 1514, Batch 1000/1000: LR=1.48e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 20:14:59,616 | INFO | Epoch 1514 Train Time 36.20911502838135s

2025-10-19 20:15:35,623 | INFO | Training epoch 1515, Batch 1000/1000: LR=1.47e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 20:15:35,696 | INFO | Epoch 1515 Train Time 36.07854437828064s

2025-10-19 20:16:11,828 | INFO | Training epoch 1516, Batch 1000/1000: LR=1.47e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:16:11,920 | INFO | Epoch 1516 Train Time 36.22363877296448s

2025-10-19 20:16:47,716 | INFO | Training epoch 1517, Batch 1000/1000: LR=1.46e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:16:47,801 | INFO | Epoch 1517 Train Time 35.8790807723999s

2025-10-19 20:17:23,562 | INFO | Training epoch 1518, Batch 1000/1000: LR=1.46e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 20:17:23,651 | INFO | Epoch 1518 Train Time 35.84924030303955s

2025-10-19 20:17:59,762 | INFO | Training epoch 1519, Batch 1000/1000: LR=1.45e-05, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 20:17:59,839 | INFO | Epoch 1519 Train Time 36.187007665634155s

2025-10-19 20:18:35,611 | INFO | Training epoch 1520, Batch 1000/1000: LR=1.45e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:18:35,700 | INFO | Epoch 1520 Train Time 35.86037349700928s

2025-10-19 20:19:11,430 | INFO | Training epoch 1521, Batch 1000/1000: LR=1.44e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:19:11,522 | INFO | Epoch 1521 Train Time 35.820637226104736s

2025-10-19 20:19:47,259 | INFO | Training epoch 1522, Batch 1000/1000: LR=1.44e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:19:47,346 | INFO | Epoch 1522 Train Time 35.82247543334961s

2025-10-19 20:20:23,024 | INFO | Training epoch 1523, Batch 1000/1000: LR=1.43e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:20:23,095 | INFO | Epoch 1523 Train Time 35.74797487258911s

2025-10-19 20:20:58,719 | INFO | Training epoch 1524, Batch 1000/1000: LR=1.43e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:20:58,788 | INFO | Epoch 1524 Train Time 35.69160175323486s

2025-10-19 20:21:34,645 | INFO | Training epoch 1525, Batch 1000/1000: LR=1.42e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:21:34,733 | INFO | Epoch 1525 Train Time 35.94371962547302s

2025-10-19 20:22:10,740 | INFO | Training epoch 1526, Batch 1000/1000: LR=1.42e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:22:10,842 | INFO | Epoch 1526 Train Time 36.107678174972534s

2025-10-19 20:22:46,545 | INFO | Training epoch 1527, Batch 1000/1000: LR=1.41e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:22:46,647 | INFO | Epoch 1527 Train Time 35.80351662635803s

2025-10-19 20:23:22,414 | INFO | Training epoch 1528, Batch 1000/1000: LR=1.40e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 20:23:22,510 | INFO | Epoch 1528 Train Time 35.8620183467865s

2025-10-19 20:23:58,331 | INFO | Training epoch 1529, Batch 1000/1000: LR=1.40e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 20:23:58,402 | INFO | Epoch 1529 Train Time 35.8910493850708s

2025-10-19 20:24:34,430 | INFO | Training epoch 1530, Batch 1000/1000: LR=1.39e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 20:24:34,505 | INFO | Epoch 1530 Train Time 36.102078676223755s

2025-10-19 20:25:10,535 | INFO | Training epoch 1531, Batch 1000/1000: LR=1.39e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:25:10,628 | INFO | Epoch 1531 Train Time 36.12209177017212s

2025-10-19 20:25:46,722 | INFO | Training epoch 1532, Batch 1000/1000: LR=1.38e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 20:25:46,815 | INFO | Epoch 1532 Train Time 36.18521046638489s

2025-10-19 20:26:22,484 | INFO | Training epoch 1533, Batch 1000/1000: LR=1.38e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:26:22,569 | INFO | Epoch 1533 Train Time 35.75297141075134s

2025-10-19 20:26:58,123 | INFO | Training epoch 1534, Batch 1000/1000: LR=1.37e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:26:58,214 | INFO | Epoch 1534 Train Time 35.64441776275635s

2025-10-19 20:27:33,820 | INFO | Training epoch 1535, Batch 1000/1000: LR=1.37e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:27:33,911 | INFO | Epoch 1535 Train Time 35.69558906555176s

2025-10-19 20:28:09,332 | INFO | Training epoch 1536, Batch 1000/1000: LR=1.36e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:28:09,408 | INFO | Epoch 1536 Train Time 35.49523663520813s

2025-10-19 20:28:45,419 | INFO | Training epoch 1537, Batch 1000/1000: LR=1.36e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:28:45,514 | INFO | Epoch 1537 Train Time 36.105149030685425s

2025-10-19 20:29:21,614 | INFO | Training epoch 1538, Batch 1000/1000: LR=1.35e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:29:21,704 | INFO | Epoch 1538 Train Time 36.18865132331848s

2025-10-19 20:29:57,498 | INFO | Training epoch 1539, Batch 1000/1000: LR=1.35e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:29:57,571 | INFO | Epoch 1539 Train Time 35.86530137062073s

2025-10-19 20:30:33,333 | INFO | Training epoch 1540, Batch 1000/1000: LR=1.34e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:30:33,420 | INFO | Epoch 1540 Train Time 35.847607374191284s

2025-10-19 20:31:08,837 | INFO | Training epoch 1541, Batch 1000/1000: LR=1.34e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:31:08,909 | INFO | Epoch 1541 Train Time 35.487852573394775s

2025-10-19 20:31:44,959 | INFO | Training epoch 1542, Batch 1000/1000: LR=1.33e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:31:45,056 | INFO | Epoch 1542 Train Time 36.14614272117615s

2025-10-19 20:32:21,337 | INFO | Training epoch 1543, Batch 1000/1000: LR=1.33e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 20:32:21,418 | INFO | Epoch 1543 Train Time 36.359458923339844s

2025-10-19 20:32:57,033 | INFO | Training epoch 1544, Batch 1000/1000: LR=1.32e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:32:57,107 | INFO | Epoch 1544 Train Time 35.68709921836853s

2025-10-19 20:33:33,051 | INFO | Training epoch 1545, Batch 1000/1000: LR=1.32e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:33:33,142 | INFO | Epoch 1545 Train Time 36.03271484375s

2025-10-19 20:34:09,226 | INFO | Training epoch 1546, Batch 1000/1000: LR=1.31e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:34:09,306 | INFO | Epoch 1546 Train Time 36.16323637962341s

2025-10-19 20:34:45,256 | INFO | Training epoch 1547, Batch 1000/1000: LR=1.31e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:34:45,364 | INFO | Epoch 1547 Train Time 36.05659747123718s

2025-10-19 20:35:21,133 | INFO | Training epoch 1548, Batch 1000/1000: LR=1.30e-05, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:35:21,224 | INFO | Epoch 1548 Train Time 35.858293294906616s

2025-10-19 20:35:57,000 | INFO | Training epoch 1549, Batch 1000/1000: LR=1.30e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:35:57,088 | INFO | Epoch 1549 Train Time 35.86330461502075s

2025-10-19 20:36:32,732 | INFO | Training epoch 1550, Batch 1000/1000: LR=1.29e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 20:36:32,828 | INFO | Epoch 1550 Train Time 35.73864245414734s

2025-10-19 20:37:08,947 | INFO | Training epoch 1551, Batch 1000/1000: LR=1.29e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:37:09,050 | INFO | Epoch 1551 Train Time 36.2209415435791s

2025-10-19 20:37:45,657 | INFO | Training epoch 1552, Batch 1000/1000: LR=1.28e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:37:45,729 | INFO | Epoch 1552 Train Time 36.67769002914429s

2025-10-19 20:38:21,651 | INFO | Training epoch 1553, Batch 1000/1000: LR=1.28e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:38:21,749 | INFO | Epoch 1553 Train Time 36.01894450187683s

2025-10-19 20:38:57,026 | INFO | Training epoch 1554, Batch 1000/1000: LR=1.27e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:38:57,124 | INFO | Epoch 1554 Train Time 35.37296152114868s

2025-10-19 20:39:32,470 | INFO | Training epoch 1555, Batch 1000/1000: LR=1.27e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:39:32,549 | INFO | Epoch 1555 Train Time 35.42318773269653s

2025-10-19 20:40:08,184 | INFO | Training epoch 1556, Batch 1000/1000: LR=1.26e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:40:08,269 | INFO | Epoch 1556 Train Time 35.71889853477478s

2025-10-19 20:40:43,426 | INFO | Training epoch 1557, Batch 1000/1000: LR=1.26e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:40:43,505 | INFO | Epoch 1557 Train Time 35.23591232299805s

2025-10-19 20:41:19,848 | INFO | Training epoch 1558, Batch 1000/1000: LR=1.25e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 20:41:19,939 | INFO | Epoch 1558 Train Time 36.43200373649597s

2025-10-19 20:41:56,026 | INFO | Training epoch 1559, Batch 1000/1000: LR=1.25e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:41:56,137 | INFO | Epoch 1559 Train Time 36.19780421257019s

2025-10-19 20:42:32,233 | INFO | Training epoch 1560, Batch 1000/1000: LR=1.24e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 20:42:32,307 | INFO | Epoch 1560 Train Time 36.1681969165802s

2025-10-19 20:43:07,827 | INFO | Training epoch 1561, Batch 1000/1000: LR=1.24e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:43:07,903 | INFO | Epoch 1561 Train Time 35.59477257728577s

2025-10-19 20:43:43,716 | INFO | Training epoch 1562, Batch 1000/1000: LR=1.23e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:43:43,811 | INFO | Epoch 1562 Train Time 35.906885862350464s

2025-10-19 20:44:19,625 | INFO | Training epoch 1563, Batch 1000/1000: LR=1.23e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:44:19,728 | INFO | Epoch 1563 Train Time 35.91555190086365s

2025-10-19 20:44:55,154 | INFO | Training epoch 1564, Batch 1000/1000: LR=1.22e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 20:44:55,246 | INFO | Epoch 1564 Train Time 35.517688512802124s

2025-10-19 20:45:31,183 | INFO | Training epoch 1565, Batch 1000/1000: LR=1.22e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:45:31,253 | INFO | Epoch 1565 Train Time 36.004799365997314s

2025-10-19 20:46:07,430 | INFO | Training epoch 1566, Batch 1000/1000: LR=1.21e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:46:07,505 | INFO | Epoch 1566 Train Time 36.25111675262451s

2025-10-19 20:46:43,563 | INFO | Training epoch 1567, Batch 1000/1000: LR=1.21e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:46:43,637 | INFO | Epoch 1567 Train Time 36.130107164382935s

2025-10-19 20:47:19,530 | INFO | Training epoch 1568, Batch 1000/1000: LR=1.20e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 20:47:19,605 | INFO | Epoch 1568 Train Time 35.96800661087036s

2025-10-19 20:47:55,728 | INFO | Training epoch 1569, Batch 1000/1000: LR=1.20e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 20:47:55,824 | INFO | Epoch 1569 Train Time 36.21706438064575s

2025-10-19 20:48:32,023 | INFO | Training epoch 1570, Batch 1000/1000: LR=1.19e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:48:32,117 | INFO | Epoch 1570 Train Time 36.29211688041687s

2025-10-19 20:49:08,025 | INFO | Training epoch 1571, Batch 1000/1000: LR=1.19e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.53e-01
2025-10-19 20:49:08,099 | INFO | Epoch 1571 Train Time 35.980467557907104s

2025-10-19 20:49:44,173 | INFO | Training epoch 1572, Batch 1000/1000: LR=1.18e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:49:44,265 | INFO | Epoch 1572 Train Time 36.16450333595276s

2025-10-19 20:50:20,226 | INFO | Training epoch 1573, Batch 1000/1000: LR=1.18e-05, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 20:50:20,301 | INFO | Epoch 1573 Train Time 36.03426432609558s

2025-10-19 20:50:55,239 | INFO | Training epoch 1574, Batch 1000/1000: LR=1.17e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:50:55,326 | INFO | Epoch 1574 Train Time 35.0239896774292s

2025-10-19 20:51:31,527 | INFO | Training epoch 1575, Batch 1000/1000: LR=1.17e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:51:31,603 | INFO | Epoch 1575 Train Time 36.274288177490234s

2025-10-19 20:52:08,032 | INFO | Training epoch 1576, Batch 1000/1000: LR=1.16e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:52:08,120 | INFO | Epoch 1576 Train Time 36.515244483947754s

2025-10-19 20:52:43,633 | INFO | Training epoch 1577, Batch 1000/1000: LR=1.16e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:52:43,716 | INFO | Epoch 1577 Train Time 35.59460425376892s

2025-10-19 20:53:19,218 | INFO | Training epoch 1578, Batch 1000/1000: LR=1.15e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 20:53:19,310 | INFO | Epoch 1578 Train Time 35.59147334098816s

2025-10-19 20:53:55,126 | INFO | Training epoch 1579, Batch 1000/1000: LR=1.15e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 20:53:55,208 | INFO | Epoch 1579 Train Time 35.89552164077759s

2025-10-19 20:54:31,183 | INFO | Training epoch 1580, Batch 1000/1000: LR=1.14e-05, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 20:54:31,260 | INFO | Epoch 1580 Train Time 36.0511360168457s

2025-10-19 20:55:07,522 | INFO | Training epoch 1581, Batch 1000/1000: LR=1.14e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 20:55:07,599 | INFO | Epoch 1581 Train Time 36.336140155792236s

2025-10-19 20:55:43,734 | INFO | Training epoch 1582, Batch 1000/1000: LR=1.13e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:55:43,807 | INFO | Epoch 1582 Train Time 36.20724177360535s

2025-10-19 20:56:19,679 | INFO | Training epoch 1583, Batch 1000/1000: LR=1.13e-05, Loss=3.00e-02 BER=1.11e-02 FER=1.47e-01
2025-10-19 20:56:19,750 | INFO | Epoch 1583 Train Time 35.94180870056152s

2025-10-19 20:56:56,024 | INFO | Training epoch 1584, Batch 1000/1000: LR=1.12e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:56:56,118 | INFO | Epoch 1584 Train Time 36.36607217788696s

2025-10-19 20:57:32,128 | INFO | Training epoch 1585, Batch 1000/1000: LR=1.12e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.48e-01
2025-10-19 20:57:32,204 | INFO | Epoch 1585 Train Time 36.084426164627075s

2025-10-19 20:58:08,994 | INFO | Training epoch 1586, Batch 1000/1000: LR=1.12e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 20:58:09,071 | INFO | Epoch 1586 Train Time 36.86628484725952s

2025-10-19 20:58:44,532 | INFO | Training epoch 1587, Batch 1000/1000: LR=1.11e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 20:58:44,614 | INFO | Epoch 1587 Train Time 35.54205250740051s

2025-10-19 20:59:20,074 | INFO | Training epoch 1588, Batch 1000/1000: LR=1.11e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 20:59:20,157 | INFO | Epoch 1588 Train Time 35.541486501693726s

2025-10-19 20:59:57,262 | INFO | Training epoch 1589, Batch 1000/1000: LR=1.10e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 20:59:57,367 | INFO | Epoch 1589 Train Time 37.20883250236511s

2025-10-19 21:00:33,873 | INFO | Training epoch 1590, Batch 1000/1000: LR=1.10e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 21:00:33,946 | INFO | Epoch 1590 Train Time 36.57834601402283s

2025-10-19 21:01:09,316 | INFO | Training epoch 1591, Batch 1000/1000: LR=1.09e-05, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 21:01:09,389 | INFO | Epoch 1591 Train Time 35.44219160079956s

2025-10-19 21:01:44,528 | INFO | Training epoch 1592, Batch 1000/1000: LR=1.09e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:01:44,596 | INFO | Epoch 1592 Train Time 35.20550727844238s

2025-10-19 21:02:20,223 | INFO | Training epoch 1593, Batch 1000/1000: LR=1.08e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.47e-01
2025-10-19 21:02:20,316 | INFO | Epoch 1593 Train Time 35.718759536743164s

2025-10-19 21:02:56,053 | INFO | Training epoch 1594, Batch 1000/1000: LR=1.08e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:02:56,147 | INFO | Epoch 1594 Train Time 35.82936477661133s

2025-10-19 21:03:32,035 | INFO | Training epoch 1595, Batch 1000/1000: LR=1.07e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:03:32,154 | INFO | Epoch 1595 Train Time 36.00595951080322s

2025-10-19 21:04:08,024 | INFO | Training epoch 1596, Batch 1000/1000: LR=1.07e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:04:08,126 | INFO | Epoch 1596 Train Time 35.9698748588562s

2025-10-19 21:04:43,831 | INFO | Training epoch 1597, Batch 1000/1000: LR=1.06e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:04:43,924 | INFO | Epoch 1597 Train Time 35.797115087509155s

2025-10-19 21:05:19,749 | INFO | Training epoch 1598, Batch 1000/1000: LR=1.06e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:05:19,826 | INFO | Epoch 1598 Train Time 35.899715185165405s

2025-10-19 21:05:56,031 | INFO | Training epoch 1599, Batch 1000/1000: LR=1.05e-05, Loss=3.00e-02 BER=1.13e-02 FER=1.47e-01
2025-10-19 21:05:56,128 | INFO | Epoch 1599 Train Time 36.30112814903259s

2025-10-19 21:06:32,132 | INFO | Training epoch 1600, Batch 1000/1000: LR=1.05e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 21:06:32,211 | INFO | Epoch 1600 Train Time 36.08238124847412s

2025-10-19 21:07:08,222 | INFO | Training epoch 1601, Batch 1000/1000: LR=1.05e-05, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:07:08,299 | INFO | Epoch 1601 Train Time 36.08653950691223s

2025-10-19 21:07:44,211 | INFO | Training epoch 1602, Batch 1000/1000: LR=1.04e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 21:07:44,309 | INFO | Epoch 1602 Train Time 36.008914947509766s

2025-10-19 21:08:19,823 | INFO | Training epoch 1603, Batch 1000/1000: LR=1.04e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:08:19,898 | INFO | Epoch 1603 Train Time 35.58867883682251s

2025-10-19 21:08:56,038 | INFO | Training epoch 1604, Batch 1000/1000: LR=1.03e-05, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 21:08:56,125 | INFO | Epoch 1604 Train Time 36.22567558288574s

2025-10-19 21:09:32,556 | INFO | Training epoch 1605, Batch 1000/1000: LR=1.03e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 21:09:32,650 | INFO | Epoch 1605 Train Time 36.52328419685364s

2025-10-19 21:10:08,824 | INFO | Training epoch 1606, Batch 1000/1000: LR=1.02e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:10:08,899 | INFO | Epoch 1606 Train Time 36.247034549713135s

2025-10-19 21:10:44,766 | INFO | Training epoch 1607, Batch 1000/1000: LR=1.02e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:10:44,849 | INFO | Epoch 1607 Train Time 35.94857978820801s

2025-10-19 21:11:20,712 | INFO | Training epoch 1608, Batch 1000/1000: LR=1.01e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:11:20,821 | INFO | Epoch 1608 Train Time 35.97180700302124s

2025-10-19 21:11:56,431 | INFO | Training epoch 1609, Batch 1000/1000: LR=1.01e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:11:56,512 | INFO | Epoch 1609 Train Time 35.68990135192871s

2025-10-19 21:12:31,663 | INFO | Training epoch 1610, Batch 1000/1000: LR=1.00e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 21:12:31,736 | INFO | Epoch 1610 Train Time 35.222853660583496s

2025-10-19 21:13:07,371 | INFO | Training epoch 1611, Batch 1000/1000: LR=1.00e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:13:07,458 | INFO | Epoch 1611 Train Time 35.719173431396484s

2025-10-19 21:13:43,332 | INFO | Training epoch 1612, Batch 1000/1000: LR=9.96e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:13:43,420 | INFO | Epoch 1612 Train Time 35.96101212501526s

2025-10-19 21:14:19,134 | INFO | Training epoch 1613, Batch 1000/1000: LR=9.91e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 21:14:19,232 | INFO | Epoch 1613 Train Time 35.81063222885132s

2025-10-19 21:14:54,725 | INFO | Training epoch 1614, Batch 1000/1000: LR=9.87e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:14:54,802 | INFO | Epoch 1614 Train Time 35.56964588165283s

2025-10-19 21:15:31,272 | INFO | Training epoch 1615, Batch 1000/1000: LR=9.82e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 21:15:31,367 | INFO | Epoch 1615 Train Time 36.562747955322266s

2025-10-19 21:16:07,276 | INFO | Training epoch 1616, Batch 1000/1000: LR=9.78e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 21:16:07,379 | INFO | Epoch 1616 Train Time 36.01170825958252s

2025-10-19 21:16:43,223 | INFO | Training epoch 1617, Batch 1000/1000: LR=9.74e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 21:16:43,316 | INFO | Epoch 1617 Train Time 35.935636043548584s

2025-10-19 21:17:18,761 | INFO | Training epoch 1618, Batch 1000/1000: LR=9.69e-06, Loss=2.98e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:17:18,871 | INFO | Epoch 1618 Train Time 35.55378341674805s

2025-10-19 21:17:54,951 | INFO | Training epoch 1619, Batch 1000/1000: LR=9.65e-06, Loss=2.97e-02 BER=1.11e-02 FER=1.48e-01
2025-10-19 21:17:55,038 | INFO | Epoch 1619 Train Time 36.16520833969116s

2025-10-19 21:18:30,631 | INFO | Training epoch 1620, Batch 1000/1000: LR=9.60e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:18:30,704 | INFO | Epoch 1620 Train Time 35.665807008743286s

2025-10-19 21:19:06,522 | INFO | Training epoch 1621, Batch 1000/1000: LR=9.56e-06, Loss=3.08e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 21:19:06,595 | INFO | Epoch 1621 Train Time 35.889005184173584s

2025-10-19 21:19:42,233 | INFO | Training epoch 1622, Batch 1000/1000: LR=9.52e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:19:42,340 | INFO | Epoch 1622 Train Time 35.74401783943176s

2025-10-19 21:20:17,131 | INFO | Training epoch 1623, Batch 1000/1000: LR=9.47e-06, Loss=3.02e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:20:17,235 | INFO | Epoch 1623 Train Time 34.89298486709595s

2025-10-19 21:20:53,265 | INFO | Training epoch 1624, Batch 1000/1000: LR=9.43e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:20:53,347 | INFO | Epoch 1624 Train Time 36.11164474487305s

2025-10-19 21:21:29,258 | INFO | Training epoch 1625, Batch 1000/1000: LR=9.39e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:21:29,327 | INFO | Epoch 1625 Train Time 35.9782931804657s

2025-10-19 21:22:04,755 | INFO | Training epoch 1626, Batch 1000/1000: LR=9.34e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 21:22:04,827 | INFO | Epoch 1626 Train Time 35.49878287315369s

2025-10-19 21:22:41,360 | INFO | Training epoch 1627, Batch 1000/1000: LR=9.30e-06, Loss=3.08e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 21:22:41,462 | INFO | Epoch 1627 Train Time 36.63274121284485s

2025-10-19 21:23:17,729 | INFO | Training epoch 1628, Batch 1000/1000: LR=9.26e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:23:17,802 | INFO | Epoch 1628 Train Time 36.339274883270264s

2025-10-19 21:23:53,511 | INFO | Training epoch 1629, Batch 1000/1000: LR=9.21e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:23:53,586 | INFO | Epoch 1629 Train Time 35.78306341171265s

2025-10-19 21:24:30,859 | INFO | Training epoch 1630, Batch 1000/1000: LR=9.17e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:24:30,937 | INFO | Epoch 1630 Train Time 37.349937915802s

2025-10-19 21:25:09,564 | INFO | Training epoch 1631, Batch 1000/1000: LR=9.13e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:25:09,647 | INFO | Epoch 1631 Train Time 38.70895218849182s

2025-10-19 21:25:46,436 | INFO | Training epoch 1632, Batch 1000/1000: LR=9.08e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 21:25:46,512 | INFO | Epoch 1632 Train Time 36.86299467086792s

2025-10-19 21:26:23,548 | INFO | Training epoch 1633, Batch 1000/1000: LR=9.04e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:26:23,641 | INFO | Epoch 1633 Train Time 37.12774085998535s

2025-10-19 21:26:59,824 | INFO | Training epoch 1634, Batch 1000/1000: LR=9.00e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:26:59,900 | INFO | Epoch 1634 Train Time 36.257853984832764s

2025-10-19 21:27:35,525 | INFO | Training epoch 1635, Batch 1000/1000: LR=8.96e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 21:27:35,598 | INFO | Epoch 1635 Train Time 35.69665861129761s

2025-10-19 21:28:12,033 | INFO | Training epoch 1636, Batch 1000/1000: LR=8.92e-06, Loss=2.98e-02 BER=1.11e-02 FER=1.47e-01
2025-10-19 21:28:12,118 | INFO | Epoch 1636 Train Time 36.51922273635864s

2025-10-19 21:28:47,622 | INFO | Training epoch 1637, Batch 1000/1000: LR=8.87e-06, Loss=3.07e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:28:47,690 | INFO | Epoch 1637 Train Time 35.57100009918213s

2025-10-19 21:29:23,926 | INFO | Training epoch 1638, Batch 1000/1000: LR=8.83e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:29:24,018 | INFO | Epoch 1638 Train Time 36.326600074768066s

2025-10-19 21:29:59,531 | INFO | Training epoch 1639, Batch 1000/1000: LR=8.79e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:29:59,629 | INFO | Epoch 1639 Train Time 35.609413623809814s

2025-10-19 21:30:35,423 | INFO | Training epoch 1640, Batch 1000/1000: LR=8.75e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:30:35,497 | INFO | Epoch 1640 Train Time 35.86698293685913s

2025-10-19 21:31:11,618 | INFO | Training epoch 1641, Batch 1000/1000: LR=8.71e-06, Loss=3.02e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:31:11,700 | INFO | Epoch 1641 Train Time 36.20112323760986s

2025-10-19 21:31:47,422 | INFO | Training epoch 1642, Batch 1000/1000: LR=8.66e-06, Loss=3.08e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 21:31:47,499 | INFO | Epoch 1642 Train Time 35.797677993774414s

2025-10-19 21:32:23,315 | INFO | Training epoch 1643, Batch 1000/1000: LR=8.62e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 21:32:23,399 | INFO | Epoch 1643 Train Time 35.89962410926819s

2025-10-19 21:32:59,621 | INFO | Training epoch 1644, Batch 1000/1000: LR=8.58e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:32:59,704 | INFO | Epoch 1644 Train Time 36.30368423461914s

2025-10-19 21:33:35,218 | INFO | Training epoch 1645, Batch 1000/1000: LR=8.54e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 21:33:35,324 | INFO | Epoch 1645 Train Time 35.618720293045044s

2025-10-19 21:34:10,991 | INFO | Training epoch 1646, Batch 1000/1000: LR=8.50e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:34:11,072 | INFO | Epoch 1646 Train Time 35.74627876281738s

2025-10-19 21:34:47,123 | INFO | Training epoch 1647, Batch 1000/1000: LR=8.46e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 21:34:47,216 | INFO | Epoch 1647 Train Time 36.14300560951233s

2025-10-19 21:35:23,227 | INFO | Training epoch 1648, Batch 1000/1000: LR=8.42e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:35:23,323 | INFO | Epoch 1648 Train Time 36.106531858444214s

2025-10-19 21:35:59,147 | INFO | Training epoch 1649, Batch 1000/1000: LR=8.38e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:35:59,224 | INFO | Epoch 1649 Train Time 35.899596214294434s

2025-10-19 21:36:35,234 | INFO | Training epoch 1650, Batch 1000/1000: LR=8.33e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:36:35,314 | INFO | Epoch 1650 Train Time 36.08947229385376s

2025-10-19 21:37:11,412 | INFO | Training epoch 1651, Batch 1000/1000: LR=8.29e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 21:37:11,495 | INFO | Epoch 1651 Train Time 36.17951583862305s

2025-10-19 21:37:47,419 | INFO | Training epoch 1652, Batch 1000/1000: LR=8.25e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:37:47,505 | INFO | Epoch 1652 Train Time 36.00933480262756s

2025-10-19 21:38:23,225 | INFO | Training epoch 1653, Batch 1000/1000: LR=8.21e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 21:38:23,316 | INFO | Epoch 1653 Train Time 35.810261964797974s

2025-10-19 21:38:59,060 | INFO | Training epoch 1654, Batch 1000/1000: LR=8.17e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:38:59,128 | INFO | Epoch 1654 Train Time 35.810386180877686s

2025-10-19 21:39:35,221 | INFO | Training epoch 1655, Batch 1000/1000: LR=8.13e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 21:39:35,295 | INFO | Epoch 1655 Train Time 36.166009187698364s

2025-10-19 21:40:10,858 | INFO | Training epoch 1656, Batch 1000/1000: LR=8.09e-06, Loss=3.05e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 21:40:10,934 | INFO | Epoch 1656 Train Time 35.638211250305176s

2025-10-19 21:40:46,769 | INFO | Training epoch 1657, Batch 1000/1000: LR=8.05e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:40:46,860 | INFO | Epoch 1657 Train Time 35.925230264663696s

2025-10-19 21:41:23,251 | INFO | Training epoch 1658, Batch 1000/1000: LR=8.01e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:41:23,336 | INFO | Epoch 1658 Train Time 36.47411012649536s

2025-10-19 21:41:59,123 | INFO | Training epoch 1659, Batch 1000/1000: LR=7.97e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:41:59,197 | INFO | Epoch 1659 Train Time 35.85950541496277s

2025-10-19 21:42:34,865 | INFO | Training epoch 1660, Batch 1000/1000: LR=7.93e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:42:34,962 | INFO | Epoch 1660 Train Time 35.76429533958435s

2025-10-19 21:43:11,932 | INFO | Training epoch 1661, Batch 1000/1000: LR=7.89e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:43:12,033 | INFO | Epoch 1661 Train Time 37.06939458847046s

2025-10-19 21:43:48,226 | INFO | Training epoch 1662, Batch 1000/1000: LR=7.85e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:43:48,321 | INFO | Epoch 1662 Train Time 36.2861053943634s

2025-10-19 21:44:24,634 | INFO | Training epoch 1663, Batch 1000/1000: LR=7.81e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:44:24,715 | INFO | Epoch 1663 Train Time 36.39239549636841s

2025-10-19 21:45:00,792 | INFO | Training epoch 1664, Batch 1000/1000: LR=7.78e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:45:00,862 | INFO | Epoch 1664 Train Time 36.14596462249756s

2025-10-19 21:45:37,830 | INFO | Training epoch 1665, Batch 1000/1000: LR=7.74e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:45:37,902 | INFO | Epoch 1665 Train Time 37.038209676742554s

2025-10-19 21:46:13,446 | INFO | Training epoch 1666, Batch 1000/1000: LR=7.70e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:46:13,540 | INFO | Epoch 1666 Train Time 35.63781666755676s

2025-10-19 21:46:49,313 | INFO | Training epoch 1667, Batch 1000/1000: LR=7.66e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 21:46:49,387 | INFO | Epoch 1667 Train Time 35.84369349479675s

2025-10-19 21:47:25,632 | INFO | Training epoch 1668, Batch 1000/1000: LR=7.62e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 21:47:25,718 | INFO | Epoch 1668 Train Time 36.330081939697266s

2025-10-19 21:48:01,621 | INFO | Training epoch 1669, Batch 1000/1000: LR=7.58e-06, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:48:01,695 | INFO | Epoch 1669 Train Time 35.975677490234375s

2025-10-19 21:48:37,020 | INFO | Training epoch 1670, Batch 1000/1000: LR=7.54e-06, Loss=3.09e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 21:48:37,116 | INFO | Epoch 1670 Train Time 35.41961169242859s

2025-10-19 21:49:13,048 | INFO | Training epoch 1671, Batch 1000/1000: LR=7.50e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:49:13,132 | INFO | Epoch 1671 Train Time 36.013076066970825s

2025-10-19 21:49:49,150 | INFO | Training epoch 1672, Batch 1000/1000: LR=7.46e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.48e-01
2025-10-19 21:49:49,245 | INFO | Epoch 1672 Train Time 36.11103940010071s

2025-10-19 21:50:25,651 | INFO | Training epoch 1673, Batch 1000/1000: LR=7.43e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:50:25,739 | INFO | Epoch 1673 Train Time 36.4897677898407s

2025-10-19 21:51:01,744 | INFO | Training epoch 1674, Batch 1000/1000: LR=7.39e-06, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:51:01,814 | INFO | Epoch 1674 Train Time 36.07360625267029s

2025-10-19 21:51:38,523 | INFO | Training epoch 1675, Batch 1000/1000: LR=7.35e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 21:51:38,619 | INFO | Epoch 1675 Train Time 36.80400466918945s

2025-10-19 21:52:13,222 | INFO | Training epoch 1676, Batch 1000/1000: LR=7.31e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:52:13,297 | INFO | Epoch 1676 Train Time 34.67797374725342s

2025-10-19 21:52:48,944 | INFO | Training epoch 1677, Batch 1000/1000: LR=7.27e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 21:52:49,018 | INFO | Epoch 1677 Train Time 35.719016551971436s

2025-10-19 21:53:25,027 | INFO | Training epoch 1678, Batch 1000/1000: LR=7.24e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:53:25,106 | INFO | Epoch 1678 Train Time 36.08648371696472s

2025-10-19 21:54:00,941 | INFO | Training epoch 1679, Batch 1000/1000: LR=7.20e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:54:01,031 | INFO | Epoch 1679 Train Time 35.92240309715271s

2025-10-19 21:54:36,530 | INFO | Training epoch 1680, Batch 1000/1000: LR=7.16e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 21:54:36,595 | INFO | Epoch 1680 Train Time 35.562644243240356s

2025-10-19 21:55:12,464 | INFO | Training epoch 1681, Batch 1000/1000: LR=7.12e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:55:12,572 | INFO | Epoch 1681 Train Time 35.976882219314575s

2025-10-19 21:55:47,630 | INFO | Training epoch 1682, Batch 1000/1000: LR=7.09e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 21:55:47,707 | INFO | Epoch 1682 Train Time 35.13395023345947s

2025-10-19 21:56:23,727 | INFO | Training epoch 1683, Batch 1000/1000: LR=7.05e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:56:23,821 | INFO | Epoch 1683 Train Time 36.11262392997742s

2025-10-19 21:56:59,258 | INFO | Training epoch 1684, Batch 1000/1000: LR=7.01e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:56:59,333 | INFO | Epoch 1684 Train Time 35.51011252403259s

2025-10-19 21:57:35,428 | INFO | Training epoch 1685, Batch 1000/1000: LR=6.97e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 21:57:35,504 | INFO | Epoch 1685 Train Time 36.169875144958496s

2025-10-19 21:58:11,034 | INFO | Training epoch 1686, Batch 1000/1000: LR=6.94e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:58:11,128 | INFO | Epoch 1686 Train Time 35.62337803840637s

2025-10-19 21:58:47,036 | INFO | Training epoch 1687, Batch 1000/1000: LR=6.90e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 21:58:47,136 | INFO | Epoch 1687 Train Time 36.00637125968933s

2025-10-19 21:59:21,676 | INFO | Training epoch 1688, Batch 1000/1000: LR=6.86e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 21:59:21,757 | INFO | Epoch 1688 Train Time 34.619848012924194s

2025-10-19 21:59:57,225 | INFO | Training epoch 1689, Batch 1000/1000: LR=6.83e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 21:59:57,298 | INFO | Epoch 1689 Train Time 35.539762020111084s

2025-10-19 22:00:34,334 | INFO | Training epoch 1690, Batch 1000/1000: LR=6.79e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:00:34,405 | INFO | Epoch 1690 Train Time 37.105690240859985s

2025-10-19 22:01:10,544 | INFO | Training epoch 1691, Batch 1000/1000: LR=6.75e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:01:10,620 | INFO | Epoch 1691 Train Time 36.213677167892456s

2025-10-19 22:01:46,225 | INFO | Training epoch 1692, Batch 1000/1000: LR=6.72e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:01:46,306 | INFO | Epoch 1692 Train Time 35.68527960777283s

2025-10-19 22:02:21,957 | INFO | Training epoch 1693, Batch 1000/1000: LR=6.68e-06, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 22:02:22,059 | INFO | Epoch 1693 Train Time 35.751832008361816s

2025-10-19 22:02:57,771 | INFO | Training epoch 1694, Batch 1000/1000: LR=6.64e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:02:57,866 | INFO | Epoch 1694 Train Time 35.80580806732178s

2025-10-19 22:03:36,973 | INFO | Training epoch 1695, Batch 1000/1000: LR=6.61e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:03:37,087 | INFO | Epoch 1695 Train Time 39.219725608825684s

2025-10-19 22:04:14,746 | INFO | Training epoch 1696, Batch 1000/1000: LR=6.57e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:04:14,830 | INFO | Epoch 1696 Train Time 37.7401237487793s

2025-10-19 22:04:52,879 | INFO | Training epoch 1697, Batch 1000/1000: LR=6.54e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:04:52,979 | INFO | Epoch 1697 Train Time 38.14794993400574s

2025-10-19 22:05:30,229 | INFO | Training epoch 1698, Batch 1000/1000: LR=6.50e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:05:30,319 | INFO | Epoch 1698 Train Time 37.338916540145874s

2025-10-19 22:06:07,546 | INFO | Training epoch 1699, Batch 1000/1000: LR=6.47e-06, Loss=3.07e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 22:06:07,642 | INFO | Epoch 1699 Train Time 37.32286047935486s

2025-10-19 22:06:44,861 | INFO | Training epoch 1700, Batch 1000/1000: LR=6.43e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:06:44,956 | INFO | Epoch 1700 Train Time 37.31280493736267s

2025-10-19 22:07:21,544 | INFO | Training epoch 1701, Batch 1000/1000: LR=6.40e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:07:21,637 | INFO | Epoch 1701 Train Time 36.68001866340637s

2025-10-19 22:07:58,040 | INFO | Training epoch 1702, Batch 1000/1000: LR=6.36e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:07:58,117 | INFO | Epoch 1702 Train Time 36.47837543487549s

2025-10-19 22:08:35,565 | INFO | Training epoch 1703, Batch 1000/1000: LR=6.32e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:08:35,660 | INFO | Epoch 1703 Train Time 37.541648149490356s

2025-10-19 22:09:12,674 | INFO | Training epoch 1704, Batch 1000/1000: LR=6.29e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 22:09:12,783 | INFO | Epoch 1704 Train Time 37.121403217315674s

2025-10-19 22:09:49,845 | INFO | Training epoch 1705, Batch 1000/1000: LR=6.25e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:09:49,936 | INFO | Epoch 1705 Train Time 37.15151286125183s

2025-10-19 22:10:26,340 | INFO | Training epoch 1706, Batch 1000/1000: LR=6.22e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:10:26,454 | INFO | Epoch 1706 Train Time 36.516929149627686s

2025-10-19 22:11:03,835 | INFO | Training epoch 1707, Batch 1000/1000: LR=6.19e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:11:03,916 | INFO | Epoch 1707 Train Time 37.45998787879944s

2025-10-19 22:11:41,336 | INFO | Training epoch 1708, Batch 1000/1000: LR=6.15e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 22:11:41,436 | INFO | Epoch 1708 Train Time 37.51831865310669s

2025-10-19 22:12:18,145 | INFO | Training epoch 1709, Batch 1000/1000: LR=6.12e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:12:18,239 | INFO | Epoch 1709 Train Time 36.802414894104004s

2025-10-19 22:12:55,485 | INFO | Training epoch 1710, Batch 1000/1000: LR=6.08e-06, Loss=2.98e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 22:12:55,591 | INFO | Epoch 1710 Train Time 37.351081132888794s

2025-10-19 22:13:33,470 | INFO | Training epoch 1711, Batch 1000/1000: LR=6.05e-06, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:13:33,567 | INFO | Epoch 1711 Train Time 37.97514891624451s

2025-10-19 22:14:10,739 | INFO | Training epoch 1712, Batch 1000/1000: LR=6.01e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:14:10,822 | INFO | Epoch 1712 Train Time 37.25378155708313s

2025-10-19 22:14:47,640 | INFO | Training epoch 1713, Batch 1000/1000: LR=5.98e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.47e-01
2025-10-19 22:14:47,731 | INFO | Epoch 1713 Train Time 36.908308267593384s

2025-10-19 22:15:25,236 | INFO | Training epoch 1714, Batch 1000/1000: LR=5.95e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:15:25,326 | INFO | Epoch 1714 Train Time 37.59347343444824s

2025-10-19 22:16:02,645 | INFO | Training epoch 1715, Batch 1000/1000: LR=5.91e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 22:16:02,736 | INFO | Epoch 1715 Train Time 37.40914583206177s

2025-10-19 22:16:39,677 | INFO | Training epoch 1716, Batch 1000/1000: LR=5.88e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 22:16:39,754 | INFO | Epoch 1716 Train Time 37.01660394668579s

2025-10-19 22:17:17,270 | INFO | Training epoch 1717, Batch 1000/1000: LR=5.84e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:17:17,363 | INFO | Epoch 1717 Train Time 37.60688042640686s

2025-10-19 22:17:54,581 | INFO | Training epoch 1718, Batch 1000/1000: LR=5.81e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:17:54,693 | INFO | Epoch 1718 Train Time 37.3288197517395s

2025-10-19 22:18:32,355 | INFO | Training epoch 1719, Batch 1000/1000: LR=5.78e-06, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 22:18:32,445 | INFO | Epoch 1719 Train Time 37.75063395500183s

2025-10-19 22:19:10,241 | INFO | Training epoch 1720, Batch 1000/1000: LR=5.74e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:19:10,337 | INFO | Epoch 1720 Train Time 37.89133620262146s

2025-10-19 22:19:47,846 | INFO | Training epoch 1721, Batch 1000/1000: LR=5.71e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 22:19:47,938 | INFO | Epoch 1721 Train Time 37.59946084022522s

2025-10-19 22:20:24,342 | INFO | Training epoch 1722, Batch 1000/1000: LR=5.68e-06, Loss=2.98e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:20:24,428 | INFO | Epoch 1722 Train Time 36.4884467124939s

2025-10-19 22:21:01,868 | INFO | Training epoch 1723, Batch 1000/1000: LR=5.65e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:21:01,956 | INFO | Epoch 1723 Train Time 37.526512145996094s

2025-10-19 22:21:39,066 | INFO | Training epoch 1724, Batch 1000/1000: LR=5.61e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:21:39,155 | INFO | Epoch 1724 Train Time 37.19817781448364s

2025-10-19 22:22:16,356 | INFO | Training epoch 1725, Batch 1000/1000: LR=5.58e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 22:22:16,449 | INFO | Epoch 1725 Train Time 37.292683124542236s

2025-10-19 22:22:53,744 | INFO | Training epoch 1726, Batch 1000/1000: LR=5.55e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:22:53,841 | INFO | Epoch 1726 Train Time 37.39054322242737s

2025-10-19 22:23:31,052 | INFO | Training epoch 1727, Batch 1000/1000: LR=5.51e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 22:23:31,134 | INFO | Epoch 1727 Train Time 37.29114079475403s

2025-10-19 22:24:08,940 | INFO | Training epoch 1728, Batch 1000/1000: LR=5.48e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.48e-01
2025-10-19 22:24:09,034 | INFO | Epoch 1728 Train Time 37.89814019203186s

2025-10-19 22:24:47,067 | INFO | Training epoch 1729, Batch 1000/1000: LR=5.45e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:24:47,175 | INFO | Epoch 1729 Train Time 38.13996171951294s

2025-10-19 22:25:24,548 | INFO | Training epoch 1730, Batch 1000/1000: LR=5.42e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 22:25:24,640 | INFO | Epoch 1730 Train Time 37.46459913253784s

2025-10-19 22:26:01,843 | INFO | Training epoch 1731, Batch 1000/1000: LR=5.39e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:26:01,930 | INFO | Epoch 1731 Train Time 37.28784370422363s

2025-10-19 22:26:39,475 | INFO | Training epoch 1732, Batch 1000/1000: LR=5.35e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:26:39,567 | INFO | Epoch 1732 Train Time 37.63562273979187s

2025-10-19 22:27:16,059 | INFO | Training epoch 1733, Batch 1000/1000: LR=5.32e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:27:16,169 | INFO | Epoch 1733 Train Time 36.601284980773926s

2025-10-19 22:27:53,261 | INFO | Training epoch 1734, Batch 1000/1000: LR=5.29e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:27:53,368 | INFO | Epoch 1734 Train Time 37.197386741638184s

2025-10-19 22:28:30,739 | INFO | Training epoch 1735, Batch 1000/1000: LR=5.26e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 22:28:30,823 | INFO | Epoch 1735 Train Time 37.45403838157654s

2025-10-19 22:29:06,539 | INFO | Training epoch 1736, Batch 1000/1000: LR=5.23e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:29:06,639 | INFO | Epoch 1736 Train Time 35.813363790512085s

2025-10-19 22:29:43,940 | INFO | Training epoch 1737, Batch 1000/1000: LR=5.20e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 22:29:44,035 | INFO | Epoch 1737 Train Time 37.39459586143494s

2025-10-19 22:30:21,444 | INFO | Training epoch 1738, Batch 1000/1000: LR=5.16e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 22:30:21,524 | INFO | Epoch 1738 Train Time 37.48835778236389s

2025-10-19 22:30:58,445 | INFO | Training epoch 1739, Batch 1000/1000: LR=5.13e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 22:30:58,549 | INFO | Epoch 1739 Train Time 37.022948265075684s

2025-10-19 22:31:34,942 | INFO | Training epoch 1740, Batch 1000/1000: LR=5.10e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:31:35,039 | INFO | Epoch 1740 Train Time 36.48883652687073s

2025-10-19 22:32:12,174 | INFO | Training epoch 1741, Batch 1000/1000: LR=5.07e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:32:12,271 | INFO | Epoch 1741 Train Time 37.23120331764221s

2025-10-19 22:32:49,732 | INFO | Training epoch 1742, Batch 1000/1000: LR=5.04e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:32:49,816 | INFO | Epoch 1742 Train Time 37.5430703163147s

2025-10-19 22:33:27,338 | INFO | Training epoch 1743, Batch 1000/1000: LR=5.01e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:33:27,427 | INFO | Epoch 1743 Train Time 37.6092894077301s

2025-10-19 22:34:04,751 | INFO | Training epoch 1744, Batch 1000/1000: LR=4.98e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:34:04,833 | INFO | Epoch 1744 Train Time 37.404698848724365s

2025-10-19 22:34:42,245 | INFO | Training epoch 1745, Batch 1000/1000: LR=4.95e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:34:42,344 | INFO | Epoch 1745 Train Time 37.50998544692993s

2025-10-19 22:35:19,839 | INFO | Training epoch 1746, Batch 1000/1000: LR=4.92e-06, Loss=3.10e-02 BER=1.16e-02 FER=1.51e-01
2025-10-19 22:35:19,942 | INFO | Epoch 1746 Train Time 37.596723794937134s

2025-10-19 22:35:56,796 | INFO | Training epoch 1747, Batch 1000/1000: LR=4.89e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:35:56,886 | INFO | Epoch 1747 Train Time 36.94345688819885s

2025-10-19 22:36:33,929 | INFO | Training epoch 1748, Batch 1000/1000: LR=4.86e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 22:36:34,015 | INFO | Epoch 1748 Train Time 37.12716817855835s

2025-10-19 22:37:10,772 | INFO | Training epoch 1749, Batch 1000/1000: LR=4.83e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:37:10,859 | INFO | Epoch 1749 Train Time 36.843313455581665s

2025-10-19 22:37:47,947 | INFO | Training epoch 1750, Batch 1000/1000: LR=4.80e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 22:37:48,029 | INFO | Epoch 1750 Train Time 37.16700482368469s

2025-10-19 22:38:25,154 | INFO | Training epoch 1751, Batch 1000/1000: LR=4.77e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:38:25,256 | INFO | Epoch 1751 Train Time 37.22549605369568s

2025-10-19 22:39:02,077 | INFO | Training epoch 1752, Batch 1000/1000: LR=4.74e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:39:02,183 | INFO | Epoch 1752 Train Time 36.925445795059204s

2025-10-19 22:39:39,642 | INFO | Training epoch 1753, Batch 1000/1000: LR=4.71e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 22:39:39,739 | INFO | Epoch 1753 Train Time 37.5555477142334s

2025-10-19 22:40:17,426 | INFO | Training epoch 1754, Batch 1000/1000: LR=4.68e-06, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 22:40:17,516 | INFO | Epoch 1754 Train Time 37.77604532241821s

2025-10-19 22:40:54,973 | INFO | Training epoch 1755, Batch 1000/1000: LR=4.65e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 22:40:55,082 | INFO | Epoch 1755 Train Time 37.56439971923828s

2025-10-19 22:41:32,369 | INFO | Training epoch 1756, Batch 1000/1000: LR=4.62e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:41:32,456 | INFO | Epoch 1756 Train Time 37.37218189239502s

2025-10-19 22:42:09,543 | INFO | Training epoch 1757, Batch 1000/1000: LR=4.59e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:42:09,640 | INFO | Epoch 1757 Train Time 37.18290638923645s

2025-10-19 22:42:47,637 | INFO | Training epoch 1758, Batch 1000/1000: LR=4.56e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 22:42:47,718 | INFO | Epoch 1758 Train Time 38.07709765434265s

2025-10-19 22:43:24,973 | INFO | Training epoch 1759, Batch 1000/1000: LR=4.53e-06, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 22:43:25,058 | INFO | Epoch 1759 Train Time 37.33802890777588s

2025-10-19 22:44:02,346 | INFO | Training epoch 1760, Batch 1000/1000: LR=4.50e-06, Loss=2.99e-02 BER=1.11e-02 FER=1.48e-01
2025-10-19 22:44:02,438 | INFO | Epoch 1760 Train Time 37.37983155250549s

2025-10-19 22:44:39,868 | INFO | Training epoch 1761, Batch 1000/1000: LR=4.48e-06, Loss=3.11e-02 BER=1.16e-02 FER=1.52e-01
2025-10-19 22:44:39,964 | INFO | Epoch 1761 Train Time 37.52444338798523s

2025-10-19 22:45:17,239 | INFO | Training epoch 1762, Batch 1000/1000: LR=4.45e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:45:17,329 | INFO | Epoch 1762 Train Time 37.36353421211243s

2025-10-19 22:45:54,741 | INFO | Training epoch 1763, Batch 1000/1000: LR=4.42e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 22:45:54,836 | INFO | Epoch 1763 Train Time 37.50530695915222s

2025-10-19 22:46:32,343 | INFO | Training epoch 1764, Batch 1000/1000: LR=4.39e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:46:32,425 | INFO | Epoch 1764 Train Time 37.58811569213867s

2025-10-19 22:47:09,797 | INFO | Training epoch 1765, Batch 1000/1000: LR=4.36e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:47:09,892 | INFO | Epoch 1765 Train Time 37.46458911895752s

2025-10-19 22:47:46,952 | INFO | Training epoch 1766, Batch 1000/1000: LR=4.33e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:47:47,047 | INFO | Epoch 1766 Train Time 37.15448784828186s

2025-10-19 22:48:24,240 | INFO | Training epoch 1767, Batch 1000/1000: LR=4.31e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 22:48:24,330 | INFO | Epoch 1767 Train Time 37.2822208404541s

2025-10-19 22:49:01,175 | INFO | Training epoch 1768, Batch 1000/1000: LR=4.28e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:49:01,267 | INFO | Epoch 1768 Train Time 36.932952642440796s

2025-10-19 22:49:38,069 | INFO | Training epoch 1769, Batch 1000/1000: LR=4.25e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:49:38,165 | INFO | Epoch 1769 Train Time 36.89689564704895s

2025-10-19 22:50:15,486 | INFO | Training epoch 1770, Batch 1000/1000: LR=4.22e-06, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:50:15,572 | INFO | Epoch 1770 Train Time 37.40573263168335s

2025-10-19 22:50:53,133 | INFO | Training epoch 1771, Batch 1000/1000: LR=4.20e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:50:53,225 | INFO | Epoch 1771 Train Time 37.652060985565186s

2025-10-19 22:51:30,664 | INFO | Training epoch 1772, Batch 1000/1000: LR=4.17e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:51:30,762 | INFO | Epoch 1772 Train Time 37.53543019294739s

2025-10-19 22:52:07,971 | INFO | Training epoch 1773, Batch 1000/1000: LR=4.14e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 22:52:08,057 | INFO | Epoch 1773 Train Time 37.293577909469604s

2025-10-19 22:52:45,340 | INFO | Training epoch 1774, Batch 1000/1000: LR=4.11e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:52:45,428 | INFO | Epoch 1774 Train Time 37.368385553359985s

2025-10-19 22:53:22,645 | INFO | Training epoch 1775, Batch 1000/1000: LR=4.09e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 22:53:22,743 | INFO | Epoch 1775 Train Time 37.31410765647888s

2025-10-19 22:53:59,938 | INFO | Training epoch 1776, Batch 1000/1000: LR=4.06e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:54:00,040 | INFO | Epoch 1776 Train Time 37.29530334472656s

2025-10-19 22:54:36,798 | INFO | Training epoch 1777, Batch 1000/1000: LR=4.03e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:54:36,893 | INFO | Epoch 1777 Train Time 36.85207176208496s

2025-10-19 22:55:14,364 | INFO | Training epoch 1778, Batch 1000/1000: LR=4.01e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 22:55:14,450 | INFO | Epoch 1778 Train Time 37.55622696876526s

2025-10-19 22:55:51,543 | INFO | Training epoch 1779, Batch 1000/1000: LR=3.98e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 22:55:51,633 | INFO | Epoch 1779 Train Time 37.18038010597229s

2025-10-19 22:56:28,899 | INFO | Training epoch 1780, Batch 1000/1000: LR=3.95e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 22:56:28,987 | INFO | Epoch 1780 Train Time 37.3520610332489s

2025-10-19 22:57:06,651 | INFO | Training epoch 1781, Batch 1000/1000: LR=3.93e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.47e-01
2025-10-19 22:57:06,730 | INFO | Epoch 1781 Train Time 37.74246048927307s

2025-10-19 22:57:43,630 | INFO | Training epoch 1782, Batch 1000/1000: LR=3.90e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 22:57:43,728 | INFO | Epoch 1782 Train Time 36.996488094329834s

2025-10-19 22:58:20,746 | INFO | Training epoch 1783, Batch 1000/1000: LR=3.87e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 22:58:20,829 | INFO | Epoch 1783 Train Time 37.09940814971924s

2025-10-19 22:58:58,047 | INFO | Training epoch 1784, Batch 1000/1000: LR=3.85e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 22:58:58,146 | INFO | Epoch 1784 Train Time 37.3167142868042s

2025-10-19 22:59:35,641 | INFO | Training epoch 1785, Batch 1000/1000: LR=3.82e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 22:59:35,745 | INFO | Epoch 1785 Train Time 37.59753155708313s

2025-10-19 23:00:12,953 | INFO | Training epoch 1786, Batch 1000/1000: LR=3.80e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:00:13,036 | INFO | Epoch 1786 Train Time 37.28952431678772s

2025-10-19 23:00:50,472 | INFO | Training epoch 1787, Batch 1000/1000: LR=3.77e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:00:50,552 | INFO | Epoch 1787 Train Time 37.51550817489624s

2025-10-19 23:01:27,601 | INFO | Training epoch 1788, Batch 1000/1000: LR=3.74e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 23:01:27,693 | INFO | Epoch 1788 Train Time 37.13905572891235s

2025-10-19 23:02:04,763 | INFO | Training epoch 1789, Batch 1000/1000: LR=3.72e-06, Loss=2.97e-02 BER=1.11e-02 FER=1.48e-01
2025-10-19 23:02:04,866 | INFO | Epoch 1789 Train Time 37.17239189147949s

2025-10-19 23:02:42,552 | INFO | Training epoch 1790, Batch 1000/1000: LR=3.69e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 23:02:42,678 | INFO | Epoch 1790 Train Time 37.81097650527954s

2025-10-19 23:03:19,942 | INFO | Training epoch 1791, Batch 1000/1000: LR=3.67e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:03:20,032 | INFO | Epoch 1791 Train Time 37.35309815406799s

2025-10-19 23:03:57,462 | INFO | Training epoch 1792, Batch 1000/1000: LR=3.64e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.52e-01
2025-10-19 23:03:57,563 | INFO | Epoch 1792 Train Time 37.52831554412842s

2025-10-19 23:04:34,642 | INFO | Training epoch 1793, Batch 1000/1000: LR=3.62e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:04:34,728 | INFO | Epoch 1793 Train Time 37.163718700408936s

2025-10-19 23:05:11,668 | INFO | Training epoch 1794, Batch 1000/1000: LR=3.59e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:05:11,754 | INFO | Epoch 1794 Train Time 37.02425956726074s

2025-10-19 23:05:49,185 | INFO | Training epoch 1795, Batch 1000/1000: LR=3.57e-06, Loss=3.02e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:05:49,271 | INFO | Epoch 1795 Train Time 37.51550769805908s

2025-10-19 23:06:26,168 | INFO | Training epoch 1796, Batch 1000/1000: LR=3.54e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:06:26,268 | INFO | Epoch 1796 Train Time 36.9959659576416s

2025-10-19 23:07:03,242 | INFO | Training epoch 1797, Batch 1000/1000: LR=3.52e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:07:03,334 | INFO | Epoch 1797 Train Time 37.06559610366821s

2025-10-19 23:07:40,439 | INFO | Training epoch 1798, Batch 1000/1000: LR=3.50e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:07:40,532 | INFO | Epoch 1798 Train Time 37.194865465164185s

2025-10-19 23:08:17,538 | INFO | Training epoch 1799, Batch 1000/1000: LR=3.47e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:08:17,622 | INFO | Epoch 1799 Train Time 37.08953022956848s

2025-10-19 23:08:55,082 | INFO | Training epoch 1800, Batch 1000/1000: LR=3.45e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:08:55,169 | INFO | Epoch 1800 Train Time 37.5457866191864s

2025-10-19 23:09:32,342 | INFO | Training epoch 1801, Batch 1000/1000: LR=3.42e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:09:32,434 | INFO | Epoch 1801 Train Time 37.26369857788086s

2025-10-19 23:10:09,740 | INFO | Training epoch 1802, Batch 1000/1000: LR=3.40e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 23:10:09,836 | INFO | Epoch 1802 Train Time 37.400716066360474s

2025-10-19 23:10:47,181 | INFO | Training epoch 1803, Batch 1000/1000: LR=3.37e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:10:47,272 | INFO | Epoch 1803 Train Time 37.43441200256348s

2025-10-19 23:11:24,968 | INFO | Training epoch 1804, Batch 1000/1000: LR=3.35e-06, Loss=3.07e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 23:11:25,053 | INFO | Epoch 1804 Train Time 37.77930998802185s

2025-10-19 23:12:02,240 | INFO | Training epoch 1805, Batch 1000/1000: LR=3.33e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 23:12:02,337 | INFO | Epoch 1805 Train Time 37.28199005126953s

2025-10-19 23:12:39,735 | INFO | Training epoch 1806, Batch 1000/1000: LR=3.30e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 23:12:39,835 | INFO | Epoch 1806 Train Time 37.49625754356384s

2025-10-19 23:13:16,845 | INFO | Training epoch 1807, Batch 1000/1000: LR=3.28e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:13:16,957 | INFO | Epoch 1807 Train Time 37.118149280548096s

2025-10-19 23:13:54,176 | INFO | Training epoch 1808, Batch 1000/1000: LR=3.26e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:13:54,277 | INFO | Epoch 1808 Train Time 37.31978154182434s

2025-10-19 23:14:31,258 | INFO | Training epoch 1809, Batch 1000/1000: LR=3.23e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:14:31,352 | INFO | Epoch 1809 Train Time 37.07262110710144s

2025-10-19 23:15:08,338 | INFO | Training epoch 1810, Batch 1000/1000: LR=3.21e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 23:15:08,430 | INFO | Epoch 1810 Train Time 37.076228857040405s

2025-10-19 23:15:45,536 | INFO | Training epoch 1811, Batch 1000/1000: LR=3.19e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:15:45,618 | INFO | Epoch 1811 Train Time 37.18563961982727s

2025-10-19 23:16:22,831 | INFO | Training epoch 1812, Batch 1000/1000: LR=3.17e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:16:22,930 | INFO | Epoch 1812 Train Time 37.31040120124817s

2025-10-19 23:17:00,343 | INFO | Training epoch 1813, Batch 1000/1000: LR=3.14e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:17:00,423 | INFO | Epoch 1813 Train Time 37.491934061050415s

2025-10-19 23:17:37,846 | INFO | Training epoch 1814, Batch 1000/1000: LR=3.12e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:17:37,941 | INFO | Epoch 1814 Train Time 37.51545071601868s

2025-10-19 23:18:14,235 | INFO | Training epoch 1815, Batch 1000/1000: LR=3.10e-06, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:18:14,318 | INFO | Epoch 1815 Train Time 36.37486529350281s

2025-10-19 23:18:51,946 | INFO | Training epoch 1816, Batch 1000/1000: LR=3.08e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:18:52,032 | INFO | Epoch 1816 Train Time 37.71328902244568s

2025-10-19 23:19:29,681 | INFO | Training epoch 1817, Batch 1000/1000: LR=3.05e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 23:19:29,779 | INFO | Epoch 1817 Train Time 37.74500131607056s

2025-10-19 23:20:06,782 | INFO | Training epoch 1818, Batch 1000/1000: LR=3.03e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 23:20:06,858 | INFO | Epoch 1818 Train Time 37.07824397087097s

2025-10-19 23:20:43,750 | INFO | Training epoch 1819, Batch 1000/1000: LR=3.01e-06, Loss=3.05e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:20:43,846 | INFO | Epoch 1819 Train Time 36.987029790878296s

2025-10-19 23:21:21,289 | INFO | Training epoch 1820, Batch 1000/1000: LR=2.99e-06, Loss=2.98e-02 BER=1.11e-02 FER=1.47e-01
2025-10-19 23:21:21,380 | INFO | Epoch 1820 Train Time 37.53266000747681s

2025-10-19 23:21:58,173 | INFO | Training epoch 1821, Batch 1000/1000: LR=2.97e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:21:58,270 | INFO | Epoch 1821 Train Time 36.88867139816284s

2025-10-19 23:22:35,343 | INFO | Training epoch 1822, Batch 1000/1000: LR=2.94e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:22:35,428 | INFO | Epoch 1822 Train Time 37.156683921813965s

2025-10-19 23:23:12,145 | INFO | Training epoch 1823, Batch 1000/1000: LR=2.92e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:23:12,223 | INFO | Epoch 1823 Train Time 36.79338216781616s

2025-10-19 23:23:49,539 | INFO | Training epoch 1824, Batch 1000/1000: LR=2.90e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:23:49,627 | INFO | Epoch 1824 Train Time 37.40350580215454s

2025-10-19 23:24:26,546 | INFO | Training epoch 1825, Batch 1000/1000: LR=2.88e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:24:26,630 | INFO | Epoch 1825 Train Time 37.001325607299805s

2025-10-19 23:25:03,866 | INFO | Training epoch 1826, Batch 1000/1000: LR=2.86e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:25:03,967 | INFO | Epoch 1826 Train Time 37.335493326187134s

2025-10-19 23:25:41,116 | INFO | Training epoch 1827, Batch 1000/1000: LR=2.84e-06, Loss=3.05e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:25:41,204 | INFO | Epoch 1827 Train Time 37.2355477809906s

2025-10-19 23:26:18,444 | INFO | Training epoch 1828, Batch 1000/1000: LR=2.82e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:26:18,529 | INFO | Epoch 1828 Train Time 37.32424020767212s

2025-10-19 23:26:56,035 | INFO | Training epoch 1829, Batch 1000/1000: LR=2.80e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:26:56,118 | INFO | Epoch 1829 Train Time 37.586483001708984s

2025-10-19 23:27:32,969 | INFO | Training epoch 1830, Batch 1000/1000: LR=2.77e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:27:33,049 | INFO | Epoch 1830 Train Time 36.92981839179993s

2025-10-19 23:28:10,037 | INFO | Training epoch 1831, Batch 1000/1000: LR=2.75e-06, Loss=3.05e-02 BER=1.15e-02 FER=1.50e-01
2025-10-19 23:28:10,127 | INFO | Epoch 1831 Train Time 37.07680130004883s

2025-10-19 23:28:47,566 | INFO | Training epoch 1832, Batch 1000/1000: LR=2.73e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:28:47,649 | INFO | Epoch 1832 Train Time 37.52076268196106s

2025-10-19 23:29:25,075 | INFO | Training epoch 1833, Batch 1000/1000: LR=2.71e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:29:25,168 | INFO | Epoch 1833 Train Time 37.518420934677124s

2025-10-19 23:30:02,242 | INFO | Training epoch 1834, Batch 1000/1000: LR=2.69e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:30:02,338 | INFO | Epoch 1834 Train Time 37.168142557144165s

2025-10-19 23:30:39,747 | INFO | Training epoch 1835, Batch 1000/1000: LR=2.67e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:30:39,825 | INFO | Epoch 1835 Train Time 37.48538541793823s

2025-10-19 23:31:16,646 | INFO | Training epoch 1836, Batch 1000/1000: LR=2.65e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:31:16,732 | INFO | Epoch 1836 Train Time 36.9060115814209s

2025-10-19 23:31:54,856 | INFO | Training epoch 1837, Batch 1000/1000: LR=2.63e-06, Loss=2.98e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:31:54,941 | INFO | Epoch 1837 Train Time 38.20786428451538s

2025-10-19 23:32:32,744 | INFO | Training epoch 1838, Batch 1000/1000: LR=2.61e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:32:32,831 | INFO | Epoch 1838 Train Time 37.88945293426514s

2025-10-19 23:33:09,953 | INFO | Training epoch 1839, Batch 1000/1000: LR=2.59e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:33:10,050 | INFO | Epoch 1839 Train Time 37.217623472213745s

2025-10-19 23:33:47,228 | INFO | Training epoch 1840, Batch 1000/1000: LR=2.57e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:33:47,324 | INFO | Epoch 1840 Train Time 37.27275729179382s

2025-10-19 23:34:23,731 | INFO | Training epoch 1841, Batch 1000/1000: LR=2.56e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:34:23,830 | INFO | Epoch 1841 Train Time 36.50428056716919s

2025-10-19 23:35:00,866 | INFO | Training epoch 1842, Batch 1000/1000: LR=2.54e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:35:00,955 | INFO | Epoch 1842 Train Time 37.12371373176575s

2025-10-19 23:35:38,043 | INFO | Training epoch 1843, Batch 1000/1000: LR=2.52e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:35:38,136 | INFO | Epoch 1843 Train Time 37.18049240112305s

2025-10-19 23:36:14,144 | INFO | Training epoch 1844, Batch 1000/1000: LR=2.50e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:36:14,224 | INFO | Epoch 1844 Train Time 36.086535692214966s

2025-10-19 23:36:51,178 | INFO | Training epoch 1845, Batch 1000/1000: LR=2.48e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:36:51,268 | INFO | Epoch 1845 Train Time 37.04333019256592s

2025-10-19 23:37:27,643 | INFO | Training epoch 1846, Batch 1000/1000: LR=2.46e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:37:27,737 | INFO | Epoch 1846 Train Time 36.46672701835632s

2025-10-19 23:38:06,180 | INFO | Training epoch 1847, Batch 1000/1000: LR=2.44e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 23:38:06,274 | INFO | Epoch 1847 Train Time 38.53616237640381s

2025-10-19 23:38:44,441 | INFO | Training epoch 1848, Batch 1000/1000: LR=2.42e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:38:44,529 | INFO | Epoch 1848 Train Time 38.252649784088135s

2025-10-19 23:39:21,370 | INFO | Training epoch 1849, Batch 1000/1000: LR=2.40e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:39:21,453 | INFO | Epoch 1849 Train Time 36.92236614227295s

2025-10-19 23:39:58,668 | INFO | Training epoch 1850, Batch 1000/1000: LR=2.39e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:39:58,766 | INFO | Epoch 1850 Train Time 37.31232666969299s

2025-10-19 23:40:36,445 | INFO | Training epoch 1851, Batch 1000/1000: LR=2.37e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:40:36,541 | INFO | Epoch 1851 Train Time 37.7728385925293s

2025-10-19 23:41:13,745 | INFO | Training epoch 1852, Batch 1000/1000: LR=2.35e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 23:41:13,832 | INFO | Epoch 1852 Train Time 37.28921556472778s

2025-10-19 23:41:51,067 | INFO | Training epoch 1853, Batch 1000/1000: LR=2.33e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 23:41:51,162 | INFO | Epoch 1853 Train Time 37.32891035079956s

2025-10-19 23:42:28,344 | INFO | Training epoch 1854, Batch 1000/1000: LR=2.31e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:42:28,434 | INFO | Epoch 1854 Train Time 37.27105975151062s

2025-10-19 23:43:05,741 | INFO | Training epoch 1855, Batch 1000/1000: LR=2.30e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:43:05,841 | INFO | Epoch 1855 Train Time 37.40623617172241s

2025-10-19 23:43:43,110 | INFO | Training epoch 1856, Batch 1000/1000: LR=2.28e-06, Loss=2.98e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:43:43,196 | INFO | Epoch 1856 Train Time 37.35310173034668s

2025-10-19 23:44:20,345 | INFO | Training epoch 1857, Batch 1000/1000: LR=2.26e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:44:20,421 | INFO | Epoch 1857 Train Time 37.22327923774719s

2025-10-19 23:44:57,535 | INFO | Training epoch 1858, Batch 1000/1000: LR=2.24e-06, Loss=3.05e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:44:57,629 | INFO | Epoch 1858 Train Time 37.20774745941162s

2025-10-19 23:45:34,639 | INFO | Training epoch 1859, Batch 1000/1000: LR=2.23e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:45:34,730 | INFO | Epoch 1859 Train Time 37.09868669509888s

2025-10-19 23:46:11,950 | INFO | Training epoch 1860, Batch 1000/1000: LR=2.21e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.46e-01
2025-10-19 23:46:12,055 | INFO | Epoch 1860 Train Time 37.32304763793945s

2025-10-19 23:46:49,133 | INFO | Training epoch 1861, Batch 1000/1000: LR=2.19e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:46:49,225 | INFO | Epoch 1861 Train Time 37.166834592819214s

2025-10-19 23:47:26,365 | INFO | Training epoch 1862, Batch 1000/1000: LR=2.18e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.48e-01
2025-10-19 23:47:26,451 | INFO | Epoch 1862 Train Time 37.22437834739685s

2025-10-19 23:48:03,746 | INFO | Training epoch 1863, Batch 1000/1000: LR=2.16e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:48:03,841 | INFO | Epoch 1863 Train Time 37.388686418533325s

2025-10-19 23:48:41,568 | INFO | Training epoch 1864, Batch 1000/1000: LR=2.14e-06, Loss=3.03e-02 BER=1.15e-02 FER=1.51e-01
2025-10-19 23:48:41,659 | INFO | Epoch 1864 Train Time 37.8167085647583s

2025-10-19 23:49:18,759 | INFO | Training epoch 1865, Batch 1000/1000: LR=2.13e-06, Loss=2.98e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:49:18,847 | INFO | Epoch 1865 Train Time 37.18680238723755s

2025-10-19 23:49:56,044 | INFO | Training epoch 1866, Batch 1000/1000: LR=2.11e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-19 23:49:56,133 | INFO | Epoch 1866 Train Time 37.28544807434082s

2025-10-19 23:50:33,244 | INFO | Training epoch 1867, Batch 1000/1000: LR=2.09e-06, Loss=2.98e-02 BER=1.11e-02 FER=1.46e-01
2025-10-19 23:50:33,333 | INFO | Epoch 1867 Train Time 37.1967499256134s

2025-10-19 23:51:10,173 | INFO | Training epoch 1868, Batch 1000/1000: LR=2.08e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:51:10,268 | INFO | Epoch 1868 Train Time 36.93402051925659s

2025-10-19 23:51:47,742 | INFO | Training epoch 1869, Batch 1000/1000: LR=2.06e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:51:47,829 | INFO | Epoch 1869 Train Time 37.55912923812866s

2025-10-19 23:52:25,041 | INFO | Training epoch 1870, Batch 1000/1000: LR=2.04e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.48e-01
2025-10-19 23:52:25,149 | INFO | Epoch 1870 Train Time 37.31866240501404s

2025-10-19 23:53:01,939 | INFO | Training epoch 1871, Batch 1000/1000: LR=2.03e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-19 23:53:02,027 | INFO | Epoch 1871 Train Time 36.87685203552246s

2025-10-19 23:53:39,081 | INFO | Training epoch 1872, Batch 1000/1000: LR=2.01e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-19 23:53:39,170 | INFO | Epoch 1872 Train Time 37.142027378082275s

2025-10-19 23:54:16,571 | INFO | Training epoch 1873, Batch 1000/1000: LR=2.00e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:54:16,665 | INFO | Epoch 1873 Train Time 37.493210554122925s

2025-10-19 23:54:54,149 | INFO | Training epoch 1874, Batch 1000/1000: LR=1.98e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-19 23:54:54,237 | INFO | Epoch 1874 Train Time 37.57071352005005s

2025-10-19 23:55:31,087 | INFO | Training epoch 1875, Batch 1000/1000: LR=1.97e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:55:31,181 | INFO | Epoch 1875 Train Time 36.94282674789429s

2025-10-19 23:56:08,534 | INFO | Training epoch 1876, Batch 1000/1000: LR=1.95e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:56:08,635 | INFO | Epoch 1876 Train Time 37.45272898674011s

2025-10-19 23:56:46,242 | INFO | Training epoch 1877, Batch 1000/1000: LR=1.94e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:56:46,328 | INFO | Epoch 1877 Train Time 37.69241809844971s

2025-10-19 23:57:23,439 | INFO | Training epoch 1878, Batch 1000/1000: LR=1.92e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:57:23,537 | INFO | Epoch 1878 Train Time 37.20665740966797s

2025-10-19 23:58:01,073 | INFO | Training epoch 1879, Batch 1000/1000: LR=1.91e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-19 23:58:01,157 | INFO | Epoch 1879 Train Time 37.61920166015625s

2025-10-19 23:58:38,335 | INFO | Training epoch 1880, Batch 1000/1000: LR=1.89e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.47e-01
2025-10-19 23:58:38,422 | INFO | Epoch 1880 Train Time 37.262385845184326s

2025-10-19 23:59:15,837 | INFO | Training epoch 1881, Batch 1000/1000: LR=1.88e-06, Loss=3.05e-02 BER=1.13e-02 FER=1.50e-01
2025-10-19 23:59:15,924 | INFO | Epoch 1881 Train Time 37.5005829334259s

2025-10-19 23:59:53,385 | INFO | Training epoch 1882, Batch 1000/1000: LR=1.86e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.49e-01
2025-10-19 23:59:53,463 | INFO | Epoch 1882 Train Time 37.53713917732239s

2025-10-20 00:00:30,445 | INFO | Training epoch 1883, Batch 1000/1000: LR=1.85e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 00:00:30,555 | INFO | Epoch 1883 Train Time 37.09135985374451s

2025-10-20 00:01:07,874 | INFO | Training epoch 1884, Batch 1000/1000: LR=1.83e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:01:07,983 | INFO | Epoch 1884 Train Time 37.426844358444214s

2025-10-20 00:01:45,637 | INFO | Training epoch 1885, Batch 1000/1000: LR=1.82e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:01:45,738 | INFO | Epoch 1885 Train Time 37.7537739276886s

2025-10-20 00:02:22,546 | INFO | Training epoch 1886, Batch 1000/1000: LR=1.81e-06, Loss=3.07e-02 BER=1.16e-02 FER=1.52e-01
2025-10-20 00:02:22,636 | INFO | Epoch 1886 Train Time 36.896559953689575s

2025-10-20 00:03:00,143 | INFO | Training epoch 1887, Batch 1000/1000: LR=1.79e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:03:00,252 | INFO | Epoch 1887 Train Time 37.61469602584839s

2025-10-20 00:03:37,441 | INFO | Training epoch 1888, Batch 1000/1000: LR=1.78e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 00:03:37,536 | INFO | Epoch 1888 Train Time 37.28337121009827s

2025-10-20 00:04:14,644 | INFO | Training epoch 1889, Batch 1000/1000: LR=1.76e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:04:14,752 | INFO | Epoch 1889 Train Time 37.21531867980957s

2025-10-20 00:04:51,837 | INFO | Training epoch 1890, Batch 1000/1000: LR=1.75e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 00:04:51,924 | INFO | Epoch 1890 Train Time 37.17002463340759s

2025-10-20 00:05:29,156 | INFO | Training epoch 1891, Batch 1000/1000: LR=1.74e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:05:29,250 | INFO | Epoch 1891 Train Time 37.32485556602478s

2025-10-20 00:06:06,071 | INFO | Training epoch 1892, Batch 1000/1000: LR=1.72e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:06:06,172 | INFO | Epoch 1892 Train Time 36.91986131668091s

2025-10-20 00:06:43,570 | INFO | Training epoch 1893, Batch 1000/1000: LR=1.71e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:06:43,652 | INFO | Epoch 1893 Train Time 37.47901725769043s

2025-10-20 00:07:20,753 | INFO | Training epoch 1894, Batch 1000/1000: LR=1.70e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 00:07:20,845 | INFO | Epoch 1894 Train Time 37.19117546081543s

2025-10-20 00:07:57,784 | INFO | Training epoch 1895, Batch 1000/1000: LR=1.68e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:07:57,870 | INFO | Epoch 1895 Train Time 37.0238242149353s

2025-10-20 00:08:34,787 | INFO | Training epoch 1896, Batch 1000/1000: LR=1.67e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:08:34,878 | INFO | Epoch 1896 Train Time 37.00590658187866s

2025-10-20 00:09:12,101 | INFO | Training epoch 1897, Batch 1000/1000: LR=1.66e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-20 00:09:12,187 | INFO | Epoch 1897 Train Time 37.30785918235779s

2025-10-20 00:09:49,145 | INFO | Training epoch 1898, Batch 1000/1000: LR=1.65e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:09:49,232 | INFO | Epoch 1898 Train Time 37.04372811317444s

2025-10-20 00:10:26,029 | INFO | Training epoch 1899, Batch 1000/1000: LR=1.63e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:10:26,127 | INFO | Epoch 1899 Train Time 36.89430594444275s

2025-10-20 00:11:03,721 | INFO | Training epoch 1900, Batch 1000/1000: LR=1.62e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:11:03,813 | INFO | Epoch 1900 Train Time 37.6842622756958s

2025-10-20 00:11:40,341 | INFO | Training epoch 1901, Batch 1000/1000: LR=1.61e-06, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:11:40,426 | INFO | Epoch 1901 Train Time 36.61186456680298s

2025-10-20 00:12:17,662 | INFO | Training epoch 1902, Batch 1000/1000: LR=1.60e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.47e-01
2025-10-20 00:12:17,748 | INFO | Epoch 1902 Train Time 37.32148838043213s

2025-10-20 00:12:54,736 | INFO | Training epoch 1903, Batch 1000/1000: LR=1.59e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 00:12:54,834 | INFO | Epoch 1903 Train Time 37.08550834655762s

2025-10-20 00:13:32,341 | INFO | Training epoch 1904, Batch 1000/1000: LR=1.57e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-20 00:13:32,448 | INFO | Epoch 1904 Train Time 37.61293864250183s

2025-10-20 00:14:09,444 | INFO | Training epoch 1905, Batch 1000/1000: LR=1.56e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 00:14:09,519 | INFO | Epoch 1905 Train Time 37.069634437561035s

2025-10-20 00:14:46,642 | INFO | Training epoch 1906, Batch 1000/1000: LR=1.55e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:14:46,734 | INFO | Epoch 1906 Train Time 37.21387028694153s

2025-10-20 00:15:23,580 | INFO | Training epoch 1907, Batch 1000/1000: LR=1.54e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:15:23,669 | INFO | Epoch 1907 Train Time 36.9339816570282s

2025-10-20 00:16:01,055 | INFO | Training epoch 1908, Batch 1000/1000: LR=1.53e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:16:01,163 | INFO | Epoch 1908 Train Time 37.49118757247925s

2025-10-20 00:16:38,439 | INFO | Training epoch 1909, Batch 1000/1000: LR=1.52e-06, Loss=2.99e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:16:38,539 | INFO | Epoch 1909 Train Time 37.37501859664917s

2025-10-20 00:17:15,772 | INFO | Training epoch 1910, Batch 1000/1000: LR=1.50e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-20 00:17:15,866 | INFO | Epoch 1910 Train Time 37.32563304901123s

2025-10-20 00:17:53,362 | INFO | Training epoch 1911, Batch 1000/1000: LR=1.49e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:17:53,447 | INFO | Epoch 1911 Train Time 37.579208850860596s

2025-10-20 00:18:30,143 | INFO | Training epoch 1912, Batch 1000/1000: LR=1.48e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:18:30,224 | INFO | Epoch 1912 Train Time 36.77601861953735s

2025-10-20 00:19:07,991 | INFO | Training epoch 1913, Batch 1000/1000: LR=1.47e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:19:08,086 | INFO | Epoch 1913 Train Time 37.86080074310303s

2025-10-20 00:19:45,437 | INFO | Training epoch 1914, Batch 1000/1000: LR=1.46e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:19:45,529 | INFO | Epoch 1914 Train Time 37.441675662994385s

2025-10-20 00:20:22,990 | INFO | Training epoch 1915, Batch 1000/1000: LR=1.45e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:20:23,078 | INFO | Epoch 1915 Train Time 37.548381328582764s

2025-10-20 00:20:59,928 | INFO | Training epoch 1916, Batch 1000/1000: LR=1.44e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:21:00,007 | INFO | Epoch 1916 Train Time 36.92752456665039s

2025-10-20 00:21:37,037 | INFO | Training epoch 1917, Batch 1000/1000: LR=1.43e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:21:37,126 | INFO | Epoch 1917 Train Time 37.11781644821167s

2025-10-20 00:22:14,452 | INFO | Training epoch 1918, Batch 1000/1000: LR=1.42e-06, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:22:14,545 | INFO | Epoch 1918 Train Time 37.417969942092896s

2025-10-20 00:22:51,747 | INFO | Training epoch 1919, Batch 1000/1000: LR=1.41e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:22:51,843 | INFO | Epoch 1919 Train Time 37.2964768409729s

2025-10-20 00:23:28,944 | INFO | Training epoch 1920, Batch 1000/1000: LR=1.40e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.51e-01
2025-10-20 00:23:29,047 | INFO | Epoch 1920 Train Time 37.20268416404724s

2025-10-20 00:24:06,389 | INFO | Training epoch 1921, Batch 1000/1000: LR=1.39e-06, Loss=3.02e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:24:06,478 | INFO | Epoch 1921 Train Time 37.429850339889526s

2025-10-20 00:24:43,625 | INFO | Training epoch 1922, Batch 1000/1000: LR=1.38e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:24:43,727 | INFO | Epoch 1922 Train Time 37.24786734580994s

2025-10-20 00:25:20,566 | INFO | Training epoch 1923, Batch 1000/1000: LR=1.37e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 00:25:20,661 | INFO | Epoch 1923 Train Time 36.932990312576294s

2025-10-20 00:25:57,990 | INFO | Training epoch 1924, Batch 1000/1000: LR=1.36e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:25:58,077 | INFO | Epoch 1924 Train Time 37.4153048992157s

2025-10-20 00:26:35,048 | INFO | Training epoch 1925, Batch 1000/1000: LR=1.35e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:26:35,155 | INFO | Epoch 1925 Train Time 37.076699018478394s

2025-10-20 00:27:12,182 | INFO | Training epoch 1926, Batch 1000/1000: LR=1.34e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:27:12,269 | INFO | Epoch 1926 Train Time 37.11304473876953s

2025-10-20 00:27:49,930 | INFO | Training epoch 1927, Batch 1000/1000: LR=1.33e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.51e-01
2025-10-20 00:27:50,017 | INFO | Epoch 1927 Train Time 37.74499011039734s

2025-10-20 00:28:27,143 | INFO | Training epoch 1928, Batch 1000/1000: LR=1.33e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:28:27,229 | INFO | Epoch 1928 Train Time 37.21203017234802s

2025-10-20 00:29:04,332 | INFO | Training epoch 1929, Batch 1000/1000: LR=1.32e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:29:04,448 | INFO | Epoch 1929 Train Time 37.217034578323364s

2025-10-20 00:29:41,542 | INFO | Training epoch 1930, Batch 1000/1000: LR=1.31e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:29:41,633 | INFO | Epoch 1930 Train Time 37.183977127075195s

2025-10-20 00:30:18,859 | INFO | Training epoch 1931, Batch 1000/1000: LR=1.30e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:30:18,943 | INFO | Epoch 1931 Train Time 37.30876326560974s

2025-10-20 00:30:55,947 | INFO | Training epoch 1932, Batch 1000/1000: LR=1.29e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:30:56,025 | INFO | Epoch 1932 Train Time 37.080400466918945s

2025-10-20 00:31:33,678 | INFO | Training epoch 1933, Batch 1000/1000: LR=1.28e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.47e-01
2025-10-20 00:31:33,756 | INFO | Epoch 1933 Train Time 37.729854583740234s

2025-10-20 00:32:10,863 | INFO | Training epoch 1934, Batch 1000/1000: LR=1.27e-06, Loss=2.98e-02 BER=1.11e-02 FER=1.47e-01
2025-10-20 00:32:10,947 | INFO | Epoch 1934 Train Time 37.18901228904724s

2025-10-20 00:32:47,841 | INFO | Training epoch 1935, Batch 1000/1000: LR=1.27e-06, Loss=3.04e-02 BER=1.15e-02 FER=1.51e-01
2025-10-20 00:32:47,928 | INFO | Epoch 1935 Train Time 36.97870230674744s

2025-10-20 00:33:25,241 | INFO | Training epoch 1936, Batch 1000/1000: LR=1.26e-06, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-20 00:33:25,344 | INFO | Epoch 1936 Train Time 37.41561555862427s

2025-10-20 00:34:02,787 | INFO | Training epoch 1937, Batch 1000/1000: LR=1.25e-06, Loss=3.02e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:34:02,879 | INFO | Epoch 1937 Train Time 37.53362512588501s

2025-10-20 00:34:40,133 | INFO | Training epoch 1938, Batch 1000/1000: LR=1.24e-06, Loss=3.08e-02 BER=1.15e-02 FER=1.51e-01
2025-10-20 00:34:40,210 | INFO | Epoch 1938 Train Time 37.329285621643066s

2025-10-20 00:35:17,684 | INFO | Training epoch 1939, Batch 1000/1000: LR=1.23e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 00:35:17,768 | INFO | Epoch 1939 Train Time 37.556581020355225s

2025-10-20 00:35:54,948 | INFO | Training epoch 1940, Batch 1000/1000: LR=1.23e-06, Loss=3.01e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:35:55,048 | INFO | Epoch 1940 Train Time 37.27828574180603s

2025-10-20 00:36:31,654 | INFO | Training epoch 1941, Batch 1000/1000: LR=1.22e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:36:31,743 | INFO | Epoch 1941 Train Time 36.69306778907776s

2025-10-20 00:37:08,775 | INFO | Training epoch 1942, Batch 1000/1000: LR=1.21e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:37:08,893 | INFO | Epoch 1942 Train Time 37.148234844207764s

2025-10-20 00:37:46,043 | INFO | Training epoch 1943, Batch 1000/1000: LR=1.21e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.51e-01
2025-10-20 00:37:46,148 | INFO | Epoch 1943 Train Time 37.25412464141846s

2025-10-20 00:38:23,444 | INFO | Training epoch 1944, Batch 1000/1000: LR=1.20e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:38:23,548 | INFO | Epoch 1944 Train Time 37.39903426170349s

2025-10-20 00:39:00,843 | INFO | Training epoch 1945, Batch 1000/1000: LR=1.19e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.47e-01
2025-10-20 00:39:00,932 | INFO | Epoch 1945 Train Time 37.38260054588318s

2025-10-20 00:39:37,689 | INFO | Training epoch 1946, Batch 1000/1000: LR=1.18e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:39:37,787 | INFO | Epoch 1946 Train Time 36.85413360595703s

2025-10-20 00:40:15,155 | INFO | Training epoch 1947, Batch 1000/1000: LR=1.18e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-20 00:40:15,245 | INFO | Epoch 1947 Train Time 37.45660424232483s

2025-10-20 00:40:52,356 | INFO | Training epoch 1948, Batch 1000/1000: LR=1.17e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:40:52,449 | INFO | Epoch 1948 Train Time 37.201969146728516s

2025-10-20 00:41:29,538 | INFO | Training epoch 1949, Batch 1000/1000: LR=1.17e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:41:29,630 | INFO | Epoch 1949 Train Time 37.17977452278137s

2025-10-20 00:42:06,690 | INFO | Training epoch 1950, Batch 1000/1000: LR=1.16e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:42:06,754 | INFO | Epoch 1950 Train Time 37.12220096588135s

2025-10-20 00:42:43,869 | INFO | Training epoch 1951, Batch 1000/1000: LR=1.15e-06, Loss=3.06e-02 BER=1.14e-02 FER=1.51e-01
2025-10-20 00:42:43,952 | INFO | Epoch 1951 Train Time 37.19695520401001s

2025-10-20 00:43:21,046 | INFO | Training epoch 1952, Batch 1000/1000: LR=1.15e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 00:43:21,155 | INFO | Epoch 1952 Train Time 37.202117681503296s

2025-10-20 00:43:58,349 | INFO | Training epoch 1953, Batch 1000/1000: LR=1.14e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:43:58,436 | INFO | Epoch 1953 Train Time 37.28013896942139s

2025-10-20 00:44:35,442 | INFO | Training epoch 1954, Batch 1000/1000: LR=1.13e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:44:35,524 | INFO | Epoch 1954 Train Time 37.08541202545166s

2025-10-20 00:45:12,878 | INFO | Training epoch 1955, Batch 1000/1000: LR=1.13e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.51e-01
2025-10-20 00:45:12,964 | INFO | Epoch 1955 Train Time 37.438650608062744s

2025-10-20 00:45:50,748 | INFO | Training epoch 1956, Batch 1000/1000: LR=1.12e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:45:50,846 | INFO | Epoch 1956 Train Time 37.88126254081726s

2025-10-20 00:46:28,032 | INFO | Training epoch 1957, Batch 1000/1000: LR=1.12e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:46:28,115 | INFO | Epoch 1957 Train Time 37.26757454872131s

2025-10-20 00:47:05,185 | INFO | Training epoch 1958, Batch 1000/1000: LR=1.11e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 00:47:05,270 | INFO | Epoch 1958 Train Time 37.153056621551514s

2025-10-20 00:47:42,473 | INFO | Training epoch 1959, Batch 1000/1000: LR=1.11e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:47:42,565 | INFO | Epoch 1959 Train Time 37.294248819351196s

2025-10-20 00:48:19,137 | INFO | Training epoch 1960, Batch 1000/1000: LR=1.10e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:48:19,239 | INFO | Epoch 1960 Train Time 36.67323064804077s

2025-10-20 00:48:56,448 | INFO | Training epoch 1961, Batch 1000/1000: LR=1.10e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:48:56,536 | INFO | Epoch 1961 Train Time 37.29579162597656s

2025-10-20 00:49:33,060 | INFO | Training epoch 1962, Batch 1000/1000: LR=1.09e-06, Loss=2.99e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 00:49:33,143 | INFO | Epoch 1962 Train Time 36.60425305366516s

2025-10-20 00:50:09,942 | INFO | Training epoch 1963, Batch 1000/1000: LR=1.09e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:50:10,056 | INFO | Epoch 1963 Train Time 36.9119131565094s

2025-10-20 00:50:47,354 | INFO | Training epoch 1964, Batch 1000/1000: LR=1.08e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:50:47,437 | INFO | Epoch 1964 Train Time 37.3806586265564s

2025-10-20 00:51:24,042 | INFO | Training epoch 1965, Batch 1000/1000: LR=1.08e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.48e-01
2025-10-20 00:51:24,127 | INFO | Epoch 1965 Train Time 36.687440633773804s

2025-10-20 00:52:01,139 | INFO | Training epoch 1966, Batch 1000/1000: LR=1.07e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.50e-01
2025-10-20 00:52:01,236 | INFO | Epoch 1966 Train Time 37.10835385322571s

2025-10-20 00:52:38,070 | INFO | Training epoch 1967, Batch 1000/1000: LR=1.07e-06, Loss=3.06e-02 BER=1.15e-02 FER=1.50e-01
2025-10-20 00:52:38,164 | INFO | Epoch 1967 Train Time 36.92689895629883s

2025-10-20 00:53:15,740 | INFO | Training epoch 1968, Batch 1000/1000: LR=1.07e-06, Loss=3.07e-02 BER=1.15e-02 FER=1.52e-01
2025-10-20 00:53:15,847 | INFO | Epoch 1968 Train Time 37.681288719177246s

2025-10-20 00:53:53,268 | INFO | Training epoch 1969, Batch 1000/1000: LR=1.06e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 00:53:53,373 | INFO | Epoch 1969 Train Time 37.52468132972717s

2025-10-20 00:54:30,336 | INFO | Training epoch 1970, Batch 1000/1000: LR=1.06e-06, Loss=3.04e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:54:30,433 | INFO | Epoch 1970 Train Time 37.057437896728516s

2025-10-20 00:55:07,344 | INFO | Training epoch 1971, Batch 1000/1000: LR=1.05e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:55:07,447 | INFO | Epoch 1971 Train Time 37.013577699661255s

2025-10-20 00:55:44,145 | INFO | Training epoch 1972, Batch 1000/1000: LR=1.05e-06, Loss=3.02e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:55:44,237 | INFO | Epoch 1972 Train Time 36.78857088088989s

2025-10-20 00:56:21,345 | INFO | Training epoch 1973, Batch 1000/1000: LR=1.05e-06, Loss=2.97e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 00:56:21,433 | INFO | Epoch 1973 Train Time 37.195093870162964s

2025-10-20 00:56:58,885 | INFO | Training epoch 1974, Batch 1000/1000: LR=1.04e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:56:58,998 | INFO | Epoch 1974 Train Time 37.563568353652954s

2025-10-20 00:57:37,532 | INFO | Training epoch 1975, Batch 1000/1000: LR=1.04e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 00:57:37,637 | INFO | Epoch 1975 Train Time 38.63728427886963s

2025-10-20 00:58:15,435 | INFO | Training epoch 1976, Batch 1000/1000: LR=1.04e-06, Loss=2.99e-02 BER=1.11e-02 FER=1.47e-01
2025-10-20 00:58:15,534 | INFO | Epoch 1976 Train Time 37.89533758163452s

2025-10-20 00:58:53,042 | INFO | Training epoch 1977, Batch 1000/1000: LR=1.04e-06, Loss=3.03e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 00:58:53,131 | INFO | Epoch 1977 Train Time 37.595855712890625s

2025-10-20 00:59:30,657 | INFO | Training epoch 1978, Batch 1000/1000: LR=1.03e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 00:59:30,743 | INFO | Epoch 1978 Train Time 37.61035966873169s

2025-10-20 01:00:07,570 | INFO | Training epoch 1979, Batch 1000/1000: LR=1.03e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 01:00:07,679 | INFO | Epoch 1979 Train Time 36.93542242050171s

2025-10-20 01:00:44,848 | INFO | Training epoch 1980, Batch 1000/1000: LR=1.03e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 01:00:44,948 | INFO | Epoch 1980 Train Time 37.26784801483154s

2025-10-20 01:01:22,140 | INFO | Training epoch 1981, Batch 1000/1000: LR=1.02e-06, Loss=2.99e-02 BER=1.11e-02 FER=1.47e-01
2025-10-20 01:01:22,234 | INFO | Epoch 1981 Train Time 37.28520464897156s

2025-10-20 01:01:59,077 | INFO | Training epoch 1982, Batch 1000/1000: LR=1.02e-06, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-20 01:01:59,168 | INFO | Epoch 1982 Train Time 36.932453632354736s

2025-10-20 01:02:36,746 | INFO | Training epoch 1983, Batch 1000/1000: LR=1.02e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 01:02:36,832 | INFO | Epoch 1983 Train Time 37.662801027297974s

2025-10-20 01:03:14,051 | INFO | Training epoch 1984, Batch 1000/1000: LR=1.02e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 01:03:14,147 | INFO | Epoch 1984 Train Time 37.31265950202942s

2025-10-20 01:03:51,040 | INFO | Training epoch 1985, Batch 1000/1000: LR=1.02e-06, Loss=2.98e-02 BER=1.11e-02 FER=1.47e-01
2025-10-20 01:03:51,134 | INFO | Epoch 1985 Train Time 36.98647332191467s

2025-10-20 01:04:28,231 | INFO | Training epoch 1986, Batch 1000/1000: LR=1.01e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 01:04:28,317 | INFO | Epoch 1986 Train Time 37.18155121803284s

2025-10-20 01:05:05,747 | INFO | Training epoch 1987, Batch 1000/1000: LR=1.01e-06, Loss=3.00e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 01:05:05,856 | INFO | Epoch 1987 Train Time 37.53681468963623s

2025-10-20 01:05:43,068 | INFO | Training epoch 1988, Batch 1000/1000: LR=1.01e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.51e-01
2025-10-20 01:05:43,159 | INFO | Epoch 1988 Train Time 37.30297303199768s

2025-10-20 01:06:20,168 | INFO | Training epoch 1989, Batch 1000/1000: LR=1.01e-06, Loss=3.05e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 01:06:20,254 | INFO | Epoch 1989 Train Time 37.09148287773132s

2025-10-20 01:06:57,350 | INFO | Training epoch 1990, Batch 1000/1000: LR=1.01e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 01:06:57,441 | INFO | Epoch 1990 Train Time 37.185996770858765s

2025-10-20 01:07:34,564 | INFO | Training epoch 1991, Batch 1000/1000: LR=1.01e-06, Loss=2.99e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 01:07:34,660 | INFO | Epoch 1991 Train Time 37.21814703941345s

2025-10-20 01:08:11,839 | INFO | Training epoch 1992, Batch 1000/1000: LR=1.00e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 01:08:11,935 | INFO | Epoch 1992 Train Time 37.27393627166748s

2025-10-20 01:08:48,540 | INFO | Training epoch 1993, Batch 1000/1000: LR=1.00e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 01:08:48,652 | INFO | Epoch 1993 Train Time 36.715718507766724s

2025-10-20 01:09:25,966 | INFO | Training epoch 1994, Batch 1000/1000: LR=1.00e-06, Loss=3.01e-02 BER=1.13e-02 FER=1.49e-01
2025-10-20 01:09:26,063 | INFO | Epoch 1994 Train Time 37.40998411178589s

2025-10-20 01:10:03,145 | INFO | Training epoch 1995, Batch 1000/1000: LR=1.00e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 01:10:03,240 | INFO | Epoch 1995 Train Time 37.17526888847351s

2025-10-20 01:10:40,441 | INFO | Training epoch 1996, Batch 1000/1000: LR=1.00e-06, Loss=3.00e-02 BER=1.12e-02 FER=1.49e-01
2025-10-20 01:10:40,540 | INFO | Epoch 1996 Train Time 37.29848670959473s

2025-10-20 01:11:17,543 | INFO | Training epoch 1997, Batch 1000/1000: LR=1.00e-06, Loss=3.01e-02 BER=1.12e-02 FER=1.48e-01
2025-10-20 01:11:17,647 | INFO | Epoch 1997 Train Time 37.10539674758911s

2025-10-20 01:11:55,045 | INFO | Training epoch 1998, Batch 1000/1000: LR=1.00e-06, Loss=3.03e-02 BER=1.14e-02 FER=1.50e-01
2025-10-20 01:11:55,136 | INFO | Epoch 1998 Train Time 37.48716449737549s

2025-10-20 01:12:32,542 | INFO | Training epoch 1999, Batch 1000/1000: LR=1.00e-06, Loss=3.04e-02 BER=1.14e-02 FER=1.49e-01
2025-10-20 01:12:32,634 | INFO | Epoch 1999 Train Time 37.49736213684082s

2025-10-20 01:13:09,442 | INFO | Training epoch 2000, Batch 1000/1000: LR=1.00e-06, Loss=3.08e-02 BER=1.16e-02 FER=1.51e-01
2025-10-20 01:13:09,543 | INFO | Epoch 2000 Train Time 36.90586304664612s

2025-10-20 01:13:09,550 | INFO | Checkpoint saved: runs/20251018_155726/stage2_qat__BCH_n31_k16__Ndec2_d16_h8.pth
2025-10-20 01:13:09,555 | INFO | Checkpoint saved: runs/20251018_155726/stage2_qat__BCH_n31_k16__Ndec2_d16_h8__e2000_loss0.030843.pth
2025-10-20 01:13:09,625 | INFO | Loaded checkpoint: runs/20251018_155726/stage2_qat__BCH_n31_k16__Ndec2_d16_h8.pth (strict=False)
2025-10-20 01:13:09,635 | INFO | Checkpoint saved: runs/20251018_155726/stage2_infer_frozen__BCH_n31_k16__Ndec2_d16_h8__e1169_loss0.029627.pth
2025-10-20 01:13:15,462 | INFO | FER count threshold reached for EbN0:4
2025-10-20 01:13:15,582 | INFO | Test EbN0=4, BER=1.73e-02
2025-10-20 01:13:20,909 | INFO | FER count threshold reached for EbN0:5
2025-10-20 01:13:20,977 | INFO | Test EbN0=5, BER=5.86e-03
2025-10-20 01:13:26,150 | INFO | FER count threshold reached for EbN0:6
2025-10-20 01:13:26,262 | INFO | Test EbN0=6, BER=1.32e-03
2025-10-20 01:13:26,263 | INFO | 
Test Loss 4: 4.5510e-02 5: 1.6716e-02 6: 4.5106e-03
2025-10-20 01:13:26,263 | INFO | Test FER 4: 2.3993e-01 5: 9.8862e-02 6: 2.6726e-02
2025-10-20 01:13:26,263 | INFO | Test BER 4: 1.7264e-02 5: 5.8642e-03 6: 1.3205e-03
2025-10-20 01:13:26,264 | INFO | Test -ln(BER) 4: 4.0591e+00 5: 5.1389e+00 6: 6.6297e+00
2025-10-20 01:13:26,264 | INFO | # of testing samples: [100352.0, 100352.0, 100352.0]
 Test Time 16.62871789932251 s

2025-10-20 01:13:26,267 | INFO | Done.
