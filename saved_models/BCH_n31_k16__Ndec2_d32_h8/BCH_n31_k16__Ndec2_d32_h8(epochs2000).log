2025-10-14 11:14:37,058 | INFO | Device: cuda
2025-10-14 11:14:37,879 | INFO | Loaded checkpoint: runs/BCH_n31_k16__Ndec2_d32_h8/stage1_fp32__BCH_n31_k16__Ndec2_d32_h8__e1000_loss0.025157.pth (strict=True)
2025-10-14 11:14:56,814 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=2.61e-02 BER=9.87e-03 FER=1.26e-01
2025-10-14 11:14:56,850 | INFO | Epoch 1 Train Time 18.970271348953247s

2025-10-14 11:14:56,850 | INFO | [P1] saving best_model with loss 0.026072 at epoch 1
2025-10-14 11:15:14,856 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=2.56e-02 BER=9.70e-03 FER=1.24e-01
2025-10-14 11:15:14,907 | INFO | Epoch 2 Train Time 18.041654348373413s

2025-10-14 11:15:14,908 | INFO | [P1] saving best_model with loss 0.025551 at epoch 2
2025-10-14 11:15:32,408 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=2.59e-02 BER=9.79e-03 FER=1.25e-01
2025-10-14 11:15:32,463 | INFO | Epoch 3 Train Time 17.53518795967102s

2025-10-14 11:15:50,018 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=2.57e-02 BER=9.74e-03 FER=1.24e-01
2025-10-14 11:15:50,096 | INFO | Epoch 4 Train Time 17.631735801696777s

2025-10-14 11:16:08,924 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=2.54e-02 BER=9.60e-03 FER=1.22e-01
2025-10-14 11:16:08,988 | INFO | Epoch 5 Train Time 18.890190362930298s

2025-10-14 11:16:08,988 | INFO | [P1] saving best_model with loss 0.025398 at epoch 5
2025-10-14 11:16:27,584 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=2.56e-02 BER=9.68e-03 FER=1.23e-01
2025-10-14 11:16:27,629 | INFO | Epoch 6 Train Time 18.62861680984497s

2025-10-14 11:16:46,537 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=2.54e-02 BER=9.65e-03 FER=1.22e-01
2025-10-14 11:16:46,594 | INFO | Epoch 7 Train Time 18.964756965637207s

2025-10-14 11:17:06,426 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=2.55e-02 BER=9.69e-03 FER=1.23e-01
2025-10-14 11:17:06,478 | INFO | Epoch 8 Train Time 19.88250422477722s

2025-10-14 11:17:26,111 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=2.55e-02 BER=9.61e-03 FER=1.22e-01
2025-10-14 11:17:26,158 | INFO | Epoch 9 Train Time 19.679741144180298s

2025-10-14 11:17:47,325 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=2.55e-02 BER=9.69e-03 FER=1.24e-01
2025-10-14 11:17:47,378 | INFO | Epoch 10 Train Time 21.21888566017151s

2025-10-14 11:18:05,113 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=2.53e-02 BER=9.53e-03 FER=1.22e-01
2025-10-14 11:18:05,167 | INFO | Epoch 11 Train Time 17.788166522979736s

2025-10-14 11:18:05,168 | INFO | [P1] saving best_model with loss 0.025315 at epoch 11
2025-10-14 11:18:25,117 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=2.54e-02 BER=9.59e-03 FER=1.22e-01
2025-10-14 11:18:25,177 | INFO | Epoch 12 Train Time 19.995309114456177s

2025-10-14 11:18:46,153 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=2.54e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:18:46,227 | INFO | Epoch 13 Train Time 21.049129009246826s

2025-10-14 11:19:06,515 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=2.54e-02 BER=9.62e-03 FER=1.22e-01
2025-10-14 11:19:06,571 | INFO | Epoch 14 Train Time 20.343263864517212s

2025-10-14 11:19:25,714 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=2.51e-02 BER=9.55e-03 FER=1.23e-01
2025-10-14 11:19:25,769 | INFO | Epoch 15 Train Time 19.196861743927002s

2025-10-14 11:19:25,769 | INFO | [P1] saving best_model with loss 0.025148 at epoch 15
2025-10-14 11:19:45,338 | INFO | Training epoch 16, Batch 1000/1000: LR=9.99e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:19:45,387 | INFO | Epoch 16 Train Time 19.600038766860962s

2025-10-14 11:20:05,920 | INFO | Training epoch 17, Batch 1000/1000: LR=9.99e-05, Loss=2.55e-02 BER=9.67e-03 FER=1.22e-01
2025-10-14 11:20:05,964 | INFO | Epoch 17 Train Time 20.575664043426514s

2025-10-14 11:20:25,797 | INFO | Training epoch 18, Batch 1000/1000: LR=9.99e-05, Loss=2.55e-02 BER=9.70e-03 FER=1.23e-01
2025-10-14 11:20:25,867 | INFO | Epoch 18 Train Time 19.901817083358765s

2025-10-14 11:20:44,212 | INFO | Training epoch 19, Batch 1000/1000: LR=9.99e-05, Loss=2.53e-02 BER=9.52e-03 FER=1.23e-01
2025-10-14 11:20:44,263 | INFO | Epoch 19 Train Time 18.395469665527344s

2025-10-14 11:21:02,932 | INFO | Training epoch 20, Batch 1000/1000: LR=9.99e-05, Loss=2.53e-02 BER=9.58e-03 FER=1.22e-01
2025-10-14 11:21:02,994 | INFO | Epoch 20 Train Time 18.73030972480774s

2025-10-14 11:21:22,633 | INFO | Training epoch 21, Batch 1000/1000: LR=9.99e-05, Loss=2.53e-02 BER=9.53e-03 FER=1.23e-01
2025-10-14 11:21:22,686 | INFO | Epoch 21 Train Time 19.691120862960815s

2025-10-14 11:21:40,742 | INFO | Training epoch 22, Batch 1000/1000: LR=9.99e-05, Loss=2.56e-02 BER=9.69e-03 FER=1.23e-01
2025-10-14 11:21:40,797 | INFO | Epoch 22 Train Time 18.110467672348022s

2025-10-14 11:21:59,630 | INFO | Training epoch 23, Batch 1000/1000: LR=9.99e-05, Loss=2.54e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:21:59,696 | INFO | Epoch 23 Train Time 18.898042917251587s

2025-10-14 11:22:18,635 | INFO | Training epoch 24, Batch 1000/1000: LR=9.99e-05, Loss=2.53e-02 BER=9.54e-03 FER=1.21e-01
2025-10-14 11:22:18,707 | INFO | Epoch 24 Train Time 19.01032328605652s

2025-10-14 11:22:38,230 | INFO | Training epoch 25, Batch 1000/1000: LR=9.99e-05, Loss=2.54e-02 BER=9.56e-03 FER=1.22e-01
2025-10-14 11:22:38,292 | INFO | Epoch 25 Train Time 19.583414793014526s

2025-10-14 11:22:58,160 | INFO | Training epoch 26, Batch 1000/1000: LR=9.98e-05, Loss=2.53e-02 BER=9.55e-03 FER=1.22e-01
2025-10-14 11:22:58,220 | INFO | Epoch 26 Train Time 19.927480936050415s

2025-10-14 11:23:18,627 | INFO | Training epoch 27, Batch 1000/1000: LR=9.98e-05, Loss=2.55e-02 BER=9.68e-03 FER=1.22e-01
2025-10-14 11:23:18,681 | INFO | Epoch 27 Train Time 20.459715127944946s

2025-10-14 11:23:39,721 | INFO | Training epoch 28, Batch 1000/1000: LR=9.98e-05, Loss=2.55e-02 BER=9.64e-03 FER=1.23e-01
2025-10-14 11:23:39,786 | INFO | Epoch 28 Train Time 21.10445761680603s

2025-10-14 11:23:59,207 | INFO | Training epoch 29, Batch 1000/1000: LR=9.98e-05, Loss=2.53e-02 BER=9.55e-03 FER=1.22e-01
2025-10-14 11:23:59,257 | INFO | Epoch 29 Train Time 19.470333099365234s

2025-10-14 11:24:18,444 | INFO | Training epoch 30, Batch 1000/1000: LR=9.98e-05, Loss=2.56e-02 BER=9.69e-03 FER=1.23e-01
2025-10-14 11:24:18,496 | INFO | Epoch 30 Train Time 19.23826026916504s

2025-10-14 11:24:38,011 | INFO | Training epoch 31, Batch 1000/1000: LR=9.98e-05, Loss=2.54e-02 BER=9.67e-03 FER=1.24e-01
2025-10-14 11:24:38,071 | INFO | Epoch 31 Train Time 19.573832273483276s

2025-10-14 11:24:57,434 | INFO | Training epoch 32, Batch 1000/1000: LR=9.98e-05, Loss=2.55e-02 BER=9.64e-03 FER=1.22e-01
2025-10-14 11:24:57,488 | INFO | Epoch 32 Train Time 19.416806936264038s

2025-10-14 11:25:17,431 | INFO | Training epoch 33, Batch 1000/1000: LR=9.98e-05, Loss=2.58e-02 BER=9.75e-03 FER=1.24e-01
2025-10-14 11:25:17,481 | INFO | Epoch 33 Train Time 19.991276741027832s

2025-10-14 11:25:37,307 | INFO | Training epoch 34, Batch 1000/1000: LR=9.97e-05, Loss=2.59e-02 BER=9.75e-03 FER=1.24e-01
2025-10-14 11:25:37,365 | INFO | Epoch 34 Train Time 19.883829832077026s

2025-10-14 11:25:56,812 | INFO | Training epoch 35, Batch 1000/1000: LR=9.97e-05, Loss=2.59e-02 BER=9.78e-03 FER=1.24e-01
2025-10-14 11:25:56,879 | INFO | Epoch 35 Train Time 19.512965440750122s

2025-10-14 11:26:16,014 | INFO | Training epoch 36, Batch 1000/1000: LR=9.97e-05, Loss=2.53e-02 BER=9.61e-03 FER=1.22e-01
2025-10-14 11:26:16,067 | INFO | Epoch 36 Train Time 19.187316179275513s

2025-10-14 11:26:34,537 | INFO | Training epoch 37, Batch 1000/1000: LR=9.97e-05, Loss=2.57e-02 BER=9.78e-03 FER=1.24e-01
2025-10-14 11:26:34,588 | INFO | Epoch 37 Train Time 18.520471334457397s

2025-10-14 11:26:54,321 | INFO | Training epoch 38, Batch 1000/1000: LR=9.97e-05, Loss=2.54e-02 BER=9.53e-03 FER=1.22e-01
2025-10-14 11:26:54,368 | INFO | Epoch 38 Train Time 19.77896285057068s

2025-10-14 11:27:14,029 | INFO | Training epoch 39, Batch 1000/1000: LR=9.96e-05, Loss=2.55e-02 BER=9.63e-03 FER=1.23e-01
2025-10-14 11:27:14,074 | INFO | Epoch 39 Train Time 19.705844163894653s

2025-10-14 11:27:34,007 | INFO | Training epoch 40, Batch 1000/1000: LR=9.96e-05, Loss=2.55e-02 BER=9.68e-03 FER=1.22e-01
2025-10-14 11:27:34,053 | INFO | Epoch 40 Train Time 19.977477312088013s

2025-10-14 11:27:51,813 | INFO | Training epoch 41, Batch 1000/1000: LR=9.96e-05, Loss=2.50e-02 BER=9.47e-03 FER=1.21e-01
2025-10-14 11:27:51,871 | INFO | Epoch 41 Train Time 17.818110704421997s

2025-10-14 11:27:51,872 | INFO | [P1] saving best_model with loss 0.025021 at epoch 41
2025-10-14 11:28:11,529 | INFO | Training epoch 42, Batch 1000/1000: LR=9.96e-05, Loss=2.53e-02 BER=9.59e-03 FER=1.22e-01
2025-10-14 11:28:11,595 | INFO | Epoch 42 Train Time 19.70131802558899s

2025-10-14 11:28:31,136 | INFO | Training epoch 43, Batch 1000/1000: LR=9.96e-05, Loss=2.55e-02 BER=9.66e-03 FER=1.22e-01
2025-10-14 11:28:31,193 | INFO | Epoch 43 Train Time 19.596702098846436s

2025-10-14 11:28:51,232 | INFO | Training epoch 44, Batch 1000/1000: LR=9.95e-05, Loss=2.55e-02 BER=9.67e-03 FER=1.23e-01
2025-10-14 11:28:51,282 | INFO | Epoch 44 Train Time 20.08868718147278s

2025-10-14 11:29:11,322 | INFO | Training epoch 45, Batch 1000/1000: LR=9.95e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:29:11,375 | INFO | Epoch 45 Train Time 20.09215259552002s

2025-10-14 11:29:31,657 | INFO | Training epoch 46, Batch 1000/1000: LR=9.95e-05, Loss=2.52e-02 BER=9.52e-03 FER=1.21e-01
2025-10-14 11:29:31,718 | INFO | Epoch 46 Train Time 20.34259819984436s

2025-10-14 11:29:51,447 | INFO | Training epoch 47, Batch 1000/1000: LR=9.95e-05, Loss=2.56e-02 BER=9.70e-03 FER=1.23e-01
2025-10-14 11:29:51,496 | INFO | Epoch 47 Train Time 19.776403427124023s

2025-10-14 11:30:11,789 | INFO | Training epoch 48, Batch 1000/1000: LR=9.95e-05, Loss=2.49e-02 BER=9.44e-03 FER=1.21e-01
2025-10-14 11:30:11,840 | INFO | Epoch 48 Train Time 20.34311270713806s

2025-10-14 11:30:11,841 | INFO | [P1] saving best_model with loss 0.024893 at epoch 48
2025-10-14 11:30:30,723 | INFO | Training epoch 49, Batch 1000/1000: LR=9.94e-05, Loss=2.47e-02 BER=9.35e-03 FER=1.20e-01
2025-10-14 11:30:30,789 | INFO | Epoch 49 Train Time 18.931276559829712s

2025-10-14 11:30:30,790 | INFO | [P1] saving best_model with loss 0.024706 at epoch 49
2025-10-14 11:30:51,630 | INFO | Training epoch 50, Batch 1000/1000: LR=9.94e-05, Loss=2.55e-02 BER=9.70e-03 FER=1.23e-01
2025-10-14 11:30:51,695 | INFO | Epoch 50 Train Time 20.880755186080933s

2025-10-14 11:31:11,535 | INFO | Training epoch 51, Batch 1000/1000: LR=9.94e-05, Loss=2.57e-02 BER=9.78e-03 FER=1.24e-01
2025-10-14 11:31:11,585 | INFO | Epoch 51 Train Time 19.889255046844482s

2025-10-14 11:31:30,412 | INFO | Training epoch 52, Batch 1000/1000: LR=9.94e-05, Loss=2.56e-02 BER=9.68e-03 FER=1.23e-01
2025-10-14 11:31:30,472 | INFO | Epoch 52 Train Time 18.885921955108643s

2025-10-14 11:31:51,218 | INFO | Training epoch 53, Batch 1000/1000: LR=9.93e-05, Loss=2.52e-02 BER=9.59e-03 FER=1.22e-01
2025-10-14 11:31:51,272 | INFO | Epoch 53 Train Time 20.799882173538208s

2025-10-14 11:32:09,807 | INFO | Training epoch 54, Batch 1000/1000: LR=9.93e-05, Loss=2.53e-02 BER=9.58e-03 FER=1.22e-01
2025-10-14 11:32:09,857 | INFO | Epoch 54 Train Time 18.58316922187805s

2025-10-14 11:32:30,293 | INFO | Training epoch 55, Batch 1000/1000: LR=9.93e-05, Loss=2.55e-02 BER=9.67e-03 FER=1.23e-01
2025-10-14 11:32:30,336 | INFO | Epoch 55 Train Time 20.4783353805542s

2025-10-14 11:32:49,501 | INFO | Training epoch 56, Batch 1000/1000: LR=9.93e-05, Loss=2.50e-02 BER=9.46e-03 FER=1.20e-01
2025-10-14 11:32:49,549 | INFO | Epoch 56 Train Time 19.21271824836731s

2025-10-14 11:33:09,619 | INFO | Training epoch 57, Batch 1000/1000: LR=9.92e-05, Loss=2.57e-02 BER=9.77e-03 FER=1.24e-01
2025-10-14 11:33:09,676 | INFO | Epoch 57 Train Time 20.126461029052734s

2025-10-14 11:33:29,504 | INFO | Training epoch 58, Batch 1000/1000: LR=9.92e-05, Loss=2.53e-02 BER=9.58e-03 FER=1.22e-01
2025-10-14 11:33:29,567 | INFO | Epoch 58 Train Time 19.89046359062195s

2025-10-14 11:33:50,135 | INFO | Training epoch 59, Batch 1000/1000: LR=9.92e-05, Loss=2.50e-02 BER=9.43e-03 FER=1.20e-01
2025-10-14 11:33:50,197 | INFO | Epoch 59 Train Time 20.629529237747192s

2025-10-14 11:34:10,326 | INFO | Training epoch 60, Batch 1000/1000: LR=9.92e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:34:10,367 | INFO | Epoch 60 Train Time 20.16923451423645s

2025-10-14 11:34:29,832 | INFO | Training epoch 61, Batch 1000/1000: LR=9.91e-05, Loss=2.52e-02 BER=9.53e-03 FER=1.22e-01
2025-10-14 11:34:29,879 | INFO | Epoch 61 Train Time 19.510464906692505s

2025-10-14 11:34:49,623 | INFO | Training epoch 62, Batch 1000/1000: LR=9.91e-05, Loss=2.53e-02 BER=9.59e-03 FER=1.22e-01
2025-10-14 11:34:49,683 | INFO | Epoch 62 Train Time 19.802752256393433s

2025-10-14 11:35:07,825 | INFO | Training epoch 63, Batch 1000/1000: LR=9.91e-05, Loss=2.55e-02 BER=9.69e-03 FER=1.23e-01
2025-10-14 11:35:07,872 | INFO | Epoch 63 Train Time 18.18845510482788s

2025-10-14 11:35:26,822 | INFO | Training epoch 64, Batch 1000/1000: LR=9.90e-05, Loss=2.55e-02 BER=9.65e-03 FER=1.23e-01
2025-10-14 11:35:26,880 | INFO | Epoch 64 Train Time 19.007915496826172s

2025-10-14 11:35:45,401 | INFO | Training epoch 65, Batch 1000/1000: LR=9.90e-05, Loss=2.54e-02 BER=9.52e-03 FER=1.21e-01
2025-10-14 11:35:45,463 | INFO | Epoch 65 Train Time 18.58143973350525s

2025-10-14 11:36:05,509 | INFO | Training epoch 66, Batch 1000/1000: LR=9.90e-05, Loss=2.49e-02 BER=9.39e-03 FER=1.21e-01
2025-10-14 11:36:05,593 | INFO | Epoch 66 Train Time 20.129941701889038s

2025-10-14 11:36:26,526 | INFO | Training epoch 67, Batch 1000/1000: LR=9.89e-05, Loss=2.56e-02 BER=9.67e-03 FER=1.22e-01
2025-10-14 11:36:26,598 | INFO | Epoch 67 Train Time 21.003847122192383s

2025-10-14 11:36:46,898 | INFO | Training epoch 68, Batch 1000/1000: LR=9.89e-05, Loss=2.53e-02 BER=9.60e-03 FER=1.22e-01
2025-10-14 11:36:46,952 | INFO | Epoch 68 Train Time 20.352986097335815s

2025-10-14 11:37:06,018 | INFO | Training epoch 69, Batch 1000/1000: LR=9.89e-05, Loss=2.54e-02 BER=9.61e-03 FER=1.22e-01
2025-10-14 11:37:06,065 | INFO | Epoch 69 Train Time 19.112422704696655s

2025-10-14 11:37:25,236 | INFO | Training epoch 70, Batch 1000/1000: LR=9.88e-05, Loss=2.53e-02 BER=9.62e-03 FER=1.22e-01
2025-10-14 11:37:25,308 | INFO | Epoch 70 Train Time 19.24213719367981s

2025-10-14 11:37:45,621 | INFO | Training epoch 71, Batch 1000/1000: LR=9.88e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:37:45,670 | INFO | Epoch 71 Train Time 20.360569953918457s

2025-10-14 11:38:04,824 | INFO | Training epoch 72, Batch 1000/1000: LR=9.88e-05, Loss=2.58e-02 BER=9.77e-03 FER=1.24e-01
2025-10-14 11:38:04,882 | INFO | Epoch 72 Train Time 19.211329698562622s

2025-10-14 11:38:23,614 | INFO | Training epoch 73, Batch 1000/1000: LR=9.87e-05, Loss=2.56e-02 BER=9.67e-03 FER=1.23e-01
2025-10-14 11:38:23,664 | INFO | Epoch 73 Train Time 18.781808137893677s

2025-10-14 11:38:42,596 | INFO | Training epoch 74, Batch 1000/1000: LR=9.87e-05, Loss=2.52e-02 BER=9.57e-03 FER=1.21e-01
2025-10-14 11:38:42,658 | INFO | Epoch 74 Train Time 18.99222493171692s

2025-10-14 11:39:02,838 | INFO | Training epoch 75, Batch 1000/1000: LR=9.87e-05, Loss=2.52e-02 BER=9.57e-03 FER=1.21e-01
2025-10-14 11:39:02,889 | INFO | Epoch 75 Train Time 20.229496240615845s

2025-10-14 11:39:23,729 | INFO | Training epoch 76, Batch 1000/1000: LR=9.86e-05, Loss=2.54e-02 BER=9.60e-03 FER=1.21e-01
2025-10-14 11:39:23,792 | INFO | Epoch 76 Train Time 20.90220546722412s

2025-10-14 11:39:44,236 | INFO | Training epoch 77, Batch 1000/1000: LR=9.86e-05, Loss=2.55e-02 BER=9.67e-03 FER=1.22e-01
2025-10-14 11:39:44,288 | INFO | Epoch 77 Train Time 20.494693279266357s

2025-10-14 11:40:04,234 | INFO | Training epoch 78, Batch 1000/1000: LR=9.86e-05, Loss=2.49e-02 BER=9.47e-03 FER=1.21e-01
2025-10-14 11:40:04,278 | INFO | Epoch 78 Train Time 19.98915982246399s

2025-10-14 11:40:22,430 | INFO | Training epoch 79, Batch 1000/1000: LR=9.85e-05, Loss=2.50e-02 BER=9.44e-03 FER=1.21e-01
2025-10-14 11:40:22,483 | INFO | Epoch 79 Train Time 18.204031229019165s

2025-10-14 11:40:42,520 | INFO | Training epoch 80, Batch 1000/1000: LR=9.85e-05, Loss=2.53e-02 BER=9.59e-03 FER=1.22e-01
2025-10-14 11:40:42,575 | INFO | Epoch 80 Train Time 20.09122896194458s

2025-10-14 11:41:01,997 | INFO | Training epoch 81, Batch 1000/1000: LR=9.84e-05, Loss=2.55e-02 BER=9.66e-03 FER=1.22e-01
2025-10-14 11:41:02,057 | INFO | Epoch 81 Train Time 19.48155379295349s

2025-10-14 11:41:21,210 | INFO | Training epoch 82, Batch 1000/1000: LR=9.84e-05, Loss=2.52e-02 BER=9.45e-03 FER=1.21e-01
2025-10-14 11:41:21,267 | INFO | Epoch 82 Train Time 19.209452390670776s

2025-10-14 11:41:41,009 | INFO | Training epoch 83, Batch 1000/1000: LR=9.84e-05, Loss=2.50e-02 BER=9.52e-03 FER=1.20e-01
2025-10-14 11:41:41,060 | INFO | Epoch 83 Train Time 19.792117595672607s

2025-10-14 11:42:00,988 | INFO | Training epoch 84, Batch 1000/1000: LR=9.83e-05, Loss=2.54e-02 BER=9.56e-03 FER=1.22e-01
2025-10-14 11:42:01,042 | INFO | Epoch 84 Train Time 19.980893850326538s

2025-10-14 11:42:21,104 | INFO | Training epoch 85, Batch 1000/1000: LR=9.83e-05, Loss=2.54e-02 BER=9.60e-03 FER=1.22e-01
2025-10-14 11:42:21,144 | INFO | Epoch 85 Train Time 20.101044416427612s

2025-10-14 11:42:40,387 | INFO | Training epoch 86, Batch 1000/1000: LR=9.82e-05, Loss=2.53e-02 BER=9.59e-03 FER=1.21e-01
2025-10-14 11:42:40,446 | INFO | Epoch 86 Train Time 19.301674604415894s

2025-10-14 11:43:00,723 | INFO | Training epoch 87, Batch 1000/1000: LR=9.82e-05, Loss=2.51e-02 BER=9.53e-03 FER=1.21e-01
2025-10-14 11:43:00,779 | INFO | Epoch 87 Train Time 20.3325092792511s

2025-10-14 11:43:18,934 | INFO | Training epoch 88, Batch 1000/1000: LR=9.82e-05, Loss=2.50e-02 BER=9.44e-03 FER=1.21e-01
2025-10-14 11:43:18,989 | INFO | Epoch 88 Train Time 18.20872187614441s

2025-10-14 11:43:36,580 | INFO | Training epoch 89, Batch 1000/1000: LR=9.81e-05, Loss=2.52e-02 BER=9.51e-03 FER=1.22e-01
2025-10-14 11:43:36,631 | INFO | Epoch 89 Train Time 17.64113998413086s

2025-10-14 11:43:53,600 | INFO | Training epoch 90, Batch 1000/1000: LR=9.81e-05, Loss=2.50e-02 BER=9.43e-03 FER=1.20e-01
2025-10-14 11:43:53,647 | INFO | Epoch 90 Train Time 17.016254425048828s

2025-10-14 11:44:12,714 | INFO | Training epoch 91, Batch 1000/1000: LR=9.80e-05, Loss=2.52e-02 BER=9.58e-03 FER=1.22e-01
2025-10-14 11:44:12,765 | INFO | Epoch 91 Train Time 19.116629600524902s

2025-10-14 11:44:32,141 | INFO | Training epoch 92, Batch 1000/1000: LR=9.80e-05, Loss=2.49e-02 BER=9.38e-03 FER=1.20e-01
2025-10-14 11:44:32,196 | INFO | Epoch 92 Train Time 19.430511951446533s

2025-10-14 11:44:51,539 | INFO | Training epoch 93, Batch 1000/1000: LR=9.79e-05, Loss=2.49e-02 BER=9.44e-03 FER=1.20e-01
2025-10-14 11:44:51,588 | INFO | Epoch 93 Train Time 19.391798734664917s

2025-10-14 11:45:11,309 | INFO | Training epoch 94, Batch 1000/1000: LR=9.79e-05, Loss=2.49e-02 BER=9.40e-03 FER=1.20e-01
2025-10-14 11:45:11,360 | INFO | Epoch 94 Train Time 19.771217346191406s

2025-10-14 11:45:31,412 | INFO | Training epoch 95, Batch 1000/1000: LR=9.79e-05, Loss=2.49e-02 BER=9.43e-03 FER=1.21e-01
2025-10-14 11:45:31,483 | INFO | Epoch 95 Train Time 20.12156081199646s

2025-10-14 11:45:50,393 | INFO | Training epoch 96, Batch 1000/1000: LR=9.78e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.21e-01
2025-10-14 11:45:50,448 | INFO | Epoch 96 Train Time 18.964075803756714s

2025-10-14 11:46:09,690 | INFO | Training epoch 97, Batch 1000/1000: LR=9.78e-05, Loss=2.52e-02 BER=9.52e-03 FER=1.21e-01
2025-10-14 11:46:09,757 | INFO | Epoch 97 Train Time 19.308736324310303s

2025-10-14 11:46:29,035 | INFO | Training epoch 98, Batch 1000/1000: LR=9.77e-05, Loss=2.54e-02 BER=9.57e-03 FER=1.22e-01
2025-10-14 11:46:29,085 | INFO | Epoch 98 Train Time 19.326268911361694s

2025-10-14 11:46:48,230 | INFO | Training epoch 99, Batch 1000/1000: LR=9.77e-05, Loss=2.47e-02 BER=9.37e-03 FER=1.19e-01
2025-10-14 11:46:48,288 | INFO | Epoch 99 Train Time 19.201790809631348s

2025-10-14 11:46:48,288 | INFO | [P1] saving best_model with loss 0.024699 at epoch 99
2025-10-14 11:47:08,027 | INFO | Training epoch 100, Batch 1000/1000: LR=9.76e-05, Loss=2.56e-02 BER=9.67e-03 FER=1.22e-01
2025-10-14 11:47:08,084 | INFO | Epoch 100 Train Time 19.772059202194214s

2025-10-14 11:47:26,330 | INFO | Training epoch 101, Batch 1000/1000: LR=9.76e-05, Loss=2.52e-02 BER=9.55e-03 FER=1.21e-01
2025-10-14 11:47:26,393 | INFO | Epoch 101 Train Time 18.307864904403687s

2025-10-14 11:47:46,350 | INFO | Training epoch 102, Batch 1000/1000: LR=9.75e-05, Loss=2.51e-02 BER=9.50e-03 FER=1.20e-01
2025-10-14 11:47:46,403 | INFO | Epoch 102 Train Time 20.008594036102295s

2025-10-14 11:48:06,130 | INFO | Training epoch 103, Batch 1000/1000: LR=9.75e-05, Loss=2.48e-02 BER=9.37e-03 FER=1.20e-01
2025-10-14 11:48:06,188 | INFO | Epoch 103 Train Time 19.783692598342896s

2025-10-14 11:48:24,833 | INFO | Training epoch 104, Batch 1000/1000: LR=9.74e-05, Loss=2.49e-02 BER=9.42e-03 FER=1.20e-01
2025-10-14 11:48:24,891 | INFO | Epoch 104 Train Time 18.70237374305725s

2025-10-14 11:48:44,611 | INFO | Training epoch 105, Batch 1000/1000: LR=9.74e-05, Loss=2.51e-02 BER=9.51e-03 FER=1.21e-01
2025-10-14 11:48:44,665 | INFO | Epoch 105 Train Time 19.77343440055847s

2025-10-14 11:49:03,009 | INFO | Training epoch 106, Batch 1000/1000: LR=9.73e-05, Loss=2.54e-02 BER=9.58e-03 FER=1.22e-01
2025-10-14 11:49:03,071 | INFO | Epoch 106 Train Time 18.404709577560425s

2025-10-14 11:49:23,434 | INFO | Training epoch 107, Batch 1000/1000: LR=9.73e-05, Loss=2.50e-02 BER=9.50e-03 FER=1.21e-01
2025-10-14 11:49:23,489 | INFO | Epoch 107 Train Time 20.41772222518921s

2025-10-14 11:49:43,043 | INFO | Training epoch 108, Batch 1000/1000: LR=9.72e-05, Loss=2.49e-02 BER=9.40e-03 FER=1.20e-01
2025-10-14 11:49:43,098 | INFO | Epoch 108 Train Time 19.606353759765625s

2025-10-14 11:50:02,838 | INFO | Training epoch 109, Batch 1000/1000: LR=9.72e-05, Loss=2.51e-02 BER=9.46e-03 FER=1.20e-01
2025-10-14 11:50:02,890 | INFO | Epoch 109 Train Time 19.791597843170166s

2025-10-14 11:50:21,223 | INFO | Training epoch 110, Batch 1000/1000: LR=9.71e-05, Loss=2.48e-02 BER=9.34e-03 FER=1.20e-01
2025-10-14 11:50:21,282 | INFO | Epoch 110 Train Time 18.391168117523193s

2025-10-14 11:50:40,994 | INFO | Training epoch 111, Batch 1000/1000: LR=9.71e-05, Loss=2.51e-02 BER=9.51e-03 FER=1.20e-01
2025-10-14 11:50:41,055 | INFO | Epoch 111 Train Time 19.772270441055298s

2025-10-14 11:51:01,001 | INFO | Training epoch 112, Batch 1000/1000: LR=9.70e-05, Loss=2.50e-02 BER=9.47e-03 FER=1.20e-01
2025-10-14 11:51:01,046 | INFO | Epoch 112 Train Time 19.99077296257019s

2025-10-14 11:51:20,729 | INFO | Training epoch 113, Batch 1000/1000: LR=9.70e-05, Loss=2.51e-02 BER=9.55e-03 FER=1.21e-01
2025-10-14 11:51:20,784 | INFO | Epoch 113 Train Time 19.736888885498047s

2025-10-14 11:51:38,699 | INFO | Training epoch 114, Batch 1000/1000: LR=9.69e-05, Loss=2.54e-02 BER=9.58e-03 FER=1.22e-01
2025-10-14 11:51:38,750 | INFO | Epoch 114 Train Time 17.964606046676636s

2025-10-14 11:51:57,910 | INFO | Training epoch 115, Batch 1000/1000: LR=9.69e-05, Loss=2.51e-02 BER=9.48e-03 FER=1.21e-01
2025-10-14 11:51:57,970 | INFO | Epoch 115 Train Time 19.21954035758972s

2025-10-14 11:52:15,399 | INFO | Training epoch 116, Batch 1000/1000: LR=9.68e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.21e-01
2025-10-14 11:52:15,448 | INFO | Epoch 116 Train Time 17.476917266845703s

2025-10-14 11:52:34,995 | INFO | Training epoch 117, Batch 1000/1000: LR=9.67e-05, Loss=2.54e-02 BER=9.66e-03 FER=1.23e-01
2025-10-14 11:52:35,052 | INFO | Epoch 117 Train Time 19.603745698928833s

2025-10-14 11:52:55,627 | INFO | Training epoch 118, Batch 1000/1000: LR=9.67e-05, Loss=2.47e-02 BER=9.38e-03 FER=1.19e-01
2025-10-14 11:52:55,683 | INFO | Epoch 118 Train Time 20.629481315612793s

2025-10-14 11:52:55,684 | INFO | [P1] saving best_model with loss 0.024693 at epoch 118
2025-10-14 11:53:15,728 | INFO | Training epoch 119, Batch 1000/1000: LR=9.66e-05, Loss=2.50e-02 BER=9.43e-03 FER=1.20e-01
2025-10-14 11:53:15,795 | INFO | Epoch 119 Train Time 20.096928358078003s

2025-10-14 11:53:35,021 | INFO | Training epoch 120, Batch 1000/1000: LR=9.66e-05, Loss=2.52e-02 BER=9.54e-03 FER=1.21e-01
2025-10-14 11:53:35,074 | INFO | Epoch 120 Train Time 19.278337478637695s

2025-10-14 11:53:55,594 | INFO | Training epoch 121, Batch 1000/1000: LR=9.65e-05, Loss=2.50e-02 BER=9.41e-03 FER=1.19e-01
2025-10-14 11:53:55,652 | INFO | Epoch 121 Train Time 20.577462434768677s

2025-10-14 11:54:15,395 | INFO | Training epoch 122, Batch 1000/1000: LR=9.65e-05, Loss=2.46e-02 BER=9.34e-03 FER=1.19e-01
2025-10-14 11:54:15,450 | INFO | Epoch 122 Train Time 19.797219038009644s

2025-10-14 11:54:15,452 | INFO | [P1] saving best_model with loss 0.024554 at epoch 122
2025-10-14 11:54:33,825 | INFO | Training epoch 123, Batch 1000/1000: LR=9.64e-05, Loss=2.53e-02 BER=9.61e-03 FER=1.22e-01
2025-10-14 11:54:33,875 | INFO | Epoch 123 Train Time 18.41048765182495s

2025-10-14 11:54:53,617 | INFO | Training epoch 124, Batch 1000/1000: LR=9.64e-05, Loss=2.50e-02 BER=9.48e-03 FER=1.20e-01
2025-10-14 11:54:53,670 | INFO | Epoch 124 Train Time 19.79348850250244s

2025-10-14 11:55:13,318 | INFO | Training epoch 125, Batch 1000/1000: LR=9.63e-05, Loss=2.51e-02 BER=9.52e-03 FER=1.21e-01
2025-10-14 11:55:13,366 | INFO | Epoch 125 Train Time 19.695192575454712s

2025-10-14 11:55:34,019 | INFO | Training epoch 126, Batch 1000/1000: LR=9.62e-05, Loss=2.49e-02 BER=9.45e-03 FER=1.20e-01
2025-10-14 11:55:34,074 | INFO | Epoch 126 Train Time 20.70799446105957s

2025-10-14 11:55:53,311 | INFO | Training epoch 127, Batch 1000/1000: LR=9.62e-05, Loss=2.49e-02 BER=9.44e-03 FER=1.20e-01
2025-10-14 11:55:53,365 | INFO | Epoch 127 Train Time 19.29000163078308s

2025-10-14 11:56:13,494 | INFO | Training epoch 128, Batch 1000/1000: LR=9.61e-05, Loss=2.47e-02 BER=9.43e-03 FER=1.19e-01
2025-10-14 11:56:13,550 | INFO | Epoch 128 Train Time 20.184000492095947s

2025-10-14 11:56:31,926 | INFO | Training epoch 129, Batch 1000/1000: LR=9.61e-05, Loss=2.54e-02 BER=9.59e-03 FER=1.22e-01
2025-10-14 11:56:31,979 | INFO | Epoch 129 Train Time 18.42857074737549s

2025-10-14 11:56:51,023 | INFO | Training epoch 130, Batch 1000/1000: LR=9.60e-05, Loss=2.49e-02 BER=9.45e-03 FER=1.20e-01
2025-10-14 11:56:51,076 | INFO | Epoch 130 Train Time 19.09617280960083s

2025-10-14 11:57:10,987 | INFO | Training epoch 131, Batch 1000/1000: LR=9.59e-05, Loss=2.53e-02 BER=9.55e-03 FER=1.21e-01
2025-10-14 11:57:11,040 | INFO | Epoch 131 Train Time 19.96298575401306s

2025-10-14 11:57:31,397 | INFO | Training epoch 132, Batch 1000/1000: LR=9.59e-05, Loss=2.49e-02 BER=9.38e-03 FER=1.19e-01
2025-10-14 11:57:31,450 | INFO | Epoch 132 Train Time 20.40985608100891s

2025-10-14 11:57:48,221 | INFO | Training epoch 133, Batch 1000/1000: LR=9.58e-05, Loss=2.47e-02 BER=9.27e-03 FER=1.19e-01
2025-10-14 11:57:48,272 | INFO | Epoch 133 Train Time 16.819982767105103s

2025-10-14 11:58:08,337 | INFO | Training epoch 134, Batch 1000/1000: LR=9.57e-05, Loss=2.52e-02 BER=9.53e-03 FER=1.21e-01
2025-10-14 11:58:08,404 | INFO | Epoch 134 Train Time 20.131802558898926s

2025-10-14 11:58:27,822 | INFO | Training epoch 135, Batch 1000/1000: LR=9.57e-05, Loss=2.48e-02 BER=9.41e-03 FER=1.19e-01
2025-10-14 11:58:27,876 | INFO | Epoch 135 Train Time 19.47034192085266s

2025-10-14 11:58:45,642 | INFO | Training epoch 136, Batch 1000/1000: LR=9.56e-05, Loss=2.48e-02 BER=9.42e-03 FER=1.20e-01
2025-10-14 11:58:45,707 | INFO | Epoch 136 Train Time 17.830475091934204s

2025-10-14 11:59:05,422 | INFO | Training epoch 137, Batch 1000/1000: LR=9.56e-05, Loss=2.51e-02 BER=9.46e-03 FER=1.20e-01
2025-10-14 11:59:05,476 | INFO | Epoch 137 Train Time 19.768836498260498s

2025-10-14 11:59:25,211 | INFO | Training epoch 138, Batch 1000/1000: LR=9.55e-05, Loss=2.52e-02 BER=9.51e-03 FER=1.21e-01
2025-10-14 11:59:25,256 | INFO | Epoch 138 Train Time 19.778152227401733s

2025-10-14 11:59:41,986 | INFO | Training epoch 139, Batch 1000/1000: LR=9.54e-05, Loss=2.48e-02 BER=9.38e-03 FER=1.19e-01
2025-10-14 11:59:42,042 | INFO | Epoch 139 Train Time 16.78528356552124s

2025-10-14 12:00:00,937 | INFO | Training epoch 140, Batch 1000/1000: LR=9.54e-05, Loss=2.45e-02 BER=9.31e-03 FER=1.19e-01
2025-10-14 12:00:00,996 | INFO | Epoch 140 Train Time 18.953635692596436s

2025-10-14 12:00:00,996 | INFO | [P1] saving best_model with loss 0.024511 at epoch 140
2025-10-14 12:00:20,438 | INFO | Training epoch 141, Batch 1000/1000: LR=9.53e-05, Loss=2.49e-02 BER=9.43e-03 FER=1.19e-01
2025-10-14 12:00:20,498 | INFO | Epoch 141 Train Time 19.48951482772827s

2025-10-14 12:00:38,729 | INFO | Training epoch 142, Batch 1000/1000: LR=9.52e-05, Loss=2.47e-02 BER=9.36e-03 FER=1.20e-01
2025-10-14 12:00:38,783 | INFO | Epoch 142 Train Time 18.28471088409424s

2025-10-14 12:00:56,745 | INFO | Training epoch 143, Batch 1000/1000: LR=9.52e-05, Loss=2.49e-02 BER=9.43e-03 FER=1.19e-01
2025-10-14 12:00:56,796 | INFO | Epoch 143 Train Time 18.01213574409485s

2025-10-14 12:01:14,601 | INFO | Training epoch 144, Batch 1000/1000: LR=9.51e-05, Loss=2.48e-02 BER=9.37e-03 FER=1.20e-01
2025-10-14 12:01:14,665 | INFO | Epoch 144 Train Time 17.867499589920044s

2025-10-14 12:01:34,430 | INFO | Training epoch 145, Batch 1000/1000: LR=9.50e-05, Loss=2.49e-02 BER=9.42e-03 FER=1.19e-01
2025-10-14 12:01:34,496 | INFO | Epoch 145 Train Time 19.830370903015137s

2025-10-14 12:01:54,040 | INFO | Training epoch 146, Batch 1000/1000: LR=9.50e-05, Loss=2.47e-02 BER=9.31e-03 FER=1.19e-01
2025-10-14 12:01:54,101 | INFO | Epoch 146 Train Time 19.60492444038391s

2025-10-14 12:02:13,140 | INFO | Training epoch 147, Batch 1000/1000: LR=9.49e-05, Loss=2.47e-02 BER=9.34e-03 FER=1.19e-01
2025-10-14 12:02:13,199 | INFO | Epoch 147 Train Time 19.096694707870483s

2025-10-14 12:02:31,925 | INFO | Training epoch 148, Batch 1000/1000: LR=9.48e-05, Loss=2.48e-02 BER=9.37e-03 FER=1.19e-01
2025-10-14 12:02:31,986 | INFO | Epoch 148 Train Time 18.786673545837402s

2025-10-14 12:02:51,725 | INFO | Training epoch 149, Batch 1000/1000: LR=9.47e-05, Loss=2.52e-02 BER=9.50e-03 FER=1.21e-01
2025-10-14 12:02:51,774 | INFO | Epoch 149 Train Time 19.786146640777588s

2025-10-14 12:03:09,817 | INFO | Training epoch 150, Batch 1000/1000: LR=9.47e-05, Loss=2.47e-02 BER=9.38e-03 FER=1.19e-01
2025-10-14 12:03:09,873 | INFO | Epoch 150 Train Time 18.09858012199402s

2025-10-14 12:03:29,312 | INFO | Training epoch 151, Batch 1000/1000: LR=9.46e-05, Loss=2.47e-02 BER=9.36e-03 FER=1.20e-01
2025-10-14 12:03:29,369 | INFO | Epoch 151 Train Time 19.494131326675415s

2025-10-14 12:03:49,009 | INFO | Training epoch 152, Batch 1000/1000: LR=9.45e-05, Loss=2.45e-02 BER=9.28e-03 FER=1.18e-01
2025-10-14 12:03:49,071 | INFO | Epoch 152 Train Time 19.702219247817993s

2025-10-14 12:04:09,300 | INFO | Training epoch 153, Batch 1000/1000: LR=9.45e-05, Loss=2.49e-02 BER=9.46e-03 FER=1.20e-01
2025-10-14 12:04:09,357 | INFO | Epoch 153 Train Time 20.284199714660645s

2025-10-14 12:04:28,591 | INFO | Training epoch 154, Batch 1000/1000: LR=9.44e-05, Loss=2.47e-02 BER=9.35e-03 FER=1.19e-01
2025-10-14 12:04:28,637 | INFO | Epoch 154 Train Time 19.27942657470703s

2025-10-14 12:04:48,044 | INFO | Training epoch 155, Batch 1000/1000: LR=9.43e-05, Loss=2.51e-02 BER=9.51e-03 FER=1.21e-01
2025-10-14 12:04:48,107 | INFO | Epoch 155 Train Time 19.468860626220703s

2025-10-14 12:05:06,330 | INFO | Training epoch 156, Batch 1000/1000: LR=9.42e-05, Loss=2.49e-02 BER=9.41e-03 FER=1.20e-01
2025-10-14 12:05:06,380 | INFO | Epoch 156 Train Time 18.27215075492859s

2025-10-14 12:05:26,506 | INFO | Training epoch 157, Batch 1000/1000: LR=9.42e-05, Loss=2.50e-02 BER=9.49e-03 FER=1.20e-01
2025-10-14 12:05:26,562 | INFO | Epoch 157 Train Time 20.181041955947876s

2025-10-14 12:05:43,319 | INFO | Training epoch 158, Batch 1000/1000: LR=9.41e-05, Loss=2.50e-02 BER=9.49e-03 FER=1.20e-01
2025-10-14 12:05:43,370 | INFO | Epoch 158 Train Time 16.806297302246094s

2025-10-14 12:06:01,089 | INFO | Training epoch 159, Batch 1000/1000: LR=9.40e-05, Loss=2.48e-02 BER=9.45e-03 FER=1.19e-01
2025-10-14 12:06:01,139 | INFO | Epoch 159 Train Time 17.768709182739258s

2025-10-14 12:06:19,103 | INFO | Training epoch 160, Batch 1000/1000: LR=9.40e-05, Loss=2.44e-02 BER=9.24e-03 FER=1.17e-01
2025-10-14 12:06:19,150 | INFO | Epoch 160 Train Time 18.008727073669434s

2025-10-14 12:06:19,150 | INFO | [P1] saving best_model with loss 0.024433 at epoch 160
2025-10-14 12:06:35,901 | INFO | Training epoch 161, Batch 1000/1000: LR=9.39e-05, Loss=2.46e-02 BER=9.38e-03 FER=1.19e-01
2025-10-14 12:06:35,950 | INFO | Epoch 161 Train Time 16.786115884780884s

2025-10-14 12:06:56,024 | INFO | Training epoch 162, Batch 1000/1000: LR=9.38e-05, Loss=2.48e-02 BER=9.41e-03 FER=1.19e-01
2025-10-14 12:06:56,092 | INFO | Epoch 162 Train Time 20.140710830688477s

2025-10-14 12:07:15,914 | INFO | Training epoch 163, Batch 1000/1000: LR=9.37e-05, Loss=2.47e-02 BER=9.36e-03 FER=1.19e-01
2025-10-14 12:07:15,976 | INFO | Epoch 163 Train Time 19.883090257644653s

2025-10-14 12:07:36,514 | INFO | Training epoch 164, Batch 1000/1000: LR=9.37e-05, Loss=2.46e-02 BER=9.32e-03 FER=1.18e-01
2025-10-14 12:07:36,569 | INFO | Epoch 164 Train Time 20.591734886169434s

2025-10-14 12:07:55,035 | INFO | Training epoch 165, Batch 1000/1000: LR=9.36e-05, Loss=2.44e-02 BER=9.23e-03 FER=1.19e-01
2025-10-14 12:07:55,097 | INFO | Epoch 165 Train Time 18.52681541442871s

2025-10-14 12:07:55,098 | INFO | [P1] saving best_model with loss 0.024393 at epoch 165
2025-10-14 12:08:14,240 | INFO | Training epoch 166, Batch 1000/1000: LR=9.35e-05, Loss=2.47e-02 BER=9.41e-03 FER=1.20e-01
2025-10-14 12:08:14,295 | INFO | Epoch 166 Train Time 19.174670934677124s

2025-10-14 12:08:33,924 | INFO | Training epoch 167, Batch 1000/1000: LR=9.34e-05, Loss=2.45e-02 BER=9.28e-03 FER=1.19e-01
2025-10-14 12:08:33,984 | INFO | Epoch 167 Train Time 19.68851947784424s

2025-10-14 12:08:53,205 | INFO | Training epoch 168, Batch 1000/1000: LR=9.33e-05, Loss=2.49e-02 BER=9.44e-03 FER=1.19e-01
2025-10-14 12:08:53,272 | INFO | Epoch 168 Train Time 19.287095069885254s

2025-10-14 12:09:11,607 | INFO | Training epoch 169, Batch 1000/1000: LR=9.33e-05, Loss=2.49e-02 BER=9.43e-03 FER=1.20e-01
2025-10-14 12:09:11,661 | INFO | Epoch 169 Train Time 18.388108730316162s

2025-10-14 12:09:29,994 | INFO | Training epoch 170, Batch 1000/1000: LR=9.32e-05, Loss=2.48e-02 BER=9.44e-03 FER=1.19e-01
2025-10-14 12:09:30,043 | INFO | Epoch 170 Train Time 18.381877422332764s

2025-10-14 12:09:49,601 | INFO | Training epoch 171, Batch 1000/1000: LR=9.31e-05, Loss=2.51e-02 BER=9.49e-03 FER=1.20e-01
2025-10-14 12:09:49,652 | INFO | Epoch 171 Train Time 19.608350038528442s

2025-10-14 12:10:07,918 | INFO | Training epoch 172, Batch 1000/1000: LR=9.30e-05, Loss=2.50e-02 BER=9.48e-03 FER=1.20e-01
2025-10-14 12:10:07,975 | INFO | Epoch 172 Train Time 18.32070279121399s

2025-10-14 12:10:28,017 | INFO | Training epoch 173, Batch 1000/1000: LR=9.29e-05, Loss=2.48e-02 BER=9.42e-03 FER=1.18e-01
2025-10-14 12:10:28,075 | INFO | Epoch 173 Train Time 20.09970474243164s

2025-10-14 12:10:48,205 | INFO | Training epoch 174, Batch 1000/1000: LR=9.29e-05, Loss=2.47e-02 BER=9.35e-03 FER=1.19e-01
2025-10-14 12:10:48,270 | INFO | Epoch 174 Train Time 20.192919969558716s

2025-10-14 12:11:07,010 | INFO | Training epoch 175, Batch 1000/1000: LR=9.28e-05, Loss=2.47e-02 BER=9.40e-03 FER=1.19e-01
2025-10-14 12:11:07,080 | INFO | Epoch 175 Train Time 18.80845046043396s

2025-10-14 12:11:24,615 | INFO | Training epoch 176, Batch 1000/1000: LR=9.27e-05, Loss=2.53e-02 BER=9.57e-03 FER=1.20e-01
2025-10-14 12:11:24,668 | INFO | Epoch 176 Train Time 17.586795806884766s

2025-10-14 12:11:44,032 | INFO | Training epoch 177, Batch 1000/1000: LR=9.26e-05, Loss=2.49e-02 BER=9.48e-03 FER=1.20e-01
2025-10-14 12:11:44,087 | INFO | Epoch 177 Train Time 19.4186110496521s

2025-10-14 12:12:03,731 | INFO | Training epoch 178, Batch 1000/1000: LR=9.25e-05, Loss=2.48e-02 BER=9.42e-03 FER=1.19e-01
2025-10-14 12:12:03,792 | INFO | Epoch 178 Train Time 19.70364546775818s

2025-10-14 12:12:23,418 | INFO | Training epoch 179, Batch 1000/1000: LR=9.25e-05, Loss=2.46e-02 BER=9.36e-03 FER=1.18e-01
2025-10-14 12:12:23,472 | INFO | Epoch 179 Train Time 19.67941927909851s

2025-10-14 12:12:41,796 | INFO | Training epoch 180, Batch 1000/1000: LR=9.24e-05, Loss=2.52e-02 BER=9.56e-03 FER=1.21e-01
2025-10-14 12:12:41,852 | INFO | Epoch 180 Train Time 18.37940239906311s

2025-10-14 12:13:00,118 | INFO | Training epoch 181, Batch 1000/1000: LR=9.23e-05, Loss=2.46e-02 BER=9.34e-03 FER=1.19e-01
2025-10-14 12:13:00,189 | INFO | Epoch 181 Train Time 18.334643602371216s

2025-10-14 12:13:19,712 | INFO | Training epoch 182, Batch 1000/1000: LR=9.22e-05, Loss=2.48e-02 BER=9.37e-03 FER=1.18e-01
2025-10-14 12:13:19,774 | INFO | Epoch 182 Train Time 19.58412003517151s

2025-10-14 12:13:39,437 | INFO | Training epoch 183, Batch 1000/1000: LR=9.21e-05, Loss=2.45e-02 BER=9.24e-03 FER=1.18e-01
2025-10-14 12:13:39,492 | INFO | Epoch 183 Train Time 19.71733283996582s

2025-10-14 12:13:58,838 | INFO | Training epoch 184, Batch 1000/1000: LR=9.20e-05, Loss=2.49e-02 BER=9.43e-03 FER=1.20e-01
2025-10-14 12:13:58,891 | INFO | Epoch 184 Train Time 19.398117542266846s

2025-10-14 12:14:17,811 | INFO | Training epoch 185, Batch 1000/1000: LR=9.20e-05, Loss=2.50e-02 BER=9.48e-03 FER=1.20e-01
2025-10-14 12:14:17,866 | INFO | Epoch 185 Train Time 18.974079608917236s

2025-10-14 12:14:37,239 | INFO | Training epoch 186, Batch 1000/1000: LR=9.19e-05, Loss=2.44e-02 BER=9.31e-03 FER=1.18e-01
2025-10-14 12:14:37,292 | INFO | Epoch 186 Train Time 19.42545175552368s

2025-10-14 12:14:57,023 | INFO | Training epoch 187, Batch 1000/1000: LR=9.18e-05, Loss=2.49e-02 BER=9.44e-03 FER=1.20e-01
2025-10-14 12:14:57,072 | INFO | Epoch 187 Train Time 19.778546571731567s

2025-10-14 12:15:17,931 | INFO | Training epoch 188, Batch 1000/1000: LR=9.17e-05, Loss=2.44e-02 BER=9.24e-03 FER=1.18e-01
2025-10-14 12:15:17,994 | INFO | Epoch 188 Train Time 20.920954704284668s

2025-10-14 12:15:17,994 | INFO | [P1] saving best_model with loss 0.024374 at epoch 188
2025-10-14 12:15:36,834 | INFO | Training epoch 189, Batch 1000/1000: LR=9.16e-05, Loss=2.51e-02 BER=9.57e-03 FER=1.20e-01
2025-10-14 12:15:36,888 | INFO | Epoch 189 Train Time 18.876473903656006s

2025-10-14 12:15:55,822 | INFO | Training epoch 190, Batch 1000/1000: LR=9.15e-05, Loss=2.43e-02 BER=9.17e-03 FER=1.17e-01
2025-10-14 12:15:55,882 | INFO | Epoch 190 Train Time 18.993256330490112s

2025-10-14 12:15:55,882 | INFO | [P1] saving best_model with loss 0.024308 at epoch 190
2025-10-14 12:16:15,331 | INFO | Training epoch 191, Batch 1000/1000: LR=9.14e-05, Loss=2.55e-02 BER=9.67e-03 FER=1.21e-01
2025-10-14 12:16:15,398 | INFO | Epoch 191 Train Time 19.500105381011963s

2025-10-14 12:16:32,838 | INFO | Training epoch 192, Batch 1000/1000: LR=9.14e-05, Loss=2.46e-02 BER=9.34e-03 FER=1.19e-01
2025-10-14 12:16:32,890 | INFO | Epoch 192 Train Time 17.491342306137085s

2025-10-14 12:16:52,801 | INFO | Training epoch 193, Batch 1000/1000: LR=9.13e-05, Loss=2.48e-02 BER=9.39e-03 FER=1.20e-01
2025-10-14 12:16:52,867 | INFO | Epoch 193 Train Time 19.97600507736206s

2025-10-14 12:17:12,336 | INFO | Training epoch 194, Batch 1000/1000: LR=9.12e-05, Loss=2.48e-02 BER=9.46e-03 FER=1.19e-01
2025-10-14 12:17:12,401 | INFO | Epoch 194 Train Time 19.533326148986816s

2025-10-14 12:17:31,309 | INFO | Training epoch 195, Batch 1000/1000: LR=9.11e-05, Loss=2.45e-02 BER=9.25e-03 FER=1.18e-01
2025-10-14 12:17:31,358 | INFO | Epoch 195 Train Time 18.95499038696289s

2025-10-14 12:17:48,426 | INFO | Training epoch 196, Batch 1000/1000: LR=9.10e-05, Loss=2.46e-02 BER=9.33e-03 FER=1.19e-01
2025-10-14 12:17:48,483 | INFO | Epoch 196 Train Time 17.12368392944336s

2025-10-14 12:18:05,212 | INFO | Training epoch 197, Batch 1000/1000: LR=9.09e-05, Loss=2.46e-02 BER=9.27e-03 FER=1.18e-01
2025-10-14 12:18:05,262 | INFO | Epoch 197 Train Time 16.777600288391113s

2025-10-14 12:18:24,893 | INFO | Training epoch 198, Batch 1000/1000: LR=9.08e-05, Loss=2.48e-02 BER=9.39e-03 FER=1.19e-01
2025-10-14 12:18:24,939 | INFO | Epoch 198 Train Time 19.67668390274048s

2025-10-14 12:18:44,047 | INFO | Training epoch 199, Batch 1000/1000: LR=9.07e-05, Loss=2.47e-02 BER=9.34e-03 FER=1.18e-01
2025-10-14 12:18:44,113 | INFO | Epoch 199 Train Time 19.17281699180603s

2025-10-14 12:19:03,797 | INFO | Training epoch 200, Batch 1000/1000: LR=9.06e-05, Loss=2.45e-02 BER=9.32e-03 FER=1.18e-01
2025-10-14 12:19:03,854 | INFO | Epoch 200 Train Time 19.740691661834717s

2025-10-14 12:19:20,023 | INFO | Training epoch 201, Batch 1000/1000: LR=9.05e-05, Loss=2.45e-02 BER=9.36e-03 FER=1.17e-01
2025-10-14 12:19:20,074 | INFO | Epoch 201 Train Time 16.218222618103027s

2025-10-14 12:19:39,518 | INFO | Training epoch 202, Batch 1000/1000: LR=9.05e-05, Loss=2.47e-02 BER=9.34e-03 FER=1.18e-01
2025-10-14 12:19:39,574 | INFO | Epoch 202 Train Time 19.49793839454651s

2025-10-14 12:20:00,734 | INFO | Training epoch 203, Batch 1000/1000: LR=9.04e-05, Loss=2.45e-02 BER=9.27e-03 FER=1.19e-01
2025-10-14 12:20:00,796 | INFO | Epoch 203 Train Time 21.221667528152466s

2025-10-14 12:20:20,121 | INFO | Training epoch 204, Batch 1000/1000: LR=9.03e-05, Loss=2.46e-02 BER=9.31e-03 FER=1.17e-01
2025-10-14 12:20:20,181 | INFO | Epoch 204 Train Time 19.384231328964233s

2025-10-14 12:20:39,913 | INFO | Training epoch 205, Batch 1000/1000: LR=9.02e-05, Loss=2.48e-02 BER=9.42e-03 FER=1.20e-01
2025-10-14 12:20:39,978 | INFO | Epoch 205 Train Time 19.79679298400879s

2025-10-14 12:20:59,709 | INFO | Training epoch 206, Batch 1000/1000: LR=9.01e-05, Loss=2.48e-02 BER=9.39e-03 FER=1.19e-01
2025-10-14 12:20:59,765 | INFO | Epoch 206 Train Time 19.786558628082275s

2025-10-14 12:21:18,247 | INFO | Training epoch 207, Batch 1000/1000: LR=9.00e-05, Loss=2.45e-02 BER=9.30e-03 FER=1.18e-01
2025-10-14 12:21:18,296 | INFO | Epoch 207 Train Time 18.528139114379883s

2025-10-14 12:21:36,493 | INFO | Training epoch 208, Batch 1000/1000: LR=8.99e-05, Loss=2.44e-02 BER=9.27e-03 FER=1.19e-01
2025-10-14 12:21:36,546 | INFO | Epoch 208 Train Time 18.249653577804565s

2025-10-14 12:21:56,013 | INFO | Training epoch 209, Batch 1000/1000: LR=8.98e-05, Loss=2.47e-02 BER=9.36e-03 FER=1.18e-01
2025-10-14 12:21:56,068 | INFO | Epoch 209 Train Time 19.52077889442444s

2025-10-14 12:22:15,626 | INFO | Training epoch 210, Batch 1000/1000: LR=8.97e-05, Loss=2.47e-02 BER=9.31e-03 FER=1.19e-01
2025-10-14 12:22:15,693 | INFO | Epoch 210 Train Time 19.624824047088623s

2025-10-14 12:22:35,837 | INFO | Training epoch 211, Batch 1000/1000: LR=8.96e-05, Loss=2.46e-02 BER=9.39e-03 FER=1.18e-01
2025-10-14 12:22:35,891 | INFO | Epoch 211 Train Time 20.196500062942505s

2025-10-14 12:22:53,611 | INFO | Training epoch 212, Batch 1000/1000: LR=8.95e-05, Loss=2.47e-02 BER=9.32e-03 FER=1.19e-01
2025-10-14 12:22:53,663 | INFO | Epoch 212 Train Time 17.77164340019226s

2025-10-14 12:23:13,803 | INFO | Training epoch 213, Batch 1000/1000: LR=8.94e-05, Loss=2.47e-02 BER=9.35e-03 FER=1.18e-01
2025-10-14 12:23:13,858 | INFO | Epoch 213 Train Time 20.193795442581177s

2025-10-14 12:23:33,627 | INFO | Training epoch 214, Batch 1000/1000: LR=8.93e-05, Loss=2.46e-02 BER=9.32e-03 FER=1.19e-01
2025-10-14 12:23:33,703 | INFO | Epoch 214 Train Time 19.843761682510376s

2025-10-14 12:23:54,244 | INFO | Training epoch 215, Batch 1000/1000: LR=8.92e-05, Loss=2.45e-02 BER=9.27e-03 FER=1.18e-01
2025-10-14 12:23:54,311 | INFO | Epoch 215 Train Time 20.606595277786255s

2025-10-14 12:24:13,727 | INFO | Training epoch 216, Batch 1000/1000: LR=8.91e-05, Loss=2.44e-02 BER=9.20e-03 FER=1.18e-01
2025-10-14 12:24:13,779 | INFO | Epoch 216 Train Time 19.4675190448761s

2025-10-14 12:24:33,117 | INFO | Training epoch 217, Batch 1000/1000: LR=8.90e-05, Loss=2.45e-02 BER=9.28e-03 FER=1.18e-01
2025-10-14 12:24:33,163 | INFO | Epoch 217 Train Time 19.381609439849854s

2025-10-14 12:24:53,934 | INFO | Training epoch 218, Batch 1000/1000: LR=8.89e-05, Loss=2.49e-02 BER=9.48e-03 FER=1.20e-01
2025-10-14 12:24:53,979 | INFO | Epoch 218 Train Time 20.815218687057495s

2025-10-14 12:25:12,333 | INFO | Training epoch 219, Batch 1000/1000: LR=8.88e-05, Loss=2.43e-02 BER=9.21e-03 FER=1.17e-01
2025-10-14 12:25:12,380 | INFO | Epoch 219 Train Time 18.399498462677002s

2025-10-14 12:25:12,380 | INFO | [P1] saving best_model with loss 0.024267 at epoch 219
2025-10-14 12:25:31,322 | INFO | Training epoch 220, Batch 1000/1000: LR=8.87e-05, Loss=2.45e-02 BER=9.28e-03 FER=1.18e-01
2025-10-14 12:25:31,375 | INFO | Epoch 220 Train Time 18.970903635025024s

2025-10-14 12:25:50,812 | INFO | Training epoch 221, Batch 1000/1000: LR=8.86e-05, Loss=2.45e-02 BER=9.32e-03 FER=1.18e-01
2025-10-14 12:25:50,864 | INFO | Epoch 221 Train Time 19.48889136314392s

2025-10-14 12:26:10,343 | INFO | Training epoch 222, Batch 1000/1000: LR=8.85e-05, Loss=2.50e-02 BER=9.44e-03 FER=1.20e-01
2025-10-14 12:26:10,392 | INFO | Epoch 222 Train Time 19.52705216407776s

2025-10-14 12:26:29,604 | INFO | Training epoch 223, Batch 1000/1000: LR=8.84e-05, Loss=2.48e-02 BER=9.37e-03 FER=1.18e-01
2025-10-14 12:26:29,659 | INFO | Epoch 223 Train Time 19.266014575958252s

2025-10-14 12:26:49,723 | INFO | Training epoch 224, Batch 1000/1000: LR=8.83e-05, Loss=2.48e-02 BER=9.40e-03 FER=1.19e-01
2025-10-14 12:26:49,778 | INFO | Epoch 224 Train Time 20.118163347244263s

2025-10-14 12:27:09,110 | INFO | Training epoch 225, Batch 1000/1000: LR=8.82e-05, Loss=2.44e-02 BER=9.21e-03 FER=1.18e-01
2025-10-14 12:27:09,153 | INFO | Epoch 225 Train Time 19.373260736465454s

2025-10-14 12:27:28,226 | INFO | Training epoch 226, Batch 1000/1000: LR=8.81e-05, Loss=2.43e-02 BER=9.21e-03 FER=1.17e-01
2025-10-14 12:27:28,286 | INFO | Epoch 226 Train Time 19.132384061813354s

2025-10-14 12:27:46,415 | INFO | Training epoch 227, Batch 1000/1000: LR=8.80e-05, Loss=2.45e-02 BER=9.29e-03 FER=1.17e-01
2025-10-14 12:27:46,464 | INFO | Epoch 227 Train Time 18.17780637741089s

2025-10-14 12:28:03,704 | INFO | Training epoch 228, Batch 1000/1000: LR=8.79e-05, Loss=2.47e-02 BER=9.33e-03 FER=1.19e-01
2025-10-14 12:28:03,764 | INFO | Epoch 228 Train Time 17.29850959777832s

2025-10-14 12:28:21,520 | INFO | Training epoch 229, Batch 1000/1000: LR=8.78e-05, Loss=2.48e-02 BER=9.44e-03 FER=1.19e-01
2025-10-14 12:28:21,570 | INFO | Epoch 229 Train Time 17.80527901649475s

2025-10-14 12:28:41,092 | INFO | Training epoch 230, Batch 1000/1000: LR=8.77e-05, Loss=2.45e-02 BER=9.30e-03 FER=1.19e-01
2025-10-14 12:28:41,142 | INFO | Epoch 230 Train Time 19.571496963500977s

2025-10-14 12:29:00,314 | INFO | Training epoch 231, Batch 1000/1000: LR=8.76e-05, Loss=2.48e-02 BER=9.34e-03 FER=1.19e-01
2025-10-14 12:29:00,373 | INFO | Epoch 231 Train Time 19.229479789733887s

2025-10-14 12:29:18,021 | INFO | Training epoch 232, Batch 1000/1000: LR=8.75e-05, Loss=2.50e-02 BER=9.55e-03 FER=1.20e-01
2025-10-14 12:29:18,079 | INFO | Epoch 232 Train Time 17.705373525619507s

2025-10-14 12:29:35,588 | INFO | Training epoch 233, Batch 1000/1000: LR=8.74e-05, Loss=2.48e-02 BER=9.41e-03 FER=1.19e-01
2025-10-14 12:29:35,644 | INFO | Epoch 233 Train Time 17.564290523529053s

2025-10-14 12:29:54,322 | INFO | Training epoch 234, Batch 1000/1000: LR=8.73e-05, Loss=2.49e-02 BER=9.39e-03 FER=1.19e-01
2025-10-14 12:29:54,371 | INFO | Epoch 234 Train Time 18.725377321243286s

2025-10-14 12:30:13,525 | INFO | Training epoch 235, Batch 1000/1000: LR=8.72e-05, Loss=2.49e-02 BER=9.40e-03 FER=1.19e-01
2025-10-14 12:30:13,583 | INFO | Epoch 235 Train Time 19.21124267578125s

2025-10-14 12:30:33,306 | INFO | Training epoch 236, Batch 1000/1000: LR=8.71e-05, Loss=2.47e-02 BER=9.42e-03 FER=1.19e-01
2025-10-14 12:30:33,362 | INFO | Epoch 236 Train Time 19.77784776687622s

2025-10-14 12:30:51,730 | INFO | Training epoch 237, Batch 1000/1000: LR=8.70e-05, Loss=2.46e-02 BER=9.33e-03 FER=1.18e-01
2025-10-14 12:30:51,780 | INFO | Epoch 237 Train Time 18.4177885055542s

2025-10-14 12:31:09,518 | INFO | Training epoch 238, Batch 1000/1000: LR=8.69e-05, Loss=2.46e-02 BER=9.35e-03 FER=1.18e-01
2025-10-14 12:31:09,573 | INFO | Epoch 238 Train Time 17.792011499404907s

2025-10-14 12:31:28,295 | INFO | Training epoch 239, Batch 1000/1000: LR=8.68e-05, Loss=2.45e-02 BER=9.30e-03 FER=1.18e-01
2025-10-14 12:31:28,339 | INFO | Epoch 239 Train Time 18.76563835144043s

2025-10-14 12:31:49,709 | INFO | Training epoch 240, Batch 1000/1000: LR=8.67e-05, Loss=2.51e-02 BER=9.51e-03 FER=1.19e-01
2025-10-14 12:31:49,781 | INFO | Epoch 240 Train Time 21.44143533706665s

2025-10-14 12:32:09,929 | INFO | Training epoch 241, Batch 1000/1000: LR=8.66e-05, Loss=2.44e-02 BER=9.25e-03 FER=1.17e-01
2025-10-14 12:32:09,993 | INFO | Epoch 241 Train Time 20.210992574691772s

2025-10-14 12:32:29,815 | INFO | Training epoch 242, Batch 1000/1000: LR=8.65e-05, Loss=2.43e-02 BER=9.29e-03 FER=1.18e-01
2025-10-14 12:32:29,868 | INFO | Epoch 242 Train Time 19.87352156639099s

2025-10-14 12:32:50,628 | INFO | Training epoch 243, Batch 1000/1000: LR=8.64e-05, Loss=2.46e-02 BER=9.35e-03 FER=1.19e-01
2025-10-14 12:32:50,682 | INFO | Epoch 243 Train Time 20.81321430206299s

2025-10-14 12:33:11,620 | INFO | Training epoch 244, Batch 1000/1000: LR=8.63e-05, Loss=2.42e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 12:33:11,675 | INFO | Epoch 244 Train Time 20.992350578308105s

2025-10-14 12:33:11,675 | INFO | [P1] saving best_model with loss 0.024227 at epoch 244
2025-10-14 12:33:31,213 | INFO | Training epoch 245, Batch 1000/1000: LR=8.62e-05, Loss=2.43e-02 BER=9.25e-03 FER=1.16e-01
2025-10-14 12:33:31,254 | INFO | Epoch 245 Train Time 19.56521487236023s

2025-10-14 12:33:51,326 | INFO | Training epoch 246, Batch 1000/1000: LR=8.60e-05, Loss=2.47e-02 BER=9.32e-03 FER=1.19e-01
2025-10-14 12:33:51,373 | INFO | Epoch 246 Train Time 20.11822819709778s

2025-10-14 12:34:11,927 | INFO | Training epoch 247, Batch 1000/1000: LR=8.59e-05, Loss=2.48e-02 BER=9.41e-03 FER=1.19e-01
2025-10-14 12:34:11,986 | INFO | Epoch 247 Train Time 20.612105131149292s

2025-10-14 12:34:32,003 | INFO | Training epoch 248, Batch 1000/1000: LR=8.58e-05, Loss=2.46e-02 BER=9.31e-03 FER=1.18e-01
2025-10-14 12:34:32,058 | INFO | Epoch 248 Train Time 20.07105588912964s

2025-10-14 12:34:52,217 | INFO | Training epoch 249, Batch 1000/1000: LR=8.57e-05, Loss=2.45e-02 BER=9.23e-03 FER=1.17e-01
2025-10-14 12:34:52,273 | INFO | Epoch 249 Train Time 20.21371340751648s

2025-10-14 12:35:12,135 | INFO | Training epoch 250, Batch 1000/1000: LR=8.56e-05, Loss=2.40e-02 BER=9.07e-03 FER=1.17e-01
2025-10-14 12:35:12,187 | INFO | Epoch 250 Train Time 19.91269588470459s

2025-10-14 12:35:12,188 | INFO | [P1] saving best_model with loss 0.024035 at epoch 250
2025-10-14 12:35:32,115 | INFO | Training epoch 251, Batch 1000/1000: LR=8.55e-05, Loss=2.44e-02 BER=9.29e-03 FER=1.18e-01
2025-10-14 12:35:32,164 | INFO | Epoch 251 Train Time 19.959476232528687s

2025-10-14 12:35:50,290 | INFO | Training epoch 252, Batch 1000/1000: LR=8.54e-05, Loss=2.43e-02 BER=9.15e-03 FER=1.16e-01
2025-10-14 12:35:50,347 | INFO | Epoch 252 Train Time 18.18199896812439s

2025-10-14 12:36:12,519 | INFO | Training epoch 253, Batch 1000/1000: LR=8.53e-05, Loss=2.45e-02 BER=9.35e-03 FER=1.18e-01
2025-10-14 12:36:12,589 | INFO | Epoch 253 Train Time 22.241440773010254s

2025-10-14 12:36:33,525 | INFO | Training epoch 254, Batch 1000/1000: LR=8.52e-05, Loss=2.46e-02 BER=9.38e-03 FER=1.17e-01
2025-10-14 12:36:33,597 | INFO | Epoch 254 Train Time 21.006611347198486s

2025-10-14 12:36:52,426 | INFO | Training epoch 255, Batch 1000/1000: LR=8.51e-05, Loss=2.44e-02 BER=9.31e-03 FER=1.17e-01
2025-10-14 12:36:52,489 | INFO | Epoch 255 Train Time 18.890624523162842s

2025-10-14 12:37:11,625 | INFO | Training epoch 256, Batch 1000/1000: LR=8.49e-05, Loss=2.43e-02 BER=9.22e-03 FER=1.18e-01
2025-10-14 12:37:11,677 | INFO | Epoch 256 Train Time 19.187430143356323s

2025-10-14 12:37:32,593 | INFO | Training epoch 257, Batch 1000/1000: LR=8.48e-05, Loss=2.45e-02 BER=9.30e-03 FER=1.18e-01
2025-10-14 12:37:32,648 | INFO | Epoch 257 Train Time 20.969393014907837s

2025-10-14 12:37:53,021 | INFO | Training epoch 258, Batch 1000/1000: LR=8.47e-05, Loss=2.43e-02 BER=9.22e-03 FER=1.17e-01
2025-10-14 12:37:53,079 | INFO | Epoch 258 Train Time 20.43114447593689s

2025-10-14 12:38:11,332 | INFO | Training epoch 259, Batch 1000/1000: LR=8.46e-05, Loss=2.43e-02 BER=9.24e-03 FER=1.17e-01
2025-10-14 12:38:11,381 | INFO | Epoch 259 Train Time 18.299399614334106s

2025-10-14 12:38:31,729 | INFO | Training epoch 260, Batch 1000/1000: LR=8.45e-05, Loss=2.41e-02 BER=9.17e-03 FER=1.17e-01
2025-10-14 12:38:31,794 | INFO | Epoch 260 Train Time 20.412195444107056s

2025-10-14 12:38:53,837 | INFO | Training epoch 261, Batch 1000/1000: LR=8.44e-05, Loss=2.47e-02 BER=9.35e-03 FER=1.19e-01
2025-10-14 12:38:53,884 | INFO | Epoch 261 Train Time 22.089552879333496s

2025-10-14 12:39:13,826 | INFO | Training epoch 262, Batch 1000/1000: LR=8.43e-05, Loss=2.48e-02 BER=9.40e-03 FER=1.18e-01
2025-10-14 12:39:13,877 | INFO | Epoch 262 Train Time 19.991087913513184s

2025-10-14 12:39:35,185 | INFO | Training epoch 263, Batch 1000/1000: LR=8.42e-05, Loss=2.42e-02 BER=9.19e-03 FER=1.17e-01
2025-10-14 12:39:35,227 | INFO | Epoch 263 Train Time 21.349480390548706s

2025-10-14 12:39:55,206 | INFO | Training epoch 264, Batch 1000/1000: LR=8.40e-05, Loss=2.45e-02 BER=9.30e-03 FER=1.17e-01
2025-10-14 12:39:55,258 | INFO | Epoch 264 Train Time 20.030991792678833s

2025-10-14 12:40:15,316 | INFO | Training epoch 265, Batch 1000/1000: LR=8.39e-05, Loss=2.45e-02 BER=9.32e-03 FER=1.19e-01
2025-10-14 12:40:15,372 | INFO | Epoch 265 Train Time 20.11243748664856s

2025-10-14 12:40:36,336 | INFO | Training epoch 266, Batch 1000/1000: LR=8.38e-05, Loss=2.45e-02 BER=9.32e-03 FER=1.18e-01
2025-10-14 12:40:36,388 | INFO | Epoch 266 Train Time 21.015103101730347s

2025-10-14 12:40:57,544 | INFO | Training epoch 267, Batch 1000/1000: LR=8.37e-05, Loss=2.41e-02 BER=9.14e-03 FER=1.17e-01
2025-10-14 12:40:57,617 | INFO | Epoch 267 Train Time 21.227958917617798s

2025-10-14 12:41:18,233 | INFO | Training epoch 268, Batch 1000/1000: LR=8.36e-05, Loss=2.42e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 12:41:18,282 | INFO | Epoch 268 Train Time 20.663974285125732s

2025-10-14 12:41:38,641 | INFO | Training epoch 269, Batch 1000/1000: LR=8.35e-05, Loss=2.44e-02 BER=9.21e-03 FER=1.18e-01
2025-10-14 12:41:38,701 | INFO | Epoch 269 Train Time 20.417948484420776s

2025-10-14 12:41:58,739 | INFO | Training epoch 270, Batch 1000/1000: LR=8.34e-05, Loss=2.43e-02 BER=9.14e-03 FER=1.16e-01
2025-10-14 12:41:58,802 | INFO | Epoch 270 Train Time 20.099729537963867s

2025-10-14 12:42:17,624 | INFO | Training epoch 271, Batch 1000/1000: LR=8.32e-05, Loss=2.43e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 12:42:17,677 | INFO | Epoch 271 Train Time 18.874016523361206s

2025-10-14 12:42:38,721 | INFO | Training epoch 272, Batch 1000/1000: LR=8.31e-05, Loss=2.46e-02 BER=9.34e-03 FER=1.17e-01
2025-10-14 12:42:38,774 | INFO | Epoch 272 Train Time 21.09647035598755s

2025-10-14 12:42:57,614 | INFO | Training epoch 273, Batch 1000/1000: LR=8.30e-05, Loss=2.44e-02 BER=9.25e-03 FER=1.17e-01
2025-10-14 12:42:57,668 | INFO | Epoch 273 Train Time 18.892910957336426s

2025-10-14 12:43:17,536 | INFO | Training epoch 274, Batch 1000/1000: LR=8.29e-05, Loss=2.43e-02 BER=9.20e-03 FER=1.16e-01
2025-10-14 12:43:17,603 | INFO | Epoch 274 Train Time 19.934046030044556s

2025-10-14 12:43:39,735 | INFO | Training epoch 275, Batch 1000/1000: LR=8.28e-05, Loss=2.44e-02 BER=9.19e-03 FER=1.17e-01
2025-10-14 12:43:39,789 | INFO | Epoch 275 Train Time 22.185227155685425s

2025-10-14 12:43:59,525 | INFO | Training epoch 276, Batch 1000/1000: LR=8.26e-05, Loss=2.42e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 12:43:59,590 | INFO | Epoch 276 Train Time 19.799540519714355s

2025-10-14 12:44:19,512 | INFO | Training epoch 277, Batch 1000/1000: LR=8.25e-05, Loss=2.43e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 12:44:19,555 | INFO | Epoch 277 Train Time 19.96390438079834s

2025-10-14 12:44:39,725 | INFO | Training epoch 278, Batch 1000/1000: LR=8.24e-05, Loss=2.47e-02 BER=9.41e-03 FER=1.19e-01
2025-10-14 12:44:39,782 | INFO | Epoch 278 Train Time 20.225454330444336s

2025-10-14 12:44:59,498 | INFO | Training epoch 279, Batch 1000/1000: LR=8.23e-05, Loss=2.43e-02 BER=9.21e-03 FER=1.18e-01
2025-10-14 12:44:59,540 | INFO | Epoch 279 Train Time 19.757616758346558s

2025-10-14 12:45:19,410 | INFO | Training epoch 280, Batch 1000/1000: LR=8.22e-05, Loss=2.43e-02 BER=9.25e-03 FER=1.17e-01
2025-10-14 12:45:19,468 | INFO | Epoch 280 Train Time 19.927147150039673s

2025-10-14 12:45:38,591 | INFO | Training epoch 281, Batch 1000/1000: LR=8.21e-05, Loss=2.45e-02 BER=9.33e-03 FER=1.18e-01
2025-10-14 12:45:38,639 | INFO | Epoch 281 Train Time 19.169328689575195s

2025-10-14 12:45:56,114 | INFO | Training epoch 282, Batch 1000/1000: LR=8.19e-05, Loss=2.47e-02 BER=9.35e-03 FER=1.18e-01
2025-10-14 12:45:56,164 | INFO | Epoch 282 Train Time 17.52506732940674s

2025-10-14 12:46:14,534 | INFO | Training epoch 283, Batch 1000/1000: LR=8.18e-05, Loss=2.42e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 12:46:14,581 | INFO | Epoch 283 Train Time 18.41571593284607s

2025-10-14 12:46:33,931 | INFO | Training epoch 284, Batch 1000/1000: LR=8.17e-05, Loss=2.43e-02 BER=9.22e-03 FER=1.17e-01
2025-10-14 12:46:33,988 | INFO | Epoch 284 Train Time 19.406894207000732s

2025-10-14 12:46:53,342 | INFO | Training epoch 285, Batch 1000/1000: LR=8.16e-05, Loss=2.45e-02 BER=9.32e-03 FER=1.17e-01
2025-10-14 12:46:53,407 | INFO | Epoch 285 Train Time 19.41618013381958s

2025-10-14 12:47:12,729 | INFO | Training epoch 286, Batch 1000/1000: LR=8.14e-05, Loss=2.40e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 12:47:12,800 | INFO | Epoch 286 Train Time 19.39208173751831s

2025-10-14 12:47:12,800 | INFO | [P1] saving best_model with loss 0.024022 at epoch 286
2025-10-14 12:47:32,626 | INFO | Training epoch 287, Batch 1000/1000: LR=8.13e-05, Loss=2.42e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 12:47:32,676 | INFO | Epoch 287 Train Time 19.857882499694824s

2025-10-14 12:47:50,400 | INFO | Training epoch 288, Batch 1000/1000: LR=8.12e-05, Loss=2.47e-02 BER=9.32e-03 FER=1.18e-01
2025-10-14 12:47:50,439 | INFO | Epoch 288 Train Time 17.76148247718811s

2025-10-14 12:48:09,616 | INFO | Training epoch 289, Batch 1000/1000: LR=8.11e-05, Loss=2.43e-02 BER=9.23e-03 FER=1.17e-01
2025-10-14 12:48:09,682 | INFO | Epoch 289 Train Time 19.24199891090393s

2025-10-14 12:48:29,707 | INFO | Training epoch 290, Batch 1000/1000: LR=8.10e-05, Loss=2.41e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 12:48:29,751 | INFO | Epoch 290 Train Time 20.067678928375244s

2025-10-14 12:48:50,506 | INFO | Training epoch 291, Batch 1000/1000: LR=8.08e-05, Loss=2.44e-02 BER=9.27e-03 FER=1.17e-01
2025-10-14 12:48:50,554 | INFO | Epoch 291 Train Time 20.802404403686523s

2025-10-14 12:49:09,916 | INFO | Training epoch 292, Batch 1000/1000: LR=8.07e-05, Loss=2.46e-02 BER=9.36e-03 FER=1.18e-01
2025-10-14 12:49:09,970 | INFO | Epoch 292 Train Time 19.4144549369812s

2025-10-14 12:49:28,935 | INFO | Training epoch 293, Batch 1000/1000: LR=8.06e-05, Loss=2.42e-02 BER=9.21e-03 FER=1.16e-01
2025-10-14 12:49:28,995 | INFO | Epoch 293 Train Time 19.02473473548889s

2025-10-14 12:49:47,141 | INFO | Training epoch 294, Batch 1000/1000: LR=8.05e-05, Loss=2.43e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 12:49:47,192 | INFO | Epoch 294 Train Time 18.196198225021362s

2025-10-14 12:50:07,818 | INFO | Training epoch 295, Batch 1000/1000: LR=8.03e-05, Loss=2.40e-02 BER=9.11e-03 FER=1.16e-01
2025-10-14 12:50:07,877 | INFO | Epoch 295 Train Time 20.683342695236206s

2025-10-14 12:50:07,877 | INFO | [P1] saving best_model with loss 0.023978 at epoch 295
2025-10-14 12:50:27,112 | INFO | Training epoch 296, Batch 1000/1000: LR=8.02e-05, Loss=2.44e-02 BER=9.25e-03 FER=1.18e-01
2025-10-14 12:50:27,161 | INFO | Epoch 296 Train Time 19.26827335357666s

2025-10-14 12:50:46,890 | INFO | Training epoch 297, Batch 1000/1000: LR=8.01e-05, Loss=2.39e-02 BER=9.03e-03 FER=1.14e-01
2025-10-14 12:50:46,936 | INFO | Epoch 297 Train Time 19.77331018447876s

2025-10-14 12:50:46,936 | INFO | [P1] saving best_model with loss 0.023900 at epoch 297
2025-10-14 12:51:07,228 | INFO | Training epoch 298, Batch 1000/1000: LR=8.00e-05, Loss=2.43e-02 BER=9.20e-03 FER=1.16e-01
2025-10-14 12:51:07,297 | INFO | Epoch 298 Train Time 20.34742283821106s

2025-10-14 12:51:24,497 | INFO | Training epoch 299, Batch 1000/1000: LR=7.98e-05, Loss=2.44e-02 BER=9.33e-03 FER=1.17e-01
2025-10-14 12:51:24,543 | INFO | Epoch 299 Train Time 17.245407342910767s

2025-10-14 12:51:43,220 | INFO | Training epoch 300, Batch 1000/1000: LR=7.97e-05, Loss=2.43e-02 BER=9.19e-03 FER=1.16e-01
2025-10-14 12:51:43,275 | INFO | Epoch 300 Train Time 18.7310848236084s

2025-10-14 12:52:03,301 | INFO | Training epoch 301, Batch 1000/1000: LR=7.96e-05, Loss=2.40e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 12:52:03,345 | INFO | Epoch 301 Train Time 20.06835913658142s

2025-10-14 12:52:24,251 | INFO | Training epoch 302, Batch 1000/1000: LR=7.95e-05, Loss=2.43e-02 BER=9.22e-03 FER=1.16e-01
2025-10-14 12:52:24,299 | INFO | Epoch 302 Train Time 20.953166961669922s

2025-10-14 12:52:42,937 | INFO | Training epoch 303, Batch 1000/1000: LR=7.93e-05, Loss=2.44e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 12:52:42,991 | INFO | Epoch 303 Train Time 18.691543102264404s

2025-10-14 12:53:01,625 | INFO | Training epoch 304, Batch 1000/1000: LR=7.92e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.17e-01
2025-10-14 12:53:01,671 | INFO | Epoch 304 Train Time 18.679121494293213s

2025-10-14 12:53:21,090 | INFO | Training epoch 305, Batch 1000/1000: LR=7.91e-05, Loss=2.42e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 12:53:21,143 | INFO | Epoch 305 Train Time 19.47062587738037s

2025-10-14 12:53:39,490 | INFO | Training epoch 306, Batch 1000/1000: LR=7.90e-05, Loss=2.43e-02 BER=9.19e-03 FER=1.16e-01
2025-10-14 12:53:39,540 | INFO | Epoch 306 Train Time 18.39671564102173s

2025-10-14 12:53:57,821 | INFO | Training epoch 307, Batch 1000/1000: LR=7.88e-05, Loss=2.43e-02 BER=9.29e-03 FER=1.18e-01
2025-10-14 12:53:57,887 | INFO | Epoch 307 Train Time 18.346567630767822s

2025-10-14 12:54:17,102 | INFO | Training epoch 308, Batch 1000/1000: LR=7.87e-05, Loss=2.41e-02 BER=9.24e-03 FER=1.17e-01
2025-10-14 12:54:17,159 | INFO | Epoch 308 Train Time 19.27110743522644s

2025-10-14 12:54:36,820 | INFO | Training epoch 309, Batch 1000/1000: LR=7.86e-05, Loss=2.41e-02 BER=9.14e-03 FER=1.16e-01
2025-10-14 12:54:36,885 | INFO | Epoch 309 Train Time 19.725131273269653s

2025-10-14 12:54:56,609 | INFO | Training epoch 310, Batch 1000/1000: LR=7.85e-05, Loss=2.41e-02 BER=9.15e-03 FER=1.15e-01
2025-10-14 12:54:56,654 | INFO | Epoch 310 Train Time 19.768500566482544s

2025-10-14 12:55:17,642 | INFO | Training epoch 311, Batch 1000/1000: LR=7.83e-05, Loss=2.44e-02 BER=9.24e-03 FER=1.17e-01
2025-10-14 12:55:17,712 | INFO | Epoch 311 Train Time 21.057219743728638s

2025-10-14 12:55:35,625 | INFO | Training epoch 312, Batch 1000/1000: LR=7.82e-05, Loss=2.44e-02 BER=9.27e-03 FER=1.17e-01
2025-10-14 12:55:35,671 | INFO | Epoch 312 Train Time 17.957960844039917s

2025-10-14 12:55:54,927 | INFO | Training epoch 313, Batch 1000/1000: LR=7.81e-05, Loss=2.42e-02 BER=9.19e-03 FER=1.16e-01
2025-10-14 12:55:54,980 | INFO | Epoch 313 Train Time 19.308273792266846s

2025-10-14 12:56:15,193 | INFO | Training epoch 314, Batch 1000/1000: LR=7.79e-05, Loss=2.42e-02 BER=9.18e-03 FER=1.17e-01
2025-10-14 12:56:15,233 | INFO | Epoch 314 Train Time 20.25167942047119s

2025-10-14 12:56:34,540 | INFO | Training epoch 315, Batch 1000/1000: LR=7.78e-05, Loss=2.44e-02 BER=9.24e-03 FER=1.16e-01
2025-10-14 12:56:34,605 | INFO | Epoch 315 Train Time 19.370880842208862s

2025-10-14 12:56:54,152 | INFO | Training epoch 316, Batch 1000/1000: LR=7.77e-05, Loss=2.46e-02 BER=9.42e-03 FER=1.19e-01
2025-10-14 12:56:54,205 | INFO | Epoch 316 Train Time 19.598263025283813s

2025-10-14 12:57:14,546 | INFO | Training epoch 317, Batch 1000/1000: LR=7.75e-05, Loss=2.41e-02 BER=9.14e-03 FER=1.17e-01
2025-10-14 12:57:14,598 | INFO | Epoch 317 Train Time 20.39147973060608s

2025-10-14 12:57:34,126 | INFO | Training epoch 318, Batch 1000/1000: LR=7.74e-05, Loss=2.39e-02 BER=9.15e-03 FER=1.16e-01
2025-10-14 12:57:34,176 | INFO | Epoch 318 Train Time 19.57724380493164s

2025-10-14 12:57:52,903 | INFO | Training epoch 319, Batch 1000/1000: LR=7.73e-05, Loss=2.41e-02 BER=9.24e-03 FER=1.17e-01
2025-10-14 12:57:52,950 | INFO | Epoch 319 Train Time 18.77258324623108s

2025-10-14 12:58:10,894 | INFO | Training epoch 320, Batch 1000/1000: LR=7.72e-05, Loss=2.40e-02 BER=9.13e-03 FER=1.17e-01
2025-10-14 12:58:10,936 | INFO | Epoch 320 Train Time 17.98560857772827s

2025-10-14 12:58:31,204 | INFO | Training epoch 321, Batch 1000/1000: LR=7.70e-05, Loss=2.41e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 12:58:31,252 | INFO | Epoch 321 Train Time 20.315785884857178s

2025-10-14 12:58:51,099 | INFO | Training epoch 322, Batch 1000/1000: LR=7.69e-05, Loss=2.42e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 12:58:51,145 | INFO | Epoch 322 Train Time 19.891997575759888s

2025-10-14 12:59:10,730 | INFO | Training epoch 323, Batch 1000/1000: LR=7.68e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.15e-01
2025-10-14 12:59:10,783 | INFO | Epoch 323 Train Time 19.635807514190674s

2025-10-14 12:59:30,702 | INFO | Training epoch 324, Batch 1000/1000: LR=7.66e-05, Loss=2.39e-02 BER=9.07e-03 FER=1.16e-01
2025-10-14 12:59:30,764 | INFO | Epoch 324 Train Time 19.980201482772827s

2025-10-14 12:59:30,764 | INFO | [P1] saving best_model with loss 0.023862 at epoch 324
2025-10-14 12:59:49,527 | INFO | Training epoch 325, Batch 1000/1000: LR=7.65e-05, Loss=2.42e-02 BER=9.21e-03 FER=1.16e-01
2025-10-14 12:59:49,583 | INFO | Epoch 325 Train Time 18.79495859146118s

2025-10-14 13:00:09,632 | INFO | Training epoch 326, Batch 1000/1000: LR=7.64e-05, Loss=2.44e-02 BER=9.23e-03 FER=1.16e-01
2025-10-14 13:00:09,689 | INFO | Epoch 326 Train Time 20.106026887893677s

2025-10-14 13:00:30,133 | INFO | Training epoch 327, Batch 1000/1000: LR=7.62e-05, Loss=2.40e-02 BER=9.15e-03 FER=1.15e-01
2025-10-14 13:00:30,181 | INFO | Epoch 327 Train Time 20.49059510231018s

2025-10-14 13:00:50,019 | INFO | Training epoch 328, Batch 1000/1000: LR=7.61e-05, Loss=2.41e-02 BER=9.15e-03 FER=1.15e-01
2025-10-14 13:00:50,062 | INFO | Epoch 328 Train Time 19.880337238311768s

2025-10-14 13:01:09,616 | INFO | Training epoch 329, Batch 1000/1000: LR=7.60e-05, Loss=2.42e-02 BER=9.21e-03 FER=1.15e-01
2025-10-14 13:01:09,674 | INFO | Epoch 329 Train Time 19.610612630844116s

2025-10-14 13:01:27,216 | INFO | Training epoch 330, Batch 1000/1000: LR=7.58e-05, Loss=2.42e-02 BER=9.20e-03 FER=1.16e-01
2025-10-14 13:01:27,264 | INFO | Epoch 330 Train Time 17.589253425598145s

2025-10-14 13:01:46,110 | INFO | Training epoch 331, Batch 1000/1000: LR=7.57e-05, Loss=2.42e-02 BER=9.28e-03 FER=1.17e-01
2025-10-14 13:01:46,163 | INFO | Epoch 331 Train Time 18.89779782295227s

2025-10-14 13:02:05,908 | INFO | Training epoch 332, Batch 1000/1000: LR=7.56e-05, Loss=2.41e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 13:02:05,952 | INFO | Epoch 332 Train Time 19.78815197944641s

2025-10-14 13:02:24,234 | INFO | Training epoch 333, Batch 1000/1000: LR=7.54e-05, Loss=2.37e-02 BER=9.05e-03 FER=1.16e-01
2025-10-14 13:02:24,281 | INFO | Epoch 333 Train Time 18.32748031616211s

2025-10-14 13:02:24,281 | INFO | [P1] saving best_model with loss 0.023718 at epoch 333
2025-10-14 13:02:45,134 | INFO | Training epoch 334, Batch 1000/1000: LR=7.53e-05, Loss=2.44e-02 BER=9.28e-03 FER=1.17e-01
2025-10-14 13:02:45,185 | INFO | Epoch 334 Train Time 20.89131736755371s

2025-10-14 13:03:04,215 | INFO | Training epoch 335, Batch 1000/1000: LR=7.52e-05, Loss=2.40e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 13:03:04,270 | INFO | Epoch 335 Train Time 19.084320545196533s

2025-10-14 13:03:23,111 | INFO | Training epoch 336, Batch 1000/1000: LR=7.50e-05, Loss=2.41e-02 BER=9.25e-03 FER=1.17e-01
2025-10-14 13:03:23,160 | INFO | Epoch 336 Train Time 18.888901472091675s

2025-10-14 13:03:41,712 | INFO | Training epoch 337, Batch 1000/1000: LR=7.49e-05, Loss=2.42e-02 BER=9.23e-03 FER=1.17e-01
2025-10-14 13:03:41,759 | INFO | Epoch 337 Train Time 18.59833002090454s

2025-10-14 13:04:02,515 | INFO | Training epoch 338, Batch 1000/1000: LR=7.48e-05, Loss=2.42e-02 BER=9.22e-03 FER=1.17e-01
2025-10-14 13:04:02,581 | INFO | Epoch 338 Train Time 20.821638584136963s

2025-10-14 13:04:22,850 | INFO | Training epoch 339, Batch 1000/1000: LR=7.46e-05, Loss=2.39e-02 BER=9.08e-03 FER=1.16e-01
2025-10-14 13:04:22,895 | INFO | Epoch 339 Train Time 20.313130140304565s

2025-10-14 13:04:40,213 | INFO | Training epoch 340, Batch 1000/1000: LR=7.45e-05, Loss=2.46e-02 BER=9.37e-03 FER=1.18e-01
2025-10-14 13:04:40,252 | INFO | Epoch 340 Train Time 17.356135606765747s

2025-10-14 13:04:58,996 | INFO | Training epoch 341, Batch 1000/1000: LR=7.43e-05, Loss=2.38e-02 BER=9.08e-03 FER=1.15e-01
2025-10-14 13:04:59,042 | INFO | Epoch 341 Train Time 18.789520740509033s

2025-10-14 13:05:18,835 | INFO | Training epoch 342, Batch 1000/1000: LR=7.42e-05, Loss=2.42e-02 BER=9.19e-03 FER=1.16e-01
2025-10-14 13:05:18,891 | INFO | Epoch 342 Train Time 19.847908973693848s

2025-10-14 13:05:39,226 | INFO | Training epoch 343, Batch 1000/1000: LR=7.41e-05, Loss=2.41e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 13:05:39,273 | INFO | Epoch 343 Train Time 20.38076877593994s

2025-10-14 13:05:58,215 | INFO | Training epoch 344, Batch 1000/1000: LR=7.39e-05, Loss=2.40e-02 BER=9.10e-03 FER=1.16e-01
2025-10-14 13:05:58,267 | INFO | Epoch 344 Train Time 18.992915868759155s

2025-10-14 13:06:17,523 | INFO | Training epoch 345, Batch 1000/1000: LR=7.38e-05, Loss=2.41e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 13:06:17,576 | INFO | Epoch 345 Train Time 19.308307647705078s

2025-10-14 13:06:36,822 | INFO | Training epoch 346, Batch 1000/1000: LR=7.37e-05, Loss=2.40e-02 BER=9.14e-03 FER=1.15e-01
2025-10-14 13:06:36,872 | INFO | Epoch 346 Train Time 19.294956922531128s

2025-10-14 13:06:55,319 | INFO | Training epoch 347, Batch 1000/1000: LR=7.35e-05, Loss=2.41e-02 BER=9.22e-03 FER=1.16e-01
2025-10-14 13:06:55,364 | INFO | Epoch 347 Train Time 18.491640329360962s

2025-10-14 13:07:13,739 | INFO | Training epoch 348, Batch 1000/1000: LR=7.34e-05, Loss=2.42e-02 BER=9.17e-03 FER=1.15e-01
2025-10-14 13:07:13,788 | INFO | Epoch 348 Train Time 18.42298460006714s

2025-10-14 13:07:30,427 | INFO | Training epoch 349, Batch 1000/1000: LR=7.32e-05, Loss=2.42e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 13:07:30,486 | INFO | Epoch 349 Train Time 16.6967875957489s

2025-10-14 13:07:50,651 | INFO | Training epoch 350, Batch 1000/1000: LR=7.31e-05, Loss=2.43e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 13:07:50,729 | INFO | Epoch 350 Train Time 20.242140293121338s

2025-10-14 13:08:10,810 | INFO | Training epoch 351, Batch 1000/1000: LR=7.30e-05, Loss=2.42e-02 BER=9.20e-03 FER=1.16e-01
2025-10-14 13:08:10,855 | INFO | Epoch 351 Train Time 20.12313723564148s

2025-10-14 13:08:29,295 | INFO | Training epoch 352, Batch 1000/1000: LR=7.28e-05, Loss=2.43e-02 BER=9.20e-03 FER=1.17e-01
2025-10-14 13:08:29,344 | INFO | Epoch 352 Train Time 18.48857617378235s

2025-10-14 13:08:48,647 | INFO | Training epoch 353, Batch 1000/1000: LR=7.27e-05, Loss=2.38e-02 BER=9.03e-03 FER=1.15e-01
2025-10-14 13:08:48,713 | INFO | Epoch 353 Train Time 19.36847710609436s

2025-10-14 13:09:08,900 | INFO | Training epoch 354, Batch 1000/1000: LR=7.26e-05, Loss=2.41e-02 BER=9.09e-03 FER=1.16e-01
2025-10-14 13:09:08,954 | INFO | Epoch 354 Train Time 20.240061283111572s

2025-10-14 13:09:27,031 | INFO | Training epoch 355, Batch 1000/1000: LR=7.24e-05, Loss=2.39e-02 BER=9.08e-03 FER=1.15e-01
2025-10-14 13:09:27,086 | INFO | Epoch 355 Train Time 18.130691051483154s

2025-10-14 13:09:46,811 | INFO | Training epoch 356, Batch 1000/1000: LR=7.23e-05, Loss=2.41e-02 BER=9.12e-03 FER=1.15e-01
2025-10-14 13:09:46,860 | INFO | Epoch 356 Train Time 19.773562908172607s

2025-10-14 13:10:07,413 | INFO | Training epoch 357, Batch 1000/1000: LR=7.21e-05, Loss=2.44e-02 BER=9.34e-03 FER=1.17e-01
2025-10-14 13:10:07,459 | INFO | Epoch 357 Train Time 20.597400665283203s

2025-10-14 13:10:28,019 | INFO | Training epoch 358, Batch 1000/1000: LR=7.20e-05, Loss=2.43e-02 BER=9.24e-03 FER=1.17e-01
2025-10-14 13:10:28,068 | INFO | Epoch 358 Train Time 20.607642889022827s

2025-10-14 13:10:47,899 | INFO | Training epoch 359, Batch 1000/1000: LR=7.19e-05, Loss=2.42e-02 BER=9.25e-03 FER=1.17e-01
2025-10-14 13:10:47,943 | INFO | Epoch 359 Train Time 19.87398886680603s

2025-10-14 13:11:08,317 | INFO | Training epoch 360, Batch 1000/1000: LR=7.17e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 13:11:08,373 | INFO | Epoch 360 Train Time 20.42853355407715s

2025-10-14 13:11:28,542 | INFO | Training epoch 361, Batch 1000/1000: LR=7.16e-05, Loss=2.41e-02 BER=9.13e-03 FER=1.14e-01
2025-10-14 13:11:28,602 | INFO | Epoch 361 Train Time 20.228060245513916s

2025-10-14 13:11:47,220 | INFO | Training epoch 362, Batch 1000/1000: LR=7.14e-05, Loss=2.39e-02 BER=9.15e-03 FER=1.15e-01
2025-10-14 13:11:47,266 | INFO | Epoch 362 Train Time 18.663124561309814s

2025-10-14 13:12:07,019 | INFO | Training epoch 363, Batch 1000/1000: LR=7.13e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.15e-01
2025-10-14 13:12:07,063 | INFO | Epoch 363 Train Time 19.79655957221985s

2025-10-14 13:12:25,716 | INFO | Training epoch 364, Batch 1000/1000: LR=7.12e-05, Loss=2.37e-02 BER=9.01e-03 FER=1.14e-01
2025-10-14 13:12:25,766 | INFO | Epoch 364 Train Time 18.702636003494263s

2025-10-14 13:12:45,639 | INFO | Training epoch 365, Batch 1000/1000: LR=7.10e-05, Loss=2.41e-02 BER=9.13e-03 FER=1.16e-01
2025-10-14 13:12:45,707 | INFO | Epoch 365 Train Time 19.9390766620636s

2025-10-14 13:13:06,238 | INFO | Training epoch 366, Batch 1000/1000: LR=7.09e-05, Loss=2.47e-02 BER=9.36e-03 FER=1.18e-01
2025-10-14 13:13:06,291 | INFO | Epoch 366 Train Time 20.582821130752563s

2025-10-14 13:13:24,695 | INFO | Training epoch 367, Batch 1000/1000: LR=7.07e-05, Loss=2.40e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 13:13:24,742 | INFO | Epoch 367 Train Time 18.449912786483765s

2025-10-14 13:13:42,515 | INFO | Training epoch 368, Batch 1000/1000: LR=7.06e-05, Loss=2.44e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 13:13:42,577 | INFO | Epoch 368 Train Time 17.834837198257446s

2025-10-14 13:14:01,220 | INFO | Training epoch 369, Batch 1000/1000: LR=7.04e-05, Loss=2.39e-02 BER=9.08e-03 FER=1.16e-01
2025-10-14 13:14:01,267 | INFO | Epoch 369 Train Time 18.68942642211914s

2025-10-14 13:14:21,001 | INFO | Training epoch 370, Batch 1000/1000: LR=7.03e-05, Loss=2.37e-02 BER=9.01e-03 FER=1.15e-01
2025-10-14 13:14:21,062 | INFO | Epoch 370 Train Time 19.79401206970215s

2025-10-14 13:14:21,062 | INFO | [P1] saving best_model with loss 0.023714 at epoch 370
2025-10-14 13:14:40,334 | INFO | Training epoch 371, Batch 1000/1000: LR=7.02e-05, Loss=2.41e-02 BER=9.11e-03 FER=1.16e-01
2025-10-14 13:14:40,384 | INFO | Epoch 371 Train Time 19.30835199356079s

2025-10-14 13:14:59,513 | INFO | Training epoch 372, Batch 1000/1000: LR=7.00e-05, Loss=2.40e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 13:14:59,564 | INFO | Epoch 372 Train Time 19.17955207824707s

2025-10-14 13:15:19,827 | INFO | Training epoch 373, Batch 1000/1000: LR=6.99e-05, Loss=2.42e-02 BER=9.23e-03 FER=1.17e-01
2025-10-14 13:15:19,882 | INFO | Epoch 373 Train Time 20.31728982925415s

2025-10-14 13:15:39,308 | INFO | Training epoch 374, Batch 1000/1000: LR=6.97e-05, Loss=2.40e-02 BER=9.15e-03 FER=1.16e-01
2025-10-14 13:15:39,364 | INFO | Epoch 374 Train Time 19.481059074401855s

2025-10-14 13:15:55,925 | INFO | Training epoch 375, Batch 1000/1000: LR=6.96e-05, Loss=2.39e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 13:15:55,970 | INFO | Epoch 375 Train Time 16.604877710342407s

2025-10-14 13:16:16,116 | INFO | Training epoch 376, Batch 1000/1000: LR=6.94e-05, Loss=2.39e-02 BER=9.07e-03 FER=1.15e-01
2025-10-14 13:16:16,164 | INFO | Epoch 376 Train Time 20.194114208221436s

2025-10-14 13:16:34,993 | INFO | Training epoch 377, Batch 1000/1000: LR=6.93e-05, Loss=2.42e-02 BER=9.15e-03 FER=1.16e-01
2025-10-14 13:16:35,050 | INFO | Epoch 377 Train Time 18.88492226600647s

2025-10-14 13:16:54,905 | INFO | Training epoch 378, Batch 1000/1000: LR=6.92e-05, Loss=2.39e-02 BER=9.03e-03 FER=1.14e-01
2025-10-14 13:16:54,949 | INFO | Epoch 378 Train Time 19.89782977104187s

2025-10-14 13:17:14,629 | INFO | Training epoch 379, Batch 1000/1000: LR=6.90e-05, Loss=2.42e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 13:17:14,683 | INFO | Epoch 379 Train Time 19.73299551010132s

2025-10-14 13:17:34,443 | INFO | Training epoch 380, Batch 1000/1000: LR=6.89e-05, Loss=2.43e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 13:17:34,496 | INFO | Epoch 380 Train Time 19.812764644622803s

2025-10-14 13:17:53,508 | INFO | Training epoch 381, Batch 1000/1000: LR=6.87e-05, Loss=2.43e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 13:17:53,555 | INFO | Epoch 381 Train Time 19.057007789611816s

2025-10-14 13:18:13,115 | INFO | Training epoch 382, Batch 1000/1000: LR=6.86e-05, Loss=2.42e-02 BER=9.20e-03 FER=1.16e-01
2025-10-14 13:18:13,167 | INFO | Epoch 382 Train Time 19.6116361618042s

2025-10-14 13:18:32,505 | INFO | Training epoch 383, Batch 1000/1000: LR=6.84e-05, Loss=2.41e-02 BER=9.12e-03 FER=1.14e-01
2025-10-14 13:18:32,558 | INFO | Epoch 383 Train Time 19.389906406402588s

2025-10-14 13:18:49,575 | INFO | Training epoch 384, Batch 1000/1000: LR=6.83e-05, Loss=2.39e-02 BER=9.04e-03 FER=1.15e-01
2025-10-14 13:18:49,626 | INFO | Epoch 384 Train Time 17.067626237869263s

2025-10-14 13:19:07,948 | INFO | Training epoch 385, Batch 1000/1000: LR=6.81e-05, Loss=2.45e-02 BER=9.32e-03 FER=1.17e-01
2025-10-14 13:19:08,002 | INFO | Epoch 385 Train Time 18.374868154525757s

2025-10-14 13:19:27,531 | INFO | Training epoch 386, Batch 1000/1000: LR=6.80e-05, Loss=2.38e-02 BER=9.06e-03 FER=1.14e-01
2025-10-14 13:19:27,586 | INFO | Epoch 386 Train Time 19.582621574401855s

2025-10-14 13:19:44,510 | INFO | Training epoch 387, Batch 1000/1000: LR=6.79e-05, Loss=2.40e-02 BER=9.13e-03 FER=1.15e-01
2025-10-14 13:19:44,561 | INFO | Epoch 387 Train Time 16.973913431167603s

2025-10-14 13:20:02,603 | INFO | Training epoch 388, Batch 1000/1000: LR=6.77e-05, Loss=2.37e-02 BER=8.95e-03 FER=1.14e-01
2025-10-14 13:20:02,644 | INFO | Epoch 388 Train Time 18.082083702087402s

2025-10-14 13:20:02,644 | INFO | [P1] saving best_model with loss 0.023687 at epoch 388
2025-10-14 13:20:19,908 | INFO | Training epoch 389, Batch 1000/1000: LR=6.76e-05, Loss=2.41e-02 BER=9.20e-03 FER=1.16e-01
2025-10-14 13:20:19,954 | INFO | Epoch 389 Train Time 17.29432487487793s

2025-10-14 13:20:39,903 | INFO | Training epoch 390, Batch 1000/1000: LR=6.74e-05, Loss=2.38e-02 BER=9.04e-03 FER=1.15e-01
2025-10-14 13:20:39,952 | INFO | Epoch 390 Train Time 19.99787735939026s

2025-10-14 13:20:57,630 | INFO | Training epoch 391, Batch 1000/1000: LR=6.73e-05, Loss=2.38e-02 BER=9.07e-03 FER=1.14e-01
2025-10-14 13:20:57,690 | INFO | Epoch 391 Train Time 17.736814737319946s

2025-10-14 13:21:17,532 | INFO | Training epoch 392, Batch 1000/1000: LR=6.71e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 13:21:17,592 | INFO | Epoch 392 Train Time 19.900572061538696s

2025-10-14 13:21:17,592 | INFO | [P1] saving best_model with loss 0.023678 at epoch 392
2025-10-14 13:21:37,834 | INFO | Training epoch 393, Batch 1000/1000: LR=6.70e-05, Loss=2.43e-02 BER=9.22e-03 FER=1.15e-01
2025-10-14 13:21:37,893 | INFO | Epoch 393 Train Time 20.274617910385132s

2025-10-14 13:21:57,732 | INFO | Training epoch 394, Batch 1000/1000: LR=6.68e-05, Loss=2.38e-02 BER=9.06e-03 FER=1.15e-01
2025-10-14 13:21:57,780 | INFO | Epoch 394 Train Time 19.886297464370728s

2025-10-14 13:22:17,046 | INFO | Training epoch 395, Batch 1000/1000: LR=6.67e-05, Loss=2.41e-02 BER=9.24e-03 FER=1.15e-01
2025-10-14 13:22:17,119 | INFO | Epoch 395 Train Time 19.33872079849243s

2025-10-14 13:22:38,339 | INFO | Training epoch 396, Batch 1000/1000: LR=6.65e-05, Loss=2.39e-02 BER=9.13e-03 FER=1.16e-01
2025-10-14 13:22:38,395 | INFO | Epoch 396 Train Time 21.27526092529297s

2025-10-14 13:22:58,353 | INFO | Training epoch 397, Batch 1000/1000: LR=6.64e-05, Loss=2.41e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 13:22:58,397 | INFO | Epoch 397 Train Time 20.000322580337524s

2025-10-14 13:23:17,321 | INFO | Training epoch 398, Batch 1000/1000: LR=6.62e-05, Loss=2.43e-02 BER=9.26e-03 FER=1.17e-01
2025-10-14 13:23:17,370 | INFO | Epoch 398 Train Time 18.972229719161987s

2025-10-14 13:23:36,615 | INFO | Training epoch 399, Batch 1000/1000: LR=6.61e-05, Loss=2.39e-02 BER=9.07e-03 FER=1.14e-01
2025-10-14 13:23:36,662 | INFO | Epoch 399 Train Time 19.291216373443604s

2025-10-14 13:23:55,729 | INFO | Training epoch 400, Batch 1000/1000: LR=6.59e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 13:23:55,786 | INFO | Epoch 400 Train Time 19.123655796051025s

2025-10-14 13:24:15,321 | INFO | Training epoch 401, Batch 1000/1000: LR=6.58e-05, Loss=2.37e-02 BER=9.00e-03 FER=1.14e-01
2025-10-14 13:24:15,378 | INFO | Epoch 401 Train Time 19.591251134872437s

2025-10-14 13:24:15,379 | INFO | [P1] saving best_model with loss 0.023656 at epoch 401
2025-10-14 13:24:34,741 | INFO | Training epoch 402, Batch 1000/1000: LR=6.56e-05, Loss=2.40e-02 BER=9.10e-03 FER=1.15e-01
2025-10-14 13:24:34,789 | INFO | Epoch 402 Train Time 19.388936042785645s

2025-10-14 13:24:54,622 | INFO | Training epoch 403, Batch 1000/1000: LR=6.55e-05, Loss=2.35e-02 BER=8.96e-03 FER=1.14e-01
2025-10-14 13:24:54,677 | INFO | Epoch 403 Train Time 19.886576414108276s

2025-10-14 13:24:54,677 | INFO | [P1] saving best_model with loss 0.023474 at epoch 403
2025-10-14 13:25:14,194 | INFO | Training epoch 404, Batch 1000/1000: LR=6.54e-05, Loss=2.40e-02 BER=9.13e-03 FER=1.16e-01
2025-10-14 13:25:14,244 | INFO | Epoch 404 Train Time 19.546162605285645s

2025-10-14 13:25:33,102 | INFO | Training epoch 405, Batch 1000/1000: LR=6.52e-05, Loss=2.38e-02 BER=9.04e-03 FER=1.15e-01
2025-10-14 13:25:33,145 | INFO | Epoch 405 Train Time 18.90005612373352s

2025-10-14 13:25:52,728 | INFO | Training epoch 406, Batch 1000/1000: LR=6.51e-05, Loss=2.34e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 13:25:52,790 | INFO | Epoch 406 Train Time 19.643222332000732s

2025-10-14 13:25:52,792 | INFO | [P1] saving best_model with loss 0.023440 at epoch 406
2025-10-14 13:26:10,826 | INFO | Training epoch 407, Batch 1000/1000: LR=6.49e-05, Loss=2.38e-02 BER=9.13e-03 FER=1.14e-01
2025-10-14 13:26:10,878 | INFO | Epoch 407 Train Time 18.06968641281128s

2025-10-14 13:26:30,904 | INFO | Training epoch 408, Batch 1000/1000: LR=6.48e-05, Loss=2.39e-02 BER=9.07e-03 FER=1.15e-01
2025-10-14 13:26:30,949 | INFO | Epoch 408 Train Time 20.069153547286987s

2025-10-14 13:26:51,416 | INFO | Training epoch 409, Batch 1000/1000: LR=6.46e-05, Loss=2.40e-02 BER=9.08e-03 FER=1.15e-01
2025-10-14 13:26:51,471 | INFO | Epoch 409 Train Time 20.52151107788086s

2025-10-14 13:27:11,000 | INFO | Training epoch 410, Batch 1000/1000: LR=6.45e-05, Loss=2.41e-02 BER=9.19e-03 FER=1.16e-01
2025-10-14 13:27:11,049 | INFO | Epoch 410 Train Time 19.576752185821533s

2025-10-14 13:27:31,820 | INFO | Training epoch 411, Batch 1000/1000: LR=6.43e-05, Loss=2.35e-02 BER=8.92e-03 FER=1.13e-01
2025-10-14 13:27:31,884 | INFO | Epoch 411 Train Time 20.834119081497192s

2025-10-14 13:27:51,309 | INFO | Training epoch 412, Batch 1000/1000: LR=6.42e-05, Loss=2.38e-02 BER=9.06e-03 FER=1.14e-01
2025-10-14 13:27:51,373 | INFO | Epoch 412 Train Time 19.48896074295044s

2025-10-14 13:28:08,517 | INFO | Training epoch 413, Batch 1000/1000: LR=6.40e-05, Loss=2.42e-02 BER=9.17e-03 FER=1.16e-01
2025-10-14 13:28:08,574 | INFO | Epoch 413 Train Time 17.199227571487427s

2025-10-14 13:28:28,043 | INFO | Training epoch 414, Batch 1000/1000: LR=6.39e-05, Loss=2.38e-02 BER=9.08e-03 FER=1.14e-01
2025-10-14 13:28:28,120 | INFO | Epoch 414 Train Time 19.54549217224121s

2025-10-14 13:28:49,029 | INFO | Training epoch 415, Batch 1000/1000: LR=6.37e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 13:28:49,079 | INFO | Epoch 415 Train Time 20.958125114440918s

2025-10-14 13:28:49,080 | INFO | [P1] saving best_model with loss 0.023422 at epoch 415
2025-10-14 13:29:08,514 | INFO | Training epoch 416, Batch 1000/1000: LR=6.36e-05, Loss=2.38e-02 BER=9.03e-03 FER=1.15e-01
2025-10-14 13:29:08,563 | INFO | Epoch 416 Train Time 19.462736129760742s

2025-10-14 13:29:29,004 | INFO | Training epoch 417, Batch 1000/1000: LR=6.34e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 13:29:29,046 | INFO | Epoch 417 Train Time 20.482341289520264s

2025-10-14 13:29:47,215 | INFO | Training epoch 418, Batch 1000/1000: LR=6.33e-05, Loss=2.37e-02 BER=9.04e-03 FER=1.15e-01
2025-10-14 13:29:47,269 | INFO | Epoch 418 Train Time 18.22218346595764s

2025-10-14 13:30:04,112 | INFO | Training epoch 419, Batch 1000/1000: LR=6.31e-05, Loss=2.40e-02 BER=9.18e-03 FER=1.16e-01
2025-10-14 13:30:04,169 | INFO | Epoch 419 Train Time 16.899446725845337s

2025-10-14 13:30:24,125 | INFO | Training epoch 420, Batch 1000/1000: LR=6.30e-05, Loss=2.40e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 13:30:24,172 | INFO | Epoch 420 Train Time 20.00216054916382s

2025-10-14 13:30:43,712 | INFO | Training epoch 421, Batch 1000/1000: LR=6.28e-05, Loss=2.35e-02 BER=8.94e-03 FER=1.14e-01
2025-10-14 13:30:43,765 | INFO | Epoch 421 Train Time 19.592480421066284s

2025-10-14 13:31:02,099 | INFO | Training epoch 422, Batch 1000/1000: LR=6.27e-05, Loss=2.38e-02 BER=9.12e-03 FER=1.15e-01
2025-10-14 13:31:02,152 | INFO | Epoch 422 Train Time 18.386569023132324s

2025-10-14 13:31:21,417 | INFO | Training epoch 423, Batch 1000/1000: LR=6.25e-05, Loss=2.40e-02 BER=9.13e-03 FER=1.16e-01
2025-10-14 13:31:21,471 | INFO | Epoch 423 Train Time 19.318103551864624s

2025-10-14 13:31:42,208 | INFO | Training epoch 424, Batch 1000/1000: LR=6.24e-05, Loss=2.36e-02 BER=9.01e-03 FER=1.14e-01
2025-10-14 13:31:42,254 | INFO | Epoch 424 Train Time 20.781230449676514s

2025-10-14 13:32:01,326 | INFO | Training epoch 425, Batch 1000/1000: LR=6.22e-05, Loss=2.40e-02 BER=9.15e-03 FER=1.15e-01
2025-10-14 13:32:01,376 | INFO | Epoch 425 Train Time 19.12094807624817s

2025-10-14 13:32:21,843 | INFO | Training epoch 426, Batch 1000/1000: LR=6.21e-05, Loss=2.37e-02 BER=8.98e-03 FER=1.14e-01
2025-10-14 13:32:21,908 | INFO | Epoch 426 Train Time 20.530505418777466s

2025-10-14 13:32:42,140 | INFO | Training epoch 427, Batch 1000/1000: LR=6.19e-05, Loss=2.38e-02 BER=9.03e-03 FER=1.14e-01
2025-10-14 13:32:42,206 | INFO | Epoch 427 Train Time 20.297467708587646s

2025-10-14 13:33:01,626 | INFO | Training epoch 428, Batch 1000/1000: LR=6.18e-05, Loss=2.39e-02 BER=9.07e-03 FER=1.15e-01
2025-10-14 13:33:01,685 | INFO | Epoch 428 Train Time 19.4767644405365s

2025-10-14 13:33:20,330 | INFO | Training epoch 429, Batch 1000/1000: LR=6.16e-05, Loss=2.41e-02 BER=9.15e-03 FER=1.14e-01
2025-10-14 13:33:20,378 | INFO | Epoch 429 Train Time 18.69202733039856s

2025-10-14 13:33:38,809 | INFO | Training epoch 430, Batch 1000/1000: LR=6.14e-05, Loss=2.44e-02 BER=9.28e-03 FER=1.15e-01
2025-10-14 13:33:38,857 | INFO | Epoch 430 Train Time 18.477992057800293s

2025-10-14 13:33:58,240 | INFO | Training epoch 431, Batch 1000/1000: LR=6.13e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 13:33:58,311 | INFO | Epoch 431 Train Time 19.453205347061157s

2025-10-14 13:34:17,936 | INFO | Training epoch 432, Batch 1000/1000: LR=6.11e-05, Loss=2.41e-02 BER=9.21e-03 FER=1.16e-01
2025-10-14 13:34:17,986 | INFO | Epoch 432 Train Time 19.674310207366943s

2025-10-14 13:34:36,499 | INFO | Training epoch 433, Batch 1000/1000: LR=6.10e-05, Loss=2.40e-02 BER=9.15e-03 FER=1.16e-01
2025-10-14 13:34:36,548 | INFO | Epoch 433 Train Time 18.561346769332886s

2025-10-14 13:34:56,934 | INFO | Training epoch 434, Batch 1000/1000: LR=6.08e-05, Loss=2.39e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 13:34:57,009 | INFO | Epoch 434 Train Time 20.46003222465515s

2025-10-14 13:35:16,502 | INFO | Training epoch 435, Batch 1000/1000: LR=6.07e-05, Loss=2.41e-02 BER=9.15e-03 FER=1.16e-01
2025-10-14 13:35:16,553 | INFO | Epoch 435 Train Time 19.541945934295654s

2025-10-14 13:35:35,642 | INFO | Training epoch 436, Batch 1000/1000: LR=6.05e-05, Loss=2.36e-02 BER=8.97e-03 FER=1.14e-01
2025-10-14 13:35:35,710 | INFO | Epoch 436 Train Time 19.156920194625854s

2025-10-14 13:35:55,501 | INFO | Training epoch 437, Batch 1000/1000: LR=6.04e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 13:35:55,546 | INFO | Epoch 437 Train Time 19.834913730621338s

2025-10-14 13:36:15,099 | INFO | Training epoch 438, Batch 1000/1000: LR=6.02e-05, Loss=2.42e-02 BER=9.21e-03 FER=1.16e-01
2025-10-14 13:36:15,146 | INFO | Epoch 438 Train Time 19.598886489868164s

2025-10-14 13:36:34,947 | INFO | Training epoch 439, Batch 1000/1000: LR=6.01e-05, Loss=2.35e-02 BER=8.91e-03 FER=1.14e-01
2025-10-14 13:36:34,995 | INFO | Epoch 439 Train Time 19.848246097564697s

2025-10-14 13:36:55,791 | INFO | Training epoch 440, Batch 1000/1000: LR=5.99e-05, Loss=2.40e-02 BER=9.18e-03 FER=1.15e-01
2025-10-14 13:36:55,846 | INFO | Epoch 440 Train Time 20.849987745285034s

2025-10-14 13:37:15,189 | INFO | Training epoch 441, Batch 1000/1000: LR=5.98e-05, Loss=2.37e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 13:37:15,231 | INFO | Epoch 441 Train Time 19.385097980499268s

2025-10-14 13:37:36,216 | INFO | Training epoch 442, Batch 1000/1000: LR=5.96e-05, Loss=2.38e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 13:37:36,265 | INFO | Epoch 442 Train Time 21.03339147567749s

2025-10-14 13:37:55,796 | INFO | Training epoch 443, Batch 1000/1000: LR=5.95e-05, Loss=2.39e-02 BER=9.06e-03 FER=1.15e-01
2025-10-14 13:37:55,850 | INFO | Epoch 443 Train Time 19.583752155303955s

2025-10-14 13:38:14,022 | INFO | Training epoch 444, Batch 1000/1000: LR=5.93e-05, Loss=2.40e-02 BER=9.14e-03 FER=1.16e-01
2025-10-14 13:38:14,085 | INFO | Epoch 444 Train Time 18.233816385269165s

2025-10-14 13:38:34,840 | INFO | Training epoch 445, Batch 1000/1000: LR=5.92e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.14e-01
2025-10-14 13:38:34,889 | INFO | Epoch 445 Train Time 20.802850008010864s

2025-10-14 13:38:55,327 | INFO | Training epoch 446, Batch 1000/1000: LR=5.90e-05, Loss=2.36e-02 BER=9.03e-03 FER=1.14e-01
2025-10-14 13:38:55,377 | INFO | Epoch 446 Train Time 20.487014055252075s

2025-10-14 13:39:15,725 | INFO | Training epoch 447, Batch 1000/1000: LR=5.89e-05, Loss=2.41e-02 BER=9.24e-03 FER=1.15e-01
2025-10-14 13:39:15,779 | INFO | Epoch 447 Train Time 20.40088939666748s

2025-10-14 13:39:36,902 | INFO | Training epoch 448, Batch 1000/1000: LR=5.87e-05, Loss=2.37e-02 BER=8.99e-03 FER=1.14e-01
2025-10-14 13:39:36,945 | INFO | Epoch 448 Train Time 21.16549801826477s

2025-10-14 13:39:55,922 | INFO | Training epoch 449, Batch 1000/1000: LR=5.86e-05, Loss=2.37e-02 BER=9.00e-03 FER=1.14e-01
2025-10-14 13:39:55,970 | INFO | Epoch 449 Train Time 19.024591207504272s

2025-10-14 13:40:15,327 | INFO | Training epoch 450, Batch 1000/1000: LR=5.84e-05, Loss=2.41e-02 BER=9.16e-03 FER=1.16e-01
2025-10-14 13:40:15,381 | INFO | Epoch 450 Train Time 19.409469604492188s

2025-10-14 13:40:33,282 | INFO | Training epoch 451, Batch 1000/1000: LR=5.82e-05, Loss=2.36e-02 BER=9.01e-03 FER=1.14e-01
2025-10-14 13:40:33,328 | INFO | Epoch 451 Train Time 17.945324897766113s

2025-10-14 13:40:50,312 | INFO | Training epoch 452, Batch 1000/1000: LR=5.81e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.14e-01
2025-10-14 13:40:50,360 | INFO | Epoch 452 Train Time 17.03164315223694s

2025-10-14 13:41:08,415 | INFO | Training epoch 453, Batch 1000/1000: LR=5.79e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.14e-01
2025-10-14 13:41:08,465 | INFO | Epoch 453 Train Time 18.10420513153076s

2025-10-14 13:41:28,026 | INFO | Training epoch 454, Batch 1000/1000: LR=5.78e-05, Loss=2.38e-02 BER=9.04e-03 FER=1.14e-01
2025-10-14 13:41:28,076 | INFO | Epoch 454 Train Time 19.608949184417725s

2025-10-14 13:41:47,717 | INFO | Training epoch 455, Batch 1000/1000: LR=5.76e-05, Loss=2.36e-02 BER=8.96e-03 FER=1.14e-01
2025-10-14 13:41:47,790 | INFO | Epoch 455 Train Time 19.71329379081726s

2025-10-14 13:42:08,040 | INFO | Training epoch 456, Batch 1000/1000: LR=5.75e-05, Loss=2.36e-02 BER=8.97e-03 FER=1.14e-01
2025-10-14 13:42:08,090 | INFO | Epoch 456 Train Time 20.29905128479004s

2025-10-14 13:42:26,122 | INFO | Training epoch 457, Batch 1000/1000: LR=5.73e-05, Loss=2.39e-02 BER=9.06e-03 FER=1.15e-01
2025-10-14 13:42:26,170 | INFO | Epoch 457 Train Time 18.078938484191895s

2025-10-14 13:42:44,130 | INFO | Training epoch 458, Batch 1000/1000: LR=5.72e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.14e-01
2025-10-14 13:42:44,177 | INFO | Epoch 458 Train Time 18.006916522979736s

2025-10-14 13:43:02,514 | INFO | Training epoch 459, Batch 1000/1000: LR=5.70e-05, Loss=2.38e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 13:43:02,562 | INFO | Epoch 459 Train Time 18.38350486755371s

2025-10-14 13:43:22,140 | INFO | Training epoch 460, Batch 1000/1000: LR=5.69e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.15e-01
2025-10-14 13:43:22,198 | INFO | Epoch 460 Train Time 19.635127305984497s

2025-10-14 13:43:41,515 | INFO | Training epoch 461, Batch 1000/1000: LR=5.67e-05, Loss=2.38e-02 BER=9.09e-03 FER=1.14e-01
2025-10-14 13:43:41,562 | INFO | Epoch 461 Train Time 19.364047288894653s

2025-10-14 13:44:01,215 | INFO | Training epoch 462, Batch 1000/1000: LR=5.65e-05, Loss=2.42e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 13:44:01,268 | INFO | Epoch 462 Train Time 19.704964876174927s

2025-10-14 13:44:19,488 | INFO | Training epoch 463, Batch 1000/1000: LR=5.64e-05, Loss=2.40e-02 BER=9.12e-03 FER=1.15e-01
2025-10-14 13:44:19,540 | INFO | Epoch 463 Train Time 18.271809339523315s

2025-10-14 13:44:39,515 | INFO | Training epoch 464, Batch 1000/1000: LR=5.62e-05, Loss=2.39e-02 BER=9.06e-03 FER=1.14e-01
2025-10-14 13:44:39,564 | INFO | Epoch 464 Train Time 20.022702932357788s

2025-10-14 13:44:59,926 | INFO | Training epoch 465, Batch 1000/1000: LR=5.61e-05, Loss=2.41e-02 BER=9.18e-03 FER=1.15e-01
2025-10-14 13:44:59,979 | INFO | Epoch 465 Train Time 20.41447877883911s

2025-10-14 13:45:19,492 | INFO | Training epoch 466, Batch 1000/1000: LR=5.59e-05, Loss=2.41e-02 BER=9.23e-03 FER=1.15e-01
2025-10-14 13:45:19,538 | INFO | Epoch 466 Train Time 19.558013439178467s

2025-10-14 13:45:40,506 | INFO | Training epoch 467, Batch 1000/1000: LR=5.58e-05, Loss=2.38e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 13:45:40,555 | INFO | Epoch 467 Train Time 21.016048431396484s

2025-10-14 13:45:59,530 | INFO | Training epoch 468, Batch 1000/1000: LR=5.56e-05, Loss=2.37e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 13:45:59,581 | INFO | Epoch 468 Train Time 19.024653673171997s

2025-10-14 13:46:19,408 | INFO | Training epoch 469, Batch 1000/1000: LR=5.55e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 13:46:19,450 | INFO | Epoch 469 Train Time 19.868738651275635s

2025-10-14 13:46:39,232 | INFO | Training epoch 470, Batch 1000/1000: LR=5.53e-05, Loss=2.36e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 13:46:39,286 | INFO | Epoch 470 Train Time 19.834885120391846s

2025-10-14 13:46:58,308 | INFO | Training epoch 471, Batch 1000/1000: LR=5.52e-05, Loss=2.39e-02 BER=9.09e-03 FER=1.15e-01
2025-10-14 13:46:58,361 | INFO | Epoch 471 Train Time 19.074426412582397s

2025-10-14 13:47:19,613 | INFO | Training epoch 472, Batch 1000/1000: LR=5.50e-05, Loss=2.40e-02 BER=9.13e-03 FER=1.15e-01
2025-10-14 13:47:19,661 | INFO | Epoch 472 Train Time 21.29914355278015s

2025-10-14 13:47:39,701 | INFO | Training epoch 473, Batch 1000/1000: LR=5.48e-05, Loss=2.40e-02 BER=9.10e-03 FER=1.14e-01
2025-10-14 13:47:39,748 | INFO | Epoch 473 Train Time 20.085691690444946s

2025-10-14 13:47:59,713 | INFO | Training epoch 474, Batch 1000/1000: LR=5.47e-05, Loss=2.37e-02 BER=9.06e-03 FER=1.15e-01
2025-10-14 13:47:59,758 | INFO | Epoch 474 Train Time 20.009647130966187s

2025-10-14 13:48:17,626 | INFO | Training epoch 475, Batch 1000/1000: LR=5.45e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 13:48:17,673 | INFO | Epoch 475 Train Time 17.914153575897217s

2025-10-14 13:48:37,410 | INFO | Training epoch 476, Batch 1000/1000: LR=5.44e-05, Loss=2.38e-02 BER=9.03e-03 FER=1.14e-01
2025-10-14 13:48:37,456 | INFO | Epoch 476 Train Time 19.78190279006958s

2025-10-14 13:48:57,843 | INFO | Training epoch 477, Batch 1000/1000: LR=5.42e-05, Loss=2.38e-02 BER=9.06e-03 FER=1.14e-01
2025-10-14 13:48:57,913 | INFO | Epoch 477 Train Time 20.45618724822998s

2025-10-14 13:49:17,636 | INFO | Training epoch 478, Batch 1000/1000: LR=5.41e-05, Loss=2.38e-02 BER=9.07e-03 FER=1.14e-01
2025-10-14 13:49:17,690 | INFO | Epoch 478 Train Time 19.775899410247803s

2025-10-14 13:49:36,147 | INFO | Training epoch 479, Batch 1000/1000: LR=5.39e-05, Loss=2.39e-02 BER=9.08e-03 FER=1.14e-01
2025-10-14 13:49:36,208 | INFO | Epoch 479 Train Time 18.517164945602417s

2025-10-14 13:49:55,101 | INFO | Training epoch 480, Batch 1000/1000: LR=5.38e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 13:49:55,165 | INFO | Epoch 480 Train Time 18.95632576942444s

2025-10-14 13:50:14,932 | INFO | Training epoch 481, Batch 1000/1000: LR=5.36e-05, Loss=2.38e-02 BER=9.03e-03 FER=1.15e-01
2025-10-14 13:50:14,991 | INFO | Epoch 481 Train Time 19.824804306030273s

2025-10-14 13:50:35,145 | INFO | Training epoch 482, Batch 1000/1000: LR=5.35e-05, Loss=2.37e-02 BER=9.06e-03 FER=1.14e-01
2025-10-14 13:50:35,194 | INFO | Epoch 482 Train Time 20.202184915542603s

2025-10-14 13:50:54,412 | INFO | Training epoch 483, Batch 1000/1000: LR=5.33e-05, Loss=2.39e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 13:50:54,463 | INFO | Epoch 483 Train Time 19.266544103622437s

2025-10-14 13:51:12,001 | INFO | Training epoch 484, Batch 1000/1000: LR=5.31e-05, Loss=2.39e-02 BER=9.10e-03 FER=1.16e-01
2025-10-14 13:51:12,059 | INFO | Epoch 484 Train Time 17.59548568725586s

2025-10-14 13:51:31,105 | INFO | Training epoch 485, Batch 1000/1000: LR=5.30e-05, Loss=2.37e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 13:51:31,161 | INFO | Epoch 485 Train Time 19.10084557533264s

2025-10-14 13:51:50,538 | INFO | Training epoch 486, Batch 1000/1000: LR=5.28e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 13:51:50,596 | INFO | Epoch 486 Train Time 19.434438943862915s

2025-10-14 13:52:11,010 | INFO | Training epoch 487, Batch 1000/1000: LR=5.27e-05, Loss=2.37e-02 BER=8.96e-03 FER=1.14e-01
2025-10-14 13:52:11,058 | INFO | Epoch 487 Train Time 20.460153341293335s

2025-10-14 13:52:31,427 | INFO | Training epoch 488, Batch 1000/1000: LR=5.25e-05, Loss=2.37e-02 BER=9.06e-03 FER=1.14e-01
2025-10-14 13:52:31,475 | INFO | Epoch 488 Train Time 20.415496110916138s

2025-10-14 13:52:49,808 | INFO | Training epoch 489, Batch 1000/1000: LR=5.24e-05, Loss=2.34e-02 BER=8.89e-03 FER=1.13e-01
2025-10-14 13:52:49,854 | INFO | Epoch 489 Train Time 18.378547430038452s

2025-10-14 13:53:09,743 | INFO | Training epoch 490, Batch 1000/1000: LR=5.22e-05, Loss=2.35e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 13:53:09,798 | INFO | Epoch 490 Train Time 19.94225764274597s

2025-10-14 13:53:29,829 | INFO | Training epoch 491, Batch 1000/1000: LR=5.21e-05, Loss=2.37e-02 BER=9.01e-03 FER=1.14e-01
2025-10-14 13:53:29,876 | INFO | Epoch 491 Train Time 20.0779287815094s

2025-10-14 13:53:47,139 | INFO | Training epoch 492, Batch 1000/1000: LR=5.19e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.13e-01
2025-10-14 13:53:47,187 | INFO | Epoch 492 Train Time 17.30942678451538s

2025-10-14 13:53:47,187 | INFO | [P1] saving best_model with loss 0.023373 at epoch 492
2025-10-14 13:54:08,042 | INFO | Training epoch 493, Batch 1000/1000: LR=5.17e-05, Loss=2.40e-02 BER=9.08e-03 FER=1.15e-01
2025-10-14 13:54:08,091 | INFO | Epoch 493 Train Time 20.88924741744995s

2025-10-14 13:54:26,935 | INFO | Training epoch 494, Batch 1000/1000: LR=5.16e-05, Loss=2.38e-02 BER=9.10e-03 FER=1.14e-01
2025-10-14 13:54:26,985 | INFO | Epoch 494 Train Time 18.89185857772827s

2025-10-14 13:54:44,920 | INFO | Training epoch 495, Batch 1000/1000: LR=5.14e-05, Loss=2.39e-02 BER=9.12e-03 FER=1.15e-01
2025-10-14 13:54:44,962 | INFO | Epoch 495 Train Time 17.975374460220337s

2025-10-14 13:55:03,392 | INFO | Training epoch 496, Batch 1000/1000: LR=5.13e-05, Loss=2.36e-02 BER=8.92e-03 FER=1.14e-01
2025-10-14 13:55:03,446 | INFO | Epoch 496 Train Time 18.48302721977234s

2025-10-14 13:55:22,122 | INFO | Training epoch 497, Batch 1000/1000: LR=5.11e-05, Loss=2.36e-02 BER=8.96e-03 FER=1.14e-01
2025-10-14 13:55:22,178 | INFO | Epoch 497 Train Time 18.731569528579712s

2025-10-14 13:55:40,927 | INFO | Training epoch 498, Batch 1000/1000: LR=5.10e-05, Loss=2.38e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 13:55:40,973 | INFO | Epoch 498 Train Time 18.794228553771973s

2025-10-14 13:55:59,816 | INFO | Training epoch 499, Batch 1000/1000: LR=5.08e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.12e-01
2025-10-14 13:55:59,859 | INFO | Epoch 499 Train Time 18.884532690048218s

2025-10-14 13:56:19,484 | INFO | Training epoch 500, Batch 1000/1000: LR=5.07e-05, Loss=2.34e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 13:56:19,555 | INFO | Epoch 500 Train Time 19.695273399353027s

2025-10-14 13:56:40,039 | INFO | Training epoch 501, Batch 1000/1000: LR=5.05e-05, Loss=2.37e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 13:56:40,089 | INFO | Epoch 501 Train Time 20.5339994430542s

2025-10-14 13:56:58,714 | INFO | Training epoch 502, Batch 1000/1000: LR=5.03e-05, Loss=2.37e-02 BER=9.10e-03 FER=1.14e-01
2025-10-14 13:56:58,760 | INFO | Epoch 502 Train Time 18.6694438457489s

2025-10-14 13:57:17,218 | INFO | Training epoch 503, Batch 1000/1000: LR=5.02e-05, Loss=2.34e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 13:57:17,267 | INFO | Epoch 503 Train Time 18.505079984664917s

2025-10-14 13:57:17,267 | INFO | [P1] saving best_model with loss 0.023362 at epoch 503
2025-10-14 13:57:36,991 | INFO | Training epoch 504, Batch 1000/1000: LR=5.00e-05, Loss=2.36e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 13:57:37,035 | INFO | Epoch 504 Train Time 19.750884294509888s

2025-10-14 13:57:54,603 | INFO | Training epoch 505, Batch 1000/1000: LR=4.99e-05, Loss=2.34e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 13:57:54,651 | INFO | Epoch 505 Train Time 17.61501121520996s

2025-10-14 13:58:14,538 | INFO | Training epoch 506, Batch 1000/1000: LR=4.97e-05, Loss=2.37e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 13:58:14,592 | INFO | Epoch 506 Train Time 19.94041347503662s

2025-10-14 13:58:33,136 | INFO | Training epoch 507, Batch 1000/1000: LR=4.96e-05, Loss=2.38e-02 BER=9.07e-03 FER=1.14e-01
2025-10-14 13:58:33,208 | INFO | Epoch 507 Train Time 18.615197896957397s

2025-10-14 13:58:52,120 | INFO | Training epoch 508, Batch 1000/1000: LR=4.94e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.12e-01
2025-10-14 13:58:52,173 | INFO | Epoch 508 Train Time 18.963703870773315s

2025-10-14 13:59:12,146 | INFO | Training epoch 509, Batch 1000/1000: LR=4.93e-05, Loss=2.35e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 13:59:12,200 | INFO | Epoch 509 Train Time 20.02564263343811s

2025-10-14 13:59:31,311 | INFO | Training epoch 510, Batch 1000/1000: LR=4.91e-05, Loss=2.39e-02 BER=9.14e-03 FER=1.15e-01
2025-10-14 13:59:31,365 | INFO | Epoch 510 Train Time 19.164960384368896s

2025-10-14 13:59:51,616 | INFO | Training epoch 511, Batch 1000/1000: LR=4.89e-05, Loss=2.34e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 13:59:51,672 | INFO | Epoch 511 Train Time 20.304912567138672s

2025-10-14 13:59:51,672 | INFO | [P1] saving best_model with loss 0.023355 at epoch 511
2025-10-14 14:00:10,234 | INFO | Training epoch 512, Batch 1000/1000: LR=4.88e-05, Loss=2.34e-02 BER=8.95e-03 FER=1.14e-01
2025-10-14 14:00:10,283 | INFO | Epoch 512 Train Time 18.587661027908325s

2025-10-14 14:00:29,028 | INFO | Training epoch 513, Batch 1000/1000: LR=4.86e-05, Loss=2.40e-02 BER=9.15e-03 FER=1.14e-01
2025-10-14 14:00:29,086 | INFO | Epoch 513 Train Time 18.80196785926819s

2025-10-14 14:00:48,137 | INFO | Training epoch 514, Batch 1000/1000: LR=4.85e-05, Loss=2.39e-02 BER=9.11e-03 FER=1.15e-01
2025-10-14 14:00:48,186 | INFO | Epoch 514 Train Time 19.099876642227173s

2025-10-14 14:01:07,405 | INFO | Training epoch 515, Batch 1000/1000: LR=4.83e-05, Loss=2.38e-02 BER=9.06e-03 FER=1.15e-01
2025-10-14 14:01:07,457 | INFO | Epoch 515 Train Time 19.269530534744263s

2025-10-14 14:01:26,100 | INFO | Training epoch 516, Batch 1000/1000: LR=4.82e-05, Loss=2.40e-02 BER=9.19e-03 FER=1.15e-01
2025-10-14 14:01:26,146 | INFO | Epoch 516 Train Time 18.689212799072266s

2025-10-14 14:01:43,434 | INFO | Training epoch 517, Batch 1000/1000: LR=4.80e-05, Loss=2.38e-02 BER=9.07e-03 FER=1.14e-01
2025-10-14 14:01:43,487 | INFO | Epoch 517 Train Time 17.339845180511475s

2025-10-14 14:02:02,999 | INFO | Training epoch 518, Batch 1000/1000: LR=4.79e-05, Loss=2.36e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 14:02:03,047 | INFO | Epoch 518 Train Time 19.559051275253296s

2025-10-14 14:02:22,321 | INFO | Training epoch 519, Batch 1000/1000: LR=4.77e-05, Loss=2.33e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 14:02:22,371 | INFO | Epoch 519 Train Time 19.323185205459595s

2025-10-14 14:02:22,371 | INFO | [P1] saving best_model with loss 0.023256 at epoch 519
2025-10-14 14:02:42,650 | INFO | Training epoch 520, Batch 1000/1000: LR=4.75e-05, Loss=2.37e-02 BER=9.06e-03 FER=1.15e-01
2025-10-14 14:02:42,718 | INFO | Epoch 520 Train Time 20.33065891265869s

2025-10-14 14:03:02,591 | INFO | Training epoch 521, Batch 1000/1000: LR=4.74e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.12e-01
2025-10-14 14:03:02,642 | INFO | Epoch 521 Train Time 19.92336416244507s

2025-10-14 14:03:21,334 | INFO | Training epoch 522, Batch 1000/1000: LR=4.72e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 14:03:21,379 | INFO | Epoch 522 Train Time 18.736157417297363s

2025-10-14 14:03:38,520 | INFO | Training epoch 523, Batch 1000/1000: LR=4.71e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.11e-01
2025-10-14 14:03:38,572 | INFO | Epoch 523 Train Time 17.192211389541626s

2025-10-14 14:03:55,406 | INFO | Training epoch 524, Batch 1000/1000: LR=4.69e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.13e-01
2025-10-14 14:03:55,452 | INFO | Epoch 524 Train Time 16.879518270492554s

2025-10-14 14:04:14,607 | INFO | Training epoch 525, Batch 1000/1000: LR=4.68e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.13e-01
2025-10-14 14:04:14,659 | INFO | Epoch 525 Train Time 19.205920219421387s

2025-10-14 14:04:31,625 | INFO | Training epoch 526, Batch 1000/1000: LR=4.66e-05, Loss=2.38e-02 BER=9.10e-03 FER=1.15e-01
2025-10-14 14:04:31,671 | INFO | Epoch 526 Train Time 17.01121711730957s

2025-10-14 14:04:50,235 | INFO | Training epoch 527, Batch 1000/1000: LR=4.65e-05, Loss=2.38e-02 BER=9.09e-03 FER=1.14e-01
2025-10-14 14:04:50,293 | INFO | Epoch 527 Train Time 18.62104821205139s

2025-10-14 14:05:10,032 | INFO | Training epoch 528, Batch 1000/1000: LR=4.63e-05, Loss=2.40e-02 BER=9.15e-03 FER=1.15e-01
2025-10-14 14:05:10,096 | INFO | Epoch 528 Train Time 19.801490783691406s

2025-10-14 14:05:28,322 | INFO | Training epoch 529, Batch 1000/1000: LR=4.62e-05, Loss=2.36e-02 BER=9.05e-03 FER=1.13e-01
2025-10-14 14:05:28,380 | INFO | Epoch 529 Train Time 18.283169984817505s

2025-10-14 14:05:46,711 | INFO | Training epoch 530, Batch 1000/1000: LR=4.60e-05, Loss=2.42e-02 BER=9.30e-03 FER=1.16e-01
2025-10-14 14:05:46,757 | INFO | Epoch 530 Train Time 18.37685990333557s

2025-10-14 14:06:05,388 | INFO | Training epoch 531, Batch 1000/1000: LR=4.58e-05, Loss=2.38e-02 BER=9.10e-03 FER=1.14e-01
2025-10-14 14:06:05,431 | INFO | Epoch 531 Train Time 18.67276358604431s

2025-10-14 14:06:25,001 | INFO | Training epoch 532, Batch 1000/1000: LR=4.57e-05, Loss=2.36e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 14:06:25,054 | INFO | Epoch 532 Train Time 19.621373891830444s

2025-10-14 14:06:42,992 | INFO | Training epoch 533, Batch 1000/1000: LR=4.55e-05, Loss=2.35e-02 BER=8.91e-03 FER=1.13e-01
2025-10-14 14:06:43,041 | INFO | Epoch 533 Train Time 17.986807346343994s

2025-10-14 14:07:02,442 | INFO | Training epoch 534, Batch 1000/1000: LR=4.54e-05, Loss=2.37e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 14:07:02,500 | INFO | Epoch 534 Train Time 19.457797050476074s

2025-10-14 14:07:19,710 | INFO | Training epoch 535, Batch 1000/1000: LR=4.52e-05, Loss=2.36e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 14:07:19,761 | INFO | Epoch 535 Train Time 17.260639667510986s

2025-10-14 14:07:39,545 | INFO | Training epoch 536, Batch 1000/1000: LR=4.51e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:07:39,584 | INFO | Epoch 536 Train Time 19.82261061668396s

2025-10-14 14:07:58,728 | INFO | Training epoch 537, Batch 1000/1000: LR=4.49e-05, Loss=2.37e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 14:07:58,783 | INFO | Epoch 537 Train Time 19.198256492614746s

2025-10-14 14:08:17,410 | INFO | Training epoch 538, Batch 1000/1000: LR=4.48e-05, Loss=2.35e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 14:08:17,471 | INFO | Epoch 538 Train Time 18.68658757209778s

2025-10-14 14:08:37,314 | INFO | Training epoch 539, Batch 1000/1000: LR=4.46e-05, Loss=2.34e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 14:08:37,375 | INFO | Epoch 539 Train Time 19.903745889663696s

2025-10-14 14:08:56,406 | INFO | Training epoch 540, Batch 1000/1000: LR=4.45e-05, Loss=2.37e-02 BER=9.08e-03 FER=1.14e-01
2025-10-14 14:08:56,461 | INFO | Epoch 540 Train Time 19.084857940673828s

2025-10-14 14:09:16,344 | INFO | Training epoch 541, Batch 1000/1000: LR=4.43e-05, Loss=2.34e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 14:09:16,396 | INFO | Epoch 541 Train Time 19.93377375602722s

2025-10-14 14:09:36,224 | INFO | Training epoch 542, Batch 1000/1000: LR=4.41e-05, Loss=2.36e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 14:09:36,274 | INFO | Epoch 542 Train Time 19.8767671585083s

2025-10-14 14:09:54,136 | INFO | Training epoch 543, Batch 1000/1000: LR=4.40e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 14:09:54,189 | INFO | Epoch 543 Train Time 17.91448402404785s

2025-10-14 14:10:10,901 | INFO | Training epoch 544, Batch 1000/1000: LR=4.38e-05, Loss=2.35e-02 BER=8.94e-03 FER=1.13e-01
2025-10-14 14:10:10,938 | INFO | Epoch 544 Train Time 16.746440887451172s

2025-10-14 14:10:30,237 | INFO | Training epoch 545, Batch 1000/1000: LR=4.37e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 14:10:30,284 | INFO | Epoch 545 Train Time 19.345844745635986s

2025-10-14 14:10:50,434 | INFO | Training epoch 546, Batch 1000/1000: LR=4.35e-05, Loss=2.35e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 14:10:50,480 | INFO | Epoch 546 Train Time 20.19476842880249s

2025-10-14 14:11:09,093 | INFO | Training epoch 547, Batch 1000/1000: LR=4.34e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 14:11:09,132 | INFO | Epoch 547 Train Time 18.65069341659546s

2025-10-14 14:11:28,928 | INFO | Training epoch 548, Batch 1000/1000: LR=4.32e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.13e-01
2025-10-14 14:11:28,984 | INFO | Epoch 548 Train Time 19.85124158859253s

2025-10-14 14:11:48,396 | INFO | Training epoch 549, Batch 1000/1000: LR=4.31e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:11:48,440 | INFO | Epoch 549 Train Time 19.45479917526245s

2025-10-14 14:12:07,635 | INFO | Training epoch 550, Batch 1000/1000: LR=4.29e-05, Loss=2.39e-02 BER=9.11e-03 FER=1.14e-01
2025-10-14 14:12:07,688 | INFO | Epoch 550 Train Time 19.246726274490356s

2025-10-14 14:12:27,121 | INFO | Training epoch 551, Batch 1000/1000: LR=4.28e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.14e-01
2025-10-14 14:12:27,184 | INFO | Epoch 551 Train Time 19.4941086769104s

2025-10-14 14:12:46,440 | INFO | Training epoch 552, Batch 1000/1000: LR=4.26e-05, Loss=2.32e-02 BER=8.79e-03 FER=1.11e-01
2025-10-14 14:12:46,486 | INFO | Epoch 552 Train Time 19.301457405090332s

2025-10-14 14:12:46,487 | INFO | [P1] saving best_model with loss 0.023226 at epoch 552
2025-10-14 14:13:06,239 | INFO | Training epoch 553, Batch 1000/1000: LR=4.24e-05, Loss=2.36e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 14:13:06,299 | INFO | Epoch 553 Train Time 19.794198036193848s

2025-10-14 14:13:25,520 | INFO | Training epoch 554, Batch 1000/1000: LR=4.23e-05, Loss=2.35e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:13:25,568 | INFO | Epoch 554 Train Time 19.267956495285034s

2025-10-14 14:13:44,210 | INFO | Training epoch 555, Batch 1000/1000: LR=4.21e-05, Loss=2.31e-02 BER=8.79e-03 FER=1.11e-01
2025-10-14 14:13:44,257 | INFO | Epoch 555 Train Time 18.68811798095703s

2025-10-14 14:13:44,257 | INFO | [P1] saving best_model with loss 0.023088 at epoch 555
2025-10-14 14:14:02,886 | INFO | Training epoch 556, Batch 1000/1000: LR=4.20e-05, Loss=2.37e-02 BER=9.00e-03 FER=1.14e-01
2025-10-14 14:14:02,929 | INFO | Epoch 556 Train Time 18.657671689987183s

2025-10-14 14:14:22,517 | INFO | Training epoch 557, Batch 1000/1000: LR=4.18e-05, Loss=2.38e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 14:14:22,580 | INFO | Epoch 557 Train Time 19.65051245689392s

2025-10-14 14:14:42,811 | INFO | Training epoch 558, Batch 1000/1000: LR=4.17e-05, Loss=2.34e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 14:14:42,870 | INFO | Epoch 558 Train Time 20.288974285125732s

2025-10-14 14:15:03,213 | INFO | Training epoch 559, Batch 1000/1000: LR=4.15e-05, Loss=2.34e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 14:15:03,271 | INFO | Epoch 559 Train Time 20.399688243865967s

2025-10-14 14:15:21,803 | INFO | Training epoch 560, Batch 1000/1000: LR=4.14e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 14:15:21,850 | INFO | Epoch 560 Train Time 18.57838487625122s

2025-10-14 14:15:40,210 | INFO | Training epoch 561, Batch 1000/1000: LR=4.12e-05, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 14:15:40,258 | INFO | Epoch 561 Train Time 18.407729625701904s

2025-10-14 14:15:59,721 | INFO | Training epoch 562, Batch 1000/1000: LR=4.11e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:15:59,779 | INFO | Epoch 562 Train Time 19.518753051757812s

2025-10-14 14:16:19,512 | INFO | Training epoch 563, Batch 1000/1000: LR=4.09e-05, Loss=2.37e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:16:19,571 | INFO | Epoch 563 Train Time 19.791215896606445s

2025-10-14 14:16:39,429 | INFO | Training epoch 564, Batch 1000/1000: LR=4.08e-05, Loss=2.37e-02 BER=9.04e-03 FER=1.14e-01
2025-10-14 14:16:39,480 | INFO | Epoch 564 Train Time 19.90791893005371s

2025-10-14 14:16:59,688 | INFO | Training epoch 565, Batch 1000/1000: LR=4.06e-05, Loss=2.40e-02 BER=9.12e-03 FER=1.14e-01
2025-10-14 14:16:59,736 | INFO | Epoch 565 Train Time 20.255547523498535s

2025-10-14 14:17:19,722 | INFO | Training epoch 566, Batch 1000/1000: LR=4.05e-05, Loss=2.37e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 14:17:19,770 | INFO | Epoch 566 Train Time 20.033583402633667s

2025-10-14 14:17:40,031 | INFO | Training epoch 567, Batch 1000/1000: LR=4.03e-05, Loss=2.38e-02 BER=9.08e-03 FER=1.13e-01
2025-10-14 14:17:40,079 | INFO | Epoch 567 Train Time 20.307311534881592s

2025-10-14 14:18:00,064 | INFO | Training epoch 568, Batch 1000/1000: LR=4.02e-05, Loss=2.34e-02 BER=8.87e-03 FER=1.11e-01
2025-10-14 14:18:00,131 | INFO | Epoch 568 Train Time 20.0508131980896s

2025-10-14 14:18:20,218 | INFO | Training epoch 569, Batch 1000/1000: LR=4.00e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.13e-01
2025-10-14 14:18:20,268 | INFO | Epoch 569 Train Time 20.136166095733643s

2025-10-14 14:18:40,428 | INFO | Training epoch 570, Batch 1000/1000: LR=3.99e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:18:40,480 | INFO | Epoch 570 Train Time 20.211583137512207s

2025-10-14 14:18:59,025 | INFO | Training epoch 571, Batch 1000/1000: LR=3.97e-05, Loss=2.38e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 14:18:59,071 | INFO | Epoch 571 Train Time 18.589725017547607s

2025-10-14 14:19:18,633 | INFO | Training epoch 572, Batch 1000/1000: LR=3.96e-05, Loss=2.38e-02 BER=9.09e-03 FER=1.14e-01
2025-10-14 14:19:18,679 | INFO | Epoch 572 Train Time 19.607203245162964s

2025-10-14 14:19:36,945 | INFO | Training epoch 573, Batch 1000/1000: LR=3.94e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.13e-01
2025-10-14 14:19:36,996 | INFO | Epoch 573 Train Time 18.316545486450195s

2025-10-14 14:19:55,728 | INFO | Training epoch 574, Batch 1000/1000: LR=3.92e-05, Loss=2.38e-02 BER=9.10e-03 FER=1.15e-01
2025-10-14 14:19:55,779 | INFO | Epoch 574 Train Time 18.7816481590271s

2025-10-14 14:20:15,206 | INFO | Training epoch 575, Batch 1000/1000: LR=3.91e-05, Loss=2.33e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 14:20:15,246 | INFO | Epoch 575 Train Time 19.46714758872986s

2025-10-14 14:20:35,115 | INFO | Training epoch 576, Batch 1000/1000: LR=3.89e-05, Loss=2.40e-02 BER=9.11e-03 FER=1.14e-01
2025-10-14 14:20:35,165 | INFO | Epoch 576 Train Time 19.917896270751953s

2025-10-14 14:20:52,620 | INFO | Training epoch 577, Batch 1000/1000: LR=3.88e-05, Loss=2.32e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 14:20:52,666 | INFO | Epoch 577 Train Time 17.49973773956299s

2025-10-14 14:21:13,511 | INFO | Training epoch 578, Batch 1000/1000: LR=3.86e-05, Loss=2.36e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 14:21:13,563 | INFO | Epoch 578 Train Time 20.89685606956482s

2025-10-14 14:21:32,103 | INFO | Training epoch 579, Batch 1000/1000: LR=3.85e-05, Loss=2.33e-02 BER=8.91e-03 FER=1.12e-01
2025-10-14 14:21:32,167 | INFO | Epoch 579 Train Time 18.60269260406494s

2025-10-14 14:21:50,496 | INFO | Training epoch 580, Batch 1000/1000: LR=3.83e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:21:50,544 | INFO | Epoch 580 Train Time 18.376129150390625s

2025-10-14 14:22:09,505 | INFO | Training epoch 581, Batch 1000/1000: LR=3.82e-05, Loss=2.34e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 14:22:09,556 | INFO | Epoch 581 Train Time 19.011134147644043s

2025-10-14 14:22:29,234 | INFO | Training epoch 582, Batch 1000/1000: LR=3.80e-05, Loss=2.36e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 14:22:29,307 | INFO | Epoch 582 Train Time 19.750386238098145s

2025-10-14 14:22:47,102 | INFO | Training epoch 583, Batch 1000/1000: LR=3.79e-05, Loss=2.34e-02 BER=8.92e-03 FER=1.13e-01
2025-10-14 14:22:47,147 | INFO | Epoch 583 Train Time 17.838298320770264s

2025-10-14 14:23:07,132 | INFO | Training epoch 584, Batch 1000/1000: LR=3.77e-05, Loss=2.37e-02 BER=8.97e-03 FER=1.14e-01
2025-10-14 14:23:07,180 | INFO | Epoch 584 Train Time 20.032167434692383s

2025-10-14 14:23:26,442 | INFO | Training epoch 585, Batch 1000/1000: LR=3.76e-05, Loss=2.36e-02 BER=8.94e-03 FER=1.13e-01
2025-10-14 14:23:26,487 | INFO | Epoch 585 Train Time 19.30571460723877s

2025-10-14 14:23:46,207 | INFO | Training epoch 586, Batch 1000/1000: LR=3.74e-05, Loss=2.35e-02 BER=8.92e-03 FER=1.13e-01
2025-10-14 14:23:46,259 | INFO | Epoch 586 Train Time 19.770764350891113s

2025-10-14 14:24:05,924 | INFO | Training epoch 587, Batch 1000/1000: LR=3.73e-05, Loss=2.35e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 14:24:05,968 | INFO | Epoch 587 Train Time 19.708137273788452s

2025-10-14 14:24:26,621 | INFO | Training epoch 588, Batch 1000/1000: LR=3.71e-05, Loss=2.32e-02 BER=8.89e-03 FER=1.13e-01
2025-10-14 14:24:26,665 | INFO | Epoch 588 Train Time 20.696494102478027s

2025-10-14 14:24:46,724 | INFO | Training epoch 589, Batch 1000/1000: LR=3.70e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.13e-01
2025-10-14 14:24:46,774 | INFO | Epoch 589 Train Time 20.108194589614868s

2025-10-14 14:25:07,015 | INFO | Training epoch 590, Batch 1000/1000: LR=3.68e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.11e-01
2025-10-14 14:25:07,072 | INFO | Epoch 590 Train Time 20.297404527664185s

2025-10-14 14:25:26,601 | INFO | Training epoch 591, Batch 1000/1000: LR=3.67e-05, Loss=2.37e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:25:26,664 | INFO | Epoch 591 Train Time 19.59106206893921s

2025-10-14 14:25:45,771 | INFO | Training epoch 592, Batch 1000/1000: LR=3.65e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 14:25:45,820 | INFO | Epoch 592 Train Time 19.15543818473816s

2025-10-14 14:26:04,541 | INFO | Training epoch 593, Batch 1000/1000: LR=3.64e-05, Loss=2.33e-02 BER=8.82e-03 FER=1.12e-01
2025-10-14 14:26:04,605 | INFO | Epoch 593 Train Time 18.78289031982422s

2025-10-14 14:26:24,320 | INFO | Training epoch 594, Batch 1000/1000: LR=3.62e-05, Loss=2.32e-02 BER=8.83e-03 FER=1.12e-01
2025-10-14 14:26:24,374 | INFO | Epoch 594 Train Time 19.767762184143066s

2025-10-14 14:26:43,431 | INFO | Training epoch 595, Batch 1000/1000: LR=3.61e-05, Loss=2.38e-02 BER=9.11e-03 FER=1.14e-01
2025-10-14 14:26:43,505 | INFO | Epoch 595 Train Time 19.13014578819275s

2025-10-14 14:27:03,336 | INFO | Training epoch 596, Batch 1000/1000: LR=3.59e-05, Loss=2.37e-02 BER=9.02e-03 FER=1.14e-01
2025-10-14 14:27:03,392 | INFO | Epoch 596 Train Time 19.885257959365845s

2025-10-14 14:27:23,333 | INFO | Training epoch 597, Batch 1000/1000: LR=3.58e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 14:27:23,386 | INFO | Epoch 597 Train Time 19.99217176437378s

2025-10-14 14:27:44,017 | INFO | Training epoch 598, Batch 1000/1000: LR=3.56e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:27:44,072 | INFO | Epoch 598 Train Time 20.685088872909546s

2025-10-14 14:28:04,025 | INFO | Training epoch 599, Batch 1000/1000: LR=3.55e-05, Loss=2.35e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 14:28:04,085 | INFO | Epoch 599 Train Time 20.01127862930298s

2025-10-14 14:28:23,449 | INFO | Training epoch 600, Batch 1000/1000: LR=3.54e-05, Loss=2.36e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 14:28:23,496 | INFO | Epoch 600 Train Time 19.409451007843018s

2025-10-14 14:28:42,692 | INFO | Training epoch 601, Batch 1000/1000: LR=3.52e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.12e-01
2025-10-14 14:28:42,764 | INFO | Epoch 601 Train Time 19.26711869239807s

2025-10-14 14:29:01,928 | INFO | Training epoch 602, Batch 1000/1000: LR=3.51e-05, Loss=2.35e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 14:29:01,977 | INFO | Epoch 602 Train Time 19.21243667602539s

2025-10-14 14:29:21,720 | INFO | Training epoch 603, Batch 1000/1000: LR=3.49e-05, Loss=2.35e-02 BER=8.92e-03 FER=1.13e-01
2025-10-14 14:29:21,767 | INFO | Epoch 603 Train Time 19.789490938186646s

2025-10-14 14:29:41,437 | INFO | Training epoch 604, Batch 1000/1000: LR=3.48e-05, Loss=2.35e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:29:41,487 | INFO | Epoch 604 Train Time 19.719176292419434s

2025-10-14 14:30:00,703 | INFO | Training epoch 605, Batch 1000/1000: LR=3.46e-05, Loss=2.36e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 14:30:00,755 | INFO | Epoch 605 Train Time 19.26611638069153s

2025-10-14 14:30:20,328 | INFO | Training epoch 606, Batch 1000/1000: LR=3.45e-05, Loss=2.31e-02 BER=8.82e-03 FER=1.12e-01
2025-10-14 14:30:20,382 | INFO | Epoch 606 Train Time 19.626320362091064s

2025-10-14 14:30:20,383 | INFO | [P1] saving best_model with loss 0.023085 at epoch 606
2025-10-14 14:30:40,216 | INFO | Training epoch 607, Batch 1000/1000: LR=3.43e-05, Loss=2.35e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 14:30:40,262 | INFO | Epoch 607 Train Time 19.85328960418701s

2025-10-14 14:31:00,844 | INFO | Training epoch 608, Batch 1000/1000: LR=3.42e-05, Loss=2.38e-02 BER=9.13e-03 FER=1.14e-01
2025-10-14 14:31:00,886 | INFO | Epoch 608 Train Time 20.622690439224243s

2025-10-14 14:31:19,402 | INFO | Training epoch 609, Batch 1000/1000: LR=3.40e-05, Loss=2.36e-02 BER=8.99e-03 FER=1.14e-01
2025-10-14 14:31:19,449 | INFO | Epoch 609 Train Time 18.562227725982666s

2025-10-14 14:31:39,434 | INFO | Training epoch 610, Batch 1000/1000: LR=3.39e-05, Loss=2.33e-02 BER=8.94e-03 FER=1.13e-01
2025-10-14 14:31:39,487 | INFO | Epoch 610 Train Time 20.036807537078857s

2025-10-14 14:31:59,020 | INFO | Training epoch 611, Batch 1000/1000: LR=3.37e-05, Loss=2.36e-02 BER=9.04e-03 FER=1.13e-01
2025-10-14 14:31:59,076 | INFO | Epoch 611 Train Time 19.589092254638672s

2025-10-14 14:32:18,841 | INFO | Training epoch 612, Batch 1000/1000: LR=3.36e-05, Loss=2.34e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 14:32:18,892 | INFO | Epoch 612 Train Time 19.815064907073975s

2025-10-14 14:32:35,112 | INFO | Training epoch 613, Batch 1000/1000: LR=3.34e-05, Loss=2.36e-02 BER=8.98e-03 FER=1.12e-01
2025-10-14 14:32:35,160 | INFO | Epoch 613 Train Time 16.267367362976074s

2025-10-14 14:32:54,714 | INFO | Training epoch 614, Batch 1000/1000: LR=3.33e-05, Loss=2.34e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 14:32:54,772 | INFO | Epoch 614 Train Time 19.610633373260498s

2025-10-14 14:33:15,181 | INFO | Training epoch 615, Batch 1000/1000: LR=3.31e-05, Loss=2.31e-02 BER=8.79e-03 FER=1.11e-01
2025-10-14 14:33:15,222 | INFO | Epoch 615 Train Time 20.44925355911255s

2025-10-14 14:33:33,978 | INFO | Training epoch 616, Batch 1000/1000: LR=3.30e-05, Loss=2.35e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 14:33:34,029 | INFO | Epoch 616 Train Time 18.806177854537964s

2025-10-14 14:33:51,725 | INFO | Training epoch 617, Batch 1000/1000: LR=3.29e-05, Loss=2.32e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 14:33:51,779 | INFO | Epoch 617 Train Time 17.748770236968994s

2025-10-14 14:34:11,216 | INFO | Training epoch 618, Batch 1000/1000: LR=3.27e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:34:11,265 | INFO | Epoch 618 Train Time 19.48515558242798s

2025-10-14 14:34:31,535 | INFO | Training epoch 619, Batch 1000/1000: LR=3.26e-05, Loss=2.34e-02 BER=8.97e-03 FER=1.14e-01
2025-10-14 14:34:31,598 | INFO | Epoch 619 Train Time 20.331876277923584s

2025-10-14 14:34:50,640 | INFO | Training epoch 620, Batch 1000/1000: LR=3.24e-05, Loss=2.36e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 14:34:50,703 | INFO | Epoch 620 Train Time 19.102274656295776s

2025-10-14 14:35:07,018 | INFO | Training epoch 621, Batch 1000/1000: LR=3.23e-05, Loss=2.35e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:35:07,067 | INFO | Epoch 621 Train Time 16.36216926574707s

2025-10-14 14:35:26,914 | INFO | Training epoch 622, Batch 1000/1000: LR=3.21e-05, Loss=2.36e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:35:26,964 | INFO | Epoch 622 Train Time 19.89681100845337s

2025-10-14 14:35:47,229 | INFO | Training epoch 623, Batch 1000/1000: LR=3.20e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 14:35:47,284 | INFO | Epoch 623 Train Time 20.319607734680176s

2025-10-14 14:36:04,099 | INFO | Training epoch 624, Batch 1000/1000: LR=3.18e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 14:36:04,148 | INFO | Epoch 624 Train Time 16.86274790763855s

2025-10-14 14:36:24,412 | INFO | Training epoch 625, Batch 1000/1000: LR=3.17e-05, Loss=2.34e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 14:36:24,467 | INFO | Epoch 625 Train Time 20.31740403175354s

2025-10-14 14:36:44,012 | INFO | Training epoch 626, Batch 1000/1000: LR=3.16e-05, Loss=2.33e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 14:36:44,074 | INFO | Epoch 626 Train Time 19.605894804000854s

2025-10-14 14:37:03,644 | INFO | Training epoch 627, Batch 1000/1000: LR=3.14e-05, Loss=2.37e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 14:37:03,710 | INFO | Epoch 627 Train Time 19.635340452194214s

2025-10-14 14:37:22,935 | INFO | Training epoch 628, Batch 1000/1000: LR=3.13e-05, Loss=2.36e-02 BER=9.00e-03 FER=1.14e-01
2025-10-14 14:37:22,982 | INFO | Epoch 628 Train Time 19.27080488204956s

2025-10-14 14:37:41,517 | INFO | Training epoch 629, Batch 1000/1000: LR=3.11e-05, Loss=2.32e-02 BER=8.86e-03 FER=1.11e-01
2025-10-14 14:37:41,564 | INFO | Epoch 629 Train Time 18.5810329914093s

2025-10-14 14:38:01,415 | INFO | Training epoch 630, Batch 1000/1000: LR=3.10e-05, Loss=2.30e-02 BER=8.78e-03 FER=1.10e-01
2025-10-14 14:38:01,460 | INFO | Epoch 630 Train Time 19.89474892616272s

2025-10-14 14:38:01,460 | INFO | [P1] saving best_model with loss 0.023005 at epoch 630
2025-10-14 14:38:20,543 | INFO | Training epoch 631, Batch 1000/1000: LR=3.08e-05, Loss=2.34e-02 BER=8.92e-03 FER=1.13e-01
2025-10-14 14:38:20,594 | INFO | Epoch 631 Train Time 19.117578506469727s

2025-10-14 14:38:38,019 | INFO | Training epoch 632, Batch 1000/1000: LR=3.07e-05, Loss=2.37e-02 BER=9.06e-03 FER=1.13e-01
2025-10-14 14:38:38,068 | INFO | Epoch 632 Train Time 17.47358727455139s

2025-10-14 14:38:57,540 | INFO | Training epoch 633, Batch 1000/1000: LR=3.06e-05, Loss=2.29e-02 BER=8.74e-03 FER=1.10e-01
2025-10-14 14:38:57,599 | INFO | Epoch 633 Train Time 19.53011727333069s

2025-10-14 14:38:57,600 | INFO | [P1] saving best_model with loss 0.022885 at epoch 633
2025-10-14 14:39:17,134 | INFO | Training epoch 634, Batch 1000/1000: LR=3.04e-05, Loss=2.37e-02 BER=9.01e-03 FER=1.14e-01
2025-10-14 14:39:17,185 | INFO | Epoch 634 Train Time 19.56926655769348s

2025-10-14 14:39:36,530 | INFO | Training epoch 635, Batch 1000/1000: LR=3.03e-05, Loss=2.32e-02 BER=8.84e-03 FER=1.12e-01
2025-10-14 14:39:36,587 | INFO | Epoch 635 Train Time 19.40035080909729s

2025-10-14 14:39:55,626 | INFO | Training epoch 636, Batch 1000/1000: LR=3.01e-05, Loss=2.38e-02 BER=9.05e-03 FER=1.14e-01
2025-10-14 14:39:55,675 | INFO | Epoch 636 Train Time 19.08756113052368s

2025-10-14 14:40:14,511 | INFO | Training epoch 637, Batch 1000/1000: LR=3.00e-05, Loss=2.35e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:40:14,557 | INFO | Epoch 637 Train Time 18.881463050842285s

2025-10-14 14:40:34,345 | INFO | Training epoch 638, Batch 1000/1000: LR=2.98e-05, Loss=2.33e-02 BER=8.88e-03 FER=1.13e-01
2025-10-14 14:40:34,397 | INFO | Epoch 638 Train Time 19.838839292526245s

2025-10-14 14:40:53,246 | INFO | Training epoch 639, Batch 1000/1000: LR=2.97e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.13e-01
2025-10-14 14:40:53,294 | INFO | Epoch 639 Train Time 18.896657705307007s

2025-10-14 14:41:12,530 | INFO | Training epoch 640, Batch 1000/1000: LR=2.96e-05, Loss=2.31e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 14:41:12,595 | INFO | Epoch 640 Train Time 19.299814462661743s

2025-10-14 14:41:32,938 | INFO | Training epoch 641, Batch 1000/1000: LR=2.94e-05, Loss=2.33e-02 BER=8.85e-03 FER=1.12e-01
2025-10-14 14:41:32,996 | INFO | Epoch 641 Train Time 20.40079975128174s

2025-10-14 14:41:53,322 | INFO | Training epoch 642, Batch 1000/1000: LR=2.93e-05, Loss=2.39e-02 BER=9.16e-03 FER=1.15e-01
2025-10-14 14:41:53,377 | INFO | Epoch 642 Train Time 20.379851818084717s

2025-10-14 14:42:11,508 | INFO | Training epoch 643, Batch 1000/1000: LR=2.91e-05, Loss=2.29e-02 BER=8.74e-03 FER=1.11e-01
2025-10-14 14:42:11,561 | INFO | Epoch 643 Train Time 18.183466911315918s

2025-10-14 14:42:11,561 | INFO | [P1] saving best_model with loss 0.022853 at epoch 643
2025-10-14 14:42:30,731 | INFO | Training epoch 644, Batch 1000/1000: LR=2.90e-05, Loss=2.35e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 14:42:30,781 | INFO | Epoch 644 Train Time 19.206359148025513s

2025-10-14 14:42:48,203 | INFO | Training epoch 645, Batch 1000/1000: LR=2.89e-05, Loss=2.34e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 14:42:48,249 | INFO | Epoch 645 Train Time 17.466639280319214s

2025-10-14 14:43:06,094 | INFO | Training epoch 646, Batch 1000/1000: LR=2.87e-05, Loss=2.36e-02 BER=8.97e-03 FER=1.12e-01
2025-10-14 14:43:06,146 | INFO | Epoch 646 Train Time 17.896559953689575s

2025-10-14 14:43:25,603 | INFO | Training epoch 647, Batch 1000/1000: LR=2.86e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 14:43:25,656 | INFO | Epoch 647 Train Time 19.50914454460144s

2025-10-14 14:43:45,739 | INFO | Training epoch 648, Batch 1000/1000: LR=2.84e-05, Loss=2.29e-02 BER=8.72e-03 FER=1.10e-01
2025-10-14 14:43:45,815 | INFO | Epoch 648 Train Time 20.15754747390747s

2025-10-14 14:44:05,670 | INFO | Training epoch 649, Batch 1000/1000: LR=2.83e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:44:05,719 | INFO | Epoch 649 Train Time 19.90346622467041s

2025-10-14 14:44:23,402 | INFO | Training epoch 650, Batch 1000/1000: LR=2.82e-05, Loss=2.34e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 14:44:23,452 | INFO | Epoch 650 Train Time 17.731123208999634s

2025-10-14 14:44:42,525 | INFO | Training epoch 651, Batch 1000/1000: LR=2.80e-05, Loss=2.32e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 14:44:42,589 | INFO | Epoch 651 Train Time 19.136085271835327s

2025-10-14 14:45:02,739 | INFO | Training epoch 652, Batch 1000/1000: LR=2.79e-05, Loss=2.32e-02 BER=8.81e-03 FER=1.12e-01
2025-10-14 14:45:02,794 | INFO | Epoch 652 Train Time 20.204248189926147s

2025-10-14 14:45:23,124 | INFO | Training epoch 653, Batch 1000/1000: LR=2.78e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:45:23,183 | INFO | Epoch 653 Train Time 20.3877112865448s

2025-10-14 14:45:41,899 | INFO | Training epoch 654, Batch 1000/1000: LR=2.76e-05, Loss=2.35e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 14:45:41,948 | INFO | Epoch 654 Train Time 18.76410722732544s

2025-10-14 14:45:59,812 | INFO | Training epoch 655, Batch 1000/1000: LR=2.75e-05, Loss=2.38e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 14:45:59,871 | INFO | Epoch 655 Train Time 17.92137575149536s

2025-10-14 14:46:18,723 | INFO | Training epoch 656, Batch 1000/1000: LR=2.73e-05, Loss=2.35e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 14:46:18,771 | INFO | Epoch 656 Train Time 18.899108171463013s

2025-10-14 14:46:37,840 | INFO | Training epoch 657, Batch 1000/1000: LR=2.72e-05, Loss=2.33e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 14:46:37,887 | INFO | Epoch 657 Train Time 19.11534571647644s

2025-10-14 14:46:58,633 | INFO | Training epoch 658, Batch 1000/1000: LR=2.71e-05, Loss=2.32e-02 BER=8.84e-03 FER=1.13e-01
2025-10-14 14:46:58,681 | INFO | Epoch 658 Train Time 20.792909860610962s

2025-10-14 14:47:19,226 | INFO | Training epoch 659, Batch 1000/1000: LR=2.69e-05, Loss=2.30e-02 BER=8.84e-03 FER=1.12e-01
2025-10-14 14:47:19,272 | INFO | Epoch 659 Train Time 20.590490341186523s

2025-10-14 14:47:39,911 | INFO | Training epoch 660, Batch 1000/1000: LR=2.68e-05, Loss=2.31e-02 BER=8.85e-03 FER=1.12e-01
2025-10-14 14:47:39,980 | INFO | Epoch 660 Train Time 20.706727027893066s

2025-10-14 14:47:59,137 | INFO | Training epoch 661, Batch 1000/1000: LR=2.67e-05, Loss=2.30e-02 BER=8.75e-03 FER=1.12e-01
2025-10-14 14:47:59,180 | INFO | Epoch 661 Train Time 19.199808359146118s

2025-10-14 14:48:16,513 | INFO | Training epoch 662, Batch 1000/1000: LR=2.65e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.13e-01
2025-10-14 14:48:16,559 | INFO | Epoch 662 Train Time 17.377217769622803s

2025-10-14 14:48:37,025 | INFO | Training epoch 663, Batch 1000/1000: LR=2.64e-05, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 14:48:37,068 | INFO | Epoch 663 Train Time 20.50824809074402s

2025-10-14 14:48:55,991 | INFO | Training epoch 664, Batch 1000/1000: LR=2.62e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 14:48:56,051 | INFO | Epoch 664 Train Time 18.982617378234863s

2025-10-14 14:49:15,659 | INFO | Training epoch 665, Batch 1000/1000: LR=2.61e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.13e-01
2025-10-14 14:49:15,715 | INFO | Epoch 665 Train Time 19.663252592086792s

2025-10-14 14:49:36,337 | INFO | Training epoch 666, Batch 1000/1000: LR=2.60e-05, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 14:49:36,383 | INFO | Epoch 666 Train Time 20.66698384284973s

2025-10-14 14:49:54,463 | INFO | Training epoch 667, Batch 1000/1000: LR=2.58e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 14:49:54,517 | INFO | Epoch 667 Train Time 18.133002281188965s

2025-10-14 14:50:13,536 | INFO | Training epoch 668, Batch 1000/1000: LR=2.57e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.11e-01
2025-10-14 14:50:13,589 | INFO | Epoch 668 Train Time 19.070635080337524s

2025-10-14 14:50:33,728 | INFO | Training epoch 669, Batch 1000/1000: LR=2.56e-05, Loss=2.33e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 14:50:33,797 | INFO | Epoch 669 Train Time 20.208043098449707s

2025-10-14 14:50:53,938 | INFO | Training epoch 670, Batch 1000/1000: LR=2.54e-05, Loss=2.34e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 14:50:53,989 | INFO | Epoch 670 Train Time 20.190626859664917s

2025-10-14 14:51:14,733 | INFO | Training epoch 671, Batch 1000/1000: LR=2.53e-05, Loss=2.36e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 14:51:14,789 | INFO | Epoch 671 Train Time 20.797987699508667s

2025-10-14 14:51:33,342 | INFO | Training epoch 672, Batch 1000/1000: LR=2.52e-05, Loss=2.34e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 14:51:33,389 | INFO | Epoch 672 Train Time 18.599050760269165s

2025-10-14 14:51:53,420 | INFO | Training epoch 673, Batch 1000/1000: LR=2.50e-05, Loss=2.32e-02 BER=8.85e-03 FER=1.12e-01
2025-10-14 14:51:53,477 | INFO | Epoch 673 Train Time 20.08755660057068s

2025-10-14 14:52:11,423 | INFO | Training epoch 674, Batch 1000/1000: LR=2.49e-05, Loss=2.35e-02 BER=9.01e-03 FER=1.12e-01
2025-10-14 14:52:11,472 | INFO | Epoch 674 Train Time 17.99370527267456s

2025-10-14 14:52:29,691 | INFO | Training epoch 675, Batch 1000/1000: LR=2.48e-05, Loss=2.35e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 14:52:29,742 | INFO | Epoch 675 Train Time 18.268323183059692s

2025-10-14 14:52:49,053 | INFO | Training epoch 676, Batch 1000/1000: LR=2.46e-05, Loss=2.38e-02 BER=9.02e-03 FER=1.13e-01
2025-10-14 14:52:49,102 | INFO | Epoch 676 Train Time 19.35963249206543s

2025-10-14 14:53:08,938 | INFO | Training epoch 677, Batch 1000/1000: LR=2.45e-05, Loss=2.34e-02 BER=8.87e-03 FER=1.13e-01
2025-10-14 14:53:08,988 | INFO | Epoch 677 Train Time 19.884848833084106s

2025-10-14 14:53:28,510 | INFO | Training epoch 678, Batch 1000/1000: LR=2.44e-05, Loss=2.32e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 14:53:28,557 | INFO | Epoch 678 Train Time 19.56852626800537s

2025-10-14 14:53:48,333 | INFO | Training epoch 679, Batch 1000/1000: LR=2.42e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.12e-01
2025-10-14 14:53:48,383 | INFO | Epoch 679 Train Time 19.82509160041809s

2025-10-14 14:54:07,659 | INFO | Training epoch 680, Batch 1000/1000: LR=2.41e-05, Loss=2.35e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 14:54:07,720 | INFO | Epoch 680 Train Time 19.335522174835205s

2025-10-14 14:54:27,035 | INFO | Training epoch 681, Batch 1000/1000: LR=2.40e-05, Loss=2.32e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 14:54:27,080 | INFO | Epoch 681 Train Time 19.359781503677368s

2025-10-14 14:54:47,021 | INFO | Training epoch 682, Batch 1000/1000: LR=2.38e-05, Loss=2.30e-02 BER=8.76e-03 FER=1.10e-01
2025-10-14 14:54:47,070 | INFO | Epoch 682 Train Time 19.989203691482544s

2025-10-14 14:55:07,333 | INFO | Training epoch 683, Batch 1000/1000: LR=2.37e-05, Loss=2.35e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 14:55:07,406 | INFO | Epoch 683 Train Time 20.33515977859497s

2025-10-14 14:55:26,906 | INFO | Training epoch 684, Batch 1000/1000: LR=2.36e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.11e-01
2025-10-14 14:55:26,974 | INFO | Epoch 684 Train Time 19.566663026809692s

2025-10-14 14:55:46,599 | INFO | Training epoch 685, Batch 1000/1000: LR=2.35e-05, Loss=2.37e-02 BER=9.04e-03 FER=1.13e-01
2025-10-14 14:55:46,651 | INFO | Epoch 685 Train Time 19.67465353012085s

2025-10-14 14:56:06,119 | INFO | Training epoch 686, Batch 1000/1000: LR=2.33e-05, Loss=2.33e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 14:56:06,177 | INFO | Epoch 686 Train Time 19.525079011917114s

2025-10-14 14:56:25,220 | INFO | Training epoch 687, Batch 1000/1000: LR=2.32e-05, Loss=2.32e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 14:56:25,267 | INFO | Epoch 687 Train Time 19.08915662765503s

2025-10-14 14:56:42,723 | INFO | Training epoch 688, Batch 1000/1000: LR=2.31e-05, Loss=2.36e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 14:56:42,775 | INFO | Epoch 688 Train Time 17.506681203842163s

2025-10-14 14:57:03,012 | INFO | Training epoch 689, Batch 1000/1000: LR=2.29e-05, Loss=2.33e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 14:57:03,058 | INFO | Epoch 689 Train Time 20.28248143196106s

2025-10-14 14:57:20,618 | INFO | Training epoch 690, Batch 1000/1000: LR=2.28e-05, Loss=2.32e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 14:57:20,668 | INFO | Epoch 690 Train Time 17.6093111038208s

2025-10-14 14:57:41,745 | INFO | Training epoch 691, Batch 1000/1000: LR=2.27e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.13e-01
2025-10-14 14:57:41,796 | INFO | Epoch 691 Train Time 21.12685990333557s

2025-10-14 14:58:01,388 | INFO | Training epoch 692, Batch 1000/1000: LR=2.25e-05, Loss=2.34e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 14:58:01,427 | INFO | Epoch 692 Train Time 19.62936305999756s

2025-10-14 14:58:20,715 | INFO | Training epoch 693, Batch 1000/1000: LR=2.24e-05, Loss=2.30e-02 BER=8.80e-03 FER=1.12e-01
2025-10-14 14:58:20,760 | INFO | Epoch 693 Train Time 19.330747842788696s

2025-10-14 14:58:40,025 | INFO | Training epoch 694, Batch 1000/1000: LR=2.23e-05, Loss=2.35e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 14:58:40,098 | INFO | Epoch 694 Train Time 19.337726354599s

2025-10-14 14:59:00,942 | INFO | Training epoch 695, Batch 1000/1000: LR=2.22e-05, Loss=2.32e-02 BER=8.86e-03 FER=1.11e-01
2025-10-14 14:59:00,992 | INFO | Epoch 695 Train Time 20.892055988311768s

2025-10-14 14:59:21,113 | INFO | Training epoch 696, Batch 1000/1000: LR=2.20e-05, Loss=2.29e-02 BER=8.75e-03 FER=1.10e-01
2025-10-14 14:59:21,159 | INFO | Epoch 696 Train Time 20.165788888931274s

2025-10-14 14:59:40,806 | INFO | Training epoch 697, Batch 1000/1000: LR=2.19e-05, Loss=2.30e-02 BER=8.72e-03 FER=1.11e-01
2025-10-14 14:59:40,855 | INFO | Epoch 697 Train Time 19.69459366798401s

2025-10-14 14:59:58,205 | INFO | Training epoch 698, Batch 1000/1000: LR=2.18e-05, Loss=2.33e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 14:59:58,249 | INFO | Epoch 698 Train Time 17.393823862075806s

2025-10-14 15:00:18,341 | INFO | Training epoch 699, Batch 1000/1000: LR=2.17e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:00:18,395 | INFO | Epoch 699 Train Time 20.14512324333191s

2025-10-14 15:00:35,902 | INFO | Training epoch 700, Batch 1000/1000: LR=2.15e-05, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:00:35,949 | INFO | Epoch 700 Train Time 17.552952527999878s

2025-10-14 15:00:55,406 | INFO | Training epoch 701, Batch 1000/1000: LR=2.14e-05, Loss=2.33e-02 BER=8.95e-03 FER=1.11e-01
2025-10-14 15:00:55,474 | INFO | Epoch 701 Train Time 19.524208545684814s

2025-10-14 15:01:13,797 | INFO | Training epoch 702, Batch 1000/1000: LR=2.13e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.12e-01
2025-10-14 15:01:13,840 | INFO | Epoch 702 Train Time 18.364931106567383s

2025-10-14 15:01:31,929 | INFO | Training epoch 703, Batch 1000/1000: LR=2.12e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 15:01:31,974 | INFO | Epoch 703 Train Time 18.133815050125122s

2025-10-14 15:01:51,211 | INFO | Training epoch 704, Batch 1000/1000: LR=2.10e-05, Loss=2.30e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:01:51,264 | INFO | Epoch 704 Train Time 19.28952693939209s

2025-10-14 15:02:09,904 | INFO | Training epoch 705, Batch 1000/1000: LR=2.09e-05, Loss=2.33e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 15:02:09,950 | INFO | Epoch 705 Train Time 18.68483018875122s

2025-10-14 15:02:29,436 | INFO | Training epoch 706, Batch 1000/1000: LR=2.08e-05, Loss=2.34e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 15:02:29,486 | INFO | Epoch 706 Train Time 19.536154747009277s

2025-10-14 15:02:48,530 | INFO | Training epoch 707, Batch 1000/1000: LR=2.07e-05, Loss=2.32e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 15:02:48,581 | INFO | Epoch 707 Train Time 19.09432864189148s

2025-10-14 15:03:07,490 | INFO | Training epoch 708, Batch 1000/1000: LR=2.05e-05, Loss=2.33e-02 BER=8.91e-03 FER=1.12e-01
2025-10-14 15:03:07,552 | INFO | Epoch 708 Train Time 18.970011472702026s

2025-10-14 15:03:27,317 | INFO | Training epoch 709, Batch 1000/1000: LR=2.04e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.13e-01
2025-10-14 15:03:27,377 | INFO | Epoch 709 Train Time 19.824251651763916s

2025-10-14 15:03:46,840 | INFO | Training epoch 710, Batch 1000/1000: LR=2.03e-05, Loss=2.31e-02 BER=8.83e-03 FER=1.12e-01
2025-10-14 15:03:46,902 | INFO | Epoch 710 Train Time 19.524118423461914s

2025-10-14 15:04:06,483 | INFO | Training epoch 711, Batch 1000/1000: LR=2.02e-05, Loss=2.32e-02 BER=8.85e-03 FER=1.12e-01
2025-10-14 15:04:06,530 | INFO | Epoch 711 Train Time 19.62748670578003s

2025-10-14 15:04:26,030 | INFO | Training epoch 712, Batch 1000/1000: LR=2.00e-05, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:04:26,107 | INFO | Epoch 712 Train Time 19.57527256011963s

2025-10-14 15:04:46,395 | INFO | Training epoch 713, Batch 1000/1000: LR=1.99e-05, Loss=2.34e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 15:04:46,448 | INFO | Epoch 713 Train Time 20.340306520462036s

2025-10-14 15:05:03,219 | INFO | Training epoch 714, Batch 1000/1000: LR=1.98e-05, Loss=2.37e-02 BER=9.04e-03 FER=1.13e-01
2025-10-14 15:05:03,265 | INFO | Epoch 714 Train Time 16.81599235534668s

2025-10-14 15:05:23,205 | INFO | Training epoch 715, Batch 1000/1000: LR=1.97e-05, Loss=2.33e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 15:05:23,257 | INFO | Epoch 715 Train Time 19.991467475891113s

2025-10-14 15:05:41,103 | INFO | Training epoch 716, Batch 1000/1000: LR=1.96e-05, Loss=2.34e-02 BER=8.96e-03 FER=1.13e-01
2025-10-14 15:05:41,160 | INFO | Epoch 716 Train Time 17.902345657348633s

2025-10-14 15:05:59,927 | INFO | Training epoch 717, Batch 1000/1000: LR=1.94e-05, Loss=2.32e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 15:05:59,982 | INFO | Epoch 717 Train Time 18.821427583694458s

2025-10-14 15:06:18,703 | INFO | Training epoch 718, Batch 1000/1000: LR=1.93e-05, Loss=2.35e-02 BER=8.97e-03 FER=1.12e-01
2025-10-14 15:06:18,750 | INFO | Epoch 718 Train Time 18.765496253967285s

2025-10-14 15:06:35,512 | INFO | Training epoch 719, Batch 1000/1000: LR=1.92e-05, Loss=2.34e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 15:06:35,563 | INFO | Epoch 719 Train Time 16.812320470809937s

2025-10-14 15:06:55,942 | INFO | Training epoch 720, Batch 1000/1000: LR=1.91e-05, Loss=2.39e-02 BER=9.12e-03 FER=1.14e-01
2025-10-14 15:06:55,985 | INFO | Epoch 720 Train Time 20.421002626419067s

2025-10-14 15:07:15,947 | INFO | Training epoch 721, Batch 1000/1000: LR=1.89e-05, Loss=2.35e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 15:07:16,008 | INFO | Epoch 721 Train Time 20.021970510482788s

2025-10-14 15:07:33,704 | INFO | Training epoch 722, Batch 1000/1000: LR=1.88e-05, Loss=2.32e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:07:33,749 | INFO | Epoch 722 Train Time 17.73988127708435s

2025-10-14 15:07:54,810 | INFO | Training epoch 723, Batch 1000/1000: LR=1.87e-05, Loss=2.34e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:07:54,863 | INFO | Epoch 723 Train Time 21.11250400543213s

2025-10-14 15:08:14,820 | INFO | Training epoch 724, Batch 1000/1000: LR=1.86e-05, Loss=2.34e-02 BER=8.91e-03 FER=1.11e-01
2025-10-14 15:08:14,871 | INFO | Epoch 724 Train Time 20.008112907409668s

2025-10-14 15:08:34,825 | INFO | Training epoch 725, Batch 1000/1000: LR=1.85e-05, Loss=2.32e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:08:34,880 | INFO | Epoch 725 Train Time 20.007649421691895s

2025-10-14 15:08:55,930 | INFO | Training epoch 726, Batch 1000/1000: LR=1.84e-05, Loss=2.33e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 15:08:55,998 | INFO | Epoch 726 Train Time 21.117162227630615s

2025-10-14 15:09:16,725 | INFO | Training epoch 727, Batch 1000/1000: LR=1.82e-05, Loss=2.37e-02 BER=9.09e-03 FER=1.13e-01
2025-10-14 15:09:16,791 | INFO | Epoch 727 Train Time 20.792293787002563s

2025-10-14 15:09:36,212 | INFO | Training epoch 728, Batch 1000/1000: LR=1.81e-05, Loss=2.31e-02 BER=8.80e-03 FER=1.12e-01
2025-10-14 15:09:36,258 | INFO | Epoch 728 Train Time 19.465769052505493s

2025-10-14 15:09:56,331 | INFO | Training epoch 729, Batch 1000/1000: LR=1.80e-05, Loss=2.35e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 15:09:56,375 | INFO | Epoch 729 Train Time 20.11717414855957s

2025-10-14 15:10:16,837 | INFO | Training epoch 730, Batch 1000/1000: LR=1.79e-05, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 15:10:16,888 | INFO | Epoch 730 Train Time 20.510887622833252s

2025-10-14 15:10:36,854 | INFO | Training epoch 731, Batch 1000/1000: LR=1.78e-05, Loss=2.32e-02 BER=8.85e-03 FER=1.12e-01
2025-10-14 15:10:36,925 | INFO | Epoch 731 Train Time 20.03588628768921s

2025-10-14 15:10:55,732 | INFO | Training epoch 732, Batch 1000/1000: LR=1.76e-05, Loss=2.33e-02 BER=8.91e-03 FER=1.11e-01
2025-10-14 15:10:55,778 | INFO | Epoch 732 Train Time 18.852041482925415s

2025-10-14 15:11:13,910 | INFO | Training epoch 733, Batch 1000/1000: LR=1.75e-05, Loss=2.32e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:11:13,964 | INFO | Epoch 733 Train Time 18.18475604057312s

2025-10-14 15:11:33,730 | INFO | Training epoch 734, Batch 1000/1000: LR=1.74e-05, Loss=2.30e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:11:33,780 | INFO | Epoch 734 Train Time 19.815837383270264s

2025-10-14 15:11:52,402 | INFO | Training epoch 735, Batch 1000/1000: LR=1.73e-05, Loss=2.33e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 15:11:52,450 | INFO | Epoch 735 Train Time 18.668320178985596s

2025-10-14 15:12:10,918 | INFO | Training epoch 736, Batch 1000/1000: LR=1.72e-05, Loss=2.29e-02 BER=8.77e-03 FER=1.10e-01
2025-10-14 15:12:10,964 | INFO | Epoch 736 Train Time 18.513798713684082s

2025-10-14 15:12:30,912 | INFO | Training epoch 737, Batch 1000/1000: LR=1.71e-05, Loss=2.34e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 15:12:30,968 | INFO | Epoch 737 Train Time 20.002262830734253s

2025-10-14 15:12:50,622 | INFO | Training epoch 738, Batch 1000/1000: LR=1.70e-05, Loss=2.30e-02 BER=8.79e-03 FER=1.10e-01
2025-10-14 15:12:50,676 | INFO | Epoch 738 Train Time 19.70763850212097s

2025-10-14 15:13:10,136 | INFO | Training epoch 739, Batch 1000/1000: LR=1.68e-05, Loss=2.27e-02 BER=8.69e-03 FER=1.10e-01
2025-10-14 15:13:10,190 | INFO | Epoch 739 Train Time 19.513848543167114s

2025-10-14 15:13:10,191 | INFO | [P1] saving best_model with loss 0.022748 at epoch 739
2025-10-14 15:13:30,334 | INFO | Training epoch 740, Batch 1000/1000: LR=1.67e-05, Loss=2.32e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 15:13:30,388 | INFO | Epoch 740 Train Time 20.176265716552734s

2025-10-14 15:13:50,832 | INFO | Training epoch 741, Batch 1000/1000: LR=1.66e-05, Loss=2.30e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:13:50,889 | INFO | Epoch 741 Train Time 20.500181913375854s

2025-10-14 15:14:10,024 | INFO | Training epoch 742, Batch 1000/1000: LR=1.65e-05, Loss=2.32e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 15:14:10,071 | INFO | Epoch 742 Train Time 19.18109369277954s

2025-10-14 15:14:29,499 | INFO | Training epoch 743, Batch 1000/1000: LR=1.64e-05, Loss=2.30e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 15:14:29,550 | INFO | Epoch 743 Train Time 19.47784113883972s

2025-10-14 15:14:47,832 | INFO | Training epoch 744, Batch 1000/1000: LR=1.63e-05, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:14:47,886 | INFO | Epoch 744 Train Time 18.33530879020691s

2025-10-14 15:15:07,837 | INFO | Training epoch 745, Batch 1000/1000: LR=1.62e-05, Loss=2.31e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:15:07,887 | INFO | Epoch 745 Train Time 20.00067925453186s

2025-10-14 15:15:27,657 | INFO | Training epoch 746, Batch 1000/1000: LR=1.61e-05, Loss=2.36e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 15:15:27,702 | INFO | Epoch 746 Train Time 19.81368613243103s

2025-10-14 15:15:46,609 | INFO | Training epoch 747, Batch 1000/1000: LR=1.59e-05, Loss=2.28e-02 BER=8.70e-03 FER=1.10e-01
2025-10-14 15:15:46,651 | INFO | Epoch 747 Train Time 18.948506593704224s

2025-10-14 15:16:07,103 | INFO | Training epoch 748, Batch 1000/1000: LR=1.58e-05, Loss=2.29e-02 BER=8.71e-03 FER=1.10e-01
2025-10-14 15:16:07,167 | INFO | Epoch 748 Train Time 20.51514959335327s

2025-10-14 15:16:27,515 | INFO | Training epoch 749, Batch 1000/1000: LR=1.57e-05, Loss=2.36e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 15:16:27,569 | INFO | Epoch 749 Train Time 20.4006450176239s

2025-10-14 15:16:47,282 | INFO | Training epoch 750, Batch 1000/1000: LR=1.56e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 15:16:47,323 | INFO | Epoch 750 Train Time 19.754258155822754s

2025-10-14 15:17:05,131 | INFO | Training epoch 751, Batch 1000/1000: LR=1.55e-05, Loss=2.34e-02 BER=8.95e-03 FER=1.12e-01
2025-10-14 15:17:05,184 | INFO | Epoch 751 Train Time 17.860554695129395s

2025-10-14 15:17:24,220 | INFO | Training epoch 752, Batch 1000/1000: LR=1.54e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 15:17:24,271 | INFO | Epoch 752 Train Time 19.085855722427368s

2025-10-14 15:17:43,123 | INFO | Training epoch 753, Batch 1000/1000: LR=1.53e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:17:43,176 | INFO | Epoch 753 Train Time 18.90419578552246s

2025-10-14 15:18:03,233 | INFO | Training epoch 754, Batch 1000/1000: LR=1.52e-05, Loss=2.38e-02 BER=9.08e-03 FER=1.14e-01
2025-10-14 15:18:03,285 | INFO | Epoch 754 Train Time 20.108326196670532s

2025-10-14 15:18:23,822 | INFO | Training epoch 755, Batch 1000/1000: LR=1.51e-05, Loss=2.32e-02 BER=8.84e-03 FER=1.12e-01
2025-10-14 15:18:23,870 | INFO | Epoch 755 Train Time 20.583812475204468s

2025-10-14 15:18:43,530 | INFO | Training epoch 756, Batch 1000/1000: LR=1.50e-05, Loss=2.33e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 15:18:43,582 | INFO | Epoch 756 Train Time 19.710954666137695s

2025-10-14 15:19:03,650 | INFO | Training epoch 757, Batch 1000/1000: LR=1.48e-05, Loss=2.31e-02 BER=8.77e-03 FER=1.11e-01
2025-10-14 15:19:03,720 | INFO | Epoch 757 Train Time 20.136908292770386s

2025-10-14 15:19:24,204 | INFO | Training epoch 758, Batch 1000/1000: LR=1.47e-05, Loss=2.35e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 15:19:24,257 | INFO | Epoch 758 Train Time 20.535724639892578s

2025-10-14 15:19:44,443 | INFO | Training epoch 759, Batch 1000/1000: LR=1.46e-05, Loss=2.34e-02 BER=8.95e-03 FER=1.12e-01
2025-10-14 15:19:44,496 | INFO | Epoch 759 Train Time 20.238368272781372s

2025-10-14 15:20:03,115 | INFO | Training epoch 760, Batch 1000/1000: LR=1.45e-05, Loss=2.30e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 15:20:03,170 | INFO | Epoch 760 Train Time 18.67316746711731s

2025-10-14 15:20:22,622 | INFO | Training epoch 761, Batch 1000/1000: LR=1.44e-05, Loss=2.33e-02 BER=8.85e-03 FER=1.12e-01
2025-10-14 15:20:22,669 | INFO | Epoch 761 Train Time 19.498120307922363s

2025-10-14 15:20:40,104 | INFO | Training epoch 762, Batch 1000/1000: LR=1.43e-05, Loss=2.36e-02 BER=9.07e-03 FER=1.13e-01
2025-10-14 15:20:40,157 | INFO | Epoch 762 Train Time 17.48682165145874s

2025-10-14 15:20:58,914 | INFO | Training epoch 763, Batch 1000/1000: LR=1.42e-05, Loss=2.36e-02 BER=9.04e-03 FER=1.14e-01
2025-10-14 15:20:58,964 | INFO | Epoch 763 Train Time 18.80608081817627s

2025-10-14 15:21:18,719 | INFO | Training epoch 764, Batch 1000/1000: LR=1.41e-05, Loss=2.32e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 15:21:18,771 | INFO | Epoch 764 Train Time 19.80648159980774s

2025-10-14 15:21:39,298 | INFO | Training epoch 765, Batch 1000/1000: LR=1.40e-05, Loss=2.32e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 15:21:39,345 | INFO | Epoch 765 Train Time 20.57315492630005s

2025-10-14 15:21:58,908 | INFO | Training epoch 766, Batch 1000/1000: LR=1.39e-05, Loss=2.33e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 15:21:58,959 | INFO | Epoch 766 Train Time 19.613579034805298s

2025-10-14 15:22:19,805 | INFO | Training epoch 767, Batch 1000/1000: LR=1.38e-05, Loss=2.28e-02 BER=8.74e-03 FER=1.10e-01
2025-10-14 15:22:19,858 | INFO | Epoch 767 Train Time 20.897631406784058s

2025-10-14 15:22:40,520 | INFO | Training epoch 768, Batch 1000/1000: LR=1.37e-05, Loss=2.35e-02 BER=8.91e-03 FER=1.11e-01
2025-10-14 15:22:40,571 | INFO | Epoch 768 Train Time 20.712295055389404s

2025-10-14 15:23:00,439 | INFO | Training epoch 769, Batch 1000/1000: LR=1.36e-05, Loss=2.33e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:23:00,478 | INFO | Epoch 769 Train Time 19.906402826309204s

2025-10-14 15:23:21,017 | INFO | Training epoch 770, Batch 1000/1000: LR=1.35e-05, Loss=2.33e-02 BER=8.93e-03 FER=1.11e-01
2025-10-14 15:23:21,063 | INFO | Epoch 770 Train Time 20.58406925201416s

2025-10-14 15:23:40,813 | INFO | Training epoch 771, Batch 1000/1000: LR=1.34e-05, Loss=2.33e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 15:23:40,876 | INFO | Epoch 771 Train Time 19.811994075775146s

2025-10-14 15:24:00,920 | INFO | Training epoch 772, Batch 1000/1000: LR=1.33e-05, Loss=2.34e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 15:24:00,975 | INFO | Epoch 772 Train Time 20.098262071609497s

2025-10-14 15:24:21,523 | INFO | Training epoch 773, Batch 1000/1000: LR=1.32e-05, Loss=2.33e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:24:21,563 | INFO | Epoch 773 Train Time 20.587575674057007s

2025-10-14 15:24:40,008 | INFO | Training epoch 774, Batch 1000/1000: LR=1.31e-05, Loss=2.32e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 15:24:40,059 | INFO | Epoch 774 Train Time 18.495301008224487s

2025-10-14 15:24:59,443 | INFO | Training epoch 775, Batch 1000/1000: LR=1.30e-05, Loss=2.33e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 15:24:59,502 | INFO | Epoch 775 Train Time 19.442143201828003s

2025-10-14 15:25:19,590 | INFO | Training epoch 776, Batch 1000/1000: LR=1.29e-05, Loss=2.34e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 15:25:19,648 | INFO | Epoch 776 Train Time 20.146039724349976s

2025-10-14 15:25:39,602 | INFO | Training epoch 777, Batch 1000/1000: LR=1.28e-05, Loss=2.33e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 15:25:39,643 | INFO | Epoch 777 Train Time 19.993438005447388s

2025-10-14 15:25:57,620 | INFO | Training epoch 778, Batch 1000/1000: LR=1.27e-05, Loss=2.30e-02 BER=8.72e-03 FER=1.10e-01
2025-10-14 15:25:57,694 | INFO | Epoch 778 Train Time 18.05047059059143s

2025-10-14 15:26:17,977 | INFO | Training epoch 779, Batch 1000/1000: LR=1.26e-05, Loss=2.32e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 15:26:18,024 | INFO | Epoch 779 Train Time 20.329412698745728s

2025-10-14 15:26:37,823 | INFO | Training epoch 780, Batch 1000/1000: LR=1.25e-05, Loss=2.35e-02 BER=8.97e-03 FER=1.12e-01
2025-10-14 15:26:37,893 | INFO | Epoch 780 Train Time 19.868408203125s

2025-10-14 15:26:56,409 | INFO | Training epoch 781, Batch 1000/1000: LR=1.24e-05, Loss=2.27e-02 BER=8.76e-03 FER=1.10e-01
2025-10-14 15:26:56,454 | INFO | Epoch 781 Train Time 18.560084104537964s

2025-10-14 15:26:56,455 | INFO | [P1] saving best_model with loss 0.022742 at epoch 781
2025-10-14 15:27:14,802 | INFO | Training epoch 782, Batch 1000/1000: LR=1.23e-05, Loss=2.34e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 15:27:14,856 | INFO | Epoch 782 Train Time 18.38627600669861s

2025-10-14 15:27:33,998 | INFO | Training epoch 783, Batch 1000/1000: LR=1.22e-05, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:27:34,039 | INFO | Epoch 783 Train Time 19.182024717330933s

2025-10-14 15:27:54,017 | INFO | Training epoch 784, Batch 1000/1000: LR=1.21e-05, Loss=2.33e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 15:27:54,068 | INFO | Epoch 784 Train Time 20.028839826583862s

2025-10-14 15:28:12,627 | INFO | Training epoch 785, Batch 1000/1000: LR=1.20e-05, Loss=2.34e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:28:12,683 | INFO | Epoch 785 Train Time 18.613348484039307s

2025-10-14 15:28:32,332 | INFO | Training epoch 786, Batch 1000/1000: LR=1.19e-05, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:28:32,378 | INFO | Epoch 786 Train Time 19.69417977333069s

2025-10-14 15:28:50,712 | INFO | Training epoch 787, Batch 1000/1000: LR=1.18e-05, Loss=2.31e-02 BER=8.86e-03 FER=1.11e-01
2025-10-14 15:28:50,762 | INFO | Epoch 787 Train Time 18.382946729660034s

2025-10-14 15:29:10,409 | INFO | Training epoch 788, Batch 1000/1000: LR=1.17e-05, Loss=2.31e-02 BER=8.75e-03 FER=1.10e-01
2025-10-14 15:29:10,462 | INFO | Epoch 788 Train Time 19.69855308532715s

2025-10-14 15:29:28,630 | INFO | Training epoch 789, Batch 1000/1000: LR=1.16e-05, Loss=2.31e-02 BER=8.80e-03 FER=1.10e-01
2025-10-14 15:29:28,678 | INFO | Epoch 789 Train Time 18.215584754943848s

2025-10-14 15:29:48,647 | INFO | Training epoch 790, Batch 1000/1000: LR=1.15e-05, Loss=2.31e-02 BER=8.78e-03 FER=1.10e-01
2025-10-14 15:29:48,718 | INFO | Epoch 790 Train Time 20.039916515350342s

2025-10-14 15:30:07,983 | INFO | Training epoch 791, Batch 1000/1000: LR=1.14e-05, Loss=2.33e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:30:08,030 | INFO | Epoch 791 Train Time 19.309180974960327s

2025-10-14 15:30:28,633 | INFO | Training epoch 792, Batch 1000/1000: LR=1.13e-05, Loss=2.32e-02 BER=8.87e-03 FER=1.11e-01
2025-10-14 15:30:28,682 | INFO | Epoch 792 Train Time 20.651551008224487s

2025-10-14 15:30:48,136 | INFO | Training epoch 793, Batch 1000/1000: LR=1.12e-05, Loss=2.29e-02 BER=8.67e-03 FER=1.10e-01
2025-10-14 15:30:48,187 | INFO | Epoch 793 Train Time 19.50334334373474s

2025-10-14 15:31:06,622 | INFO | Training epoch 794, Batch 1000/1000: LR=1.11e-05, Loss=2.30e-02 BER=8.73e-03 FER=1.10e-01
2025-10-14 15:31:06,664 | INFO | Epoch 794 Train Time 18.476208686828613s

2025-10-14 15:31:24,535 | INFO | Training epoch 795, Batch 1000/1000: LR=1.10e-05, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 15:31:24,594 | INFO | Epoch 795 Train Time 17.930126667022705s

2025-10-14 15:31:41,910 | INFO | Training epoch 796, Batch 1000/1000: LR=1.09e-05, Loss=2.34e-02 BER=9.01e-03 FER=1.13e-01
2025-10-14 15:31:41,951 | INFO | Epoch 796 Train Time 17.356194496154785s

2025-10-14 15:32:01,737 | INFO | Training epoch 797, Batch 1000/1000: LR=1.08e-05, Loss=2.35e-02 BER=8.95e-03 FER=1.13e-01
2025-10-14 15:32:01,807 | INFO | Epoch 797 Train Time 19.85511088371277s

2025-10-14 15:32:21,146 | INFO | Training epoch 798, Batch 1000/1000: LR=1.07e-05, Loss=2.33e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 15:32:21,202 | INFO | Epoch 798 Train Time 19.39434027671814s

2025-10-14 15:32:39,919 | INFO | Training epoch 799, Batch 1000/1000: LR=1.06e-05, Loss=2.37e-02 BER=9.04e-03 FER=1.13e-01
2025-10-14 15:32:39,977 | INFO | Epoch 799 Train Time 18.773762226104736s

2025-10-14 15:32:58,001 | INFO | Training epoch 800, Batch 1000/1000: LR=1.05e-05, Loss=2.32e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 15:32:58,052 | INFO | Epoch 800 Train Time 18.073265552520752s

2025-10-14 15:33:16,516 | INFO | Training epoch 801, Batch 1000/1000: LR=1.05e-05, Loss=2.30e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:33:16,563 | INFO | Epoch 801 Train Time 18.51025080680847s

2025-10-14 15:33:35,729 | INFO | Training epoch 802, Batch 1000/1000: LR=1.04e-05, Loss=2.35e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 15:33:35,803 | INFO | Epoch 802 Train Time 19.239855766296387s

2025-10-14 15:33:53,614 | INFO | Training epoch 803, Batch 1000/1000: LR=1.03e-05, Loss=2.32e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 15:33:53,656 | INFO | Epoch 803 Train Time 17.851558446884155s

2025-10-14 15:34:13,125 | INFO | Training epoch 804, Batch 1000/1000: LR=1.02e-05, Loss=2.32e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 15:34:13,190 | INFO | Epoch 804 Train Time 19.53274416923523s

2025-10-14 15:34:33,595 | INFO | Training epoch 805, Batch 1000/1000: LR=1.01e-05, Loss=2.29e-02 BER=8.75e-03 FER=1.10e-01
2025-10-14 15:34:33,649 | INFO | Epoch 805 Train Time 20.458327531814575s

2025-10-14 15:34:54,718 | INFO | Training epoch 806, Batch 1000/1000: LR=1.00e-05, Loss=2.35e-02 BER=8.99e-03 FER=1.13e-01
2025-10-14 15:34:54,770 | INFO | Epoch 806 Train Time 21.121087312698364s

2025-10-14 15:35:12,731 | INFO | Training epoch 807, Batch 1000/1000: LR=9.91e-06, Loss=2.33e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 15:35:12,792 | INFO | Epoch 807 Train Time 18.020038843154907s

2025-10-14 15:35:31,908 | INFO | Training epoch 808, Batch 1000/1000: LR=9.82e-06, Loss=2.36e-02 BER=9.05e-03 FER=1.13e-01
2025-10-14 15:35:31,957 | INFO | Epoch 808 Train Time 19.16423749923706s

2025-10-14 15:35:51,733 | INFO | Training epoch 809, Batch 1000/1000: LR=9.74e-06, Loss=2.31e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:35:51,783 | INFO | Epoch 809 Train Time 19.825543880462646s

2025-10-14 15:36:10,847 | INFO | Training epoch 810, Batch 1000/1000: LR=9.65e-06, Loss=2.32e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:36:10,888 | INFO | Epoch 810 Train Time 19.104120016098022s

2025-10-14 15:36:30,311 | INFO | Training epoch 811, Batch 1000/1000: LR=9.56e-06, Loss=2.33e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 15:36:30,360 | INFO | Epoch 811 Train Time 19.470532178878784s

2025-10-14 15:36:49,802 | INFO | Training epoch 812, Batch 1000/1000: LR=9.47e-06, Loss=2.29e-02 BER=8.71e-03 FER=1.10e-01
2025-10-14 15:36:49,878 | INFO | Epoch 812 Train Time 19.517695903778076s

2025-10-14 15:37:08,570 | INFO | Training epoch 813, Batch 1000/1000: LR=9.39e-06, Loss=2.30e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 15:37:08,615 | INFO | Epoch 813 Train Time 18.73526406288147s

2025-10-14 15:37:27,941 | INFO | Training epoch 814, Batch 1000/1000: LR=9.30e-06, Loss=2.30e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 15:37:28,011 | INFO | Epoch 814 Train Time 19.394922256469727s

2025-10-14 15:37:47,012 | INFO | Training epoch 815, Batch 1000/1000: LR=9.21e-06, Loss=2.33e-02 BER=8.95e-03 FER=1.11e-01
2025-10-14 15:37:47,065 | INFO | Epoch 815 Train Time 19.053661346435547s

2025-10-14 15:38:06,319 | INFO | Training epoch 816, Batch 1000/1000: LR=9.13e-06, Loss=2.31e-02 BER=8.77e-03 FER=1.11e-01
2025-10-14 15:38:06,361 | INFO | Epoch 816 Train Time 19.294838666915894s

2025-10-14 15:38:26,605 | INFO | Training epoch 817, Batch 1000/1000: LR=9.04e-06, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:38:26,663 | INFO | Epoch 817 Train Time 20.301711082458496s

2025-10-14 15:38:44,690 | INFO | Training epoch 818, Batch 1000/1000: LR=8.96e-06, Loss=2.32e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:38:44,733 | INFO | Epoch 818 Train Time 18.06859278678894s

2025-10-14 15:39:04,417 | INFO | Training epoch 819, Batch 1000/1000: LR=8.87e-06, Loss=2.32e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:39:04,465 | INFO | Epoch 819 Train Time 19.731394052505493s

2025-10-14 15:39:23,534 | INFO | Training epoch 820, Batch 1000/1000: LR=8.79e-06, Loss=2.33e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 15:39:23,583 | INFO | Epoch 820 Train Time 19.118035554885864s

2025-10-14 15:39:44,533 | INFO | Training epoch 821, Batch 1000/1000: LR=8.71e-06, Loss=2.33e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 15:39:44,590 | INFO | Epoch 821 Train Time 21.006189346313477s

2025-10-14 15:40:04,592 | INFO | Training epoch 822, Batch 1000/1000: LR=8.62e-06, Loss=2.34e-02 BER=8.97e-03 FER=1.13e-01
2025-10-14 15:40:04,655 | INFO | Epoch 822 Train Time 20.063857078552246s

2025-10-14 15:40:23,247 | INFO | Training epoch 823, Batch 1000/1000: LR=8.54e-06, Loss=2.36e-02 BER=9.01e-03 FER=1.12e-01
2025-10-14 15:40:23,300 | INFO | Epoch 823 Train Time 18.644139766693115s

2025-10-14 15:40:41,698 | INFO | Training epoch 824, Batch 1000/1000: LR=8.46e-06, Loss=2.31e-02 BER=8.79e-03 FER=1.10e-01
2025-10-14 15:40:41,751 | INFO | Epoch 824 Train Time 18.449990034103394s

2025-10-14 15:41:01,933 | INFO | Training epoch 825, Batch 1000/1000: LR=8.38e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:41:01,978 | INFO | Epoch 825 Train Time 20.225870847702026s

2025-10-14 15:41:20,106 | INFO | Training epoch 826, Batch 1000/1000: LR=8.29e-06, Loss=2.35e-02 BER=8.98e-03 FER=1.12e-01
2025-10-14 15:41:20,166 | INFO | Epoch 826 Train Time 18.186907052993774s

2025-10-14 15:41:39,676 | INFO | Training epoch 827, Batch 1000/1000: LR=8.21e-06, Loss=2.32e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 15:41:39,723 | INFO | Epoch 827 Train Time 19.55649471282959s

2025-10-14 15:41:56,833 | INFO | Training epoch 828, Batch 1000/1000: LR=8.13e-06, Loss=2.32e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 15:41:56,877 | INFO | Epoch 828 Train Time 17.15301251411438s

2025-10-14 15:42:15,732 | INFO | Training epoch 829, Batch 1000/1000: LR=8.05e-06, Loss=2.34e-02 BER=9.01e-03 FER=1.12e-01
2025-10-14 15:42:15,811 | INFO | Epoch 829 Train Time 18.933372974395752s

2025-10-14 15:42:32,493 | INFO | Training epoch 830, Batch 1000/1000: LR=7.97e-06, Loss=2.32e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:42:32,542 | INFO | Epoch 830 Train Time 16.729252099990845s

2025-10-14 15:42:51,010 | INFO | Training epoch 831, Batch 1000/1000: LR=7.89e-06, Loss=2.29e-02 BER=8.69e-03 FER=1.10e-01
2025-10-14 15:42:51,072 | INFO | Epoch 831 Train Time 18.529110431671143s

2025-10-14 15:43:09,930 | INFO | Training epoch 832, Batch 1000/1000: LR=7.81e-06, Loss=2.33e-02 BER=8.88e-03 FER=1.10e-01
2025-10-14 15:43:09,978 | INFO | Epoch 832 Train Time 18.90534281730652s

2025-10-14 15:43:29,236 | INFO | Training epoch 833, Batch 1000/1000: LR=7.74e-06, Loss=2.32e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 15:43:29,298 | INFO | Epoch 833 Train Time 19.318380117416382s

2025-10-14 15:43:49,881 | INFO | Training epoch 834, Batch 1000/1000: LR=7.66e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:43:49,932 | INFO | Epoch 834 Train Time 20.634172201156616s

2025-10-14 15:44:07,008 | INFO | Training epoch 835, Batch 1000/1000: LR=7.58e-06, Loss=2.30e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:44:07,066 | INFO | Epoch 835 Train Time 17.132829189300537s

2025-10-14 15:44:26,727 | INFO | Training epoch 836, Batch 1000/1000: LR=7.50e-06, Loss=2.32e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 15:44:26,790 | INFO | Epoch 836 Train Time 19.723769903182983s

2025-10-14 15:44:47,634 | INFO | Training epoch 837, Batch 1000/1000: LR=7.43e-06, Loss=2.34e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 15:44:47,691 | INFO | Epoch 837 Train Time 20.899574756622314s

2025-10-14 15:45:06,227 | INFO | Training epoch 838, Batch 1000/1000: LR=7.35e-06, Loss=2.25e-02 BER=8.57e-03 FER=1.09e-01
2025-10-14 15:45:06,281 | INFO | Epoch 838 Train Time 18.58882784843445s

2025-10-14 15:45:06,281 | INFO | [P1] saving best_model with loss 0.022546 at epoch 838
2025-10-14 15:45:25,739 | INFO | Training epoch 839, Batch 1000/1000: LR=7.27e-06, Loss=2.33e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:45:25,804 | INFO | Epoch 839 Train Time 19.50520658493042s

2025-10-14 15:45:44,834 | INFO | Training epoch 840, Batch 1000/1000: LR=7.20e-06, Loss=2.31e-02 BER=8.78e-03 FER=1.10e-01
2025-10-14 15:45:44,886 | INFO | Epoch 840 Train Time 19.080981016159058s

2025-10-14 15:46:03,874 | INFO | Training epoch 841, Batch 1000/1000: LR=7.12e-06, Loss=2.28e-02 BER=8.66e-03 FER=1.10e-01
2025-10-14 15:46:03,923 | INFO | Epoch 841 Train Time 19.036377429962158s

2025-10-14 15:46:23,494 | INFO | Training epoch 842, Batch 1000/1000: LR=7.05e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.12e-01
2025-10-14 15:46:23,545 | INFO | Epoch 842 Train Time 19.62045431137085s

2025-10-14 15:46:43,335 | INFO | Training epoch 843, Batch 1000/1000: LR=6.97e-06, Loss=2.32e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:46:43,383 | INFO | Epoch 843 Train Time 19.837589979171753s

2025-10-14 15:47:02,631 | INFO | Training epoch 844, Batch 1000/1000: LR=6.90e-06, Loss=2.33e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:47:02,684 | INFO | Epoch 844 Train Time 19.299940586090088s

2025-10-14 15:47:21,094 | INFO | Training epoch 845, Batch 1000/1000: LR=6.83e-06, Loss=2.31e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:47:21,140 | INFO | Epoch 845 Train Time 18.455243349075317s

2025-10-14 15:47:41,125 | INFO | Training epoch 846, Batch 1000/1000: LR=6.75e-06, Loss=2.33e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 15:47:41,182 | INFO | Epoch 846 Train Time 20.041361093521118s

2025-10-14 15:48:00,409 | INFO | Training epoch 847, Batch 1000/1000: LR=6.68e-06, Loss=2.30e-02 BER=8.77e-03 FER=1.10e-01
2025-10-14 15:48:00,469 | INFO | Epoch 847 Train Time 19.285691738128662s

2025-10-14 15:48:18,809 | INFO | Training epoch 848, Batch 1000/1000: LR=6.61e-06, Loss=2.31e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 15:48:18,863 | INFO | Epoch 848 Train Time 18.39263367652893s

2025-10-14 15:48:39,051 | INFO | Training epoch 849, Batch 1000/1000: LR=6.54e-06, Loss=2.35e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 15:48:39,117 | INFO | Epoch 849 Train Time 20.252841472625732s

2025-10-14 15:48:57,432 | INFO | Training epoch 850, Batch 1000/1000: LR=6.47e-06, Loss=2.27e-02 BER=8.65e-03 FER=1.09e-01
2025-10-14 15:48:57,490 | INFO | Epoch 850 Train Time 18.372278690338135s

2025-10-14 15:49:17,341 | INFO | Training epoch 851, Batch 1000/1000: LR=6.40e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 15:49:17,386 | INFO | Epoch 851 Train Time 19.89532470703125s

2025-10-14 15:49:37,650 | INFO | Training epoch 852, Batch 1000/1000: LR=6.32e-06, Loss=2.31e-02 BER=8.83e-03 FER=1.10e-01
2025-10-14 15:49:37,717 | INFO | Epoch 852 Train Time 20.330952167510986s

2025-10-14 15:49:56,526 | INFO | Training epoch 853, Batch 1000/1000: LR=6.25e-06, Loss=2.31e-02 BER=8.73e-03 FER=1.10e-01
2025-10-14 15:49:56,589 | INFO | Epoch 853 Train Time 18.871020317077637s

2025-10-14 15:50:16,645 | INFO | Training epoch 854, Batch 1000/1000: LR=6.19e-06, Loss=2.32e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 15:50:16,715 | INFO | Epoch 854 Train Time 20.125519037246704s

2025-10-14 15:50:35,507 | INFO | Training epoch 855, Batch 1000/1000: LR=6.12e-06, Loss=2.30e-02 BER=8.79e-03 FER=1.11e-01
2025-10-14 15:50:35,555 | INFO | Epoch 855 Train Time 18.83875298500061s

2025-10-14 15:50:54,714 | INFO | Training epoch 856, Batch 1000/1000: LR=6.05e-06, Loss=2.25e-02 BER=8.60e-03 FER=1.09e-01
2025-10-14 15:50:54,757 | INFO | Epoch 856 Train Time 19.20110273361206s

2025-10-14 15:50:54,758 | INFO | [P1] saving best_model with loss 0.022521 at epoch 856
2025-10-14 15:51:14,222 | INFO | Training epoch 857, Batch 1000/1000: LR=5.98e-06, Loss=2.32e-02 BER=8.78e-03 FER=1.10e-01
2025-10-14 15:51:14,277 | INFO | Epoch 857 Train Time 19.504454851150513s

2025-10-14 15:51:32,930 | INFO | Training epoch 858, Batch 1000/1000: LR=5.91e-06, Loss=2.34e-02 BER=8.98e-03 FER=1.13e-01
2025-10-14 15:51:32,983 | INFO | Epoch 858 Train Time 18.7053279876709s

2025-10-14 15:51:52,187 | INFO | Training epoch 859, Batch 1000/1000: LR=5.84e-06, Loss=2.29e-02 BER=8.73e-03 FER=1.10e-01
2025-10-14 15:51:52,244 | INFO | Epoch 859 Train Time 19.260211944580078s

2025-10-14 15:52:10,809 | INFO | Training epoch 860, Batch 1000/1000: LR=5.78e-06, Loss=2.35e-02 BER=8.95e-03 FER=1.12e-01
2025-10-14 15:52:10,858 | INFO | Epoch 860 Train Time 18.613782167434692s

2025-10-14 15:52:30,708 | INFO | Training epoch 861, Batch 1000/1000: LR=5.71e-06, Loss=2.32e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 15:52:30,760 | INFO | Epoch 861 Train Time 19.901347637176514s

2025-10-14 15:52:49,230 | INFO | Training epoch 862, Batch 1000/1000: LR=5.65e-06, Loss=2.28e-02 BER=8.68e-03 FER=1.10e-01
2025-10-14 15:52:49,287 | INFO | Epoch 862 Train Time 18.525877237319946s

2025-10-14 15:53:07,132 | INFO | Training epoch 863, Batch 1000/1000: LR=5.58e-06, Loss=2.34e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 15:53:07,204 | INFO | Epoch 863 Train Time 17.916016340255737s

2025-10-14 15:53:27,714 | INFO | Training epoch 864, Batch 1000/1000: LR=5.51e-06, Loss=2.30e-02 BER=8.78e-03 FER=1.10e-01
2025-10-14 15:53:27,763 | INFO | Epoch 864 Train Time 20.558331966400146s

2025-10-14 15:53:47,335 | INFO | Training epoch 865, Batch 1000/1000: LR=5.45e-06, Loss=2.33e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 15:53:47,393 | INFO | Epoch 865 Train Time 19.62870979309082s

2025-10-14 15:54:07,496 | INFO | Training epoch 866, Batch 1000/1000: LR=5.39e-06, Loss=2.34e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 15:54:07,543 | INFO | Epoch 866 Train Time 20.148547172546387s

2025-10-14 15:54:27,414 | INFO | Training epoch 867, Batch 1000/1000: LR=5.32e-06, Loss=2.30e-02 BER=8.76e-03 FER=1.11e-01
2025-10-14 15:54:27,454 | INFO | Epoch 867 Train Time 19.910334587097168s

2025-10-14 15:54:47,796 | INFO | Training epoch 868, Batch 1000/1000: LR=5.26e-06, Loss=2.32e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 15:54:47,844 | INFO | Epoch 868 Train Time 20.389147520065308s

2025-10-14 15:55:05,610 | INFO | Training epoch 869, Batch 1000/1000: LR=5.20e-06, Loss=2.34e-02 BER=8.94e-03 FER=1.13e-01
2025-10-14 15:55:05,659 | INFO | Epoch 869 Train Time 17.814476013183594s

2025-10-14 15:55:23,844 | INFO | Training epoch 870, Batch 1000/1000: LR=5.13e-06, Loss=2.29e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 15:55:23,890 | INFO | Epoch 870 Train Time 18.23017716407776s

2025-10-14 15:55:41,733 | INFO | Training epoch 871, Batch 1000/1000: LR=5.07e-06, Loss=2.34e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 15:55:41,785 | INFO | Epoch 871 Train Time 17.894071340560913s

2025-10-14 15:56:00,924 | INFO | Training epoch 872, Batch 1000/1000: LR=5.01e-06, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:56:00,974 | INFO | Epoch 872 Train Time 19.188414573669434s

2025-10-14 15:56:19,004 | INFO | Training epoch 873, Batch 1000/1000: LR=4.95e-06, Loss=2.31e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 15:56:19,053 | INFO | Epoch 873 Train Time 18.078453063964844s

2025-10-14 15:56:37,811 | INFO | Training epoch 874, Batch 1000/1000: LR=4.89e-06, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 15:56:37,860 | INFO | Epoch 874 Train Time 18.805992126464844s

2025-10-14 15:56:57,104 | INFO | Training epoch 875, Batch 1000/1000: LR=4.83e-06, Loss=2.35e-02 BER=9.02e-03 FER=1.13e-01
2025-10-14 15:56:57,152 | INFO | Epoch 875 Train Time 19.291079998016357s

2025-10-14 15:57:15,932 | INFO | Training epoch 876, Batch 1000/1000: LR=4.77e-06, Loss=2.30e-02 BER=8.79e-03 FER=1.10e-01
2025-10-14 15:57:15,978 | INFO | Epoch 876 Train Time 18.825512647628784s

2025-10-14 15:57:32,738 | INFO | Training epoch 877, Batch 1000/1000: LR=4.71e-06, Loss=2.33e-02 BER=8.94e-03 FER=1.11e-01
2025-10-14 15:57:32,796 | INFO | Epoch 877 Train Time 16.81655263900757s

2025-10-14 15:57:51,331 | INFO | Training epoch 878, Batch 1000/1000: LR=4.65e-06, Loss=2.32e-02 BER=8.84e-03 FER=1.12e-01
2025-10-14 15:57:51,383 | INFO | Epoch 878 Train Time 18.58691668510437s

2025-10-14 15:58:11,141 | INFO | Training epoch 879, Batch 1000/1000: LR=4.59e-06, Loss=2.31e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 15:58:11,201 | INFO | Epoch 879 Train Time 19.81740641593933s

2025-10-14 15:58:29,040 | INFO | Training epoch 880, Batch 1000/1000: LR=4.53e-06, Loss=2.30e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:58:29,091 | INFO | Epoch 880 Train Time 17.888848304748535s

2025-10-14 15:58:48,709 | INFO | Training epoch 881, Batch 1000/1000: LR=4.48e-06, Loss=2.32e-02 BER=8.78e-03 FER=1.10e-01
2025-10-14 15:58:48,777 | INFO | Epoch 881 Train Time 19.684998989105225s

2025-10-14 15:59:08,633 | INFO | Training epoch 882, Batch 1000/1000: LR=4.42e-06, Loss=2.30e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 15:59:08,691 | INFO | Epoch 882 Train Time 19.913668632507324s

2025-10-14 15:59:27,829 | INFO | Training epoch 883, Batch 1000/1000: LR=4.36e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 15:59:27,876 | INFO | Epoch 883 Train Time 19.184553384780884s

2025-10-14 15:59:47,022 | INFO | Training epoch 884, Batch 1000/1000: LR=4.31e-06, Loss=2.32e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 15:59:47,073 | INFO | Epoch 884 Train Time 19.196037769317627s

2025-10-14 16:00:07,186 | INFO | Training epoch 885, Batch 1000/1000: LR=4.25e-06, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 16:00:07,234 | INFO | Epoch 885 Train Time 20.159024238586426s

2025-10-14 16:00:25,389 | INFO | Training epoch 886, Batch 1000/1000: LR=4.20e-06, Loss=2.33e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 16:00:25,431 | INFO | Epoch 886 Train Time 18.196577548980713s

2025-10-14 16:00:44,207 | INFO | Training epoch 887, Batch 1000/1000: LR=4.14e-06, Loss=2.33e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 16:00:44,254 | INFO | Epoch 887 Train Time 18.823026180267334s

2025-10-14 16:01:03,285 | INFO | Training epoch 888, Batch 1000/1000: LR=4.09e-06, Loss=2.33e-02 BER=8.86e-03 FER=1.12e-01
2025-10-14 16:01:03,334 | INFO | Epoch 888 Train Time 19.078575134277344s

2025-10-14 16:01:23,306 | INFO | Training epoch 889, Batch 1000/1000: LR=4.03e-06, Loss=2.32e-02 BER=8.90e-03 FER=1.13e-01
2025-10-14 16:01:23,373 | INFO | Epoch 889 Train Time 20.038681507110596s

2025-10-14 16:01:43,136 | INFO | Training epoch 890, Batch 1000/1000: LR=3.98e-06, Loss=2.30e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 16:01:43,193 | INFO | Epoch 890 Train Time 19.81905436515808s

2025-10-14 16:02:03,729 | INFO | Training epoch 891, Batch 1000/1000: LR=3.93e-06, Loss=2.33e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:02:03,785 | INFO | Epoch 891 Train Time 20.591457843780518s

2025-10-14 16:02:22,239 | INFO | Training epoch 892, Batch 1000/1000: LR=3.87e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 16:02:22,295 | INFO | Epoch 892 Train Time 18.509811878204346s

2025-10-14 16:02:41,097 | INFO | Training epoch 893, Batch 1000/1000: LR=3.82e-06, Loss=2.32e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:02:41,141 | INFO | Epoch 893 Train Time 18.844804286956787s

2025-10-14 16:03:00,176 | INFO | Training epoch 894, Batch 1000/1000: LR=3.77e-06, Loss=2.28e-02 BER=8.73e-03 FER=1.09e-01
2025-10-14 16:03:00,218 | INFO | Epoch 894 Train Time 19.07609987258911s

2025-10-14 16:03:19,021 | INFO | Training epoch 895, Batch 1000/1000: LR=3.72e-06, Loss=2.34e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 16:03:19,068 | INFO | Epoch 895 Train Time 18.849841594696045s

2025-10-14 16:03:39,432 | INFO | Training epoch 896, Batch 1000/1000: LR=3.67e-06, Loss=2.32e-02 BER=8.91e-03 FER=1.12e-01
2025-10-14 16:03:39,485 | INFO | Epoch 896 Train Time 20.415568828582764s

2025-10-14 16:03:57,522 | INFO | Training epoch 897, Batch 1000/1000: LR=3.62e-06, Loss=2.31e-02 BER=8.86e-03 FER=1.11e-01
2025-10-14 16:03:57,574 | INFO | Epoch 897 Train Time 18.087891101837158s

2025-10-14 16:04:16,911 | INFO | Training epoch 898, Batch 1000/1000: LR=3.57e-06, Loss=2.33e-02 BER=8.89e-03 FER=1.12e-01
2025-10-14 16:04:16,980 | INFO | Epoch 898 Train Time 19.404311895370483s

2025-10-14 16:04:35,609 | INFO | Training epoch 899, Batch 1000/1000: LR=3.52e-06, Loss=2.34e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 16:04:35,662 | INFO | Epoch 899 Train Time 18.681485414505005s

2025-10-14 16:04:54,899 | INFO | Training epoch 900, Batch 1000/1000: LR=3.47e-06, Loss=2.30e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 16:04:54,941 | INFO | Epoch 900 Train Time 19.278586864471436s

2025-10-14 16:05:13,710 | INFO | Training epoch 901, Batch 1000/1000: LR=3.42e-06, Loss=2.33e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 16:05:13,758 | INFO | Epoch 901 Train Time 18.816287994384766s

2025-10-14 16:05:33,234 | INFO | Training epoch 902, Batch 1000/1000: LR=3.37e-06, Loss=2.32e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 16:05:33,286 | INFO | Epoch 902 Train Time 19.527016162872314s

2025-10-14 16:05:51,416 | INFO | Training epoch 903, Batch 1000/1000: LR=3.33e-06, Loss=2.29e-02 BER=8.71e-03 FER=1.10e-01
2025-10-14 16:05:51,470 | INFO | Epoch 903 Train Time 18.182602167129517s

2025-10-14 16:06:10,532 | INFO | Training epoch 904, Batch 1000/1000: LR=3.28e-06, Loss=2.32e-02 BER=8.85e-03 FER=1.10e-01
2025-10-14 16:06:10,589 | INFO | Epoch 904 Train Time 19.11740493774414s

2025-10-14 16:06:30,534 | INFO | Training epoch 905, Batch 1000/1000: LR=3.23e-06, Loss=2.33e-02 BER=8.93e-03 FER=1.12e-01
2025-10-14 16:06:30,592 | INFO | Epoch 905 Train Time 20.002010107040405s

2025-10-14 16:06:49,540 | INFO | Training epoch 906, Batch 1000/1000: LR=3.19e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:06:49,591 | INFO | Epoch 906 Train Time 18.998618602752686s

2025-10-14 16:07:08,514 | INFO | Training epoch 907, Batch 1000/1000: LR=3.14e-06, Loss=2.31e-02 BER=8.76e-03 FER=1.10e-01
2025-10-14 16:07:08,562 | INFO | Epoch 907 Train Time 18.970327138900757s

2025-10-14 16:07:25,922 | INFO | Training epoch 908, Batch 1000/1000: LR=3.10e-06, Loss=2.34e-02 BER=8.96e-03 FER=1.12e-01
2025-10-14 16:07:25,992 | INFO | Epoch 908 Train Time 17.4292471408844s

2025-10-14 16:07:45,841 | INFO | Training epoch 909, Batch 1000/1000: LR=3.05e-06, Loss=2.32e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 16:07:45,893 | INFO | Epoch 909 Train Time 19.89931583404541s

2025-10-14 16:08:05,631 | INFO | Training epoch 910, Batch 1000/1000: LR=3.01e-06, Loss=2.28e-02 BER=8.72e-03 FER=1.09e-01
2025-10-14 16:08:05,684 | INFO | Epoch 910 Train Time 19.790709257125854s

2025-10-14 16:08:25,284 | INFO | Training epoch 911, Batch 1000/1000: LR=2.97e-06, Loss=2.33e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 16:08:25,345 | INFO | Epoch 911 Train Time 19.660174131393433s

2025-10-14 16:08:44,031 | INFO | Training epoch 912, Batch 1000/1000: LR=2.92e-06, Loss=2.31e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 16:08:44,083 | INFO | Epoch 912 Train Time 18.73653554916382s

2025-10-14 16:09:02,400 | INFO | Training epoch 913, Batch 1000/1000: LR=2.88e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:09:02,450 | INFO | Epoch 913 Train Time 18.36676836013794s

2025-10-14 16:09:22,129 | INFO | Training epoch 914, Batch 1000/1000: LR=2.84e-06, Loss=2.27e-02 BER=8.65e-03 FER=1.09e-01
2025-10-14 16:09:22,176 | INFO | Epoch 914 Train Time 19.724764585494995s

2025-10-14 16:09:41,310 | INFO | Training epoch 915, Batch 1000/1000: LR=2.80e-06, Loss=2.33e-02 BER=8.98e-03 FER=1.12e-01
2025-10-14 16:09:41,367 | INFO | Epoch 915 Train Time 19.190574407577515s

2025-10-14 16:10:00,518 | INFO | Training epoch 916, Batch 1000/1000: LR=2.75e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 16:10:00,574 | INFO | Epoch 916 Train Time 19.206318378448486s

2025-10-14 16:10:20,337 | INFO | Training epoch 917, Batch 1000/1000: LR=2.71e-06, Loss=2.32e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 16:10:20,392 | INFO | Epoch 917 Train Time 19.81724452972412s

2025-10-14 16:10:40,346 | INFO | Training epoch 918, Batch 1000/1000: LR=2.67e-06, Loss=2.30e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 16:10:40,404 | INFO | Epoch 918 Train Time 20.011214017868042s

2025-10-14 16:11:00,232 | INFO | Training epoch 919, Batch 1000/1000: LR=2.63e-06, Loss=2.33e-02 BER=8.90e-03 FER=1.13e-01
2025-10-14 16:11:00,289 | INFO | Epoch 919 Train Time 19.883575201034546s

2025-10-14 16:11:20,899 | INFO | Training epoch 920, Batch 1000/1000: LR=2.59e-06, Loss=2.31e-02 BER=8.80e-03 FER=1.10e-01
2025-10-14 16:11:20,950 | INFO | Epoch 920 Train Time 20.660598278045654s

2025-10-14 16:11:41,604 | INFO | Training epoch 921, Batch 1000/1000: LR=2.56e-06, Loss=2.30e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 16:11:41,650 | INFO | Epoch 921 Train Time 20.69845962524414s

2025-10-14 16:12:00,062 | INFO | Training epoch 922, Batch 1000/1000: LR=2.52e-06, Loss=2.32e-02 BER=8.79e-03 FER=1.11e-01
2025-10-14 16:12:00,120 | INFO | Epoch 922 Train Time 18.469119787216187s

2025-10-14 16:12:19,429 | INFO | Training epoch 923, Batch 1000/1000: LR=2.48e-06, Loss=2.30e-02 BER=8.81e-03 FER=1.10e-01
2025-10-14 16:12:19,485 | INFO | Epoch 923 Train Time 19.36415696144104s

2025-10-14 16:12:39,042 | INFO | Training epoch 924, Batch 1000/1000: LR=2.44e-06, Loss=2.32e-02 BER=8.86e-03 FER=1.11e-01
2025-10-14 16:12:39,106 | INFO | Epoch 924 Train Time 19.620665311813354s

2025-10-14 16:12:57,703 | INFO | Training epoch 925, Batch 1000/1000: LR=2.40e-06, Loss=2.28e-02 BER=8.73e-03 FER=1.11e-01
2025-10-14 16:12:57,741 | INFO | Epoch 925 Train Time 18.633917808532715s

2025-10-14 16:13:16,034 | INFO | Training epoch 926, Batch 1000/1000: LR=2.37e-06, Loss=2.32e-02 BER=8.86e-03 FER=1.11e-01
2025-10-14 16:13:16,086 | INFO | Epoch 926 Train Time 18.34398317337036s

2025-10-14 16:13:35,305 | INFO | Training epoch 927, Batch 1000/1000: LR=2.33e-06, Loss=2.29e-02 BER=8.68e-03 FER=1.09e-01
2025-10-14 16:13:35,356 | INFO | Epoch 927 Train Time 19.26949644088745s

2025-10-14 16:13:53,325 | INFO | Training epoch 928, Batch 1000/1000: LR=2.30e-06, Loss=2.32e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 16:13:53,379 | INFO | Epoch 928 Train Time 18.02177381515503s

2025-10-14 16:14:11,196 | INFO | Training epoch 929, Batch 1000/1000: LR=2.26e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.10e-01
2025-10-14 16:14:11,246 | INFO | Epoch 929 Train Time 17.86617922782898s

2025-10-14 16:14:31,759 | INFO | Training epoch 930, Batch 1000/1000: LR=2.23e-06, Loss=2.29e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 16:14:31,828 | INFO | Epoch 930 Train Time 20.58157753944397s

2025-10-14 16:14:52,212 | INFO | Training epoch 931, Batch 1000/1000: LR=2.19e-06, Loss=2.32e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 16:14:52,266 | INFO | Epoch 931 Train Time 20.435752391815186s

2025-10-14 16:15:12,746 | INFO | Training epoch 932, Batch 1000/1000: LR=2.16e-06, Loss=2.28e-02 BER=8.77e-03 FER=1.10e-01
2025-10-14 16:15:12,814 | INFO | Epoch 932 Train Time 20.547167778015137s

2025-10-14 16:15:33,530 | INFO | Training epoch 933, Batch 1000/1000: LR=2.13e-06, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 16:15:33,581 | INFO | Epoch 933 Train Time 20.766168117523193s

2025-10-14 16:15:53,316 | INFO | Training epoch 934, Batch 1000/1000: LR=2.09e-06, Loss=2.35e-02 BER=8.91e-03 FER=1.11e-01
2025-10-14 16:15:53,378 | INFO | Epoch 934 Train Time 19.79617953300476s

2025-10-14 16:16:14,129 | INFO | Training epoch 935, Batch 1000/1000: LR=2.06e-06, Loss=2.30e-02 BER=8.73e-03 FER=1.11e-01
2025-10-14 16:16:14,176 | INFO | Epoch 935 Train Time 20.797350645065308s

2025-10-14 16:16:35,201 | INFO | Training epoch 936, Batch 1000/1000: LR=2.03e-06, Loss=2.30e-02 BER=8.74e-03 FER=1.11e-01
2025-10-14 16:16:35,275 | INFO | Epoch 936 Train Time 21.09737229347229s

2025-10-14 16:16:56,448 | INFO | Training epoch 937, Batch 1000/1000: LR=2.00e-06, Loss=2.31e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 16:16:56,517 | INFO | Epoch 937 Train Time 21.241270065307617s

2025-10-14 16:17:16,931 | INFO | Training epoch 938, Batch 1000/1000: LR=1.97e-06, Loss=2.32e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 16:17:16,982 | INFO | Epoch 938 Train Time 20.464232921600342s

2025-10-14 16:17:39,033 | INFO | Training epoch 939, Batch 1000/1000: LR=1.94e-06, Loss=2.36e-02 BER=9.04e-03 FER=1.13e-01
2025-10-14 16:17:39,093 | INFO | Epoch 939 Train Time 22.110132455825806s

2025-10-14 16:17:59,919 | INFO | Training epoch 940, Batch 1000/1000: LR=1.91e-06, Loss=2.32e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 16:17:59,970 | INFO | Epoch 940 Train Time 20.875198125839233s

2025-10-14 16:18:21,438 | INFO | Training epoch 941, Batch 1000/1000: LR=1.88e-06, Loss=2.29e-02 BER=8.77e-03 FER=1.10e-01
2025-10-14 16:18:21,513 | INFO | Epoch 941 Train Time 21.541767597198486s

2025-10-14 16:18:42,149 | INFO | Training epoch 942, Batch 1000/1000: LR=1.85e-06, Loss=2.34e-02 BER=8.94e-03 FER=1.12e-01
2025-10-14 16:18:42,217 | INFO | Epoch 942 Train Time 20.703321933746338s

2025-10-14 16:19:02,695 | INFO | Training epoch 943, Batch 1000/1000: LR=1.82e-06, Loss=2.27e-02 BER=8.66e-03 FER=1.09e-01
2025-10-14 16:19:02,748 | INFO | Epoch 943 Train Time 20.529775857925415s

2025-10-14 16:19:23,736 | INFO | Training epoch 944, Batch 1000/1000: LR=1.79e-06, Loss=2.32e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:19:23,797 | INFO | Epoch 944 Train Time 21.04818034172058s

2025-10-14 16:19:45,218 | INFO | Training epoch 945, Batch 1000/1000: LR=1.76e-06, Loss=2.28e-02 BER=8.69e-03 FER=1.10e-01
2025-10-14 16:19:45,267 | INFO | Epoch 945 Train Time 21.468343496322632s

2025-10-14 16:20:06,520 | INFO | Training epoch 946, Batch 1000/1000: LR=1.74e-06, Loss=2.31e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 16:20:06,575 | INFO | Epoch 946 Train Time 21.3078715801239s

2025-10-14 16:20:26,319 | INFO | Training epoch 947, Batch 1000/1000: LR=1.71e-06, Loss=2.30e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 16:20:26,370 | INFO | Epoch 947 Train Time 19.793691635131836s

2025-10-14 16:20:46,959 | INFO | Training epoch 948, Batch 1000/1000: LR=1.68e-06, Loss=2.28e-02 BER=8.69e-03 FER=1.10e-01
2025-10-14 16:20:47,015 | INFO | Epoch 948 Train Time 20.64440941810608s

2025-10-14 16:21:08,455 | INFO | Training epoch 949, Batch 1000/1000: LR=1.66e-06, Loss=2.33e-02 BER=8.93e-03 FER=1.13e-01
2025-10-14 16:21:08,507 | INFO | Epoch 949 Train Time 21.49161386489868s

2025-10-14 16:21:28,135 | INFO | Training epoch 950, Batch 1000/1000: LR=1.63e-06, Loss=2.31e-02 BER=8.81e-03 FER=1.10e-01
2025-10-14 16:21:28,188 | INFO | Epoch 950 Train Time 19.679574489593506s

2025-10-14 16:21:49,105 | INFO | Training epoch 951, Batch 1000/1000: LR=1.61e-06, Loss=2.29e-02 BER=8.67e-03 FER=1.10e-01
2025-10-14 16:21:49,168 | INFO | Epoch 951 Train Time 20.980034112930298s

2025-10-14 16:22:10,142 | INFO | Training epoch 952, Batch 1000/1000: LR=1.59e-06, Loss=2.31e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 16:22:10,202 | INFO | Epoch 952 Train Time 21.032386779785156s

2025-10-14 16:22:30,903 | INFO | Training epoch 953, Batch 1000/1000: LR=1.56e-06, Loss=2.32e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 16:22:30,948 | INFO | Epoch 953 Train Time 20.743831872940063s

2025-10-14 16:22:51,789 | INFO | Training epoch 954, Batch 1000/1000: LR=1.54e-06, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 16:22:51,835 | INFO | Epoch 954 Train Time 20.88691544532776s

2025-10-14 16:23:12,835 | INFO | Training epoch 955, Batch 1000/1000: LR=1.52e-06, Loss=2.32e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 16:23:12,890 | INFO | Epoch 955 Train Time 21.053842067718506s

2025-10-14 16:23:34,052 | INFO | Training epoch 956, Batch 1000/1000: LR=1.49e-06, Loss=2.29e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 16:23:34,111 | INFO | Epoch 956 Train Time 21.21968698501587s

2025-10-14 16:23:56,074 | INFO | Training epoch 957, Batch 1000/1000: LR=1.47e-06, Loss=2.30e-02 BER=8.77e-03 FER=1.10e-01
2025-10-14 16:23:56,123 | INFO | Epoch 957 Train Time 22.011099338531494s

2025-10-14 16:24:15,612 | INFO | Training epoch 958, Batch 1000/1000: LR=1.45e-06, Loss=2.28e-02 BER=8.75e-03 FER=1.09e-01
2025-10-14 16:24:15,669 | INFO | Epoch 958 Train Time 19.54549551010132s

2025-10-14 16:24:35,628 | INFO | Training epoch 959, Batch 1000/1000: LR=1.43e-06, Loss=2.33e-02 BER=8.92e-03 FER=1.12e-01
2025-10-14 16:24:35,671 | INFO | Epoch 959 Train Time 20.001578092575073s

2025-10-14 16:24:57,512 | INFO | Training epoch 960, Batch 1000/1000: LR=1.41e-06, Loss=2.29e-02 BER=8.77e-03 FER=1.12e-01
2025-10-14 16:24:57,577 | INFO | Epoch 960 Train Time 21.904707193374634s

2025-10-14 16:25:17,906 | INFO | Training epoch 961, Batch 1000/1000: LR=1.39e-06, Loss=2.35e-02 BER=9.00e-03 FER=1.13e-01
2025-10-14 16:25:17,957 | INFO | Epoch 961 Train Time 20.378546953201294s

2025-10-14 16:25:38,251 | INFO | Training epoch 962, Batch 1000/1000: LR=1.37e-06, Loss=2.32e-02 BER=8.89e-03 FER=1.11e-01
2025-10-14 16:25:38,308 | INFO | Epoch 962 Train Time 20.350146055221558s

2025-10-14 16:25:58,340 | INFO | Training epoch 963, Batch 1000/1000: LR=1.35e-06, Loss=2.34e-02 BER=8.95e-03 FER=1.12e-01
2025-10-14 16:25:58,384 | INFO | Epoch 963 Train Time 20.07594895362854s

2025-10-14 16:26:18,533 | INFO | Training epoch 964, Batch 1000/1000: LR=1.33e-06, Loss=2.31e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 16:26:18,588 | INFO | Epoch 964 Train Time 20.202362298965454s

2025-10-14 16:26:39,602 | INFO | Training epoch 965, Batch 1000/1000: LR=1.32e-06, Loss=2.33e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:26:39,657 | INFO | Epoch 965 Train Time 21.068538904190063s

2025-10-14 16:26:59,715 | INFO | Training epoch 966, Batch 1000/1000: LR=1.30e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 16:26:59,767 | INFO | Epoch 966 Train Time 20.108777284622192s

2025-10-14 16:27:19,114 | INFO | Training epoch 967, Batch 1000/1000: LR=1.28e-06, Loss=2.37e-02 BER=9.09e-03 FER=1.14e-01
2025-10-14 16:27:19,167 | INFO | Epoch 967 Train Time 19.3988618850708s

2025-10-14 16:27:36,190 | INFO | Training epoch 968, Batch 1000/1000: LR=1.27e-06, Loss=2.33e-02 BER=8.84e-03 FER=1.10e-01
2025-10-14 16:27:36,240 | INFO | Epoch 968 Train Time 17.072739839553833s

2025-10-14 16:27:54,604 | INFO | Training epoch 969, Batch 1000/1000: LR=1.25e-06, Loss=2.31e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 16:27:54,659 | INFO | Epoch 969 Train Time 18.417946577072144s

2025-10-14 16:28:11,916 | INFO | Training epoch 970, Batch 1000/1000: LR=1.23e-06, Loss=2.31e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 16:28:11,975 | INFO | Epoch 970 Train Time 17.315754413604736s

2025-10-14 16:28:32,949 | INFO | Training epoch 971, Batch 1000/1000: LR=1.22e-06, Loss=2.32e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 16:28:33,000 | INFO | Epoch 971 Train Time 21.024596691131592s

2025-10-14 16:28:53,800 | INFO | Training epoch 972, Batch 1000/1000: LR=1.21e-06, Loss=2.26e-02 BER=8.67e-03 FER=1.10e-01
2025-10-14 16:28:53,844 | INFO | Epoch 972 Train Time 20.84279775619507s

2025-10-14 16:29:14,722 | INFO | Training epoch 973, Batch 1000/1000: LR=1.19e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:29:14,774 | INFO | Epoch 973 Train Time 20.927950859069824s

2025-10-14 16:29:35,647 | INFO | Training epoch 974, Batch 1000/1000: LR=1.18e-06, Loss=2.31e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 16:29:35,698 | INFO | Epoch 974 Train Time 20.922883987426758s

2025-10-14 16:29:55,412 | INFO | Training epoch 975, Batch 1000/1000: LR=1.17e-06, Loss=2.32e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 16:29:55,477 | INFO | Epoch 975 Train Time 19.777944326400757s

2025-10-14 16:30:15,231 | INFO | Training epoch 976, Batch 1000/1000: LR=1.15e-06, Loss=2.27e-02 BER=8.65e-03 FER=1.09e-01
2025-10-14 16:30:15,279 | INFO | Epoch 976 Train Time 19.801228284835815s

2025-10-14 16:30:34,911 | INFO | Training epoch 977, Batch 1000/1000: LR=1.14e-06, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 16:30:34,970 | INFO | Epoch 977 Train Time 19.690277099609375s

2025-10-14 16:30:55,211 | INFO | Training epoch 978, Batch 1000/1000: LR=1.13e-06, Loss=2.31e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 16:30:55,260 | INFO | Epoch 978 Train Time 20.28926157951355s

2025-10-14 16:31:14,114 | INFO | Training epoch 979, Batch 1000/1000: LR=1.12e-06, Loss=2.32e-02 BER=8.87e-03 FER=1.12e-01
2025-10-14 16:31:14,174 | INFO | Epoch 979 Train Time 18.9133780002594s

2025-10-14 16:31:35,509 | INFO | Training epoch 980, Batch 1000/1000: LR=1.11e-06, Loss=2.34e-02 BER=8.91e-03 FER=1.11e-01
2025-10-14 16:31:35,559 | INFO | Epoch 980 Train Time 21.383580684661865s

2025-10-14 16:31:56,145 | INFO | Training epoch 981, Batch 1000/1000: LR=1.10e-06, Loss=2.29e-02 BER=8.76e-03 FER=1.11e-01
2025-10-14 16:31:56,194 | INFO | Epoch 981 Train Time 20.63426375389099s

2025-10-14 16:32:15,807 | INFO | Training epoch 982, Batch 1000/1000: LR=1.09e-06, Loss=2.31e-02 BER=8.80e-03 FER=1.11e-01
2025-10-14 16:32:15,857 | INFO | Epoch 982 Train Time 19.661440134048462s

2025-10-14 16:32:35,628 | INFO | Training epoch 983, Batch 1000/1000: LR=1.08e-06, Loss=2.32e-02 BER=8.85e-03 FER=1.11e-01
2025-10-14 16:32:35,678 | INFO | Epoch 983 Train Time 19.819036722183228s

2025-10-14 16:32:54,025 | INFO | Training epoch 984, Batch 1000/1000: LR=1.07e-06, Loss=2.33e-02 BER=8.88e-03 FER=1.12e-01
2025-10-14 16:32:54,073 | INFO | Epoch 984 Train Time 18.394428730010986s

2025-10-14 16:33:12,729 | INFO | Training epoch 985, Batch 1000/1000: LR=1.06e-06, Loss=2.30e-02 BER=8.77e-03 FER=1.11e-01
2025-10-14 16:33:12,781 | INFO | Epoch 985 Train Time 18.706992864608765s

2025-10-14 16:33:31,513 | INFO | Training epoch 986, Batch 1000/1000: LR=1.05e-06, Loss=2.32e-02 BER=8.88e-03 FER=1.11e-01
2025-10-14 16:33:31,566 | INFO | Epoch 986 Train Time 18.784213304519653s

2025-10-14 16:33:51,213 | INFO | Training epoch 987, Batch 1000/1000: LR=1.05e-06, Loss=2.32e-02 BER=8.82e-03 FER=1.10e-01
2025-10-14 16:33:51,273 | INFO | Epoch 987 Train Time 19.706172466278076s

2025-10-14 16:34:11,525 | INFO | Training epoch 988, Batch 1000/1000: LR=1.04e-06, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 16:34:11,580 | INFO | Epoch 988 Train Time 20.304413080215454s

2025-10-14 16:34:32,325 | INFO | Training epoch 989, Batch 1000/1000: LR=1.04e-06, Loss=2.31e-02 BER=8.78e-03 FER=1.11e-01
2025-10-14 16:34:32,385 | INFO | Epoch 989 Train Time 20.80358648300171s

2025-10-14 16:34:52,442 | INFO | Training epoch 990, Batch 1000/1000: LR=1.03e-06, Loss=2.31e-02 BER=8.84e-03 FER=1.11e-01
2025-10-14 16:34:52,507 | INFO | Epoch 990 Train Time 20.121084213256836s

2025-10-14 16:35:11,003 | INFO | Training epoch 991, Batch 1000/1000: LR=1.02e-06, Loss=2.34e-02 BER=8.95e-03 FER=1.12e-01
2025-10-14 16:35:11,060 | INFO | Epoch 991 Train Time 18.55133628845215s

2025-10-14 16:35:32,220 | INFO | Training epoch 992, Batch 1000/1000: LR=1.02e-06, Loss=2.30e-02 BER=8.73e-03 FER=1.11e-01
2025-10-14 16:35:32,271 | INFO | Epoch 992 Train Time 21.210259675979614s

2025-10-14 16:35:52,341 | INFO | Training epoch 993, Batch 1000/1000: LR=1.02e-06, Loss=2.32e-02 BER=8.83e-03 FER=1.11e-01
2025-10-14 16:35:52,419 | INFO | Epoch 993 Train Time 20.14684748649597s

2025-10-14 16:36:10,903 | INFO | Training epoch 994, Batch 1000/1000: LR=1.01e-06, Loss=2.32e-02 BER=8.90e-03 FER=1.11e-01
2025-10-14 16:36:10,954 | INFO | Epoch 994 Train Time 18.533824920654297s

2025-10-14 16:36:32,031 | INFO | Training epoch 995, Batch 1000/1000: LR=1.01e-06, Loss=2.31e-02 BER=8.81e-03 FER=1.11e-01
2025-10-14 16:36:32,085 | INFO | Epoch 995 Train Time 21.129931449890137s

2025-10-14 16:36:53,328 | INFO | Training epoch 996, Batch 1000/1000: LR=1.01e-06, Loss=2.33e-02 BER=8.82e-03 FER=1.11e-01
2025-10-14 16:36:53,382 | INFO | Epoch 996 Train Time 21.295998096466064s

2025-10-14 16:37:11,102 | INFO | Training epoch 997, Batch 1000/1000: LR=1.00e-06, Loss=2.29e-02 BER=8.72e-03 FER=1.11e-01
2025-10-14 16:37:11,152 | INFO | Epoch 997 Train Time 17.76775860786438s

2025-10-14 16:37:31,724 | INFO | Training epoch 998, Batch 1000/1000: LR=1.00e-06, Loss=2.33e-02 BER=8.90e-03 FER=1.12e-01
2025-10-14 16:37:31,781 | INFO | Epoch 998 Train Time 20.628426551818848s

2025-10-14 16:37:51,012 | INFO | Training epoch 999, Batch 1000/1000: LR=1.00e-06, Loss=2.32e-02 BER=8.86e-03 FER=1.10e-01
2025-10-14 16:37:51,064 | INFO | Epoch 999 Train Time 19.281642198562622s

2025-10-14 16:38:11,013 | INFO | Training epoch 1000, Batch 1000/1000: LR=1.00e-06, Loss=2.32e-02 BER=8.79e-03 FER=1.11e-01
2025-10-14 16:38:11,063 | INFO | Epoch 1000 Train Time 19.998692989349365s

2025-10-14 16:38:11,070 | INFO | Checkpoint saved: runs/20251014_111436/stage1_fp32__BCH_n31_k16__Ndec2_d32_h8.pth
2025-10-14 16:38:11,076 | INFO | Checkpoint saved: runs/20251014_111436/stage1_fp32__BCH_n31_k16__Ndec2_d32_h8__e1000_loss0.023183.pth
2025-10-14 16:38:15,198 | INFO | FER count threshold reached for EbN0:4
2025-10-14 16:38:15,268 | INFO | Test EbN0=4, BER=1.26e-02
2025-10-14 16:38:19,287 | INFO | FER count threshold reached for EbN0:5
2025-10-14 16:38:19,348 | INFO | Test EbN0=5, BER=3.51e-03
2025-10-14 16:38:23,558 | INFO | FER count threshold reached for EbN0:6
2025-10-14 16:38:23,634 | INFO | Test EbN0=6, BER=6.39e-04
2025-10-14 16:38:23,634 | INFO | 
Test Loss 4: 3.3104e-02 5: 9.8601e-03 6: 2.0706e-03
2025-10-14 16:38:23,634 | INFO | Test FER 4: 1.6559e-01 5: 5.5804e-02 6: 1.2337e-02
2025-10-14 16:38:23,634 | INFO | Test BER 4: 1.2590e-02 5: 3.5089e-03 6: 6.3872e-04
2025-10-14 16:38:23,634 | INFO | Test -ln(BER) 4: 4.3748e+00 5: 5.6524e+00 6: 7.3560e+00
2025-10-14 16:38:23,635 | INFO | # of testing samples: [100352.0, 100352.0, 100352.0]
 Test Time 12.557927131652832 s

2025-10-14 16:38:23,703 | INFO | Loaded checkpoint: runs/20251014_111436/stage1_fp32__BCH_n31_k16__Ndec2_d32_h8.pth (strict=False)
2025-10-14 16:38:54,506 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=1.13e-01 BER=3.82e-02 FER=4.12e-01
2025-10-14 16:38:54,555 | INFO | Epoch 1 Train Time 30.850977659225464s

2025-10-14 16:38:54,557 | INFO | [P2] saving best_model (QAT) with loss 0.112503 at epoch 1
2025-10-14 16:39:25,001 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=5.28e-02 BER=1.85e-02 FER=2.59e-01
2025-10-14 16:39:25,052 | INFO | Epoch 2 Train Time 30.476596355438232s

2025-10-14 16:39:25,053 | INFO | [P2] saving best_model (QAT) with loss 0.052798 at epoch 2
2025-10-14 16:39:56,599 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=4.50e-02 BER=1.61e-02 FER=2.24e-01
2025-10-14 16:39:56,650 | INFO | Epoch 3 Train Time 31.58272361755371s

2025-10-14 16:39:56,652 | INFO | [P2] saving best_model (QAT) with loss 0.045013 at epoch 3
2025-10-14 16:40:28,325 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=4.25e-02 BER=1.54e-02 FER=2.09e-01
2025-10-14 16:40:28,387 | INFO | Epoch 4 Train Time 31.717231035232544s

2025-10-14 16:40:28,388 | INFO | [P2] saving best_model (QAT) with loss 0.042517 at epoch 4
2025-10-14 16:41:00,156 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=4.01e-02 BER=1.47e-02 FER=1.99e-01
2025-10-14 16:41:00,208 | INFO | Epoch 5 Train Time 31.797277212142944s

2025-10-14 16:41:00,209 | INFO | [P2] saving best_model (QAT) with loss 0.040136 at epoch 5
2025-10-14 16:41:31,694 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=3.91e-02 BER=1.44e-02 FER=1.93e-01
2025-10-14 16:41:31,762 | INFO | Epoch 6 Train Time 31.529672145843506s

2025-10-14 16:41:31,762 | INFO | [P2] saving best_model (QAT) with loss 0.039054 at epoch 6
2025-10-14 16:42:03,503 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=3.77e-02 BER=1.40e-02 FER=1.87e-01
2025-10-14 16:42:03,556 | INFO | Epoch 7 Train Time 31.778499841690063s

2025-10-14 16:42:03,558 | INFO | [P2] saving best_model (QAT) with loss 0.037688 at epoch 7
2025-10-14 16:42:35,300 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=3.74e-02 BER=1.39e-02 FER=1.86e-01
2025-10-14 16:42:35,362 | INFO | Epoch 8 Train Time 31.789775848388672s

2025-10-14 16:42:35,363 | INFO | [P2] saving best_model (QAT) with loss 0.037410 at epoch 8
2025-10-14 16:43:07,204 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=3.67e-02 BER=1.37e-02 FER=1.81e-01
2025-10-14 16:43:07,259 | INFO | Epoch 9 Train Time 31.88106942176819s

2025-10-14 16:43:07,260 | INFO | [P2] saving best_model (QAT) with loss 0.036673 at epoch 9
2025-10-14 16:43:39,311 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=3.60e-02 BER=1.34e-02 FER=1.80e-01
2025-10-14 16:43:39,372 | INFO | Epoch 10 Train Time 32.09284520149231s

2025-10-14 16:43:39,373 | INFO | [P2] saving best_model (QAT) with loss 0.036036 at epoch 10
2025-10-14 16:44:11,699 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=3.60e-02 BER=1.34e-02 FER=1.77e-01
2025-10-14 16:44:11,752 | INFO | Epoch 11 Train Time 32.36546540260315s

2025-10-14 16:44:11,753 | INFO | [P2] saving best_model (QAT) with loss 0.036012 at epoch 11
2025-10-14 16:44:43,496 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=3.59e-02 BER=1.34e-02 FER=1.77e-01
2025-10-14 16:44:43,550 | INFO | Epoch 12 Train Time 31.77251696586609s

2025-10-14 16:44:43,551 | INFO | [P2] saving best_model (QAT) with loss 0.035906 at epoch 12
2025-10-14 16:45:15,421 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=3.56e-02 BER=1.32e-02 FER=1.74e-01
2025-10-14 16:45:15,471 | INFO | Epoch 13 Train Time 31.905998945236206s

2025-10-14 16:45:15,472 | INFO | [P2] saving best_model (QAT) with loss 0.035553 at epoch 13
2025-10-14 16:45:46,617 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=3.56e-02 BER=1.32e-02 FER=1.75e-01
2025-10-14 16:45:46,672 | INFO | Epoch 14 Train Time 31.185609102249146s

2025-10-14 16:46:18,598 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=3.49e-02 BER=1.29e-02 FER=1.72e-01
2025-10-14 16:46:18,656 | INFO | Epoch 15 Train Time 31.98283076286316s

2025-10-14 16:46:18,657 | INFO | [P2] saving best_model (QAT) with loss 0.034933 at epoch 15
2025-10-14 16:46:49,902 | INFO | Training epoch 16, Batch 1000/1000: LR=1.00e-04, Loss=3.55e-02 BER=1.32e-02 FER=1.74e-01
2025-10-14 16:46:49,957 | INFO | Epoch 16 Train Time 31.28566575050354s

2025-10-14 16:47:20,802 | INFO | Training epoch 17, Batch 1000/1000: LR=1.00e-04, Loss=3.52e-02 BER=1.31e-02 FER=1.74e-01
2025-10-14 16:47:20,870 | INFO | Epoch 17 Train Time 30.91052269935608s

2025-10-14 16:47:52,625 | INFO | Training epoch 18, Batch 1000/1000: LR=1.00e-04, Loss=3.44e-02 BER=1.27e-02 FER=1.68e-01
2025-10-14 16:47:52,675 | INFO | Epoch 18 Train Time 31.804728507995605s

2025-10-14 16:47:52,676 | INFO | [P2] saving best_model (QAT) with loss 0.034387 at epoch 18
2025-10-14 16:48:24,457 | INFO | Training epoch 19, Batch 1000/1000: LR=1.00e-04, Loss=3.44e-02 BER=1.28e-02 FER=1.69e-01
2025-10-14 16:48:24,509 | INFO | Epoch 19 Train Time 31.81359887123108s

2025-10-14 16:48:24,510 | INFO | [P2] saving best_model (QAT) with loss 0.034386 at epoch 19
2025-10-14 16:48:56,308 | INFO | Training epoch 20, Batch 1000/1000: LR=1.00e-04, Loss=3.35e-02 BER=1.24e-02 FER=1.65e-01
2025-10-14 16:48:56,364 | INFO | Epoch 20 Train Time 31.83488965034485s

2025-10-14 16:48:56,365 | INFO | [P2] saving best_model (QAT) with loss 0.033463 at epoch 20
2025-10-14 16:49:28,106 | INFO | Training epoch 21, Batch 1000/1000: LR=1.00e-04, Loss=3.35e-02 BER=1.25e-02 FER=1.63e-01
2025-10-14 16:49:28,165 | INFO | Epoch 21 Train Time 31.78615927696228s

2025-10-14 16:49:59,659 | INFO | Training epoch 22, Batch 1000/1000: LR=1.00e-04, Loss=3.35e-02 BER=1.25e-02 FER=1.62e-01
2025-10-14 16:49:59,716 | INFO | Epoch 22 Train Time 31.550316333770752s

2025-10-14 16:50:30,908 | INFO | Training epoch 23, Batch 1000/1000: LR=1.00e-04, Loss=3.36e-02 BER=1.24e-02 FER=1.64e-01
2025-10-14 16:50:30,967 | INFO | Epoch 23 Train Time 31.249685764312744s

2025-10-14 16:51:02,603 | INFO | Training epoch 24, Batch 1000/1000: LR=1.00e-04, Loss=3.29e-02 BER=1.23e-02 FER=1.61e-01
2025-10-14 16:51:02,665 | INFO | Epoch 24 Train Time 31.696935176849365s

2025-10-14 16:51:02,665 | INFO | [P2] saving best_model (QAT) with loss 0.032920 at epoch 24
2025-10-14 16:51:34,700 | INFO | Training epoch 25, Batch 1000/1000: LR=1.00e-04, Loss=3.32e-02 BER=1.23e-02 FER=1.62e-01
2025-10-14 16:51:34,749 | INFO | Epoch 25 Train Time 32.068952798843384s

2025-10-14 16:52:06,395 | INFO | Training epoch 26, Batch 1000/1000: LR=1.00e-04, Loss=3.29e-02 BER=1.22e-02 FER=1.60e-01
2025-10-14 16:52:06,457 | INFO | Epoch 26 Train Time 31.707314014434814s

2025-10-14 16:52:06,458 | INFO | [P2] saving best_model (QAT) with loss 0.032861 at epoch 26
2025-10-14 16:52:37,603 | INFO | Training epoch 27, Batch 1000/1000: LR=1.00e-04, Loss=3.30e-02 BER=1.23e-02 FER=1.62e-01
2025-10-14 16:52:37,661 | INFO | Epoch 27 Train Time 31.188173294067383s

2025-10-14 16:53:09,509 | INFO | Training epoch 28, Batch 1000/1000: LR=1.00e-04, Loss=3.26e-02 BER=1.21e-02 FER=1.58e-01
2025-10-14 16:53:09,565 | INFO | Epoch 28 Train Time 31.90314269065857s

2025-10-14 16:53:09,566 | INFO | [P2] saving best_model (QAT) with loss 0.032601 at epoch 28
2025-10-14 16:53:41,007 | INFO | Training epoch 29, Batch 1000/1000: LR=1.00e-04, Loss=3.29e-02 BER=1.22e-02 FER=1.60e-01
2025-10-14 16:53:41,063 | INFO | Epoch 29 Train Time 31.48247218132019s

2025-10-14 16:54:12,111 | INFO | Training epoch 30, Batch 1000/1000: LR=9.99e-05, Loss=3.24e-02 BER=1.20e-02 FER=1.58e-01
2025-10-14 16:54:12,167 | INFO | Epoch 30 Train Time 31.102713584899902s

2025-10-14 16:54:12,167 | INFO | [P2] saving best_model (QAT) with loss 0.032400 at epoch 30
2025-10-14 16:54:42,804 | INFO | Training epoch 31, Batch 1000/1000: LR=9.99e-05, Loss=3.26e-02 BER=1.21e-02 FER=1.58e-01
2025-10-14 16:54:42,858 | INFO | Epoch 31 Train Time 30.6649968624115s

2025-10-14 16:55:14,609 | INFO | Training epoch 32, Batch 1000/1000: LR=9.99e-05, Loss=3.23e-02 BER=1.20e-02 FER=1.57e-01
2025-10-14 16:55:14,665 | INFO | Epoch 32 Train Time 31.805280447006226s

2025-10-14 16:55:14,665 | INFO | [P2] saving best_model (QAT) with loss 0.032283 at epoch 32
2025-10-14 16:55:46,208 | INFO | Training epoch 33, Batch 1000/1000: LR=9.99e-05, Loss=3.24e-02 BER=1.21e-02 FER=1.58e-01
2025-10-14 16:55:46,267 | INFO | Epoch 33 Train Time 31.588074922561646s

2025-10-14 16:56:17,594 | INFO | Training epoch 34, Batch 1000/1000: LR=9.99e-05, Loss=3.18e-02 BER=1.18e-02 FER=1.56e-01
2025-10-14 16:56:17,663 | INFO | Epoch 34 Train Time 31.39519214630127s

2025-10-14 16:56:17,663 | INFO | [P2] saving best_model (QAT) with loss 0.031770 at epoch 34
2025-10-14 16:56:48,904 | INFO | Training epoch 35, Batch 1000/1000: LR=9.99e-05, Loss=3.21e-02 BER=1.19e-02 FER=1.56e-01
2025-10-14 16:56:48,966 | INFO | Epoch 35 Train Time 31.287871837615967s

2025-10-14 16:57:20,507 | INFO | Training epoch 36, Batch 1000/1000: LR=9.99e-05, Loss=3.23e-02 BER=1.20e-02 FER=1.57e-01
2025-10-14 16:57:20,570 | INFO | Epoch 36 Train Time 31.60328435897827s

2025-10-14 16:57:51,899 | INFO | Training epoch 37, Batch 1000/1000: LR=9.99e-05, Loss=3.24e-02 BER=1.20e-02 FER=1.57e-01
2025-10-14 16:57:51,961 | INFO | Epoch 37 Train Time 31.389219284057617s

2025-10-14 16:58:23,098 | INFO | Training epoch 38, Batch 1000/1000: LR=9.99e-05, Loss=3.24e-02 BER=1.20e-02 FER=1.57e-01
2025-10-14 16:58:23,151 | INFO | Epoch 38 Train Time 31.188645362854004s

2025-10-14 16:58:54,624 | INFO | Training epoch 39, Batch 1000/1000: LR=9.99e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.55e-01
2025-10-14 16:58:54,688 | INFO | Epoch 39 Train Time 31.536009073257446s

2025-10-14 16:59:25,519 | INFO | Training epoch 40, Batch 1000/1000: LR=9.99e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.55e-01
2025-10-14 16:59:25,612 | INFO | Epoch 40 Train Time 30.923405408859253s

2025-10-14 16:59:25,613 | INFO | [P2] saving best_model (QAT) with loss 0.031737 at epoch 40
2025-10-14 16:59:57,607 | INFO | Training epoch 41, Batch 1000/1000: LR=9.99e-05, Loss=3.19e-02 BER=1.19e-02 FER=1.56e-01
2025-10-14 16:59:57,665 | INFO | Epoch 41 Train Time 32.037376403808594s

2025-10-14 17:00:28,994 | INFO | Training epoch 42, Batch 1000/1000: LR=9.99e-05, Loss=3.23e-02 BER=1.20e-02 FER=1.56e-01
2025-10-14 17:00:29,064 | INFO | Epoch 42 Train Time 31.398661375045776s

2025-10-14 17:01:00,110 | INFO | Training epoch 43, Batch 1000/1000: LR=9.99e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.54e-01
2025-10-14 17:01:00,166 | INFO | Epoch 43 Train Time 31.10125732421875s

2025-10-14 17:01:00,167 | INFO | [P2] saving best_model (QAT) with loss 0.031737 at epoch 43
2025-10-14 17:01:31,607 | INFO | Training epoch 44, Batch 1000/1000: LR=9.99e-05, Loss=3.15e-02 BER=1.17e-02 FER=1.54e-01
2025-10-14 17:01:31,663 | INFO | Epoch 44 Train Time 31.481422185897827s

2025-10-14 17:01:31,663 | INFO | [P2] saving best_model (QAT) with loss 0.031494 at epoch 44
2025-10-14 17:02:03,292 | INFO | Training epoch 45, Batch 1000/1000: LR=9.99e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.52e-01
2025-10-14 17:02:03,360 | INFO | Epoch 45 Train Time 31.682719469070435s

2025-10-14 17:02:03,361 | INFO | [P2] saving best_model (QAT) with loss 0.031241 at epoch 45
2025-10-14 17:02:34,494 | INFO | Training epoch 46, Batch 1000/1000: LR=9.99e-05, Loss=3.15e-02 BER=1.17e-02 FER=1.52e-01
2025-10-14 17:02:34,545 | INFO | Epoch 46 Train Time 31.170628309249878s

2025-10-14 17:03:05,903 | INFO | Training epoch 47, Batch 1000/1000: LR=9.99e-05, Loss=3.17e-02 BER=1.18e-02 FER=1.53e-01
2025-10-14 17:03:05,970 | INFO | Epoch 47 Train Time 31.42279601097107s

2025-10-14 17:03:37,008 | INFO | Training epoch 48, Batch 1000/1000: LR=9.99e-05, Loss=3.14e-02 BER=1.17e-02 FER=1.52e-01
2025-10-14 17:03:37,060 | INFO | Epoch 48 Train Time 31.089768648147583s

2025-10-14 17:04:08,200 | INFO | Training epoch 49, Batch 1000/1000: LR=9.99e-05, Loss=3.14e-02 BER=1.16e-02 FER=1.53e-01
2025-10-14 17:04:08,266 | INFO | Epoch 49 Train Time 31.205352544784546s

2025-10-14 17:04:39,493 | INFO | Training epoch 50, Batch 1000/1000: LR=9.99e-05, Loss=3.13e-02 BER=1.17e-02 FER=1.52e-01
2025-10-14 17:04:39,546 | INFO | Epoch 50 Train Time 31.27815556526184s

2025-10-14 17:05:10,007 | INFO | Training epoch 51, Batch 1000/1000: LR=9.98e-05, Loss=3.21e-02 BER=1.19e-02 FER=1.54e-01
2025-10-14 17:05:10,057 | INFO | Epoch 51 Train Time 30.510481357574463s

2025-10-14 17:05:41,420 | INFO | Training epoch 52, Batch 1000/1000: LR=9.98e-05, Loss=3.15e-02 BER=1.16e-02 FER=1.52e-01
2025-10-14 17:05:41,471 | INFO | Epoch 52 Train Time 31.413395404815674s

2025-10-14 17:06:12,603 | INFO | Training epoch 53, Batch 1000/1000: LR=9.98e-05, Loss=3.15e-02 BER=1.17e-02 FER=1.53e-01
2025-10-14 17:06:12,661 | INFO | Epoch 53 Train Time 31.189634084701538s

2025-10-14 17:06:43,592 | INFO | Training epoch 54, Batch 1000/1000: LR=9.98e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.49e-01
2025-10-14 17:06:43,650 | INFO | Epoch 54 Train Time 30.987893104553223s

2025-10-14 17:06:43,650 | INFO | [P2] saving best_model (QAT) with loss 0.030832 at epoch 54
2025-10-14 17:07:15,406 | INFO | Training epoch 55, Batch 1000/1000: LR=9.98e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.51e-01
2025-10-14 17:07:15,468 | INFO | Epoch 55 Train Time 31.803608655929565s

2025-10-14 17:07:47,511 | INFO | Training epoch 56, Batch 1000/1000: LR=9.98e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.50e-01
2025-10-14 17:07:47,567 | INFO | Epoch 56 Train Time 32.0973699092865s

2025-10-14 17:08:19,021 | INFO | Training epoch 57, Batch 1000/1000: LR=9.98e-05, Loss=3.13e-02 BER=1.16e-02 FER=1.51e-01
2025-10-14 17:08:19,077 | INFO | Epoch 57 Train Time 31.509118795394897s

2025-10-14 17:08:49,592 | INFO | Training epoch 58, Batch 1000/1000: LR=9.98e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.50e-01
2025-10-14 17:08:49,647 | INFO | Epoch 58 Train Time 30.569409132003784s

2025-10-14 17:09:20,494 | INFO | Training epoch 59, Batch 1000/1000: LR=9.98e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.50e-01
2025-10-14 17:09:20,548 | INFO | Epoch 59 Train Time 30.899564743041992s

2025-10-14 17:09:51,907 | INFO | Training epoch 60, Batch 1000/1000: LR=9.98e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.49e-01
2025-10-14 17:09:51,958 | INFO | Epoch 60 Train Time 31.408931732177734s

2025-10-14 17:10:23,289 | INFO | Training epoch 61, Batch 1000/1000: LR=9.98e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.50e-01
2025-10-14 17:10:23,343 | INFO | Epoch 61 Train Time 31.383514881134033s

2025-10-14 17:10:54,904 | INFO | Training epoch 62, Batch 1000/1000: LR=9.98e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:10:54,958 | INFO | Epoch 62 Train Time 31.61343502998352s

2025-10-14 17:10:54,958 | INFO | [P2] saving best_model (QAT) with loss 0.030516 at epoch 62
2025-10-14 17:11:26,296 | INFO | Training epoch 63, Batch 1000/1000: LR=9.98e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.49e-01
2025-10-14 17:11:26,353 | INFO | Epoch 63 Train Time 31.37969136238098s

2025-10-14 17:11:57,504 | INFO | Training epoch 64, Batch 1000/1000: LR=9.98e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.50e-01
2025-10-14 17:11:57,570 | INFO | Epoch 64 Train Time 31.216177701950073s

2025-10-14 17:12:28,403 | INFO | Training epoch 65, Batch 1000/1000: LR=9.98e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.50e-01
2025-10-14 17:12:28,469 | INFO | Epoch 65 Train Time 30.89763569831848s

2025-10-14 17:12:59,712 | INFO | Training epoch 66, Batch 1000/1000: LR=9.97e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.51e-01
2025-10-14 17:12:59,757 | INFO | Epoch 66 Train Time 31.286170721054077s

2025-10-14 17:13:30,620 | INFO | Training epoch 67, Batch 1000/1000: LR=9.97e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.49e-01
2025-10-14 17:13:30,680 | INFO | Epoch 67 Train Time 30.922276258468628s

2025-10-14 17:14:01,803 | INFO | Training epoch 68, Batch 1000/1000: LR=9.97e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.49e-01
2025-10-14 17:14:01,873 | INFO | Epoch 68 Train Time 31.192846298217773s

2025-10-14 17:14:33,101 | INFO | Training epoch 69, Batch 1000/1000: LR=9.97e-05, Loss=3.03e-02 BER=1.12e-02 FER=1.47e-01
2025-10-14 17:14:33,156 | INFO | Epoch 69 Train Time 31.281916856765747s

2025-10-14 17:14:33,156 | INFO | [P2] saving best_model (QAT) with loss 0.030258 at epoch 69
2025-10-14 17:15:04,689 | INFO | Training epoch 70, Batch 1000/1000: LR=9.97e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.47e-01
2025-10-14 17:15:04,746 | INFO | Epoch 70 Train Time 31.56836700439453s

2025-10-14 17:15:36,494 | INFO | Training epoch 71, Batch 1000/1000: LR=9.97e-05, Loss=3.12e-02 BER=1.16e-02 FER=1.50e-01
2025-10-14 17:15:36,559 | INFO | Epoch 71 Train Time 31.812403917312622s

2025-10-14 17:16:07,325 | INFO | Training epoch 72, Batch 1000/1000: LR=9.97e-05, Loss=3.11e-02 BER=1.15e-02 FER=1.50e-01
2025-10-14 17:16:07,382 | INFO | Epoch 72 Train Time 30.82188582420349s

2025-10-14 17:16:38,402 | INFO | Training epoch 73, Batch 1000/1000: LR=9.97e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:16:38,457 | INFO | Epoch 73 Train Time 31.07411813735962s

2025-10-14 17:17:09,203 | INFO | Training epoch 74, Batch 1000/1000: LR=9.97e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:17:09,275 | INFO | Epoch 74 Train Time 30.816964864730835s

2025-10-14 17:17:40,806 | INFO | Training epoch 75, Batch 1000/1000: LR=9.97e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:17:40,863 | INFO | Epoch 75 Train Time 31.587383031845093s

2025-10-14 17:18:12,005 | INFO | Training epoch 76, Batch 1000/1000: LR=9.97e-05, Loss=3.09e-02 BER=1.16e-02 FER=1.49e-01
2025-10-14 17:18:12,059 | INFO | Epoch 76 Train Time 31.194005727767944s

2025-10-14 17:18:43,798 | INFO | Training epoch 77, Batch 1000/1000: LR=9.96e-05, Loss=3.11e-02 BER=1.16e-02 FER=1.50e-01
2025-10-14 17:18:43,869 | INFO | Epoch 77 Train Time 31.809824228286743s

2025-10-14 17:19:15,048 | INFO | Training epoch 78, Batch 1000/1000: LR=9.96e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:19:15,099 | INFO | Epoch 78 Train Time 31.229007244110107s

2025-10-14 17:19:46,018 | INFO | Training epoch 79, Batch 1000/1000: LR=9.96e-05, Loss=3.07e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:19:46,067 | INFO | Epoch 79 Train Time 30.9675235748291s

2025-10-14 17:20:17,427 | INFO | Training epoch 80, Batch 1000/1000: LR=9.96e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:20:17,485 | INFO | Epoch 80 Train Time 31.41702127456665s

2025-10-14 17:20:48,893 | INFO | Training epoch 81, Batch 1000/1000: LR=9.96e-05, Loss=3.10e-02 BER=1.15e-02 FER=1.49e-01
2025-10-14 17:20:48,942 | INFO | Epoch 81 Train Time 31.45606803894043s

2025-10-14 17:21:20,824 | INFO | Training epoch 82, Batch 1000/1000: LR=9.96e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.48e-01
2025-10-14 17:21:20,878 | INFO | Epoch 82 Train Time 31.935245513916016s

2025-10-14 17:21:51,995 | INFO | Training epoch 83, Batch 1000/1000: LR=9.96e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.48e-01
2025-10-14 17:21:52,050 | INFO | Epoch 83 Train Time 31.171335697174072s

2025-10-14 17:22:23,304 | INFO | Training epoch 84, Batch 1000/1000: LR=9.96e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:22:23,351 | INFO | Epoch 84 Train Time 31.300832509994507s

2025-10-14 17:22:55,002 | INFO | Training epoch 85, Batch 1000/1000: LR=9.96e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.48e-01
2025-10-14 17:22:55,054 | INFO | Epoch 85 Train Time 31.70217800140381s

2025-10-14 17:23:26,398 | INFO | Training epoch 86, Batch 1000/1000: LR=9.96e-05, Loss=3.08e-02 BER=1.15e-02 FER=1.48e-01
2025-10-14 17:23:26,443 | INFO | Epoch 86 Train Time 31.38694667816162s

2025-10-14 17:23:57,893 | INFO | Training epoch 87, Batch 1000/1000: LR=9.95e-05, Loss=3.05e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:23:57,948 | INFO | Epoch 87 Train Time 31.503584623336792s

2025-10-14 17:24:28,904 | INFO | Training epoch 88, Batch 1000/1000: LR=9.95e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.46e-01
2025-10-14 17:24:28,963 | INFO | Epoch 88 Train Time 31.01319122314453s

2025-10-14 17:25:00,611 | INFO | Training epoch 89, Batch 1000/1000: LR=9.95e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.47e-01
2025-10-14 17:25:00,674 | INFO | Epoch 89 Train Time 31.70987057685852s

2025-10-14 17:25:31,608 | INFO | Training epoch 90, Batch 1000/1000: LR=9.95e-05, Loss=3.06e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:25:31,667 | INFO | Epoch 90 Train Time 30.992666721343994s

2025-10-14 17:26:02,611 | INFO | Training epoch 91, Batch 1000/1000: LR=9.95e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:26:02,664 | INFO | Epoch 91 Train Time 30.995980739593506s

2025-10-14 17:26:34,006 | INFO | Training epoch 92, Batch 1000/1000: LR=9.95e-05, Loss=3.00e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:26:34,056 | INFO | Epoch 92 Train Time 31.390300750732422s

2025-10-14 17:26:34,057 | INFO | [P2] saving best_model (QAT) with loss 0.029978 at epoch 92
2025-10-14 17:27:05,509 | INFO | Training epoch 93, Batch 1000/1000: LR=9.95e-05, Loss=3.09e-02 BER=1.15e-02 FER=1.49e-01
2025-10-14 17:27:05,555 | INFO | Epoch 93 Train Time 31.472663402557373s

2025-10-14 17:27:36,585 | INFO | Training epoch 94, Batch 1000/1000: LR=9.95e-05, Loss=3.08e-02 BER=1.14e-02 FER=1.50e-01
2025-10-14 17:27:36,630 | INFO | Epoch 94 Train Time 31.073251724243164s

2025-10-14 17:28:08,096 | INFO | Training epoch 95, Batch 1000/1000: LR=9.95e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:28:08,145 | INFO | Epoch 95 Train Time 31.51427388191223s

2025-10-14 17:28:39,089 | INFO | Training epoch 96, Batch 1000/1000: LR=9.94e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.48e-01
2025-10-14 17:28:39,135 | INFO | Epoch 96 Train Time 30.98843479156494s

2025-10-14 17:29:10,499 | INFO | Training epoch 97, Batch 1000/1000: LR=9.94e-05, Loss=3.03e-02 BER=1.12e-02 FER=1.47e-01
2025-10-14 17:29:10,551 | INFO | Epoch 97 Train Time 31.415392637252808s

2025-10-14 17:29:41,790 | INFO | Training epoch 98, Batch 1000/1000: LR=9.94e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:29:41,847 | INFO | Epoch 98 Train Time 31.29531240463257s

2025-10-14 17:30:13,099 | INFO | Training epoch 99, Batch 1000/1000: LR=9.94e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:30:13,148 | INFO | Epoch 99 Train Time 31.300629138946533s

2025-10-14 17:30:44,909 | INFO | Training epoch 100, Batch 1000/1000: LR=9.94e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:30:44,957 | INFO | Epoch 100 Train Time 31.808547019958496s

2025-10-14 17:31:16,505 | INFO | Training epoch 101, Batch 1000/1000: LR=9.94e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:31:16,552 | INFO | Epoch 101 Train Time 31.593613862991333s

2025-10-14 17:31:47,604 | INFO | Training epoch 102, Batch 1000/1000: LR=9.94e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.48e-01
2025-10-14 17:31:47,654 | INFO | Epoch 102 Train Time 31.100838661193848s

2025-10-14 17:32:19,196 | INFO | Training epoch 103, Batch 1000/1000: LR=9.94e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:32:19,257 | INFO | Epoch 103 Train Time 31.602641344070435s

2025-10-14 17:32:50,198 | INFO | Training epoch 104, Batch 1000/1000: LR=9.94e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.48e-01
2025-10-14 17:32:50,259 | INFO | Epoch 104 Train Time 31.00081157684326s

2025-10-14 17:33:21,098 | INFO | Training epoch 105, Batch 1000/1000: LR=9.93e-05, Loss=3.03e-02 BER=1.12e-02 FER=1.47e-01
2025-10-14 17:33:21,162 | INFO | Epoch 105 Train Time 30.902268886566162s

2025-10-14 17:33:51,997 | INFO | Training epoch 106, Batch 1000/1000: LR=9.93e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.47e-01
2025-10-14 17:33:52,047 | INFO | Epoch 106 Train Time 30.883569717407227s

2025-10-14 17:34:23,419 | INFO | Training epoch 107, Batch 1000/1000: LR=9.93e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:34:23,466 | INFO | Epoch 107 Train Time 31.41911029815674s

2025-10-14 17:34:55,002 | INFO | Training epoch 108, Batch 1000/1000: LR=9.93e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:34:55,047 | INFO | Epoch 108 Train Time 31.580443143844604s

2025-10-14 17:35:25,899 | INFO | Training epoch 109, Batch 1000/1000: LR=9.93e-05, Loss=2.99e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:35:25,969 | INFO | Epoch 109 Train Time 30.920846939086914s

2025-10-14 17:35:25,969 | INFO | [P2] saving best_model (QAT) with loss 0.029946 at epoch 109
2025-10-14 17:35:58,115 | INFO | Training epoch 110, Batch 1000/1000: LR=9.93e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:35:58,172 | INFO | Epoch 110 Train Time 32.180914878845215s

2025-10-14 17:36:29,890 | INFO | Training epoch 111, Batch 1000/1000: LR=9.93e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:36:29,946 | INFO | Epoch 111 Train Time 31.773550510406494s

2025-10-14 17:37:00,808 | INFO | Training epoch 112, Batch 1000/1000: LR=9.92e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:37:00,854 | INFO | Epoch 112 Train Time 30.907621145248413s

2025-10-14 17:37:31,824 | INFO | Training epoch 113, Batch 1000/1000: LR=9.92e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:37:31,886 | INFO | Epoch 113 Train Time 31.030874729156494s

2025-10-14 17:38:03,203 | INFO | Training epoch 114, Batch 1000/1000: LR=9.92e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:38:03,254 | INFO | Epoch 114 Train Time 31.367883920669556s

2025-10-14 17:38:34,894 | INFO | Training epoch 115, Batch 1000/1000: LR=9.92e-05, Loss=3.00e-02 BER=1.11e-02 FER=1.46e-01
2025-10-14 17:38:34,942 | INFO | Epoch 115 Train Time 31.687246322631836s

2025-10-14 17:39:05,803 | INFO | Training epoch 116, Batch 1000/1000: LR=9.92e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:39:05,859 | INFO | Epoch 116 Train Time 30.9162917137146s

2025-10-14 17:39:34,796 | INFO | Training epoch 117, Batch 1000/1000: LR=9.92e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:39:34,850 | INFO | Epoch 117 Train Time 28.990583658218384s

2025-10-14 17:39:34,851 | INFO | [P2] saving best_model (QAT) with loss 0.029936 at epoch 117
2025-10-14 17:40:06,304 | INFO | Training epoch 118, Batch 1000/1000: LR=9.92e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 17:40:06,360 | INFO | Epoch 118 Train Time 31.494656324386597s

2025-10-14 17:40:38,012 | INFO | Training epoch 119, Batch 1000/1000: LR=9.92e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:40:38,068 | INFO | Epoch 119 Train Time 31.7064106464386s

2025-10-14 17:41:09,397 | INFO | Training epoch 120, Batch 1000/1000: LR=9.91e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:41:09,452 | INFO | Epoch 120 Train Time 31.38329243659973s

2025-10-14 17:41:09,452 | INFO | [P2] saving best_model (QAT) with loss 0.029765 at epoch 120
2025-10-14 17:41:41,089 | INFO | Training epoch 121, Batch 1000/1000: LR=9.91e-05, Loss=3.04e-02 BER=1.14e-02 FER=1.47e-01
2025-10-14 17:41:41,144 | INFO | Epoch 121 Train Time 31.67600393295288s

2025-10-14 17:42:12,723 | INFO | Training epoch 122, Batch 1000/1000: LR=9.91e-05, Loss=3.04e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:42:12,769 | INFO | Epoch 122 Train Time 31.624989986419678s

2025-10-14 17:42:44,597 | INFO | Training epoch 123, Batch 1000/1000: LR=9.91e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.47e-01
2025-10-14 17:42:44,653 | INFO | Epoch 123 Train Time 31.882713556289673s

2025-10-14 17:43:16,474 | INFO | Training epoch 124, Batch 1000/1000: LR=9.91e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:43:16,524 | INFO | Epoch 124 Train Time 31.87058138847351s

2025-10-14 17:43:48,004 | INFO | Training epoch 125, Batch 1000/1000: LR=9.91e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.46e-01
2025-10-14 17:43:48,056 | INFO | Epoch 125 Train Time 31.531566381454468s

2025-10-14 17:44:19,669 | INFO | Training epoch 126, Batch 1000/1000: LR=9.90e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:44:19,723 | INFO | Epoch 126 Train Time 31.665474891662598s

2025-10-14 17:44:52,192 | INFO | Training epoch 127, Batch 1000/1000: LR=9.90e-05, Loss=2.99e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:44:52,244 | INFO | Epoch 127 Train Time 32.51907920837402s

2025-10-14 17:45:24,207 | INFO | Training epoch 128, Batch 1000/1000: LR=9.90e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 17:45:24,265 | INFO | Epoch 128 Train Time 32.020851135253906s

2025-10-14 17:45:24,265 | INFO | [P2] saving best_model (QAT) with loss 0.029572 at epoch 128
2025-10-14 17:45:55,799 | INFO | Training epoch 129, Batch 1000/1000: LR=9.90e-05, Loss=3.05e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:45:55,867 | INFO | Epoch 129 Train Time 31.58737277984619s

2025-10-14 17:46:26,696 | INFO | Training epoch 130, Batch 1000/1000: LR=9.90e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.45e-01
2025-10-14 17:46:26,758 | INFO | Epoch 130 Train Time 30.88992428779602s

2025-10-14 17:46:57,606 | INFO | Training epoch 131, Batch 1000/1000: LR=9.90e-05, Loss=3.03e-02 BER=1.13e-02 FER=1.47e-01
2025-10-14 17:46:57,652 | INFO | Epoch 131 Train Time 30.893315315246582s

2025-10-14 17:47:29,297 | INFO | Training epoch 132, Batch 1000/1000: LR=9.90e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 17:47:29,341 | INFO | Epoch 132 Train Time 31.687745094299316s

2025-10-14 17:48:01,315 | INFO | Training epoch 133, Batch 1000/1000: LR=9.89e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.47e-01
2025-10-14 17:48:01,361 | INFO | Epoch 133 Train Time 32.019526720047s

2025-10-14 17:48:32,822 | INFO | Training epoch 134, Batch 1000/1000: LR=9.89e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 17:48:32,879 | INFO | Epoch 134 Train Time 31.516902685165405s

2025-10-14 17:49:03,710 | INFO | Training epoch 135, Batch 1000/1000: LR=9.89e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:49:03,767 | INFO | Epoch 135 Train Time 30.887855768203735s

2025-10-14 17:49:35,031 | INFO | Training epoch 136, Batch 1000/1000: LR=9.89e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 17:49:35,069 | INFO | Epoch 136 Train Time 31.300894021987915s

2025-10-14 17:50:05,913 | INFO | Training epoch 137, Batch 1000/1000: LR=9.89e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 17:50:05,958 | INFO | Epoch 137 Train Time 30.88639760017395s

2025-10-14 17:50:37,698 | INFO | Training epoch 138, Batch 1000/1000: LR=9.89e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 17:50:37,744 | INFO | Epoch 138 Train Time 31.785739421844482s

2025-10-14 17:51:08,501 | INFO | Training epoch 139, Batch 1000/1000: LR=9.88e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:51:08,549 | INFO | Epoch 139 Train Time 30.803926944732666s

2025-10-14 17:51:39,663 | INFO | Training epoch 140, Batch 1000/1000: LR=9.88e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:51:39,713 | INFO | Epoch 140 Train Time 31.164116621017456s

2025-10-14 17:52:11,094 | INFO | Training epoch 141, Batch 1000/1000: LR=9.88e-05, Loss=3.01e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:52:11,141 | INFO | Epoch 141 Train Time 31.426173448562622s

2025-10-14 17:52:42,901 | INFO | Training epoch 142, Batch 1000/1000: LR=9.88e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 17:52:42,950 | INFO | Epoch 142 Train Time 31.80687117576599s

2025-10-14 17:53:14,118 | INFO | Training epoch 143, Batch 1000/1000: LR=9.88e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 17:53:14,163 | INFO | Epoch 143 Train Time 31.211952686309814s

2025-10-14 17:53:45,598 | INFO | Training epoch 144, Batch 1000/1000: LR=9.88e-05, Loss=3.02e-02 BER=1.13e-02 FER=1.46e-01
2025-10-14 17:53:45,649 | INFO | Epoch 144 Train Time 31.485454559326172s

2025-10-14 17:54:17,296 | INFO | Training epoch 145, Batch 1000/1000: LR=9.87e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 17:54:17,351 | INFO | Epoch 145 Train Time 31.700321912765503s

2025-10-14 17:54:47,998 | INFO | Training epoch 146, Batch 1000/1000: LR=9.87e-05, Loss=2.97e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 17:54:48,040 | INFO | Epoch 146 Train Time 30.68828797340393s

2025-10-14 17:55:19,301 | INFO | Training epoch 147, Batch 1000/1000: LR=9.87e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 17:55:19,353 | INFO | Epoch 147 Train Time 31.312541484832764s

2025-10-14 17:55:50,294 | INFO | Training epoch 148, Batch 1000/1000: LR=9.87e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.44e-01
2025-10-14 17:55:50,343 | INFO | Epoch 148 Train Time 30.989637851715088s

2025-10-14 17:56:22,003 | INFO | Training epoch 149, Batch 1000/1000: LR=9.87e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 17:56:22,066 | INFO | Epoch 149 Train Time 31.72165012359619s

2025-10-14 17:56:53,705 | INFO | Training epoch 150, Batch 1000/1000: LR=9.87e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 17:56:53,760 | INFO | Epoch 150 Train Time 31.691812992095947s

2025-10-14 17:57:25,305 | INFO | Training epoch 151, Batch 1000/1000: LR=9.86e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.43e-01
2025-10-14 17:57:25,359 | INFO | Epoch 151 Train Time 31.598564624786377s

2025-10-14 17:57:25,361 | INFO | [P2] saving best_model (QAT) with loss 0.029402 at epoch 151
2025-10-14 17:57:56,710 | INFO | Training epoch 152, Batch 1000/1000: LR=9.86e-05, Loss=2.98e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 17:57:56,759 | INFO | Epoch 152 Train Time 31.37587809562683s

2025-10-14 17:58:27,791 | INFO | Training epoch 153, Batch 1000/1000: LR=9.86e-05, Loss=3.00e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 17:58:27,848 | INFO | Epoch 153 Train Time 31.089019298553467s

2025-10-14 17:58:59,896 | INFO | Training epoch 154, Batch 1000/1000: LR=9.86e-05, Loss=2.99e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 17:58:59,945 | INFO | Epoch 154 Train Time 32.09629583358765s

2025-10-14 17:59:31,409 | INFO | Training epoch 155, Batch 1000/1000: LR=9.86e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 17:59:31,472 | INFO | Epoch 155 Train Time 31.52577519416809s

2025-10-14 18:00:03,296 | INFO | Training epoch 156, Batch 1000/1000: LR=9.85e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 18:00:03,337 | INFO | Epoch 156 Train Time 31.863622903823853s

2025-10-14 18:00:34,931 | INFO | Training epoch 157, Batch 1000/1000: LR=9.85e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:00:34,988 | INFO | Epoch 157 Train Time 31.650351762771606s

2025-10-14 18:01:06,001 | INFO | Training epoch 158, Batch 1000/1000: LR=9.85e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:01:06,058 | INFO | Epoch 158 Train Time 31.06892466545105s

2025-10-14 18:01:37,794 | INFO | Training epoch 159, Batch 1000/1000: LR=9.85e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:01:37,852 | INFO | Epoch 159 Train Time 31.792996883392334s

2025-10-14 18:02:08,811 | INFO | Training epoch 160, Batch 1000/1000: LR=9.85e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:02:08,862 | INFO | Epoch 160 Train Time 31.00905156135559s

2025-10-14 18:02:39,398 | INFO | Training epoch 161, Batch 1000/1000: LR=9.84e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:02:39,439 | INFO | Epoch 161 Train Time 30.57611346244812s

2025-10-14 18:03:10,911 | INFO | Training epoch 162, Batch 1000/1000: LR=9.84e-05, Loss=3.02e-02 BER=1.12e-02 FER=1.46e-01
2025-10-14 18:03:10,963 | INFO | Epoch 162 Train Time 31.52313256263733s

2025-10-14 18:03:42,604 | INFO | Training epoch 163, Batch 1000/1000: LR=9.84e-05, Loss=2.99e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:03:42,654 | INFO | Epoch 163 Train Time 31.690819025039673s

2025-10-14 18:04:13,596 | INFO | Training epoch 164, Batch 1000/1000: LR=9.84e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 18:04:13,646 | INFO | Epoch 164 Train Time 30.991268396377563s

2025-10-14 18:04:45,159 | INFO | Training epoch 165, Batch 1000/1000: LR=9.84e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:04:45,204 | INFO | Epoch 165 Train Time 31.55743670463562s

2025-10-14 18:05:16,704 | INFO | Training epoch 166, Batch 1000/1000: LR=9.83e-05, Loss=2.97e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:05:16,762 | INFO | Epoch 166 Train Time 31.556546211242676s

2025-10-14 18:05:47,625 | INFO | Training epoch 167, Batch 1000/1000: LR=9.83e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 18:05:47,676 | INFO | Epoch 167 Train Time 30.91324257850647s

2025-10-14 18:06:18,602 | INFO | Training epoch 168, Batch 1000/1000: LR=9.83e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:06:18,653 | INFO | Epoch 168 Train Time 30.976332426071167s

2025-10-14 18:06:18,654 | INFO | [P2] saving best_model (QAT) with loss 0.029267 at epoch 168
2025-10-14 18:06:50,108 | INFO | Training epoch 169, Batch 1000/1000: LR=9.83e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:06:50,160 | INFO | Epoch 169 Train Time 31.49239182472229s

2025-10-14 18:07:22,206 | INFO | Training epoch 170, Batch 1000/1000: LR=9.83e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.44e-01
2025-10-14 18:07:22,256 | INFO | Epoch 170 Train Time 32.09453725814819s

2025-10-14 18:07:53,207 | INFO | Training epoch 171, Batch 1000/1000: LR=9.82e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:07:53,264 | INFO | Epoch 171 Train Time 31.006702423095703s

2025-10-14 18:08:23,908 | INFO | Training epoch 172, Batch 1000/1000: LR=9.82e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:08:23,956 | INFO | Epoch 172 Train Time 30.69195795059204s

2025-10-14 18:08:55,614 | INFO | Training epoch 173, Batch 1000/1000: LR=9.82e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:08:55,662 | INFO | Epoch 173 Train Time 31.705135583877563s

2025-10-14 18:09:27,495 | INFO | Training epoch 174, Batch 1000/1000: LR=9.82e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:09:27,564 | INFO | Epoch 174 Train Time 31.900646209716797s

2025-10-14 18:09:59,505 | INFO | Training epoch 175, Batch 1000/1000: LR=9.82e-05, Loss=3.01e-02 BER=1.13e-02 FER=1.45e-01
2025-10-14 18:09:59,556 | INFO | Epoch 175 Train Time 31.99115777015686s

2025-10-14 18:10:30,821 | INFO | Training epoch 176, Batch 1000/1000: LR=9.81e-05, Loss=3.00e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:10:30,868 | INFO | Epoch 176 Train Time 31.31089687347412s

2025-10-14 18:11:02,542 | INFO | Training epoch 177, Batch 1000/1000: LR=9.81e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 18:11:02,592 | INFO | Epoch 177 Train Time 31.72350239753723s

2025-10-14 18:11:33,105 | INFO | Training epoch 178, Batch 1000/1000: LR=9.81e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:11:33,155 | INFO | Epoch 178 Train Time 30.561837673187256s

2025-10-14 18:12:03,790 | INFO | Training epoch 179, Batch 1000/1000: LR=9.81e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:12:03,842 | INFO | Epoch 179 Train Time 30.684598922729492s

2025-10-14 18:12:35,414 | INFO | Training epoch 180, Batch 1000/1000: LR=9.81e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:12:35,464 | INFO | Epoch 180 Train Time 31.620399951934814s

2025-10-14 18:13:06,845 | INFO | Training epoch 181, Batch 1000/1000: LR=9.80e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:13:06,886 | INFO | Epoch 181 Train Time 31.421387195587158s

2025-10-14 18:13:37,704 | INFO | Training epoch 182, Batch 1000/1000: LR=9.80e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 18:13:37,755 | INFO | Epoch 182 Train Time 30.86798334121704s

2025-10-14 18:14:09,308 | INFO | Training epoch 183, Batch 1000/1000: LR=9.80e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 18:14:09,351 | INFO | Epoch 183 Train Time 31.59488558769226s

2025-10-14 18:14:40,833 | INFO | Training epoch 184, Batch 1000/1000: LR=9.80e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 18:14:40,888 | INFO | Epoch 184 Train Time 31.535427808761597s

2025-10-14 18:15:12,013 | INFO | Training epoch 185, Batch 1000/1000: LR=9.79e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 18:15:12,066 | INFO | Epoch 185 Train Time 31.177577018737793s

2025-10-14 18:15:43,690 | INFO | Training epoch 186, Batch 1000/1000: LR=9.79e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:15:43,776 | INFO | Epoch 186 Train Time 31.707671642303467s

2025-10-14 18:16:15,619 | INFO | Training epoch 187, Batch 1000/1000: LR=9.79e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:16:15,669 | INFO | Epoch 187 Train Time 31.892822980880737s

2025-10-14 18:16:46,606 | INFO | Training epoch 188, Batch 1000/1000: LR=9.79e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 18:16:46,660 | INFO | Epoch 188 Train Time 30.989635705947876s

2025-10-14 18:17:19,421 | INFO | Training epoch 189, Batch 1000/1000: LR=9.79e-05, Loss=2.99e-02 BER=1.12e-02 FER=1.44e-01
2025-10-14 18:17:19,468 | INFO | Epoch 189 Train Time 32.80742835998535s

2025-10-14 18:17:51,084 | INFO | Training epoch 190, Batch 1000/1000: LR=9.78e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:17:51,135 | INFO | Epoch 190 Train Time 31.666154384613037s

2025-10-14 18:18:22,523 | INFO | Training epoch 191, Batch 1000/1000: LR=9.78e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:18:22,563 | INFO | Epoch 191 Train Time 31.427048206329346s

2025-10-14 18:18:54,399 | INFO | Training epoch 192, Batch 1000/1000: LR=9.78e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:18:54,454 | INFO | Epoch 192 Train Time 31.889522790908813s

2025-10-14 18:19:26,094 | INFO | Training epoch 193, Batch 1000/1000: LR=9.78e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.44e-01
2025-10-14 18:19:26,149 | INFO | Epoch 193 Train Time 31.694050788879395s

2025-10-14 18:19:57,917 | INFO | Training epoch 194, Batch 1000/1000: LR=9.77e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:19:57,968 | INFO | Epoch 194 Train Time 31.819083213806152s

2025-10-14 18:19:57,969 | INFO | [P2] saving best_model (QAT) with loss 0.029265 at epoch 194
2025-10-14 18:20:29,401 | INFO | Training epoch 195, Batch 1000/1000: LR=9.77e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:20:29,447 | INFO | Epoch 195 Train Time 31.460782289505005s

2025-10-14 18:21:00,819 | INFO | Training epoch 196, Batch 1000/1000: LR=9.77e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:21:00,870 | INFO | Epoch 196 Train Time 31.421926259994507s

2025-10-14 18:21:33,188 | INFO | Training epoch 197, Batch 1000/1000: LR=9.77e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:21:33,265 | INFO | Epoch 197 Train Time 32.39286017417908s

2025-10-14 18:22:05,405 | INFO | Training epoch 198, Batch 1000/1000: LR=9.76e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.43e-01
2025-10-14 18:22:05,474 | INFO | Epoch 198 Train Time 32.209067583084106s

2025-10-14 18:22:36,716 | INFO | Training epoch 199, Batch 1000/1000: LR=9.76e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:22:36,777 | INFO | Epoch 199 Train Time 31.301937103271484s

2025-10-14 18:23:07,702 | INFO | Training epoch 200, Batch 1000/1000: LR=9.76e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.45e-01
2025-10-14 18:23:07,761 | INFO | Epoch 200 Train Time 30.982922315597534s

2025-10-14 18:23:38,500 | INFO | Training epoch 201, Batch 1000/1000: LR=9.76e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:23:38,548 | INFO | Epoch 201 Train Time 30.78583860397339s

2025-10-14 18:24:10,603 | INFO | Training epoch 202, Batch 1000/1000: LR=9.76e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:24:10,668 | INFO | Epoch 202 Train Time 32.119691371917725s

2025-10-14 18:24:42,048 | INFO | Training epoch 203, Batch 1000/1000: LR=9.75e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:24:42,104 | INFO | Epoch 203 Train Time 31.43364906311035s

2025-10-14 18:25:13,505 | INFO | Training epoch 204, Batch 1000/1000: LR=9.75e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:25:13,551 | INFO | Epoch 204 Train Time 31.446797132492065s

2025-10-14 18:25:45,008 | INFO | Training epoch 205, Batch 1000/1000: LR=9.75e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:25:45,062 | INFO | Epoch 205 Train Time 31.509948253631592s

2025-10-14 18:26:16,513 | INFO | Training epoch 206, Batch 1000/1000: LR=9.75e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:26:16,562 | INFO | Epoch 206 Train Time 31.499500274658203s

2025-10-14 18:26:46,300 | INFO | Training epoch 207, Batch 1000/1000: LR=9.74e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:26:46,342 | INFO | Epoch 207 Train Time 29.779226541519165s

2025-10-14 18:27:17,791 | INFO | Training epoch 208, Batch 1000/1000: LR=9.74e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:27:17,840 | INFO | Epoch 208 Train Time 31.497730016708374s

2025-10-14 18:27:48,806 | INFO | Training epoch 209, Batch 1000/1000: LR=9.74e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:27:48,858 | INFO | Epoch 209 Train Time 31.017237663269043s

2025-10-14 18:28:20,096 | INFO | Training epoch 210, Batch 1000/1000: LR=9.74e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:28:20,152 | INFO | Epoch 210 Train Time 31.292209148406982s

2025-10-14 18:28:51,502 | INFO | Training epoch 211, Batch 1000/1000: LR=9.73e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:28:51,553 | INFO | Epoch 211 Train Time 31.400086164474487s

2025-10-14 18:29:23,296 | INFO | Training epoch 212, Batch 1000/1000: LR=9.73e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:29:23,347 | INFO | Epoch 212 Train Time 31.791611671447754s

2025-10-14 18:29:54,810 | INFO | Training epoch 213, Batch 1000/1000: LR=9.73e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:29:54,868 | INFO | Epoch 213 Train Time 31.52096152305603s

2025-10-14 18:30:26,513 | INFO | Training epoch 214, Batch 1000/1000: LR=9.73e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 18:30:26,579 | INFO | Epoch 214 Train Time 31.70913052558899s

2025-10-14 18:30:57,809 | INFO | Training epoch 215, Batch 1000/1000: LR=9.72e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:30:57,875 | INFO | Epoch 215 Train Time 31.295435428619385s

2025-10-14 18:31:29,526 | INFO | Training epoch 216, Batch 1000/1000: LR=9.72e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 18:31:29,575 | INFO | Epoch 216 Train Time 31.69947838783264s

2025-10-14 18:31:29,576 | INFO | [P2] saving best_model (QAT) with loss 0.029025 at epoch 216
2025-10-14 18:32:00,604 | INFO | Training epoch 217, Batch 1000/1000: LR=9.72e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:32:00,661 | INFO | Epoch 217 Train Time 31.070008039474487s

2025-10-14 18:32:31,793 | INFO | Training epoch 218, Batch 1000/1000: LR=9.72e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:32:31,843 | INFO | Epoch 218 Train Time 31.18167543411255s

2025-10-14 18:33:03,201 | INFO | Training epoch 219, Batch 1000/1000: LR=9.71e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:33:03,249 | INFO | Epoch 219 Train Time 31.405301809310913s

2025-10-14 18:33:34,602 | INFO | Training epoch 220, Batch 1000/1000: LR=9.71e-05, Loss=2.97e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:33:34,652 | INFO | Epoch 220 Train Time 31.402053356170654s

2025-10-14 18:34:06,103 | INFO | Training epoch 221, Batch 1000/1000: LR=9.71e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:34:06,156 | INFO | Epoch 221 Train Time 31.503172159194946s

2025-10-14 18:34:37,204 | INFO | Training epoch 222, Batch 1000/1000: LR=9.70e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:34:37,262 | INFO | Epoch 222 Train Time 31.104612112045288s

2025-10-14 18:35:08,311 | INFO | Training epoch 223, Batch 1000/1000: LR=9.70e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:35:08,360 | INFO | Epoch 223 Train Time 31.096730709075928s

2025-10-14 18:35:39,996 | INFO | Training epoch 224, Batch 1000/1000: LR=9.70e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:35:40,069 | INFO | Epoch 224 Train Time 31.70758032798767s

2025-10-14 18:36:11,405 | INFO | Training epoch 225, Batch 1000/1000: LR=9.70e-05, Loss=3.01e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 18:36:11,461 | INFO | Epoch 225 Train Time 31.39171552658081s

2025-10-14 18:36:42,994 | INFO | Training epoch 226, Batch 1000/1000: LR=9.69e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:36:43,048 | INFO | Epoch 226 Train Time 31.5865261554718s

2025-10-14 18:37:13,890 | INFO | Training epoch 227, Batch 1000/1000: LR=9.69e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.45e-01
2025-10-14 18:37:13,940 | INFO | Epoch 227 Train Time 30.89017629623413s

2025-10-14 18:37:45,492 | INFO | Training epoch 228, Batch 1000/1000: LR=9.69e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:37:45,539 | INFO | Epoch 228 Train Time 31.597712993621826s

2025-10-14 18:38:17,200 | INFO | Training epoch 229, Batch 1000/1000: LR=9.69e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.44e-01
2025-10-14 18:38:17,264 | INFO | Epoch 229 Train Time 31.724480867385864s

2025-10-14 18:38:49,089 | INFO | Training epoch 230, Batch 1000/1000: LR=9.68e-05, Loss=3.01e-02 BER=1.12e-02 FER=1.44e-01
2025-10-14 18:38:49,145 | INFO | Epoch 230 Train Time 31.88064956665039s

2025-10-14 18:39:20,102 | INFO | Training epoch 231, Batch 1000/1000: LR=9.68e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 18:39:20,169 | INFO | Epoch 231 Train Time 31.023081302642822s

2025-10-14 18:39:50,503 | INFO | Training epoch 232, Batch 1000/1000: LR=9.68e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:39:50,553 | INFO | Epoch 232 Train Time 30.382616996765137s

2025-10-14 18:40:21,794 | INFO | Training epoch 233, Batch 1000/1000: LR=9.67e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:40:21,849 | INFO | Epoch 233 Train Time 31.29567003250122s

2025-10-14 18:40:53,432 | INFO | Training epoch 234, Batch 1000/1000: LR=9.67e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:40:53,504 | INFO | Epoch 234 Train Time 31.653727054595947s

2025-10-14 18:41:24,516 | INFO | Training epoch 235, Batch 1000/1000: LR=9.67e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:41:24,564 | INFO | Epoch 235 Train Time 31.05981755256653s

2025-10-14 18:41:55,700 | INFO | Training epoch 236, Batch 1000/1000: LR=9.67e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 18:41:55,750 | INFO | Epoch 236 Train Time 31.184836387634277s

2025-10-14 18:42:27,313 | INFO | Training epoch 237, Batch 1000/1000: LR=9.66e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:42:27,366 | INFO | Epoch 237 Train Time 31.61538791656494s

2025-10-14 18:42:58,222 | INFO | Training epoch 238, Batch 1000/1000: LR=9.66e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:42:58,272 | INFO | Epoch 238 Train Time 30.90520405769348s

2025-10-14 18:43:29,813 | INFO | Training epoch 239, Batch 1000/1000: LR=9.66e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:43:29,859 | INFO | Epoch 239 Train Time 31.585386276245117s

2025-10-14 18:44:01,792 | INFO | Training epoch 240, Batch 1000/1000: LR=9.66e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 18:44:01,841 | INFO | Epoch 240 Train Time 31.979804754257202s

2025-10-14 18:44:33,394 | INFO | Training epoch 241, Batch 1000/1000: LR=9.65e-05, Loss=2.97e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:44:33,436 | INFO | Epoch 241 Train Time 31.59445548057556s

2025-10-14 18:45:05,203 | INFO | Training epoch 242, Batch 1000/1000: LR=9.65e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:45:05,263 | INFO | Epoch 242 Train Time 31.826563596725464s

2025-10-14 18:45:37,004 | INFO | Training epoch 243, Batch 1000/1000: LR=9.65e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 18:45:37,057 | INFO | Epoch 243 Train Time 31.792702436447144s

2025-10-14 18:46:07,901 | INFO | Training epoch 244, Batch 1000/1000: LR=9.64e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:46:07,960 | INFO | Epoch 244 Train Time 30.902576208114624s

2025-10-14 18:46:39,515 | INFO | Training epoch 245, Batch 1000/1000: LR=9.64e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:46:39,557 | INFO | Epoch 245 Train Time 31.59658193588257s

2025-10-14 18:47:10,604 | INFO | Training epoch 246, Batch 1000/1000: LR=9.64e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:47:10,644 | INFO | Epoch 246 Train Time 31.085633277893066s

2025-10-14 18:47:42,720 | INFO | Training epoch 247, Batch 1000/1000: LR=9.64e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:47:42,784 | INFO | Epoch 247 Train Time 32.13960790634155s

2025-10-14 18:48:14,205 | INFO | Training epoch 248, Batch 1000/1000: LR=9.63e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:48:14,251 | INFO | Epoch 248 Train Time 31.463961839675903s

2025-10-14 18:48:45,406 | INFO | Training epoch 249, Batch 1000/1000: LR=9.63e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 18:48:45,462 | INFO | Epoch 249 Train Time 31.2100567817688s

2025-10-14 18:48:45,463 | INFO | [P2] saving best_model (QAT) with loss 0.028756 at epoch 249
2025-10-14 18:49:16,104 | INFO | Training epoch 250, Batch 1000/1000: LR=9.63e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 18:49:16,154 | INFO | Epoch 250 Train Time 30.650790452957153s

2025-10-14 18:49:47,400 | INFO | Training epoch 251, Batch 1000/1000: LR=9.62e-05, Loss=2.93e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 18:49:47,454 | INFO | Epoch 251 Train Time 31.29933476448059s

2025-10-14 18:50:19,012 | INFO | Training epoch 252, Batch 1000/1000: LR=9.62e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:50:19,061 | INFO | Epoch 252 Train Time 31.6064190864563s

2025-10-14 18:50:50,193 | INFO | Training epoch 253, Batch 1000/1000: LR=9.62e-05, Loss=3.00e-02 BER=1.12e-02 FER=1.45e-01
2025-10-14 18:50:50,253 | INFO | Epoch 253 Train Time 31.190866947174072s

2025-10-14 18:51:21,488 | INFO | Training epoch 254, Batch 1000/1000: LR=9.61e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:51:21,529 | INFO | Epoch 254 Train Time 31.27551031112671s

2025-10-14 18:51:52,799 | INFO | Training epoch 255, Batch 1000/1000: LR=9.61e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 18:51:52,856 | INFO | Epoch 255 Train Time 31.325748682022095s

2025-10-14 18:52:23,501 | INFO | Training epoch 256, Batch 1000/1000: LR=9.61e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 18:52:23,544 | INFO | Epoch 256 Train Time 30.68739604949951s

2025-10-14 18:52:54,504 | INFO | Training epoch 257, Batch 1000/1000: LR=9.61e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 18:52:54,553 | INFO | Epoch 257 Train Time 31.00848627090454s

2025-10-14 18:53:25,494 | INFO | Training epoch 258, Batch 1000/1000: LR=9.60e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:53:25,540 | INFO | Epoch 258 Train Time 30.986427783966064s

2025-10-14 18:53:56,815 | INFO | Training epoch 259, Batch 1000/1000: LR=9.60e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 18:53:56,872 | INFO | Epoch 259 Train Time 31.33053421974182s

2025-10-14 18:54:28,504 | INFO | Training epoch 260, Batch 1000/1000: LR=9.60e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:54:28,556 | INFO | Epoch 260 Train Time 31.68313956260681s

2025-10-14 18:55:00,201 | INFO | Training epoch 261, Batch 1000/1000: LR=9.59e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 18:55:00,251 | INFO | Epoch 261 Train Time 31.69480299949646s

2025-10-14 18:55:31,816 | INFO | Training epoch 262, Batch 1000/1000: LR=9.59e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 18:55:31,868 | INFO | Epoch 262 Train Time 31.616243839263916s

2025-10-14 18:56:03,104 | INFO | Training epoch 263, Batch 1000/1000: LR=9.59e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 18:56:03,153 | INFO | Epoch 263 Train Time 31.28334903717041s

2025-10-14 18:56:35,108 | INFO | Training epoch 264, Batch 1000/1000: LR=9.58e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:56:35,151 | INFO | Epoch 264 Train Time 31.998218536376953s

2025-10-14 18:57:06,118 | INFO | Training epoch 265, Batch 1000/1000: LR=9.58e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:57:06,181 | INFO | Epoch 265 Train Time 31.02750539779663s

2025-10-14 18:57:37,823 | INFO | Training epoch 266, Batch 1000/1000: LR=9.58e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 18:57:37,865 | INFO | Epoch 266 Train Time 31.68342423439026s

2025-10-14 18:58:09,911 | INFO | Training epoch 267, Batch 1000/1000: LR=9.57e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 18:58:09,956 | INFO | Epoch 267 Train Time 32.08842468261719s

2025-10-14 18:58:41,289 | INFO | Training epoch 268, Batch 1000/1000: LR=9.57e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 18:58:41,359 | INFO | Epoch 268 Train Time 31.40244197845459s

2025-10-14 18:59:12,818 | INFO | Training epoch 269, Batch 1000/1000: LR=9.57e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 18:59:12,864 | INFO | Epoch 269 Train Time 31.504183292388916s

2025-10-14 18:59:44,703 | INFO | Training epoch 270, Batch 1000/1000: LR=9.56e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 18:59:44,753 | INFO | Epoch 270 Train Time 31.888166666030884s

2025-10-14 19:00:16,107 | INFO | Training epoch 271, Batch 1000/1000: LR=9.56e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:00:16,165 | INFO | Epoch 271 Train Time 31.411123752593994s

2025-10-14 19:00:47,702 | INFO | Training epoch 272, Batch 1000/1000: LR=9.56e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:00:47,751 | INFO | Epoch 272 Train Time 31.58479642868042s

2025-10-14 19:01:19,307 | INFO | Training epoch 273, Batch 1000/1000: LR=9.56e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:01:19,366 | INFO | Epoch 273 Train Time 31.614879369735718s

2025-10-14 19:01:50,613 | INFO | Training epoch 274, Batch 1000/1000: LR=9.55e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:01:50,656 | INFO | Epoch 274 Train Time 31.288464069366455s

2025-10-14 19:02:22,495 | INFO | Training epoch 275, Batch 1000/1000: LR=9.55e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:02:22,551 | INFO | Epoch 275 Train Time 31.892887353897095s

2025-10-14 19:02:53,191 | INFO | Training epoch 276, Batch 1000/1000: LR=9.55e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:02:53,244 | INFO | Epoch 276 Train Time 30.692224979400635s

2025-10-14 19:03:24,824 | INFO | Training epoch 277, Batch 1000/1000: LR=9.54e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 19:03:24,874 | INFO | Epoch 277 Train Time 31.629635334014893s

2025-10-14 19:03:56,202 | INFO | Training epoch 278, Batch 1000/1000: LR=9.54e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:03:56,255 | INFO | Epoch 278 Train Time 31.379470586776733s

2025-10-14 19:04:28,206 | INFO | Training epoch 279, Batch 1000/1000: LR=9.54e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 19:04:28,274 | INFO | Epoch 279 Train Time 32.01825737953186s

2025-10-14 19:04:59,908 | INFO | Training epoch 280, Batch 1000/1000: LR=9.53e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:04:59,959 | INFO | Epoch 280 Train Time 31.68342351913452s

2025-10-14 19:05:31,603 | INFO | Training epoch 281, Batch 1000/1000: LR=9.53e-05, Loss=2.95e-02 BER=1.09e-02 FER=1.43e-01
2025-10-14 19:05:31,655 | INFO | Epoch 281 Train Time 31.695476055145264s

2025-10-14 19:06:02,999 | INFO | Training epoch 282, Batch 1000/1000: LR=9.53e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:06:03,041 | INFO | Epoch 282 Train Time 31.385534048080444s

2025-10-14 19:06:33,991 | INFO | Training epoch 283, Batch 1000/1000: LR=9.52e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:06:34,044 | INFO | Epoch 283 Train Time 31.002814054489136s

2025-10-14 19:07:07,002 | INFO | Training epoch 284, Batch 1000/1000: LR=9.52e-05, Loss=2.95e-02 BER=1.11e-02 FER=1.42e-01
2025-10-14 19:07:07,049 | INFO | Epoch 284 Train Time 33.003085136413574s

2025-10-14 19:07:38,214 | INFO | Training epoch 285, Batch 1000/1000: LR=9.52e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:07:38,275 | INFO | Epoch 285 Train Time 31.224007844924927s

2025-10-14 19:08:09,589 | INFO | Training epoch 286, Batch 1000/1000: LR=9.51e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:08:09,644 | INFO | Epoch 286 Train Time 31.368220806121826s

2025-10-14 19:08:40,410 | INFO | Training epoch 287, Batch 1000/1000: LR=9.51e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:08:40,466 | INFO | Epoch 287 Train Time 30.820701837539673s

2025-10-14 19:09:11,601 | INFO | Training epoch 288, Batch 1000/1000: LR=9.51e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:09:11,662 | INFO | Epoch 288 Train Time 31.195077657699585s

2025-10-14 19:09:43,434 | INFO | Training epoch 289, Batch 1000/1000: LR=9.50e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:09:43,479 | INFO | Epoch 289 Train Time 31.81556749343872s

2025-10-14 19:10:14,596 | INFO | Training epoch 290, Batch 1000/1000: LR=9.50e-05, Loss=2.95e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:10:14,673 | INFO | Epoch 290 Train Time 31.193116426467896s

2025-10-14 19:10:45,911 | INFO | Training epoch 291, Batch 1000/1000: LR=9.50e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:10:45,964 | INFO | Epoch 291 Train Time 31.289649963378906s

2025-10-14 19:11:17,391 | INFO | Training epoch 292, Batch 1000/1000: LR=9.49e-05, Loss=2.98e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 19:11:17,445 | INFO | Epoch 292 Train Time 31.48092794418335s

2025-10-14 19:11:48,200 | INFO | Training epoch 293, Batch 1000/1000: LR=9.49e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:11:48,248 | INFO | Epoch 293 Train Time 30.800759315490723s

2025-10-14 19:12:19,503 | INFO | Training epoch 294, Batch 1000/1000: LR=9.48e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:12:19,548 | INFO | Epoch 294 Train Time 31.299666166305542s

2025-10-14 19:12:51,622 | INFO | Training epoch 295, Batch 1000/1000: LR=9.48e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:12:51,690 | INFO | Epoch 295 Train Time 32.14053988456726s

2025-10-14 19:13:23,416 | INFO | Training epoch 296, Batch 1000/1000: LR=9.48e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.44e-01
2025-10-14 19:13:23,467 | INFO | Epoch 296 Train Time 31.775079011917114s

2025-10-14 19:13:54,598 | INFO | Training epoch 297, Batch 1000/1000: LR=9.47e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:13:54,651 | INFO | Epoch 297 Train Time 31.183727025985718s

2025-10-14 19:14:26,007 | INFO | Training epoch 298, Batch 1000/1000: LR=9.47e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:14:26,056 | INFO | Epoch 298 Train Time 31.403918981552124s

2025-10-14 19:14:57,818 | INFO | Training epoch 299, Batch 1000/1000: LR=9.47e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.44e-01
2025-10-14 19:14:57,875 | INFO | Epoch 299 Train Time 31.81800389289856s

2025-10-14 19:15:28,696 | INFO | Training epoch 300, Batch 1000/1000: LR=9.46e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:15:28,750 | INFO | Epoch 300 Train Time 30.873688220977783s

2025-10-14 19:16:00,122 | INFO | Training epoch 301, Batch 1000/1000: LR=9.46e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:16:00,170 | INFO | Epoch 301 Train Time 31.418535709381104s

2025-10-14 19:16:31,026 | INFO | Training epoch 302, Batch 1000/1000: LR=9.46e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:16:31,077 | INFO | Epoch 302 Train Time 30.905622243881226s

2025-10-14 19:17:03,399 | INFO | Training epoch 303, Batch 1000/1000: LR=9.45e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:17:03,448 | INFO | Epoch 303 Train Time 32.37076163291931s

2025-10-14 19:17:35,308 | INFO | Training epoch 304, Batch 1000/1000: LR=9.45e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:17:35,367 | INFO | Epoch 304 Train Time 31.91702938079834s

2025-10-14 19:18:06,389 | INFO | Training epoch 305, Batch 1000/1000: LR=9.45e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 19:18:06,443 | INFO | Epoch 305 Train Time 31.0743088722229s

2025-10-14 19:18:37,284 | INFO | Training epoch 306, Batch 1000/1000: LR=9.44e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:18:37,340 | INFO | Epoch 306 Train Time 30.896273136138916s

2025-10-14 19:19:09,027 | INFO | Training epoch 307, Batch 1000/1000: LR=9.44e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:19:09,074 | INFO | Epoch 307 Train Time 31.732085943222046s

2025-10-14 19:19:40,114 | INFO | Training epoch 308, Batch 1000/1000: LR=9.44e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:19:40,164 | INFO | Epoch 308 Train Time 31.089093446731567s

2025-10-14 19:20:11,699 | INFO | Training epoch 309, Batch 1000/1000: LR=9.43e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:20:11,753 | INFO | Epoch 309 Train Time 31.58835744857788s

2025-10-14 19:20:43,096 | INFO | Training epoch 310, Batch 1000/1000: LR=9.43e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:20:43,146 | INFO | Epoch 310 Train Time 31.39131188392639s

2025-10-14 19:21:14,517 | INFO | Training epoch 311, Batch 1000/1000: LR=9.42e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:21:14,567 | INFO | Epoch 311 Train Time 31.420520544052124s

2025-10-14 19:21:45,810 | INFO | Training epoch 312, Batch 1000/1000: LR=9.42e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:21:45,865 | INFO | Epoch 312 Train Time 31.29815912246704s

2025-10-14 19:22:16,694 | INFO | Training epoch 313, Batch 1000/1000: LR=9.42e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:22:16,745 | INFO | Epoch 313 Train Time 30.879314422607422s

2025-10-14 19:22:48,304 | INFO | Training epoch 314, Batch 1000/1000: LR=9.41e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:22:48,356 | INFO | Epoch 314 Train Time 31.609108686447144s

2025-10-14 19:23:19,091 | INFO | Training epoch 315, Batch 1000/1000: LR=9.41e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:23:19,140 | INFO | Epoch 315 Train Time 30.78382110595703s

2025-10-14 19:23:50,230 | INFO | Training epoch 316, Batch 1000/1000: LR=9.41e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:23:50,280 | INFO | Epoch 316 Train Time 31.139190196990967s

2025-10-14 19:24:21,606 | INFO | Training epoch 317, Batch 1000/1000: LR=9.40e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:24:21,663 | INFO | Epoch 317 Train Time 31.38113570213318s

2025-10-14 19:24:52,909 | INFO | Training epoch 318, Batch 1000/1000: LR=9.40e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.43e-01
2025-10-14 19:24:52,957 | INFO | Epoch 318 Train Time 31.294079780578613s

2025-10-14 19:25:24,401 | INFO | Training epoch 319, Batch 1000/1000: LR=9.40e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:25:24,455 | INFO | Epoch 319 Train Time 31.49709701538086s

2025-10-14 19:25:55,620 | INFO | Training epoch 320, Batch 1000/1000: LR=9.39e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:25:55,671 | INFO | Epoch 320 Train Time 31.214555501937866s

2025-10-14 19:26:27,303 | INFO | Training epoch 321, Batch 1000/1000: LR=9.39e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:26:27,351 | INFO | Epoch 321 Train Time 31.67993664741516s

2025-10-14 19:26:59,300 | INFO | Training epoch 322, Batch 1000/1000: LR=9.38e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:26:59,342 | INFO | Epoch 322 Train Time 31.98937487602234s

2025-10-14 19:27:30,900 | INFO | Training epoch 323, Batch 1000/1000: LR=9.38e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 19:27:30,950 | INFO | Epoch 323 Train Time 31.60734796524048s

2025-10-14 19:28:02,103 | INFO | Training epoch 324, Batch 1000/1000: LR=9.38e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:28:02,162 | INFO | Epoch 324 Train Time 31.210737705230713s

2025-10-14 19:28:33,600 | INFO | Training epoch 325, Batch 1000/1000: LR=9.37e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:28:33,655 | INFO | Epoch 325 Train Time 31.491734981536865s

2025-10-14 19:29:05,113 | INFO | Training epoch 326, Batch 1000/1000: LR=9.37e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:29:05,163 | INFO | Epoch 326 Train Time 31.507927417755127s

2025-10-14 19:29:36,207 | INFO | Training epoch 327, Batch 1000/1000: LR=9.37e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:29:36,268 | INFO | Epoch 327 Train Time 31.10403037071228s

2025-10-14 19:30:07,697 | INFO | Training epoch 328, Batch 1000/1000: LR=9.36e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:30:07,745 | INFO | Epoch 328 Train Time 31.47667622566223s

2025-10-14 19:30:39,711 | INFO | Training epoch 329, Batch 1000/1000: LR=9.36e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:30:39,765 | INFO | Epoch 329 Train Time 32.01863694190979s

2025-10-14 19:31:10,998 | INFO | Training epoch 330, Batch 1000/1000: LR=9.35e-05, Loss=2.92e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:31:11,053 | INFO | Epoch 330 Train Time 31.28727388381958s

2025-10-14 19:31:42,809 | INFO | Training epoch 331, Batch 1000/1000: LR=9.35e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:31:42,855 | INFO | Epoch 331 Train Time 31.799986124038696s

2025-10-14 19:32:14,515 | INFO | Training epoch 332, Batch 1000/1000: LR=9.35e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:32:14,577 | INFO | Epoch 332 Train Time 31.721243381500244s

2025-10-14 19:32:46,000 | INFO | Training epoch 333, Batch 1000/1000: LR=9.34e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:32:46,049 | INFO | Epoch 333 Train Time 31.47102642059326s

2025-10-14 19:33:17,691 | INFO | Training epoch 334, Batch 1000/1000: LR=9.34e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:33:17,743 | INFO | Epoch 334 Train Time 31.693341493606567s

2025-10-14 19:33:49,299 | INFO | Training epoch 335, Batch 1000/1000: LR=9.33e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.43e-01
2025-10-14 19:33:49,358 | INFO | Epoch 335 Train Time 31.614081382751465s

2025-10-14 19:34:20,993 | INFO | Training epoch 336, Batch 1000/1000: LR=9.33e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 19:34:21,052 | INFO | Epoch 336 Train Time 31.692599296569824s

2025-10-14 19:34:21,052 | INFO | [P2] saving best_model (QAT) with loss 0.028483 at epoch 336
2025-10-14 19:34:52,004 | INFO | Training epoch 337, Batch 1000/1000: LR=9.33e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:34:52,063 | INFO | Epoch 337 Train Time 30.992408275604248s

2025-10-14 19:35:23,098 | INFO | Training epoch 338, Batch 1000/1000: LR=9.32e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:35:23,151 | INFO | Epoch 338 Train Time 31.086371660232544s

2025-10-14 19:35:54,491 | INFO | Training epoch 339, Batch 1000/1000: LR=9.32e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:35:54,542 | INFO | Epoch 339 Train Time 31.38957452774048s

2025-10-14 19:36:26,102 | INFO | Training epoch 340, Batch 1000/1000: LR=9.31e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:36:26,153 | INFO | Epoch 340 Train Time 31.609776973724365s

2025-10-14 19:36:57,900 | INFO | Training epoch 341, Batch 1000/1000: LR=9.31e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:36:57,953 | INFO | Epoch 341 Train Time 31.798879623413086s

2025-10-14 19:37:28,593 | INFO | Training epoch 342, Batch 1000/1000: LR=9.31e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:37:28,644 | INFO | Epoch 342 Train Time 30.68985605239868s

2025-10-14 19:37:59,302 | INFO | Training epoch 343, Batch 1000/1000: LR=9.30e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:37:59,354 | INFO | Epoch 343 Train Time 30.708723068237305s

2025-10-14 19:38:31,308 | INFO | Training epoch 344, Batch 1000/1000: LR=9.30e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 19:38:31,351 | INFO | Epoch 344 Train Time 31.996310710906982s

2025-10-14 19:39:03,004 | INFO | Training epoch 345, Batch 1000/1000: LR=9.29e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:39:03,060 | INFO | Epoch 345 Train Time 31.706964015960693s

2025-10-14 19:39:34,600 | INFO | Training epoch 346, Batch 1000/1000: LR=9.29e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:39:34,655 | INFO | Epoch 346 Train Time 31.594179153442383s

2025-10-14 19:40:05,798 | INFO | Training epoch 347, Batch 1000/1000: LR=9.29e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:40:05,845 | INFO | Epoch 347 Train Time 31.1881320476532s

2025-10-14 19:40:37,798 | INFO | Training epoch 348, Batch 1000/1000: LR=9.28e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:40:37,844 | INFO | Epoch 348 Train Time 31.998854875564575s

2025-10-14 19:41:09,396 | INFO | Training epoch 349, Batch 1000/1000: LR=9.28e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:41:09,443 | INFO | Epoch 349 Train Time 31.597396850585938s

2025-10-14 19:41:40,591 | INFO | Training epoch 350, Batch 1000/1000: LR=9.27e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:41:40,644 | INFO | Epoch 350 Train Time 31.200830459594727s

2025-10-14 19:42:11,603 | INFO | Training epoch 351, Batch 1000/1000: LR=9.27e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:42:11,654 | INFO | Epoch 351 Train Time 31.007882833480835s

2025-10-14 19:42:42,593 | INFO | Training epoch 352, Batch 1000/1000: LR=9.27e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:42:42,646 | INFO | Epoch 352 Train Time 30.99190878868103s

2025-10-14 19:43:14,213 | INFO | Training epoch 353, Batch 1000/1000: LR=9.26e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:43:14,267 | INFO | Epoch 353 Train Time 31.620164394378662s

2025-10-14 19:43:45,998 | INFO | Training epoch 354, Batch 1000/1000: LR=9.26e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:43:46,061 | INFO | Epoch 354 Train Time 31.79245162010193s

2025-10-14 19:44:17,709 | INFO | Training epoch 355, Batch 1000/1000: LR=9.25e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:44:17,760 | INFO | Epoch 355 Train Time 31.697853565216064s

2025-10-14 19:44:48,231 | INFO | Training epoch 356, Batch 1000/1000: LR=9.25e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:44:48,277 | INFO | Epoch 356 Train Time 30.516062259674072s

2025-10-14 19:45:20,105 | INFO | Training epoch 357, Batch 1000/1000: LR=9.25e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:45:20,157 | INFO | Epoch 357 Train Time 31.87889003753662s

2025-10-14 19:45:51,209 | INFO | Training epoch 358, Batch 1000/1000: LR=9.24e-05, Loss=2.91e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 19:45:51,260 | INFO | Epoch 358 Train Time 31.101807832717896s

2025-10-14 19:46:22,197 | INFO | Training epoch 359, Batch 1000/1000: LR=9.24e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.40e-01
2025-10-14 19:46:22,244 | INFO | Epoch 359 Train Time 30.982762575149536s

2025-10-14 19:46:53,090 | INFO | Training epoch 360, Batch 1000/1000: LR=9.23e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:46:53,132 | INFO | Epoch 360 Train Time 30.886714935302734s

2025-10-14 19:47:24,132 | INFO | Training epoch 361, Batch 1000/1000: LR=9.23e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:47:24,189 | INFO | Epoch 361 Train Time 31.05586266517639s

2025-10-14 19:47:55,301 | INFO | Training epoch 362, Batch 1000/1000: LR=9.23e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 19:47:55,354 | INFO | Epoch 362 Train Time 31.16500687599182s

2025-10-14 19:48:26,016 | INFO | Training epoch 363, Batch 1000/1000: LR=9.22e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:48:26,066 | INFO | Epoch 363 Train Time 30.71056818962097s

2025-10-14 19:48:57,304 | INFO | Training epoch 364, Batch 1000/1000: LR=9.22e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:48:57,354 | INFO | Epoch 364 Train Time 31.287749767303467s

2025-10-14 19:49:28,420 | INFO | Training epoch 365, Batch 1000/1000: LR=9.21e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:49:28,475 | INFO | Epoch 365 Train Time 31.12035870552063s

2025-10-14 19:50:00,103 | INFO | Training epoch 366, Batch 1000/1000: LR=9.21e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:50:00,156 | INFO | Epoch 366 Train Time 31.67954421043396s

2025-10-14 19:50:31,807 | INFO | Training epoch 367, Batch 1000/1000: LR=9.20e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:50:31,858 | INFO | Epoch 367 Train Time 31.701093673706055s

2025-10-14 19:51:03,292 | INFO | Training epoch 368, Batch 1000/1000: LR=9.20e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:51:03,342 | INFO | Epoch 368 Train Time 31.48231792449951s

2025-10-14 19:51:34,706 | INFO | Training epoch 369, Batch 1000/1000: LR=9.20e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.42e-01
2025-10-14 19:51:34,753 | INFO | Epoch 369 Train Time 31.409257411956787s

2025-10-14 19:52:05,802 | INFO | Training epoch 370, Batch 1000/1000: LR=9.19e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:52:05,853 | INFO | Epoch 370 Train Time 31.098419904708862s

2025-10-14 19:52:36,931 | INFO | Training epoch 371, Batch 1000/1000: LR=9.19e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:52:36,978 | INFO | Epoch 371 Train Time 31.124478816986084s

2025-10-14 19:53:08,191 | INFO | Training epoch 372, Batch 1000/1000: LR=9.18e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:53:08,239 | INFO | Epoch 372 Train Time 31.259610176086426s

2025-10-14 19:53:39,694 | INFO | Training epoch 373, Batch 1000/1000: LR=9.18e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 19:53:39,750 | INFO | Epoch 373 Train Time 31.511310815811157s

2025-10-14 19:54:11,005 | INFO | Training epoch 374, Batch 1000/1000: LR=9.17e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:54:11,060 | INFO | Epoch 374 Train Time 31.308573007583618s

2025-10-14 19:54:42,607 | INFO | Training epoch 375, Batch 1000/1000: LR=9.17e-05, Loss=2.96e-02 BER=1.10e-02 FER=1.43e-01
2025-10-14 19:54:42,666 | INFO | Epoch 375 Train Time 31.60477924346924s

2025-10-14 19:55:13,500 | INFO | Training epoch 376, Batch 1000/1000: LR=9.17e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 19:55:13,555 | INFO | Epoch 376 Train Time 30.886723041534424s

2025-10-14 19:55:44,704 | INFO | Training epoch 377, Batch 1000/1000: LR=9.16e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 19:55:44,756 | INFO | Epoch 377 Train Time 31.20027732849121s

2025-10-14 19:56:16,133 | INFO | Training epoch 378, Batch 1000/1000: LR=9.16e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:56:16,199 | INFO | Epoch 378 Train Time 31.4413800239563s

2025-10-14 19:56:48,714 | INFO | Training epoch 379, Batch 1000/1000: LR=9.15e-05, Loss=2.92e-02 BER=1.08e-02 FER=1.42e-01
2025-10-14 19:56:48,765 | INFO | Epoch 379 Train Time 32.56501030921936s

2025-10-14 19:57:21,323 | INFO | Training epoch 380, Batch 1000/1000: LR=9.15e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 19:57:21,372 | INFO | Epoch 380 Train Time 32.60661864280701s

2025-10-14 19:57:51,839 | INFO | Training epoch 381, Batch 1000/1000: LR=9.14e-05, Loss=2.90e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 19:57:51,902 | INFO | Epoch 381 Train Time 30.52875280380249s

2025-10-14 19:58:23,675 | INFO | Training epoch 382, Batch 1000/1000: LR=9.14e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 19:58:23,720 | INFO | Epoch 382 Train Time 31.817452430725098s

2025-10-14 19:58:55,690 | INFO | Training epoch 383, Batch 1000/1000: LR=9.14e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 19:58:55,736 | INFO | Epoch 383 Train Time 32.01484990119934s

2025-10-14 19:59:26,396 | INFO | Training epoch 384, Batch 1000/1000: LR=9.13e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 19:59:26,452 | INFO | Epoch 384 Train Time 30.71491813659668s

2025-10-14 19:59:59,219 | INFO | Training epoch 385, Batch 1000/1000: LR=9.13e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.43e-01
2025-10-14 19:59:59,261 | INFO | Epoch 385 Train Time 32.80804491043091s

2025-10-14 20:00:31,106 | INFO | Training epoch 386, Batch 1000/1000: LR=9.12e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:00:31,159 | INFO | Epoch 386 Train Time 31.897250652313232s

2025-10-14 20:01:03,195 | INFO | Training epoch 387, Batch 1000/1000: LR=9.12e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 20:01:03,244 | INFO | Epoch 387 Train Time 32.08334732055664s

2025-10-14 20:01:36,111 | INFO | Training epoch 388, Batch 1000/1000: LR=9.11e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:01:36,164 | INFO | Epoch 388 Train Time 32.91805934906006s

2025-10-14 20:02:08,405 | INFO | Training epoch 389, Batch 1000/1000: LR=9.11e-05, Loss=2.90e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:02:08,449 | INFO | Epoch 389 Train Time 32.284207344055176s

2025-10-14 20:02:40,534 | INFO | Training epoch 390, Batch 1000/1000: LR=9.10e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:02:40,604 | INFO | Epoch 390 Train Time 32.15291714668274s

2025-10-14 20:03:13,613 | INFO | Training epoch 391, Batch 1000/1000: LR=9.10e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 20:03:13,660 | INFO | Epoch 391 Train Time 33.05588221549988s

2025-10-14 20:03:45,695 | INFO | Training epoch 392, Batch 1000/1000: LR=9.10e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:03:45,752 | INFO | Epoch 392 Train Time 32.08939528465271s

2025-10-14 20:04:18,003 | INFO | Training epoch 393, Batch 1000/1000: LR=9.09e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 20:04:18,059 | INFO | Epoch 393 Train Time 32.30523133277893s

2025-10-14 20:04:49,990 | INFO | Training epoch 394, Batch 1000/1000: LR=9.09e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 20:04:50,051 | INFO | Epoch 394 Train Time 31.991679191589355s

2025-10-14 20:05:21,703 | INFO | Training epoch 395, Batch 1000/1000: LR=9.08e-05, Loss=2.94e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:05:21,767 | INFO | Epoch 395 Train Time 31.714468240737915s

2025-10-14 20:05:53,313 | INFO | Training epoch 396, Batch 1000/1000: LR=9.08e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:05:53,368 | INFO | Epoch 396 Train Time 31.600117444992065s

2025-10-14 20:06:25,593 | INFO | Training epoch 397, Batch 1000/1000: LR=9.07e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:06:25,658 | INFO | Epoch 397 Train Time 32.289576292037964s

2025-10-14 20:06:58,202 | INFO | Training epoch 398, Batch 1000/1000: LR=9.07e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:06:58,260 | INFO | Epoch 398 Train Time 32.600241899490356s

2025-10-14 20:07:30,000 | INFO | Training epoch 399, Batch 1000/1000: LR=9.06e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 20:07:30,052 | INFO | Epoch 399 Train Time 31.79015803337097s

2025-10-14 20:08:01,500 | INFO | Training epoch 400, Batch 1000/1000: LR=9.06e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:08:01,558 | INFO | Epoch 400 Train Time 31.504491567611694s

2025-10-14 20:08:33,223 | INFO | Training epoch 401, Batch 1000/1000: LR=9.05e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:08:33,281 | INFO | Epoch 401 Train Time 31.722615718841553s

2025-10-14 20:09:05,013 | INFO | Training epoch 402, Batch 1000/1000: LR=9.05e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:09:05,069 | INFO | Epoch 402 Train Time 31.786760807037354s

2025-10-14 20:09:36,815 | INFO | Training epoch 403, Batch 1000/1000: LR=9.05e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:09:36,877 | INFO | Epoch 403 Train Time 31.80726981163025s

2025-10-14 20:10:08,098 | INFO | Training epoch 404, Batch 1000/1000: LR=9.04e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:10:08,146 | INFO | Epoch 404 Train Time 31.266934156417847s

2025-10-14 20:10:39,826 | INFO | Training epoch 405, Batch 1000/1000: LR=9.04e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:10:39,871 | INFO | Epoch 405 Train Time 31.72353982925415s

2025-10-14 20:11:10,503 | INFO | Training epoch 406, Batch 1000/1000: LR=9.03e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:11:10,557 | INFO | Epoch 406 Train Time 30.68511414527893s

2025-10-14 20:11:41,703 | INFO | Training epoch 407, Batch 1000/1000: LR=9.03e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:11:41,779 | INFO | Epoch 407 Train Time 31.22169589996338s

2025-10-14 20:12:13,306 | INFO | Training epoch 408, Batch 1000/1000: LR=9.02e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:12:13,358 | INFO | Epoch 408 Train Time 31.578048944473267s

2025-10-14 20:12:44,203 | INFO | Training epoch 409, Batch 1000/1000: LR=9.02e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:12:44,250 | INFO | Epoch 409 Train Time 30.89093255996704s

2025-10-14 20:13:16,094 | INFO | Training epoch 410, Batch 1000/1000: LR=9.01e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:13:16,145 | INFO | Epoch 410 Train Time 31.89362668991089s

2025-10-14 20:13:47,313 | INFO | Training epoch 411, Batch 1000/1000: LR=9.01e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:13:47,385 | INFO | Epoch 411 Train Time 31.23844623565674s

2025-10-14 20:14:18,494 | INFO | Training epoch 412, Batch 1000/1000: LR=9.00e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:14:18,548 | INFO | Epoch 412 Train Time 31.162296295166016s

2025-10-14 20:14:50,013 | INFO | Training epoch 413, Batch 1000/1000: LR=9.00e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 20:14:50,093 | INFO | Epoch 413 Train Time 31.543696641921997s

2025-10-14 20:15:21,599 | INFO | Training epoch 414, Batch 1000/1000: LR=8.99e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:15:21,639 | INFO | Epoch 414 Train Time 31.54529094696045s

2025-10-14 20:15:53,418 | INFO | Training epoch 415, Batch 1000/1000: LR=8.99e-05, Loss=2.97e-02 BER=1.11e-02 FER=1.42e-01
2025-10-14 20:15:53,468 | INFO | Epoch 415 Train Time 31.828136205673218s

2025-10-14 20:16:24,722 | INFO | Training epoch 416, Batch 1000/1000: LR=8.98e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:16:24,775 | INFO | Epoch 416 Train Time 31.306154012680054s

2025-10-14 20:16:55,403 | INFO | Training epoch 417, Batch 1000/1000: LR=8.98e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:16:55,452 | INFO | Epoch 417 Train Time 30.6751606464386s

2025-10-14 20:17:26,296 | INFO | Training epoch 418, Batch 1000/1000: LR=8.98e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:17:26,349 | INFO | Epoch 418 Train Time 30.8956618309021s

2025-10-14 20:17:58,409 | INFO | Training epoch 419, Batch 1000/1000: LR=8.97e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:17:58,462 | INFO | Epoch 419 Train Time 32.11260151863098s

2025-10-14 20:18:29,799 | INFO | Training epoch 420, Batch 1000/1000: LR=8.97e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 20:18:29,856 | INFO | Epoch 420 Train Time 31.392781019210815s

2025-10-14 20:19:01,294 | INFO | Training epoch 421, Batch 1000/1000: LR=8.96e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 20:19:01,353 | INFO | Epoch 421 Train Time 31.496309280395508s

2025-10-14 20:19:32,528 | INFO | Training epoch 422, Batch 1000/1000: LR=8.96e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:19:32,589 | INFO | Epoch 422 Train Time 31.235351085662842s

2025-10-14 20:20:03,503 | INFO | Training epoch 423, Batch 1000/1000: LR=8.95e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:20:03,561 | INFO | Epoch 423 Train Time 30.96968698501587s

2025-10-14 20:20:34,102 | INFO | Training epoch 424, Batch 1000/1000: LR=8.95e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:20:34,153 | INFO | Epoch 424 Train Time 30.590619802474976s

2025-10-14 20:21:04,935 | INFO | Training epoch 425, Batch 1000/1000: LR=8.94e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:21:04,986 | INFO | Epoch 425 Train Time 30.832160234451294s

2025-10-14 20:21:35,703 | INFO | Training epoch 426, Batch 1000/1000: LR=8.94e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:21:35,761 | INFO | Epoch 426 Train Time 30.774547815322876s

2025-10-14 20:22:07,010 | INFO | Training epoch 427, Batch 1000/1000: LR=8.93e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:22:07,060 | INFO | Epoch 427 Train Time 31.297200679779053s

2025-10-14 20:22:38,412 | INFO | Training epoch 428, Batch 1000/1000: LR=8.93e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:22:38,484 | INFO | Epoch 428 Train Time 31.423074960708618s

2025-10-14 20:23:10,025 | INFO | Training epoch 429, Batch 1000/1000: LR=8.92e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:23:10,083 | INFO | Epoch 429 Train Time 31.598307132720947s

2025-10-14 20:23:41,411 | INFO | Training epoch 430, Batch 1000/1000: LR=8.92e-05, Loss=2.92e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:23:41,467 | INFO | Epoch 430 Train Time 31.381623029708862s

2025-10-14 20:24:12,504 | INFO | Training epoch 431, Batch 1000/1000: LR=8.91e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:24:12,548 | INFO | Epoch 431 Train Time 31.07862639427185s

2025-10-14 20:24:44,498 | INFO | Training epoch 432, Batch 1000/1000: LR=8.91e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:24:44,551 | INFO | Epoch 432 Train Time 32.00213432312012s

2025-10-14 20:25:16,302 | INFO | Training epoch 433, Batch 1000/1000: LR=8.90e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:25:16,354 | INFO | Epoch 433 Train Time 31.801459789276123s

2025-10-14 20:25:47,316 | INFO | Training epoch 434, Batch 1000/1000: LR=8.90e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:25:47,369 | INFO | Epoch 434 Train Time 31.014503479003906s

2025-10-14 20:26:17,698 | INFO | Training epoch 435, Batch 1000/1000: LR=8.89e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:26:17,752 | INFO | Epoch 435 Train Time 30.38194751739502s

2025-10-14 20:26:49,612 | INFO | Training epoch 436, Batch 1000/1000: LR=8.89e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:26:49,656 | INFO | Epoch 436 Train Time 31.903266668319702s

2025-10-14 20:27:21,188 | INFO | Training epoch 437, Batch 1000/1000: LR=8.88e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 20:27:21,245 | INFO | Epoch 437 Train Time 31.587095499038696s

2025-10-14 20:27:52,306 | INFO | Training epoch 438, Batch 1000/1000: LR=8.88e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:27:52,355 | INFO | Epoch 438 Train Time 31.10847234725952s

2025-10-14 20:28:24,022 | INFO | Training epoch 439, Batch 1000/1000: LR=8.87e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:28:24,066 | INFO | Epoch 439 Train Time 31.7097384929657s

2025-10-14 20:28:55,299 | INFO | Training epoch 440, Batch 1000/1000: LR=8.87e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:28:55,347 | INFO | Epoch 440 Train Time 31.279852151870728s

2025-10-14 20:29:26,208 | INFO | Training epoch 441, Batch 1000/1000: LR=8.86e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 20:29:26,269 | INFO | Epoch 441 Train Time 30.92178249359131s

2025-10-14 20:29:26,270 | INFO | [P2] saving best_model (QAT) with loss 0.028294 at epoch 441
2025-10-14 20:29:57,796 | INFO | Training epoch 442, Batch 1000/1000: LR=8.86e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:29:57,849 | INFO | Epoch 442 Train Time 31.564790964126587s

2025-10-14 20:30:29,407 | INFO | Training epoch 443, Batch 1000/1000: LR=8.85e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:30:29,460 | INFO | Epoch 443 Train Time 31.609259366989136s

2025-10-14 20:31:00,992 | INFO | Training epoch 444, Batch 1000/1000: LR=8.85e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 20:31:01,042 | INFO | Epoch 444 Train Time 31.581945657730103s

2025-10-14 20:31:32,104 | INFO | Training epoch 445, Batch 1000/1000: LR=8.84e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:31:32,165 | INFO | Epoch 445 Train Time 31.121912717819214s

2025-10-14 20:32:03,094 | INFO | Training epoch 446, Batch 1000/1000: LR=8.84e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:32:03,158 | INFO | Epoch 446 Train Time 30.99170422554016s

2025-10-14 20:32:34,406 | INFO | Training epoch 447, Batch 1000/1000: LR=8.83e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 20:32:34,461 | INFO | Epoch 447 Train Time 31.302813291549683s

2025-10-14 20:33:05,698 | INFO | Training epoch 448, Batch 1000/1000: LR=8.83e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:33:05,754 | INFO | Epoch 448 Train Time 31.291579961776733s

2025-10-14 20:33:36,996 | INFO | Training epoch 449, Batch 1000/1000: LR=8.82e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:33:37,044 | INFO | Epoch 449 Train Time 31.28877592086792s

2025-10-14 20:34:08,111 | INFO | Training epoch 450, Batch 1000/1000: LR=8.82e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:34:08,159 | INFO | Epoch 450 Train Time 31.11380910873413s

2025-10-14 20:34:39,014 | INFO | Training epoch 451, Batch 1000/1000: LR=8.81e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:34:39,066 | INFO | Epoch 451 Train Time 30.90604043006897s

2025-10-14 20:35:10,309 | INFO | Training epoch 452, Batch 1000/1000: LR=8.81e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:35:10,376 | INFO | Epoch 452 Train Time 31.30915403366089s

2025-10-14 20:35:41,603 | INFO | Training epoch 453, Batch 1000/1000: LR=8.80e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:35:41,653 | INFO | Epoch 453 Train Time 31.275373458862305s

2025-10-14 20:36:13,003 | INFO | Training epoch 454, Batch 1000/1000: LR=8.80e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 20:36:13,046 | INFO | Epoch 454 Train Time 31.392126083374023s

2025-10-14 20:36:44,813 | INFO | Training epoch 455, Batch 1000/1000: LR=8.79e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:36:44,874 | INFO | Epoch 455 Train Time 31.827941179275513s

2025-10-14 20:37:16,719 | INFO | Training epoch 456, Batch 1000/1000: LR=8.79e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.43e-01
2025-10-14 20:37:16,766 | INFO | Epoch 456 Train Time 31.891342163085938s

2025-10-14 20:37:47,899 | INFO | Training epoch 457, Batch 1000/1000: LR=8.78e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 20:37:47,953 | INFO | Epoch 457 Train Time 31.186146020889282s

2025-10-14 20:38:19,492 | INFO | Training epoch 458, Batch 1000/1000: LR=8.78e-05, Loss=2.96e-02 BER=1.11e-02 FER=1.42e-01
2025-10-14 20:38:19,542 | INFO | Epoch 458 Train Time 31.58866238594055s

2025-10-14 20:38:51,116 | INFO | Training epoch 459, Batch 1000/1000: LR=8.77e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:38:51,176 | INFO | Epoch 459 Train Time 31.632217168807983s

2025-10-14 20:39:22,507 | INFO | Training epoch 460, Batch 1000/1000: LR=8.77e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 20:39:22,560 | INFO | Epoch 460 Train Time 31.38171076774597s

2025-10-14 20:39:54,315 | INFO | Training epoch 461, Batch 1000/1000: LR=8.76e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 20:39:54,361 | INFO | Epoch 461 Train Time 31.799681186676025s

2025-10-14 20:40:25,301 | INFO | Training epoch 462, Batch 1000/1000: LR=8.76e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:40:25,355 | INFO | Epoch 462 Train Time 30.991999864578247s

2025-10-14 20:40:55,908 | INFO | Training epoch 463, Batch 1000/1000: LR=8.75e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 20:40:55,957 | INFO | Epoch 463 Train Time 30.60143804550171s

2025-10-14 20:41:27,305 | INFO | Training epoch 464, Batch 1000/1000: LR=8.75e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 20:41:27,357 | INFO | Epoch 464 Train Time 31.399025440216064s

2025-10-14 20:41:59,194 | INFO | Training epoch 465, Batch 1000/1000: LR=8.74e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:41:59,246 | INFO | Epoch 465 Train Time 31.88826322555542s

2025-10-14 20:42:30,802 | INFO | Training epoch 466, Batch 1000/1000: LR=8.74e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:42:30,851 | INFO | Epoch 466 Train Time 31.60409379005432s

2025-10-14 20:43:02,202 | INFO | Training epoch 467, Batch 1000/1000: LR=8.73e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:43:02,250 | INFO | Epoch 467 Train Time 31.398053407669067s

2025-10-14 20:43:34,408 | INFO | Training epoch 468, Batch 1000/1000: LR=8.73e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:43:34,464 | INFO | Epoch 468 Train Time 32.21327090263367s

2025-10-14 20:44:06,017 | INFO | Training epoch 469, Batch 1000/1000: LR=8.72e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:44:06,067 | INFO | Epoch 469 Train Time 31.601090669631958s

2025-10-14 20:44:37,199 | INFO | Training epoch 470, Batch 1000/1000: LR=8.72e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 20:44:37,248 | INFO | Epoch 470 Train Time 31.179009437561035s

2025-10-14 20:45:08,302 | INFO | Training epoch 471, Batch 1000/1000: LR=8.71e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:45:08,350 | INFO | Epoch 471 Train Time 31.102097749710083s

2025-10-14 20:45:39,690 | INFO | Training epoch 472, Batch 1000/1000: LR=8.71e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:45:39,737 | INFO | Epoch 472 Train Time 31.385541677474976s

2025-10-14 20:46:11,393 | INFO | Training epoch 473, Batch 1000/1000: LR=8.70e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:46:11,450 | INFO | Epoch 473 Train Time 31.711146593093872s

2025-10-14 20:46:43,606 | INFO | Training epoch 474, Batch 1000/1000: LR=8.70e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:46:43,660 | INFO | Epoch 474 Train Time 32.20798087120056s

2025-10-14 20:47:15,404 | INFO | Training epoch 475, Batch 1000/1000: LR=8.69e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 20:47:15,466 | INFO | Epoch 475 Train Time 31.804780960083008s

2025-10-14 20:47:46,612 | INFO | Training epoch 476, Batch 1000/1000: LR=8.68e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:47:46,660 | INFO | Epoch 476 Train Time 31.193341732025146s

2025-10-14 20:48:18,200 | INFO | Training epoch 477, Batch 1000/1000: LR=8.68e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:48:18,260 | INFO | Epoch 477 Train Time 31.598186254501343s

2025-10-14 20:48:49,994 | INFO | Training epoch 478, Batch 1000/1000: LR=8.67e-05, Loss=2.92e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:48:50,045 | INFO | Epoch 478 Train Time 31.78321099281311s

2025-10-14 20:49:22,006 | INFO | Training epoch 479, Batch 1000/1000: LR=8.67e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:49:22,067 | INFO | Epoch 479 Train Time 32.02169752120972s

2025-10-14 20:49:53,499 | INFO | Training epoch 480, Batch 1000/1000: LR=8.66e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:49:53,548 | INFO | Epoch 480 Train Time 31.480412483215332s

2025-10-14 20:50:25,303 | INFO | Training epoch 481, Batch 1000/1000: LR=8.66e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:50:25,374 | INFO | Epoch 481 Train Time 31.825664281845093s

2025-10-14 20:50:56,303 | INFO | Training epoch 482, Batch 1000/1000: LR=8.65e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:50:56,352 | INFO | Epoch 482 Train Time 30.976625680923462s

2025-10-14 20:51:27,601 | INFO | Training epoch 483, Batch 1000/1000: LR=8.65e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:51:27,651 | INFO | Epoch 483 Train Time 31.29861545562744s

2025-10-14 20:51:58,893 | INFO | Training epoch 484, Batch 1000/1000: LR=8.64e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:51:58,953 | INFO | Epoch 484 Train Time 31.300354480743408s

2025-10-14 20:52:30,103 | INFO | Training epoch 485, Batch 1000/1000: LR=8.64e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:52:30,143 | INFO | Epoch 485 Train Time 31.1887469291687s

2025-10-14 20:53:01,296 | INFO | Training epoch 486, Batch 1000/1000: LR=8.63e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:53:01,345 | INFO | Epoch 486 Train Time 31.201255083084106s

2025-10-14 20:53:32,719 | INFO | Training epoch 487, Batch 1000/1000: LR=8.63e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 20:53:32,774 | INFO | Epoch 487 Train Time 31.428131818771362s

2025-10-14 20:54:03,803 | INFO | Training epoch 488, Batch 1000/1000: LR=8.62e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 20:54:03,854 | INFO | Epoch 488 Train Time 31.07977819442749s

2025-10-14 20:54:34,932 | INFO | Training epoch 489, Batch 1000/1000: LR=8.62e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 20:54:34,984 | INFO | Epoch 489 Train Time 31.129034757614136s

2025-10-14 20:55:06,497 | INFO | Training epoch 490, Batch 1000/1000: LR=8.61e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:55:06,548 | INFO | Epoch 490 Train Time 31.56203532218933s

2025-10-14 20:55:38,210 | INFO | Training epoch 491, Batch 1000/1000: LR=8.60e-05, Loss=2.95e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 20:55:38,264 | INFO | Epoch 491 Train Time 31.715006589889526s

2025-10-14 20:56:09,732 | INFO | Training epoch 492, Batch 1000/1000: LR=8.60e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:56:09,779 | INFO | Epoch 492 Train Time 31.514150619506836s

2025-10-14 20:56:40,999 | INFO | Training epoch 493, Batch 1000/1000: LR=8.59e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:56:41,053 | INFO | Epoch 493 Train Time 31.273002862930298s

2025-10-14 20:57:12,601 | INFO | Training epoch 494, Batch 1000/1000: LR=8.59e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 20:57:12,653 | INFO | Epoch 494 Train Time 31.597704648971558s

2025-10-14 20:57:45,105 | INFO | Training epoch 495, Batch 1000/1000: LR=8.58e-05, Loss=2.93e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:57:45,148 | INFO | Epoch 495 Train Time 32.49361181259155s

2025-10-14 20:58:15,902 | INFO | Training epoch 496, Batch 1000/1000: LR=8.58e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 20:58:15,948 | INFO | Epoch 496 Train Time 30.798922538757324s

2025-10-14 20:58:46,899 | INFO | Training epoch 497, Batch 1000/1000: LR=8.57e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 20:58:46,951 | INFO | Epoch 497 Train Time 31.00176167488098s

2025-10-14 20:59:18,396 | INFO | Training epoch 498, Batch 1000/1000: LR=8.57e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 20:59:18,453 | INFO | Epoch 498 Train Time 31.501567363739014s

2025-10-14 20:59:49,591 | INFO | Training epoch 499, Batch 1000/1000: LR=8.56e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 20:59:49,651 | INFO | Epoch 499 Train Time 31.197608470916748s

2025-10-14 21:00:20,298 | INFO | Training epoch 500, Batch 1000/1000: LR=8.56e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:00:20,357 | INFO | Epoch 500 Train Time 30.704994201660156s

2025-10-14 21:00:51,699 | INFO | Training epoch 501, Batch 1000/1000: LR=8.55e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:00:51,756 | INFO | Epoch 501 Train Time 31.398396015167236s

2025-10-14 21:01:23,201 | INFO | Training epoch 502, Batch 1000/1000: LR=8.54e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:01:23,252 | INFO | Epoch 502 Train Time 31.49527931213379s

2025-10-14 21:01:55,038 | INFO | Training epoch 503, Batch 1000/1000: LR=8.54e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 21:01:55,085 | INFO | Epoch 503 Train Time 31.831344842910767s

2025-10-14 21:02:26,205 | INFO | Training epoch 504, Batch 1000/1000: LR=8.53e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:02:26,260 | INFO | Epoch 504 Train Time 31.17324924468994s

2025-10-14 21:02:58,014 | INFO | Training epoch 505, Batch 1000/1000: LR=8.53e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 21:02:58,077 | INFO | Epoch 505 Train Time 31.81674337387085s

2025-10-14 21:03:29,500 | INFO | Training epoch 506, Batch 1000/1000: LR=8.52e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:03:29,555 | INFO | Epoch 506 Train Time 31.477323055267334s

2025-10-14 21:04:00,698 | INFO | Training epoch 507, Batch 1000/1000: LR=8.52e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:04:00,759 | INFO | Epoch 507 Train Time 31.2025465965271s

2025-10-14 21:04:32,004 | INFO | Training epoch 508, Batch 1000/1000: LR=8.51e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:04:32,061 | INFO | Epoch 508 Train Time 31.301558256149292s

2025-10-14 21:05:02,802 | INFO | Training epoch 509, Batch 1000/1000: LR=8.51e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:05:02,851 | INFO | Epoch 509 Train Time 30.788757801055908s

2025-10-14 21:05:33,994 | INFO | Training epoch 510, Batch 1000/1000: LR=8.50e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 21:05:34,058 | INFO | Epoch 510 Train Time 31.205906867980957s

2025-10-14 21:06:05,605 | INFO | Training epoch 511, Batch 1000/1000: LR=8.49e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:06:05,656 | INFO | Epoch 511 Train Time 31.59721350669861s

2025-10-14 21:06:37,242 | INFO | Training epoch 512, Batch 1000/1000: LR=8.49e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 21:06:37,309 | INFO | Epoch 512 Train Time 31.652578353881836s

2025-10-14 21:07:08,604 | INFO | Training epoch 513, Batch 1000/1000: LR=8.48e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:07:08,652 | INFO | Epoch 513 Train Time 31.341573238372803s

2025-10-14 21:07:39,902 | INFO | Training epoch 514, Batch 1000/1000: LR=8.48e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:07:39,954 | INFO | Epoch 514 Train Time 31.301570415496826s

2025-10-14 21:08:10,395 | INFO | Training epoch 515, Batch 1000/1000: LR=8.47e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:08:10,461 | INFO | Epoch 515 Train Time 30.506248950958252s

2025-10-14 21:08:41,503 | INFO | Training epoch 516, Batch 1000/1000: LR=8.47e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 21:08:41,553 | INFO | Epoch 516 Train Time 31.089174509048462s

2025-10-14 21:09:13,810 | INFO | Training epoch 517, Batch 1000/1000: LR=8.46e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:09:13,854 | INFO | Epoch 517 Train Time 32.300639629364014s

2025-10-14 21:09:45,605 | INFO | Training epoch 518, Batch 1000/1000: LR=8.46e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.42e-01
2025-10-14 21:09:45,654 | INFO | Epoch 518 Train Time 31.798514366149902s

2025-10-14 21:10:16,704 | INFO | Training epoch 519, Batch 1000/1000: LR=8.45e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:10:16,754 | INFO | Epoch 519 Train Time 31.09909224510193s

2025-10-14 21:10:48,318 | INFO | Training epoch 520, Batch 1000/1000: LR=8.44e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:10:48,368 | INFO | Epoch 520 Train Time 31.613356828689575s

2025-10-14 21:11:19,101 | INFO | Training epoch 521, Batch 1000/1000: LR=8.44e-05, Loss=2.87e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 21:11:19,149 | INFO | Epoch 521 Train Time 30.780025243759155s

2025-10-14 21:11:50,096 | INFO | Training epoch 522, Batch 1000/1000: LR=8.43e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.39e-01
2025-10-14 21:11:50,166 | INFO | Epoch 522 Train Time 31.016127824783325s

2025-10-14 21:12:21,290 | INFO | Training epoch 523, Batch 1000/1000: LR=8.43e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:12:21,337 | INFO | Epoch 523 Train Time 31.169846534729004s

2025-10-14 21:12:52,995 | INFO | Training epoch 524, Batch 1000/1000: LR=8.42e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:12:53,048 | INFO | Epoch 524 Train Time 31.71009397506714s

2025-10-14 21:13:24,744 | INFO | Training epoch 525, Batch 1000/1000: LR=8.42e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:13:24,792 | INFO | Epoch 525 Train Time 31.742769718170166s

2025-10-14 21:13:56,200 | INFO | Training epoch 526, Batch 1000/1000: LR=8.41e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:13:56,252 | INFO | Epoch 526 Train Time 31.458680152893066s

2025-10-14 21:14:27,701 | INFO | Training epoch 527, Batch 1000/1000: LR=8.40e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:14:27,751 | INFO | Epoch 527 Train Time 31.498619318008423s

2025-10-14 21:14:59,198 | INFO | Training epoch 528, Batch 1000/1000: LR=8.40e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:14:59,239 | INFO | Epoch 528 Train Time 31.48668146133423s

2025-10-14 21:15:30,306 | INFO | Training epoch 529, Batch 1000/1000: LR=8.39e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:15:30,359 | INFO | Epoch 529 Train Time 31.11922860145569s

2025-10-14 21:16:02,199 | INFO | Training epoch 530, Batch 1000/1000: LR=8.39e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:16:02,250 | INFO | Epoch 530 Train Time 31.89035964012146s

2025-10-14 21:16:33,898 | INFO | Training epoch 531, Batch 1000/1000: LR=8.38e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:16:33,951 | INFO | Epoch 531 Train Time 31.69985818862915s

2025-10-14 21:17:04,523 | INFO | Training epoch 532, Batch 1000/1000: LR=8.38e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:17:04,585 | INFO | Epoch 532 Train Time 30.63293170928955s

2025-10-14 21:17:36,308 | INFO | Training epoch 533, Batch 1000/1000: LR=8.37e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:17:36,367 | INFO | Epoch 533 Train Time 31.781308889389038s

2025-10-14 21:18:07,811 | INFO | Training epoch 534, Batch 1000/1000: LR=8.36e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:18:07,860 | INFO | Epoch 534 Train Time 31.491814136505127s

2025-10-14 21:18:39,500 | INFO | Training epoch 535, Batch 1000/1000: LR=8.36e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.41e-01
2025-10-14 21:18:39,547 | INFO | Epoch 535 Train Time 31.68556571006775s

2025-10-14 21:19:11,397 | INFO | Training epoch 536, Batch 1000/1000: LR=8.35e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:19:11,449 | INFO | Epoch 536 Train Time 31.90020179748535s

2025-10-14 21:19:42,730 | INFO | Training epoch 537, Batch 1000/1000: LR=8.35e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:19:42,782 | INFO | Epoch 537 Train Time 31.332663774490356s

2025-10-14 21:20:13,410 | INFO | Training epoch 538, Batch 1000/1000: LR=8.34e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.39e-01
2025-10-14 21:20:13,457 | INFO | Epoch 538 Train Time 30.673258543014526s

2025-10-14 21:20:44,502 | INFO | Training epoch 539, Batch 1000/1000: LR=8.34e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:20:44,564 | INFO | Epoch 539 Train Time 31.105736255645752s

2025-10-14 21:21:15,610 | INFO | Training epoch 540, Batch 1000/1000: LR=8.33e-05, Loss=2.90e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:21:15,663 | INFO | Epoch 540 Train Time 31.097922563552856s

2025-10-14 21:21:47,100 | INFO | Training epoch 541, Batch 1000/1000: LR=8.32e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.42e-01
2025-10-14 21:21:47,162 | INFO | Epoch 541 Train Time 31.498507499694824s

2025-10-14 21:22:19,121 | INFO | Training epoch 542, Batch 1000/1000: LR=8.32e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:22:19,172 | INFO | Epoch 542 Train Time 32.00896954536438s

2025-10-14 21:22:51,100 | INFO | Training epoch 543, Batch 1000/1000: LR=8.31e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:22:51,150 | INFO | Epoch 543 Train Time 31.977152109146118s

2025-10-14 21:23:22,730 | INFO | Training epoch 544, Batch 1000/1000: LR=8.31e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:23:22,780 | INFO | Epoch 544 Train Time 31.628765106201172s

2025-10-14 21:23:54,392 | INFO | Training epoch 545, Batch 1000/1000: LR=8.30e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:23:54,452 | INFO | Epoch 545 Train Time 31.67119526863098s

2025-10-14 21:24:25,593 | INFO | Training epoch 546, Batch 1000/1000: LR=8.29e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 21:24:25,639 | INFO | Epoch 546 Train Time 31.185874700546265s

2025-10-14 21:24:57,019 | INFO | Training epoch 547, Batch 1000/1000: LR=8.29e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 21:24:57,064 | INFO | Epoch 547 Train Time 31.42292809486389s

2025-10-14 21:25:28,507 | INFO | Training epoch 548, Batch 1000/1000: LR=8.28e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:25:28,557 | INFO | Epoch 548 Train Time 31.492244005203247s

2025-10-14 21:25:58,900 | INFO | Training epoch 549, Batch 1000/1000: LR=8.28e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:25:58,952 | INFO | Epoch 549 Train Time 30.394773960113525s

2025-10-14 21:26:29,811 | INFO | Training epoch 550, Batch 1000/1000: LR=8.27e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:26:29,858 | INFO | Epoch 550 Train Time 30.904935836791992s

2025-10-14 21:27:01,300 | INFO | Training epoch 551, Batch 1000/1000: LR=8.26e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:27:01,369 | INFO | Epoch 551 Train Time 31.50940465927124s

2025-10-14 21:27:33,018 | INFO | Training epoch 552, Batch 1000/1000: LR=8.26e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:27:33,071 | INFO | Epoch 552 Train Time 31.699527978897095s

2025-10-14 21:28:04,591 | INFO | Training epoch 553, Batch 1000/1000: LR=8.25e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:28:04,645 | INFO | Epoch 553 Train Time 31.573244333267212s

2025-10-14 21:28:36,608 | INFO | Training epoch 554, Batch 1000/1000: LR=8.25e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:28:36,687 | INFO | Epoch 554 Train Time 32.04059314727783s

2025-10-14 21:29:08,097 | INFO | Training epoch 555, Batch 1000/1000: LR=8.24e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:29:08,155 | INFO | Epoch 555 Train Time 31.46748185157776s

2025-10-14 21:29:39,793 | INFO | Training epoch 556, Batch 1000/1000: LR=8.24e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:29:39,845 | INFO | Epoch 556 Train Time 31.688690185546875s

2025-10-14 21:30:10,706 | INFO | Training epoch 557, Batch 1000/1000: LR=8.23e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:30:10,756 | INFO | Epoch 557 Train Time 30.91071367263794s

2025-10-14 21:30:41,919 | INFO | Training epoch 558, Batch 1000/1000: LR=8.22e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:30:41,970 | INFO | Epoch 558 Train Time 31.21313190460205s

2025-10-14 21:31:13,909 | INFO | Training epoch 559, Batch 1000/1000: LR=8.22e-05, Loss=2.93e-02 BER=1.10e-02 FER=1.40e-01
2025-10-14 21:31:13,961 | INFO | Epoch 559 Train Time 31.989441394805908s

2025-10-14 21:31:45,494 | INFO | Training epoch 560, Batch 1000/1000: LR=8.21e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:31:45,548 | INFO | Epoch 560 Train Time 31.58645009994507s

2025-10-14 21:32:16,811 | INFO | Training epoch 561, Batch 1000/1000: LR=8.21e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:32:16,856 | INFO | Epoch 561 Train Time 31.30777668952942s

2025-10-14 21:32:47,694 | INFO | Training epoch 562, Batch 1000/1000: LR=8.20e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 21:32:47,737 | INFO | Epoch 562 Train Time 30.87958025932312s

2025-10-14 21:33:19,410 | INFO | Training epoch 563, Batch 1000/1000: LR=8.19e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:33:19,454 | INFO | Epoch 563 Train Time 31.716076850891113s

2025-10-14 21:33:50,897 | INFO | Training epoch 564, Batch 1000/1000: LR=8.19e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:33:50,969 | INFO | Epoch 564 Train Time 31.514719247817993s

2025-10-14 21:34:22,415 | INFO | Training epoch 565, Batch 1000/1000: LR=8.18e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:34:22,464 | INFO | Epoch 565 Train Time 31.494184494018555s

2025-10-14 21:34:54,098 | INFO | Training epoch 566, Batch 1000/1000: LR=8.18e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:34:54,157 | INFO | Epoch 566 Train Time 31.691540956497192s

2025-10-14 21:35:25,099 | INFO | Training epoch 567, Batch 1000/1000: LR=8.17e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:35:25,146 | INFO | Epoch 567 Train Time 30.98781156539917s

2025-10-14 21:35:56,308 | INFO | Training epoch 568, Batch 1000/1000: LR=8.16e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:35:56,367 | INFO | Epoch 568 Train Time 31.22036123275757s

2025-10-14 21:36:28,202 | INFO | Training epoch 569, Batch 1000/1000: LR=8.16e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:36:28,260 | INFO | Epoch 569 Train Time 31.891311168670654s

2025-10-14 21:36:59,698 | INFO | Training epoch 570, Batch 1000/1000: LR=8.15e-05, Loss=2.94e-02 BER=1.10e-02 FER=1.41e-01
2025-10-14 21:36:59,751 | INFO | Epoch 570 Train Time 31.490140676498413s

2025-10-14 21:37:30,926 | INFO | Training epoch 571, Batch 1000/1000: LR=8.14e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 21:37:30,978 | INFO | Epoch 571 Train Time 31.22561001777649s

2025-10-14 21:38:01,593 | INFO | Training epoch 572, Batch 1000/1000: LR=8.14e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:38:01,644 | INFO | Epoch 572 Train Time 30.66537380218506s

2025-10-14 21:38:32,892 | INFO | Training epoch 573, Batch 1000/1000: LR=8.13e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:38:32,940 | INFO | Epoch 573 Train Time 31.29496717453003s

2025-10-14 21:39:03,596 | INFO | Training epoch 574, Batch 1000/1000: LR=8.13e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:39:03,654 | INFO | Epoch 574 Train Time 30.71285891532898s

2025-10-14 21:39:34,599 | INFO | Training epoch 575, Batch 1000/1000: LR=8.12e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:39:34,653 | INFO | Epoch 575 Train Time 30.997716188430786s

2025-10-14 21:40:05,417 | INFO | Training epoch 576, Batch 1000/1000: LR=8.11e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:40:05,479 | INFO | Epoch 576 Train Time 30.825512409210205s

2025-10-14 21:40:37,263 | INFO | Training epoch 577, Batch 1000/1000: LR=8.11e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:40:37,319 | INFO | Epoch 577 Train Time 31.838594675064087s

2025-10-14 21:41:09,415 | INFO | Training epoch 578, Batch 1000/1000: LR=8.10e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:41:09,462 | INFO | Epoch 578 Train Time 32.14264249801636s

2025-10-14 21:41:41,292 | INFO | Training epoch 579, Batch 1000/1000: LR=8.10e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:41:41,354 | INFO | Epoch 579 Train Time 31.891019821166992s

2025-10-14 21:42:12,899 | INFO | Training epoch 580, Batch 1000/1000: LR=8.09e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 21:42:12,952 | INFO | Epoch 580 Train Time 31.59636950492859s

2025-10-14 21:42:43,513 | INFO | Training epoch 581, Batch 1000/1000: LR=8.08e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:42:43,567 | INFO | Epoch 581 Train Time 30.61414909362793s

2025-10-14 21:43:15,605 | INFO | Training epoch 582, Batch 1000/1000: LR=8.08e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:43:15,673 | INFO | Epoch 582 Train Time 32.10518288612366s

2025-10-14 21:43:46,797 | INFO | Training epoch 583, Batch 1000/1000: LR=8.07e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:43:46,859 | INFO | Epoch 583 Train Time 31.18448519706726s

2025-10-14 21:44:18,498 | INFO | Training epoch 584, Batch 1000/1000: LR=8.07e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:44:18,578 | INFO | Epoch 584 Train Time 31.718579530715942s

2025-10-14 21:44:49,507 | INFO | Training epoch 585, Batch 1000/1000: LR=8.06e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 21:44:49,560 | INFO | Epoch 585 Train Time 30.98050045967102s

2025-10-14 21:45:21,204 | INFO | Training epoch 586, Batch 1000/1000: LR=8.05e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 21:45:21,266 | INFO | Epoch 586 Train Time 31.703612565994263s

2025-10-14 21:45:52,922 | INFO | Training epoch 587, Batch 1000/1000: LR=8.05e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:45:52,987 | INFO | Epoch 587 Train Time 31.720948934555054s

2025-10-14 21:46:24,226 | INFO | Training epoch 588, Batch 1000/1000: LR=8.04e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:46:24,281 | INFO | Epoch 588 Train Time 31.29197072982788s

2025-10-14 21:46:55,401 | INFO | Training epoch 589, Batch 1000/1000: LR=8.03e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:46:55,462 | INFO | Epoch 589 Train Time 31.18074870109558s

2025-10-14 21:47:26,692 | INFO | Training epoch 590, Batch 1000/1000: LR=8.03e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 21:47:26,752 | INFO | Epoch 590 Train Time 31.288191556930542s

2025-10-14 21:47:57,892 | INFO | Training epoch 591, Batch 1000/1000: LR=8.02e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:47:57,960 | INFO | Epoch 591 Train Time 31.207422018051147s

2025-10-14 21:48:28,951 | INFO | Training epoch 592, Batch 1000/1000: LR=8.02e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:48:29,014 | INFO | Epoch 592 Train Time 31.052907466888428s

2025-10-14 21:49:00,295 | INFO | Training epoch 593, Batch 1000/1000: LR=8.01e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:49:00,359 | INFO | Epoch 593 Train Time 31.34430980682373s

2025-10-14 21:49:31,688 | INFO | Training epoch 594, Batch 1000/1000: LR=8.00e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:49:31,743 | INFO | Epoch 594 Train Time 31.382949590682983s

2025-10-14 21:50:03,408 | INFO | Training epoch 595, Batch 1000/1000: LR=8.00e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:50:03,466 | INFO | Epoch 595 Train Time 31.721208095550537s

2025-10-14 21:50:35,401 | INFO | Training epoch 596, Batch 1000/1000: LR=7.99e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 21:50:35,466 | INFO | Epoch 596 Train Time 31.99988865852356s

2025-10-14 21:51:07,003 | INFO | Training epoch 597, Batch 1000/1000: LR=7.98e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:51:07,079 | INFO | Epoch 597 Train Time 31.612339735031128s

2025-10-14 21:51:38,500 | INFO | Training epoch 598, Batch 1000/1000: LR=7.98e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:51:38,560 | INFO | Epoch 598 Train Time 31.478522062301636s

2025-10-14 21:52:10,490 | INFO | Training epoch 599, Batch 1000/1000: LR=7.97e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 21:52:10,558 | INFO | Epoch 599 Train Time 31.99721097946167s

2025-10-14 21:52:41,294 | INFO | Training epoch 600, Batch 1000/1000: LR=7.97e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:52:41,345 | INFO | Epoch 600 Train Time 30.787177085876465s

2025-10-14 21:53:12,300 | INFO | Training epoch 601, Batch 1000/1000: LR=7.96e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:53:12,358 | INFO | Epoch 601 Train Time 31.01241111755371s

2025-10-14 21:53:43,890 | INFO | Training epoch 602, Batch 1000/1000: LR=7.95e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:53:43,939 | INFO | Epoch 602 Train Time 31.57997179031372s

2025-10-14 21:54:15,892 | INFO | Training epoch 603, Batch 1000/1000: LR=7.95e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 21:54:15,954 | INFO | Epoch 603 Train Time 32.01313829421997s

2025-10-14 21:54:47,895 | INFO | Training epoch 604, Batch 1000/1000: LR=7.94e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:54:47,953 | INFO | Epoch 604 Train Time 31.99759030342102s

2025-10-14 21:55:19,795 | INFO | Training epoch 605, Batch 1000/1000: LR=7.93e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:55:19,855 | INFO | Epoch 605 Train Time 31.901293992996216s

2025-10-14 21:55:50,603 | INFO | Training epoch 606, Batch 1000/1000: LR=7.93e-05, Loss=2.89e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 21:55:50,658 | INFO | Epoch 606 Train Time 30.800928354263306s

2025-10-14 21:56:22,107 | INFO | Training epoch 607, Batch 1000/1000: LR=7.92e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:56:22,166 | INFO | Epoch 607 Train Time 31.506433963775635s

2025-10-14 21:56:53,825 | INFO | Training epoch 608, Batch 1000/1000: LR=7.92e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:56:53,887 | INFO | Epoch 608 Train Time 31.720303297042847s

2025-10-14 21:57:25,001 | INFO | Training epoch 609, Batch 1000/1000: LR=7.91e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:57:25,063 | INFO | Epoch 609 Train Time 31.174744129180908s

2025-10-14 21:57:56,025 | INFO | Training epoch 610, Batch 1000/1000: LR=7.90e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.38e-01
2025-10-14 21:57:56,082 | INFO | Epoch 610 Train Time 31.018253564834595s

2025-10-14 21:58:27,114 | INFO | Training epoch 611, Batch 1000/1000: LR=7.90e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 21:58:27,170 | INFO | Epoch 611 Train Time 31.086276054382324s

2025-10-14 21:58:58,500 | INFO | Training epoch 612, Batch 1000/1000: LR=7.89e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 21:58:58,563 | INFO | Epoch 612 Train Time 31.392013788223267s

2025-10-14 21:59:29,897 | INFO | Training epoch 613, Batch 1000/1000: LR=7.88e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 21:59:29,966 | INFO | Epoch 613 Train Time 31.403114795684814s

2025-10-14 22:00:01,200 | INFO | Training epoch 614, Batch 1000/1000: LR=7.88e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:00:01,254 | INFO | Epoch 614 Train Time 31.287262678146362s

2025-10-14 22:00:32,713 | INFO | Training epoch 615, Batch 1000/1000: LR=7.87e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:00:32,772 | INFO | Epoch 615 Train Time 31.51599383354187s

2025-10-14 22:01:04,109 | INFO | Training epoch 616, Batch 1000/1000: LR=7.86e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:01:04,172 | INFO | Epoch 616 Train Time 31.39875626564026s

2025-10-14 22:01:35,298 | INFO | Training epoch 617, Batch 1000/1000: LR=7.86e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 22:01:35,352 | INFO | Epoch 617 Train Time 31.17843747138977s

2025-10-14 22:02:07,012 | INFO | Training epoch 618, Batch 1000/1000: LR=7.85e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:02:07,063 | INFO | Epoch 618 Train Time 31.708944082260132s

2025-10-14 22:02:38,010 | INFO | Training epoch 619, Batch 1000/1000: LR=7.85e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:02:38,074 | INFO | Epoch 619 Train Time 31.010361671447754s

2025-10-14 22:03:09,416 | INFO | Training epoch 620, Batch 1000/1000: LR=7.84e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 22:03:09,476 | INFO | Epoch 620 Train Time 31.401508569717407s

2025-10-14 22:03:39,991 | INFO | Training epoch 621, Batch 1000/1000: LR=7.83e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:03:40,047 | INFO | Epoch 621 Train Time 30.569910526275635s

2025-10-14 22:04:11,093 | INFO | Training epoch 622, Batch 1000/1000: LR=7.83e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:04:11,152 | INFO | Epoch 622 Train Time 31.103562593460083s

2025-10-14 22:04:42,635 | INFO | Training epoch 623, Batch 1000/1000: LR=7.82e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:04:42,714 | INFO | Epoch 623 Train Time 31.56172776222229s

2025-10-14 22:05:14,304 | INFO | Training epoch 624, Batch 1000/1000: LR=7.81e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:05:14,360 | INFO | Epoch 624 Train Time 31.644786834716797s

2025-10-14 22:05:45,595 | INFO | Training epoch 625, Batch 1000/1000: LR=7.81e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:05:45,652 | INFO | Epoch 625 Train Time 31.29051399230957s

2025-10-14 22:06:16,899 | INFO | Training epoch 626, Batch 1000/1000: LR=7.80e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-14 22:06:16,944 | INFO | Epoch 626 Train Time 31.29150629043579s

2025-10-14 22:06:16,945 | INFO | [P2] saving best_model (QAT) with loss 0.028254 at epoch 626
2025-10-14 22:06:48,630 | INFO | Training epoch 627, Batch 1000/1000: LR=7.79e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:06:48,687 | INFO | Epoch 627 Train Time 31.72710919380188s

2025-10-14 22:07:20,408 | INFO | Training epoch 628, Batch 1000/1000: LR=7.79e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:07:20,474 | INFO | Epoch 628 Train Time 31.78591537475586s

2025-10-14 22:07:51,208 | INFO | Training epoch 629, Batch 1000/1000: LR=7.78e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:07:51,267 | INFO | Epoch 629 Train Time 30.792964935302734s

2025-10-14 22:08:22,907 | INFO | Training epoch 630, Batch 1000/1000: LR=7.77e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:08:22,975 | INFO | Epoch 630 Train Time 31.70643711090088s

2025-10-14 22:08:54,508 | INFO | Training epoch 631, Batch 1000/1000: LR=7.77e-05, Loss=2.87e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:08:54,566 | INFO | Epoch 631 Train Time 31.590333223342896s

2025-10-14 22:09:25,696 | INFO | Training epoch 632, Batch 1000/1000: LR=7.76e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:09:25,756 | INFO | Epoch 632 Train Time 31.188637256622314s

2025-10-14 22:09:56,616 | INFO | Training epoch 633, Batch 1000/1000: LR=7.75e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:09:56,670 | INFO | Epoch 633 Train Time 30.914169788360596s

2025-10-14 22:10:27,802 | INFO | Training epoch 634, Batch 1000/1000: LR=7.75e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.40e-01
2025-10-14 22:10:27,859 | INFO | Epoch 634 Train Time 31.187331676483154s

2025-10-14 22:10:58,818 | INFO | Training epoch 635, Batch 1000/1000: LR=7.74e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 22:10:58,878 | INFO | Epoch 635 Train Time 31.01860475540161s

2025-10-14 22:11:30,099 | INFO | Training epoch 636, Batch 1000/1000: LR=7.74e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 22:11:30,161 | INFO | Epoch 636 Train Time 31.281307220458984s

2025-10-14 22:12:01,504 | INFO | Training epoch 637, Batch 1000/1000: LR=7.73e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 22:12:01,563 | INFO | Epoch 637 Train Time 31.401480197906494s

2025-10-14 22:12:32,830 | INFO | Training epoch 638, Batch 1000/1000: LR=7.72e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:12:32,899 | INFO | Epoch 638 Train Time 31.33494234085083s

2025-10-14 22:13:04,500 | INFO | Training epoch 639, Batch 1000/1000: LR=7.72e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:13:04,558 | INFO | Epoch 639 Train Time 31.658010244369507s

2025-10-14 22:13:36,013 | INFO | Training epoch 640, Batch 1000/1000: LR=7.71e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:13:36,073 | INFO | Epoch 640 Train Time 31.513890743255615s

2025-10-14 22:14:06,911 | INFO | Training epoch 641, Batch 1000/1000: LR=7.70e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:14:06,969 | INFO | Epoch 641 Train Time 30.894683361053467s

2025-10-14 22:14:38,912 | INFO | Training epoch 642, Batch 1000/1000: LR=7.70e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:14:38,972 | INFO | Epoch 642 Train Time 32.00060224533081s

2025-10-14 22:15:10,306 | INFO | Training epoch 643, Batch 1000/1000: LR=7.69e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:15:10,369 | INFO | Epoch 643 Train Time 31.395474433898926s

2025-10-14 22:15:41,906 | INFO | Training epoch 644, Batch 1000/1000: LR=7.68e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:15:41,967 | INFO | Epoch 644 Train Time 31.59600329399109s

2025-10-14 22:16:13,629 | INFO | Training epoch 645, Batch 1000/1000: LR=7.68e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:16:13,678 | INFO | Epoch 645 Train Time 31.710632801055908s

2025-10-14 22:16:44,791 | INFO | Training epoch 646, Batch 1000/1000: LR=7.67e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-14 22:16:44,855 | INFO | Epoch 646 Train Time 31.1759192943573s

2025-10-14 22:17:16,917 | INFO | Training epoch 647, Batch 1000/1000: LR=7.66e-05, Loss=2.91e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:17:16,975 | INFO | Epoch 647 Train Time 32.11943054199219s

2025-10-14 22:17:48,605 | INFO | Training epoch 648, Batch 1000/1000: LR=7.66e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:17:48,658 | INFO | Epoch 648 Train Time 31.680662393569946s

2025-10-14 22:18:20,703 | INFO | Training epoch 649, Batch 1000/1000: LR=7.65e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:18:20,764 | INFO | Epoch 649 Train Time 32.105496883392334s

2025-10-14 22:18:52,806 | INFO | Training epoch 650, Batch 1000/1000: LR=7.64e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:18:52,860 | INFO | Epoch 650 Train Time 32.09540152549744s

2025-10-14 22:19:24,005 | INFO | Training epoch 651, Batch 1000/1000: LR=7.64e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:19:24,059 | INFO | Epoch 651 Train Time 31.19833731651306s

2025-10-14 22:19:55,608 | INFO | Training epoch 652, Batch 1000/1000: LR=7.63e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:19:55,674 | INFO | Epoch 652 Train Time 31.613500356674194s

2025-10-14 22:20:27,506 | INFO | Training epoch 653, Batch 1000/1000: LR=7.62e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:20:27,563 | INFO | Epoch 653 Train Time 31.888323068618774s

2025-10-14 22:20:59,397 | INFO | Training epoch 654, Batch 1000/1000: LR=7.62e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:20:59,462 | INFO | Epoch 654 Train Time 31.897422790527344s

2025-10-14 22:21:30,806 | INFO | Training epoch 655, Batch 1000/1000: LR=7.61e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:21:30,873 | INFO | Epoch 655 Train Time 31.409932374954224s

2025-10-14 22:22:02,299 | INFO | Training epoch 656, Batch 1000/1000: LR=7.60e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:22:02,357 | INFO | Epoch 656 Train Time 31.482590198516846s

2025-10-14 22:22:34,109 | INFO | Training epoch 657, Batch 1000/1000: LR=7.60e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:22:34,164 | INFO | Epoch 657 Train Time 31.806721448898315s

2025-10-14 22:23:05,498 | INFO | Training epoch 658, Batch 1000/1000: LR=7.59e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:23:05,549 | INFO | Epoch 658 Train Time 31.38357448577881s

2025-10-14 22:23:36,501 | INFO | Training epoch 659, Batch 1000/1000: LR=7.58e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:23:36,560 | INFO | Epoch 659 Train Time 31.010115146636963s

2025-10-14 22:24:08,096 | INFO | Training epoch 660, Batch 1000/1000: LR=7.58e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 22:24:08,156 | INFO | Epoch 660 Train Time 31.595391750335693s

2025-10-14 22:24:39,408 | INFO | Training epoch 661, Batch 1000/1000: LR=7.57e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:24:39,463 | INFO | Epoch 661 Train Time 31.306230783462524s

2025-10-14 22:25:11,087 | INFO | Training epoch 662, Batch 1000/1000: LR=7.56e-05, Loss=2.92e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 22:25:11,146 | INFO | Epoch 662 Train Time 31.681923866271973s

2025-10-14 22:25:42,300 | INFO | Training epoch 663, Batch 1000/1000: LR=7.56e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:25:42,355 | INFO | Epoch 663 Train Time 31.208431482315063s

2025-10-14 22:26:13,901 | INFO | Training epoch 664, Batch 1000/1000: LR=7.55e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 22:26:13,956 | INFO | Epoch 664 Train Time 31.600441217422485s

2025-10-14 22:26:45,698 | INFO | Training epoch 665, Batch 1000/1000: LR=7.54e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:26:45,765 | INFO | Epoch 665 Train Time 31.807498455047607s

2025-10-14 22:27:17,102 | INFO | Training epoch 666, Batch 1000/1000: LR=7.54e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:27:17,158 | INFO | Epoch 666 Train Time 31.39228367805481s

2025-10-14 22:27:48,699 | INFO | Training epoch 667, Batch 1000/1000: LR=7.53e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:27:48,754 | INFO | Epoch 667 Train Time 31.595306873321533s

2025-10-14 22:28:20,190 | INFO | Training epoch 668, Batch 1000/1000: LR=7.52e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 22:28:20,248 | INFO | Epoch 668 Train Time 31.493167638778687s

2025-10-14 22:28:51,607 | INFO | Training epoch 669, Batch 1000/1000: LR=7.52e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:28:51,673 | INFO | Epoch 669 Train Time 31.423874616622925s

2025-10-14 22:29:23,401 | INFO | Training epoch 670, Batch 1000/1000: LR=7.51e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:29:23,459 | INFO | Epoch 670 Train Time 31.7833571434021s

2025-10-14 22:29:54,094 | INFO | Training epoch 671, Batch 1000/1000: LR=7.50e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:29:54,145 | INFO | Epoch 671 Train Time 30.68585467338562s

2025-10-14 22:30:24,990 | INFO | Training epoch 672, Batch 1000/1000: LR=7.50e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:30:25,044 | INFO | Epoch 672 Train Time 30.89823603630066s

2025-10-14 22:30:56,408 | INFO | Training epoch 673, Batch 1000/1000: LR=7.49e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:30:56,468 | INFO | Epoch 673 Train Time 31.422805547714233s

2025-10-14 22:31:28,203 | INFO | Training epoch 674, Batch 1000/1000: LR=7.48e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:31:28,268 | INFO | Epoch 674 Train Time 31.798770904541016s

2025-10-14 22:31:59,796 | INFO | Training epoch 675, Batch 1000/1000: LR=7.48e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:31:59,857 | INFO | Epoch 675 Train Time 31.588823795318604s

2025-10-14 22:32:32,011 | INFO | Training epoch 676, Batch 1000/1000: LR=7.47e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:32:32,075 | INFO | Epoch 676 Train Time 32.21538853645325s

2025-10-14 22:33:03,299 | INFO | Training epoch 677, Batch 1000/1000: LR=7.46e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:33:03,367 | INFO | Epoch 677 Train Time 31.291502475738525s

2025-10-14 22:33:34,919 | INFO | Training epoch 678, Batch 1000/1000: LR=7.46e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-14 22:33:34,974 | INFO | Epoch 678 Train Time 31.604739665985107s

2025-10-14 22:33:34,974 | INFO | [P2] saving best_model (QAT) with loss 0.028052 at epoch 678
2025-10-14 22:34:06,910 | INFO | Training epoch 679, Batch 1000/1000: LR=7.45e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:34:06,969 | INFO | Epoch 679 Train Time 31.979968070983887s

2025-10-14 22:34:38,023 | INFO | Training epoch 680, Batch 1000/1000: LR=7.44e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 22:34:38,081 | INFO | Epoch 680 Train Time 31.111908197402954s

2025-10-14 22:35:09,803 | INFO | Training epoch 681, Batch 1000/1000: LR=7.43e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:35:09,857 | INFO | Epoch 681 Train Time 31.774807929992676s

2025-10-14 22:35:41,003 | INFO | Training epoch 682, Batch 1000/1000: LR=7.43e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:35:41,063 | INFO | Epoch 682 Train Time 31.204734563827515s

2025-10-14 22:36:12,711 | INFO | Training epoch 683, Batch 1000/1000: LR=7.42e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:36:12,778 | INFO | Epoch 683 Train Time 31.71390151977539s

2025-10-14 22:36:44,412 | INFO | Training epoch 684, Batch 1000/1000: LR=7.41e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:36:44,459 | INFO | Epoch 684 Train Time 31.679394960403442s

2025-10-14 22:37:15,703 | INFO | Training epoch 685, Batch 1000/1000: LR=7.41e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:37:15,772 | INFO | Epoch 685 Train Time 31.31265377998352s

2025-10-14 22:37:47,300 | INFO | Training epoch 686, Batch 1000/1000: LR=7.40e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:37:47,360 | INFO | Epoch 686 Train Time 31.586337089538574s

2025-10-14 22:38:18,401 | INFO | Training epoch 687, Batch 1000/1000: LR=7.39e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:38:18,464 | INFO | Epoch 687 Train Time 31.10329794883728s

2025-10-14 22:38:50,102 | INFO | Training epoch 688, Batch 1000/1000: LR=7.39e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:38:50,158 | INFO | Epoch 688 Train Time 31.693241357803345s

2025-10-14 22:39:22,009 | INFO | Training epoch 689, Batch 1000/1000: LR=7.38e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:39:22,073 | INFO | Epoch 689 Train Time 31.914103984832764s

2025-10-14 22:39:53,002 | INFO | Training epoch 690, Batch 1000/1000: LR=7.37e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:39:53,058 | INFO | Epoch 690 Train Time 30.98408818244934s

2025-10-14 22:40:24,468 | INFO | Training epoch 691, Batch 1000/1000: LR=7.37e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:40:24,524 | INFO | Epoch 691 Train Time 31.464333057403564s

2025-10-14 22:40:55,702 | INFO | Training epoch 692, Batch 1000/1000: LR=7.36e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:40:55,768 | INFO | Epoch 692 Train Time 31.243860483169556s

2025-10-14 22:41:27,199 | INFO | Training epoch 693, Batch 1000/1000: LR=7.35e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:41:27,252 | INFO | Epoch 693 Train Time 31.483111143112183s

2025-10-14 22:41:58,121 | INFO | Training epoch 694, Batch 1000/1000: LR=7.35e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:41:58,172 | INFO | Epoch 694 Train Time 30.91807985305786s

2025-10-14 22:42:30,030 | INFO | Training epoch 695, Batch 1000/1000: LR=7.34e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:42:30,080 | INFO | Epoch 695 Train Time 31.90664029121399s

2025-10-14 22:43:01,606 | INFO | Training epoch 696, Batch 1000/1000: LR=7.33e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:43:01,664 | INFO | Epoch 696 Train Time 31.583143711090088s

2025-10-14 22:43:32,811 | INFO | Training epoch 697, Batch 1000/1000: LR=7.32e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:43:32,863 | INFO | Epoch 697 Train Time 31.19718647003174s

2025-10-14 22:44:04,705 | INFO | Training epoch 698, Batch 1000/1000: LR=7.32e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:44:04,779 | INFO | Epoch 698 Train Time 31.916163206100464s

2025-10-14 22:44:36,500 | INFO | Training epoch 699, Batch 1000/1000: LR=7.31e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 22:44:36,556 | INFO | Epoch 699 Train Time 31.774579763412476s

2025-10-14 22:45:07,598 | INFO | Training epoch 700, Batch 1000/1000: LR=7.30e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:45:07,660 | INFO | Epoch 700 Train Time 31.10374641418457s

2025-10-14 22:45:39,133 | INFO | Training epoch 701, Batch 1000/1000: LR=7.30e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:45:39,185 | INFO | Epoch 701 Train Time 31.524333953857422s

2025-10-14 22:46:10,515 | INFO | Training epoch 702, Batch 1000/1000: LR=7.29e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:46:10,579 | INFO | Epoch 702 Train Time 31.392027854919434s

2025-10-14 22:46:42,131 | INFO | Training epoch 703, Batch 1000/1000: LR=7.28e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:46:42,194 | INFO | Epoch 703 Train Time 31.614389419555664s

2025-10-14 22:47:13,098 | INFO | Training epoch 704, Batch 1000/1000: LR=7.28e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:47:13,166 | INFO | Epoch 704 Train Time 30.970881700515747s

2025-10-14 22:47:44,704 | INFO | Training epoch 705, Batch 1000/1000: LR=7.27e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:47:44,785 | INFO | Epoch 705 Train Time 31.61814308166504s

2025-10-14 22:48:15,901 | INFO | Training epoch 706, Batch 1000/1000: LR=7.26e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:48:15,957 | INFO | Epoch 706 Train Time 31.170977115631104s

2025-10-14 22:48:47,326 | INFO | Training epoch 707, Batch 1000/1000: LR=7.26e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:48:47,391 | INFO | Epoch 707 Train Time 31.4326171875s

2025-10-14 22:49:19,026 | INFO | Training epoch 708, Batch 1000/1000: LR=7.25e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:49:19,086 | INFO | Epoch 708 Train Time 31.694007635116577s

2025-10-14 22:49:50,611 | INFO | Training epoch 709, Batch 1000/1000: LR=7.24e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:49:50,673 | INFO | Epoch 709 Train Time 31.585212230682373s

2025-10-14 22:50:22,018 | INFO | Training epoch 710, Batch 1000/1000: LR=7.23e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:50:22,076 | INFO | Epoch 710 Train Time 31.40174102783203s

2025-10-14 22:50:53,705 | INFO | Training epoch 711, Batch 1000/1000: LR=7.23e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:50:53,761 | INFO | Epoch 711 Train Time 31.68424892425537s

2025-10-14 22:51:25,403 | INFO | Training epoch 712, Batch 1000/1000: LR=7.22e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:51:25,462 | INFO | Epoch 712 Train Time 31.69960355758667s

2025-10-14 22:51:56,401 | INFO | Training epoch 713, Batch 1000/1000: LR=7.21e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:51:56,457 | INFO | Epoch 713 Train Time 30.993896007537842s

2025-10-14 22:52:27,691 | INFO | Training epoch 714, Batch 1000/1000: LR=7.21e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:52:27,747 | INFO | Epoch 714 Train Time 31.289711475372314s

2025-10-14 22:52:59,417 | INFO | Training epoch 715, Batch 1000/1000: LR=7.20e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 22:52:59,472 | INFO | Epoch 715 Train Time 31.72396969795227s

2025-10-14 22:53:30,818 | INFO | Training epoch 716, Batch 1000/1000: LR=7.19e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:53:30,873 | INFO | Epoch 716 Train Time 31.400626182556152s

2025-10-14 22:54:02,599 | INFO | Training epoch 717, Batch 1000/1000: LR=7.19e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:54:02,665 | INFO | Epoch 717 Train Time 31.790557622909546s

2025-10-14 22:54:34,099 | INFO | Training epoch 718, Batch 1000/1000: LR=7.18e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 22:54:34,158 | INFO | Epoch 718 Train Time 31.49291157722473s

2025-10-14 22:55:05,610 | INFO | Training epoch 719, Batch 1000/1000: LR=7.17e-05, Loss=2.84e-02 BER=1.05e-02 FER=1.37e-01
2025-10-14 22:55:05,663 | INFO | Epoch 719 Train Time 31.50352144241333s

2025-10-14 22:55:36,693 | INFO | Training epoch 720, Batch 1000/1000: LR=7.16e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:55:36,774 | INFO | Epoch 720 Train Time 31.109094619750977s

2025-10-14 22:56:08,404 | INFO | Training epoch 721, Batch 1000/1000: LR=7.16e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:56:08,466 | INFO | Epoch 721 Train Time 31.69112801551819s

2025-10-14 22:56:39,599 | INFO | Training epoch 722, Batch 1000/1000: LR=7.15e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 22:56:39,667 | INFO | Epoch 722 Train Time 31.19965410232544s

2025-10-14 22:57:10,823 | INFO | Training epoch 723, Batch 1000/1000: LR=7.14e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:57:10,891 | INFO | Epoch 723 Train Time 31.223084449768066s

2025-10-14 22:57:41,202 | INFO | Training epoch 724, Batch 1000/1000: LR=7.14e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 22:57:41,259 | INFO | Epoch 724 Train Time 30.367091178894043s

2025-10-14 22:58:12,499 | INFO | Training epoch 725, Batch 1000/1000: LR=7.13e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:58:12,552 | INFO | Epoch 725 Train Time 31.292309045791626s

2025-10-14 22:58:44,296 | INFO | Training epoch 726, Batch 1000/1000: LR=7.12e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 22:58:44,350 | INFO | Epoch 726 Train Time 31.796876668930054s

2025-10-14 22:59:16,146 | INFO | Training epoch 727, Batch 1000/1000: LR=7.12e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 22:59:16,195 | INFO | Epoch 727 Train Time 31.84369397163391s

2025-10-14 22:59:47,393 | INFO | Training epoch 728, Batch 1000/1000: LR=7.11e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 22:59:47,455 | INFO | Epoch 728 Train Time 31.259528636932373s

2025-10-14 23:00:18,688 | INFO | Training epoch 729, Batch 1000/1000: LR=7.10e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:00:18,744 | INFO | Epoch 729 Train Time 31.28810429573059s

2025-10-14 23:00:49,615 | INFO | Training epoch 730, Batch 1000/1000: LR=7.09e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:00:49,668 | INFO | Epoch 730 Train Time 30.923515558242798s

2025-10-14 23:01:20,822 | INFO | Training epoch 731, Batch 1000/1000: LR=7.09e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:01:20,875 | INFO | Epoch 731 Train Time 31.2047336101532s

2025-10-14 23:01:52,003 | INFO | Training epoch 732, Batch 1000/1000: LR=7.08e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-14 23:01:52,063 | INFO | Epoch 732 Train Time 31.18808650970459s

2025-10-14 23:02:23,503 | INFO | Training epoch 733, Batch 1000/1000: LR=7.07e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:02:23,566 | INFO | Epoch 733 Train Time 31.50138759613037s

2025-10-14 23:02:54,700 | INFO | Training epoch 734, Batch 1000/1000: LR=7.07e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:02:54,769 | INFO | Epoch 734 Train Time 31.20235824584961s

2025-10-14 23:03:26,711 | INFO | Training epoch 735, Batch 1000/1000: LR=7.06e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:03:26,762 | INFO | Epoch 735 Train Time 31.991414785385132s

2025-10-14 23:03:57,797 | INFO | Training epoch 736, Batch 1000/1000: LR=7.05e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:03:57,863 | INFO | Epoch 736 Train Time 31.100648403167725s

2025-10-14 23:04:29,401 | INFO | Training epoch 737, Batch 1000/1000: LR=7.04e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:04:29,452 | INFO | Epoch 737 Train Time 31.588502645492554s

2025-10-14 23:05:00,292 | INFO | Training epoch 738, Batch 1000/1000: LR=7.04e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:05:00,355 | INFO | Epoch 738 Train Time 30.901700496673584s

2025-10-14 23:05:31,993 | INFO | Training epoch 739, Batch 1000/1000: LR=7.03e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:05:32,052 | INFO | Epoch 739 Train Time 31.696651220321655s

2025-10-14 23:06:03,609 | INFO | Training epoch 740, Batch 1000/1000: LR=7.02e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 23:06:03,675 | INFO | Epoch 740 Train Time 31.622702598571777s

2025-10-14 23:06:35,101 | INFO | Training epoch 741, Batch 1000/1000: LR=7.02e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:06:35,153 | INFO | Epoch 741 Train Time 31.476592540740967s

2025-10-14 23:07:06,211 | INFO | Training epoch 742, Batch 1000/1000: LR=7.01e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:07:06,273 | INFO | Epoch 742 Train Time 31.11990737915039s

2025-10-14 23:07:38,127 | INFO | Training epoch 743, Batch 1000/1000: LR=7.00e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:07:38,181 | INFO | Epoch 743 Train Time 31.906498670578003s

2025-10-14 23:08:09,000 | INFO | Training epoch 744, Batch 1000/1000: LR=6.99e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:08:09,055 | INFO | Epoch 744 Train Time 30.87369179725647s

2025-10-14 23:08:40,190 | INFO | Training epoch 745, Batch 1000/1000: LR=6.99e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:08:40,241 | INFO | Epoch 745 Train Time 31.185487747192383s

2025-10-14 23:09:11,414 | INFO | Training epoch 746, Batch 1000/1000: LR=6.98e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:09:11,469 | INFO | Epoch 746 Train Time 31.226271867752075s

2025-10-14 23:09:42,295 | INFO | Training epoch 747, Batch 1000/1000: LR=6.97e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:09:42,348 | INFO | Epoch 747 Train Time 30.8787043094635s

2025-10-14 23:10:13,306 | INFO | Training epoch 748, Batch 1000/1000: LR=6.97e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:10:13,358 | INFO | Epoch 748 Train Time 31.008015155792236s

2025-10-14 23:10:44,396 | INFO | Training epoch 749, Batch 1000/1000: LR=6.96e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:10:44,447 | INFO | Epoch 749 Train Time 31.08816123008728s

2025-10-14 23:11:16,215 | INFO | Training epoch 750, Batch 1000/1000: LR=6.95e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.40e-01
2025-10-14 23:11:16,278 | INFO | Epoch 750 Train Time 31.82880663871765s

2025-10-14 23:11:48,024 | INFO | Training epoch 751, Batch 1000/1000: LR=6.94e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:11:48,083 | INFO | Epoch 751 Train Time 31.80378222465515s

2025-10-14 23:12:18,801 | INFO | Training epoch 752, Batch 1000/1000: LR=6.94e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:12:18,855 | INFO | Epoch 752 Train Time 30.771995067596436s

2025-10-14 23:12:50,714 | INFO | Training epoch 753, Batch 1000/1000: LR=6.93e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 23:12:50,774 | INFO | Epoch 753 Train Time 31.917079210281372s

2025-10-14 23:13:22,511 | INFO | Training epoch 754, Batch 1000/1000: LR=6.92e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:13:22,562 | INFO | Epoch 754 Train Time 31.787395000457764s

2025-10-14 23:13:54,199 | INFO | Training epoch 755, Batch 1000/1000: LR=6.92e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:13:54,256 | INFO | Epoch 755 Train Time 31.692784547805786s

2025-10-14 23:14:26,020 | INFO | Training epoch 756, Batch 1000/1000: LR=6.91e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:14:26,077 | INFO | Epoch 756 Train Time 31.82018804550171s

2025-10-14 23:14:57,505 | INFO | Training epoch 757, Batch 1000/1000: LR=6.90e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:14:57,570 | INFO | Epoch 757 Train Time 31.49208164215088s

2025-10-14 23:15:28,108 | INFO | Training epoch 758, Batch 1000/1000: LR=6.89e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:15:28,167 | INFO | Epoch 758 Train Time 30.596971035003662s

2025-10-14 23:15:59,799 | INFO | Training epoch 759, Batch 1000/1000: LR=6.89e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:15:59,865 | INFO | Epoch 759 Train Time 31.697269439697266s

2025-10-14 23:16:31,596 | INFO | Training epoch 760, Batch 1000/1000: LR=6.88e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 23:16:31,650 | INFO | Epoch 760 Train Time 31.78389000892639s

2025-10-14 23:17:02,504 | INFO | Training epoch 761, Batch 1000/1000: LR=6.87e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:17:02,564 | INFO | Epoch 761 Train Time 30.91255784034729s

2025-10-14 23:17:34,101 | INFO | Training epoch 762, Batch 1000/1000: LR=6.86e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.39e-01
2025-10-14 23:17:34,162 | INFO | Epoch 762 Train Time 31.59687328338623s

2025-10-14 23:18:05,303 | INFO | Training epoch 763, Batch 1000/1000: LR=6.86e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:18:05,351 | INFO | Epoch 763 Train Time 31.188190460205078s

2025-10-14 23:18:36,406 | INFO | Training epoch 764, Batch 1000/1000: LR=6.85e-05, Loss=2.92e-02 BER=1.10e-02 FER=1.40e-01
2025-10-14 23:18:36,458 | INFO | Epoch 764 Train Time 31.105345487594604s

2025-10-14 23:19:07,889 | INFO | Training epoch 765, Batch 1000/1000: LR=6.84e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:19:07,941 | INFO | Epoch 765 Train Time 31.48097538948059s

2025-10-14 23:19:39,699 | INFO | Training epoch 766, Batch 1000/1000: LR=6.84e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:19:39,762 | INFO | Epoch 766 Train Time 31.820147037506104s

2025-10-14 23:20:10,602 | INFO | Training epoch 767, Batch 1000/1000: LR=6.83e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.38e-01
2025-10-14 23:20:10,658 | INFO | Epoch 767 Train Time 30.895161390304565s

2025-10-14 23:20:42,311 | INFO | Training epoch 768, Batch 1000/1000: LR=6.82e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:20:42,375 | INFO | Epoch 768 Train Time 31.715272188186646s

2025-10-14 23:21:13,918 | INFO | Training epoch 769, Batch 1000/1000: LR=6.81e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:21:13,982 | INFO | Epoch 769 Train Time 31.606163501739502s

2025-10-14 23:21:45,217 | INFO | Training epoch 770, Batch 1000/1000: LR=6.81e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:21:45,269 | INFO | Epoch 770 Train Time 31.286758184432983s

2025-10-14 23:22:16,794 | INFO | Training epoch 771, Batch 1000/1000: LR=6.80e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:22:16,851 | INFO | Epoch 771 Train Time 31.581634759902954s

2025-10-14 23:22:47,705 | INFO | Training epoch 772, Batch 1000/1000: LR=6.79e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:22:47,776 | INFO | Epoch 772 Train Time 30.92416501045227s

2025-10-14 23:23:19,020 | INFO | Training epoch 773, Batch 1000/1000: LR=6.79e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:23:19,074 | INFO | Epoch 773 Train Time 31.296587705612183s

2025-10-14 23:23:50,114 | INFO | Training epoch 774, Batch 1000/1000: LR=6.78e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:23:50,172 | INFO | Epoch 774 Train Time 31.09672999382019s

2025-10-14 23:24:21,614 | INFO | Training epoch 775, Batch 1000/1000: LR=6.77e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:24:21,669 | INFO | Epoch 775 Train Time 31.495786905288696s

2025-10-14 23:24:53,403 | INFO | Training epoch 776, Batch 1000/1000: LR=6.76e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 23:24:53,466 | INFO | Epoch 776 Train Time 31.7963707447052s

2025-10-14 23:25:24,803 | INFO | Training epoch 777, Batch 1000/1000: LR=6.76e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:25:24,856 | INFO | Epoch 777 Train Time 31.38858914375305s

2025-10-14 23:25:56,505 | INFO | Training epoch 778, Batch 1000/1000: LR=6.75e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:25:56,568 | INFO | Epoch 778 Train Time 31.71106719970703s

2025-10-14 23:26:28,506 | INFO | Training epoch 779, Batch 1000/1000: LR=6.74e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-14 23:26:28,560 | INFO | Epoch 779 Train Time 31.990997314453125s

2025-10-14 23:26:59,895 | INFO | Training epoch 780, Batch 1000/1000: LR=6.73e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.39e-01
2025-10-14 23:26:59,958 | INFO | Epoch 780 Train Time 31.396206378936768s

2025-10-14 23:27:31,607 | INFO | Training epoch 781, Batch 1000/1000: LR=6.73e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:27:31,665 | INFO | Epoch 781 Train Time 31.705458879470825s

2025-10-14 23:28:02,406 | INFO | Training epoch 782, Batch 1000/1000: LR=6.72e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:28:02,462 | INFO | Epoch 782 Train Time 30.79644513130188s

2025-10-14 23:28:32,695 | INFO | Training epoch 783, Batch 1000/1000: LR=6.71e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:28:32,748 | INFO | Epoch 783 Train Time 30.284135103225708s

2025-10-14 23:29:04,205 | INFO | Training epoch 784, Batch 1000/1000: LR=6.70e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:29:04,285 | INFO | Epoch 784 Train Time 31.53640842437744s

2025-10-14 23:29:35,813 | INFO | Training epoch 785, Batch 1000/1000: LR=6.70e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:29:35,866 | INFO | Epoch 785 Train Time 31.58024549484253s

2025-10-14 23:30:07,628 | INFO | Training epoch 786, Batch 1000/1000: LR=6.69e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:30:07,682 | INFO | Epoch 786 Train Time 31.81452250480652s

2025-10-14 23:30:39,310 | INFO | Training epoch 787, Batch 1000/1000: LR=6.68e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:30:39,381 | INFO | Epoch 787 Train Time 31.699179649353027s

2025-10-14 23:31:10,500 | INFO | Training epoch 788, Batch 1000/1000: LR=6.68e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:31:10,558 | INFO | Epoch 788 Train Time 31.174607753753662s

2025-10-14 23:31:42,504 | INFO | Training epoch 789, Batch 1000/1000: LR=6.67e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:31:42,567 | INFO | Epoch 789 Train Time 32.00850582122803s

2025-10-14 23:32:14,232 | INFO | Training epoch 790, Batch 1000/1000: LR=6.66e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:32:14,298 | INFO | Epoch 790 Train Time 31.730777978897095s

2025-10-14 23:32:45,201 | INFO | Training epoch 791, Batch 1000/1000: LR=6.65e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:32:45,270 | INFO | Epoch 791 Train Time 30.970579147338867s

2025-10-14 23:33:17,008 | INFO | Training epoch 792, Batch 1000/1000: LR=6.65e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:33:17,082 | INFO | Epoch 792 Train Time 31.810314416885376s

2025-10-14 23:33:47,602 | INFO | Training epoch 793, Batch 1000/1000: LR=6.64e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:33:47,652 | INFO | Epoch 793 Train Time 30.569544315338135s

2025-10-14 23:34:18,904 | INFO | Training epoch 794, Batch 1000/1000: LR=6.63e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.40e-01
2025-10-14 23:34:18,974 | INFO | Epoch 794 Train Time 31.321478605270386s

2025-10-14 23:34:50,028 | INFO | Training epoch 795, Batch 1000/1000: LR=6.62e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:34:50,076 | INFO | Epoch 795 Train Time 31.10055661201477s

2025-10-14 23:35:21,608 | INFO | Training epoch 796, Batch 1000/1000: LR=6.62e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:35:21,668 | INFO | Epoch 796 Train Time 31.5909583568573s

2025-10-14 23:35:52,696 | INFO | Training epoch 797, Batch 1000/1000: LR=6.61e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:35:52,756 | INFO | Epoch 797 Train Time 31.088069438934326s

2025-10-14 23:36:23,807 | INFO | Training epoch 798, Batch 1000/1000: LR=6.60e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.41e-01
2025-10-14 23:36:23,866 | INFO | Epoch 798 Train Time 31.109370708465576s

2025-10-14 23:36:55,515 | INFO | Training epoch 799, Batch 1000/1000: LR=6.59e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:36:55,582 | INFO | Epoch 799 Train Time 31.71488642692566s

2025-10-14 23:37:27,030 | INFO | Training epoch 800, Batch 1000/1000: LR=6.59e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:37:27,103 | INFO | Epoch 800 Train Time 31.520284175872803s

2025-10-14 23:37:58,832 | INFO | Training epoch 801, Batch 1000/1000: LR=6.58e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:37:58,902 | INFO | Epoch 801 Train Time 31.7978572845459s

2025-10-14 23:38:31,088 | INFO | Training epoch 802, Batch 1000/1000: LR=6.57e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:38:31,141 | INFO | Epoch 802 Train Time 32.23864436149597s

2025-10-14 23:39:03,006 | INFO | Training epoch 803, Batch 1000/1000: LR=6.56e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:39:03,058 | INFO | Epoch 803 Train Time 31.91636061668396s

2025-10-14 23:39:34,502 | INFO | Training epoch 804, Batch 1000/1000: LR=6.56e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:39:34,553 | INFO | Epoch 804 Train Time 31.493950128555298s

2025-10-14 23:40:06,006 | INFO | Training epoch 805, Batch 1000/1000: LR=6.55e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:40:06,063 | INFO | Epoch 805 Train Time 31.508028268814087s

2025-10-14 23:40:38,707 | INFO | Training epoch 806, Batch 1000/1000: LR=6.54e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 23:40:38,784 | INFO | Epoch 806 Train Time 32.720423460006714s

2025-10-14 23:41:10,433 | INFO | Training epoch 807, Batch 1000/1000: LR=6.54e-05, Loss=2.87e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:41:10,487 | INFO | Epoch 807 Train Time 31.701258659362793s

2025-10-14 23:41:43,020 | INFO | Training epoch 808, Batch 1000/1000: LR=6.53e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 23:41:43,071 | INFO | Epoch 808 Train Time 32.58373951911926s

2025-10-14 23:42:15,504 | INFO | Training epoch 809, Batch 1000/1000: LR=6.52e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 23:42:15,569 | INFO | Epoch 809 Train Time 32.49612236022949s

2025-10-14 23:42:47,111 | INFO | Training epoch 810, Batch 1000/1000: LR=6.51e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-14 23:42:47,164 | INFO | Epoch 810 Train Time 31.593671798706055s

2025-10-14 23:43:18,603 | INFO | Training epoch 811, Batch 1000/1000: LR=6.51e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:43:18,658 | INFO | Epoch 811 Train Time 31.493608951568604s

2025-10-14 23:43:50,697 | INFO | Training epoch 812, Batch 1000/1000: LR=6.50e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:43:50,750 | INFO | Epoch 812 Train Time 32.09157371520996s

2025-10-14 23:44:21,912 | INFO | Training epoch 813, Batch 1000/1000: LR=6.49e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:44:21,969 | INFO | Epoch 813 Train Time 31.218221187591553s

2025-10-14 23:44:53,300 | INFO | Training epoch 814, Batch 1000/1000: LR=6.48e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:44:53,359 | INFO | Epoch 814 Train Time 31.389310359954834s

2025-10-14 23:45:24,905 | INFO | Training epoch 815, Batch 1000/1000: LR=6.48e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:45:24,970 | INFO | Epoch 815 Train Time 31.61066222190857s

2025-10-14 23:45:55,397 | INFO | Training epoch 816, Batch 1000/1000: LR=6.47e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:45:55,456 | INFO | Epoch 816 Train Time 30.485518217086792s

2025-10-14 23:46:27,200 | INFO | Training epoch 817, Batch 1000/1000: LR=6.46e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-14 23:46:27,270 | INFO | Epoch 817 Train Time 31.812133073806763s

2025-10-14 23:46:59,138 | INFO | Training epoch 818, Batch 1000/1000: LR=6.45e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:46:59,202 | INFO | Epoch 818 Train Time 31.93135142326355s

2025-10-14 23:47:30,809 | INFO | Training epoch 819, Batch 1000/1000: LR=6.45e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:47:30,862 | INFO | Epoch 819 Train Time 31.658611536026s

2025-10-14 23:48:02,632 | INFO | Training epoch 820, Batch 1000/1000: LR=6.44e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:48:02,704 | INFO | Epoch 820 Train Time 31.83979105949402s

2025-10-14 23:48:34,801 | INFO | Training epoch 821, Batch 1000/1000: LR=6.43e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-14 23:48:34,862 | INFO | Epoch 821 Train Time 32.15592551231384s

2025-10-14 23:49:07,009 | INFO | Training epoch 822, Batch 1000/1000: LR=6.42e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:49:07,060 | INFO | Epoch 822 Train Time 32.19665288925171s

2025-10-14 23:49:40,692 | INFO | Training epoch 823, Batch 1000/1000: LR=6.42e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:49:40,752 | INFO | Epoch 823 Train Time 33.69119644165039s

2025-10-14 23:50:12,499 | INFO | Training epoch 824, Batch 1000/1000: LR=6.41e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:50:12,563 | INFO | Epoch 824 Train Time 31.810218811035156s

2025-10-14 23:50:43,705 | INFO | Training epoch 825, Batch 1000/1000: LR=6.40e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:50:43,757 | INFO | Epoch 825 Train Time 31.191770553588867s

2025-10-14 23:51:15,406 | INFO | Training epoch 826, Batch 1000/1000: LR=6.39e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:51:15,471 | INFO | Epoch 826 Train Time 31.712843656539917s

2025-10-14 23:51:46,426 | INFO | Training epoch 827, Batch 1000/1000: LR=6.39e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:51:46,484 | INFO | Epoch 827 Train Time 31.012519359588623s

2025-10-14 23:52:17,206 | INFO | Training epoch 828, Batch 1000/1000: LR=6.38e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:52:17,259 | INFO | Epoch 828 Train Time 30.77455234527588s

2025-10-14 23:52:49,015 | INFO | Training epoch 829, Batch 1000/1000: LR=6.37e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:52:49,077 | INFO | Epoch 829 Train Time 31.81726837158203s

2025-10-14 23:53:20,808 | INFO | Training epoch 830, Batch 1000/1000: LR=6.36e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:53:20,857 | INFO | Epoch 830 Train Time 31.778974056243896s

2025-10-14 23:53:52,303 | INFO | Training epoch 831, Batch 1000/1000: LR=6.36e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-14 23:53:52,359 | INFO | Epoch 831 Train Time 31.500991344451904s

2025-10-14 23:54:23,697 | INFO | Training epoch 832, Batch 1000/1000: LR=6.35e-05, Loss=2.91e-02 BER=1.09e-02 FER=1.39e-01
2025-10-14 23:54:23,753 | INFO | Epoch 832 Train Time 31.394018173217773s

2025-10-14 23:54:55,816 | INFO | Training epoch 833, Batch 1000/1000: LR=6.34e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:54:55,870 | INFO | Epoch 833 Train Time 32.11626696586609s

2025-10-14 23:55:27,506 | INFO | Training epoch 834, Batch 1000/1000: LR=6.33e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-14 23:55:27,555 | INFO | Epoch 834 Train Time 31.683789253234863s

2025-10-14 23:55:58,892 | INFO | Training epoch 835, Batch 1000/1000: LR=6.33e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:55:58,942 | INFO | Epoch 835 Train Time 31.386707544326782s

2025-10-14 23:56:30,602 | INFO | Training epoch 836, Batch 1000/1000: LR=6.32e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:56:30,644 | INFO | Epoch 836 Train Time 31.700944423675537s

2025-10-14 23:57:02,317 | INFO | Training epoch 837, Batch 1000/1000: LR=6.31e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:57:02,371 | INFO | Epoch 837 Train Time 31.72559690475464s

2025-10-14 23:57:33,789 | INFO | Training epoch 838, Batch 1000/1000: LR=6.30e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-14 23:57:33,841 | INFO | Epoch 838 Train Time 31.469681978225708s

2025-10-14 23:58:05,423 | INFO | Training epoch 839, Batch 1000/1000: LR=6.30e-05, Loss=2.87e-02 BER=1.08e-02 FER=1.39e-01
2025-10-14 23:58:05,491 | INFO | Epoch 839 Train Time 31.64844274520874s

2025-10-14 23:58:36,496 | INFO | Training epoch 840, Batch 1000/1000: LR=6.29e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-14 23:58:36,547 | INFO | Epoch 840 Train Time 31.054101943969727s

2025-10-14 23:59:07,909 | INFO | Training epoch 841, Batch 1000/1000: LR=6.28e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:59:07,961 | INFO | Epoch 841 Train Time 31.413994312286377s

2025-10-14 23:59:39,552 | INFO | Training epoch 842, Batch 1000/1000: LR=6.27e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-14 23:59:39,620 | INFO | Epoch 842 Train Time 31.65827178955078s

2025-10-15 00:00:11,216 | INFO | Training epoch 843, Batch 1000/1000: LR=6.27e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:00:11,273 | INFO | Epoch 843 Train Time 31.651299953460693s

2025-10-15 00:00:42,789 | INFO | Training epoch 844, Batch 1000/1000: LR=6.26e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:00:42,840 | INFO | Epoch 844 Train Time 31.565937519073486s

2025-10-15 00:01:13,615 | INFO | Training epoch 845, Batch 1000/1000: LR=6.25e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:01:13,664 | INFO | Epoch 845 Train Time 30.822285652160645s

2025-10-15 00:01:45,225 | INFO | Training epoch 846, Batch 1000/1000: LR=6.24e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:01:45,266 | INFO | Epoch 846 Train Time 31.601258993148804s

2025-10-15 00:02:16,315 | INFO | Training epoch 847, Batch 1000/1000: LR=6.24e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.40e-01
2025-10-15 00:02:16,362 | INFO | Epoch 847 Train Time 31.094715356826782s

2025-10-15 00:02:47,605 | INFO | Training epoch 848, Batch 1000/1000: LR=6.23e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:02:47,654 | INFO | Epoch 848 Train Time 31.290201902389526s

2025-10-15 00:03:19,198 | INFO | Training epoch 849, Batch 1000/1000: LR=6.22e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:03:19,250 | INFO | Epoch 849 Train Time 31.59512495994568s

2025-10-15 00:03:50,719 | INFO | Training epoch 850, Batch 1000/1000: LR=6.21e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:03:50,767 | INFO | Epoch 850 Train Time 31.515704870224s

2025-10-15 00:04:22,408 | INFO | Training epoch 851, Batch 1000/1000: LR=6.21e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:04:22,461 | INFO | Epoch 851 Train Time 31.692585229873657s

2025-10-15 00:04:53,301 | INFO | Training epoch 852, Batch 1000/1000: LR=6.20e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:04:53,343 | INFO | Epoch 852 Train Time 30.881444215774536s

2025-10-15 00:05:25,001 | INFO | Training epoch 853, Batch 1000/1000: LR=6.19e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:05:25,051 | INFO | Epoch 853 Train Time 31.707077741622925s

2025-10-15 00:05:56,705 | INFO | Training epoch 854, Batch 1000/1000: LR=6.18e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:05:56,768 | INFO | Epoch 854 Train Time 31.71625304222107s

2025-10-15 00:06:28,539 | INFO | Training epoch 855, Batch 1000/1000: LR=6.18e-05, Loss=2.89e-02 BER=1.09e-02 FER=1.41e-01
2025-10-15 00:06:28,595 | INFO | Epoch 855 Train Time 31.82561445236206s

2025-10-15 00:07:00,224 | INFO | Training epoch 856, Batch 1000/1000: LR=6.17e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:07:00,284 | INFO | Epoch 856 Train Time 31.688583850860596s

2025-10-15 00:07:31,299 | INFO | Training epoch 857, Batch 1000/1000: LR=6.16e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:07:31,348 | INFO | Epoch 857 Train Time 31.062880039215088s

2025-10-15 00:08:02,193 | INFO | Training epoch 858, Batch 1000/1000: LR=6.15e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:08:02,251 | INFO | Epoch 858 Train Time 30.90248465538025s

2025-10-15 00:08:33,191 | INFO | Training epoch 859, Batch 1000/1000: LR=6.14e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 00:08:33,239 | INFO | Epoch 859 Train Time 30.986955881118774s

2025-10-15 00:09:04,402 | INFO | Training epoch 860, Batch 1000/1000: LR=6.14e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 00:09:04,455 | INFO | Epoch 860 Train Time 31.21456289291382s

2025-10-15 00:09:35,256 | INFO | Training epoch 861, Batch 1000/1000: LR=6.13e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 00:09:35,307 | INFO | Epoch 861 Train Time 30.8510320186615s

2025-10-15 00:10:07,208 | INFO | Training epoch 862, Batch 1000/1000: LR=6.12e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:10:07,260 | INFO | Epoch 862 Train Time 31.952022075653076s

2025-10-15 00:10:38,824 | INFO | Training epoch 863, Batch 1000/1000: LR=6.11e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:10:38,869 | INFO | Epoch 863 Train Time 31.607876539230347s

2025-10-15 00:11:10,609 | INFO | Training epoch 864, Batch 1000/1000: LR=6.11e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:11:10,673 | INFO | Epoch 864 Train Time 31.803821086883545s

2025-10-15 00:11:41,996 | INFO | Training epoch 865, Batch 1000/1000: LR=6.10e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 00:11:42,060 | INFO | Epoch 865 Train Time 31.385149717330933s

2025-10-15 00:12:13,700 | INFO | Training epoch 866, Batch 1000/1000: LR=6.09e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:12:13,749 | INFO | Epoch 866 Train Time 31.68817973136902s

2025-10-15 00:12:45,312 | INFO | Training epoch 867, Batch 1000/1000: LR=6.08e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:12:45,372 | INFO | Epoch 867 Train Time 31.620424270629883s

2025-10-15 00:13:16,614 | INFO | Training epoch 868, Batch 1000/1000: LR=6.08e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.38e-01
2025-10-15 00:13:16,673 | INFO | Epoch 868 Train Time 31.30001163482666s

2025-10-15 00:13:47,721 | INFO | Training epoch 869, Batch 1000/1000: LR=6.07e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:13:47,778 | INFO | Epoch 869 Train Time 31.103935956954956s

2025-10-15 00:14:19,687 | INFO | Training epoch 870, Batch 1000/1000: LR=6.06e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:14:19,742 | INFO | Epoch 870 Train Time 31.963311910629272s

2025-10-15 00:14:50,902 | INFO | Training epoch 871, Batch 1000/1000: LR=6.05e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:14:50,954 | INFO | Epoch 871 Train Time 31.212040662765503s

2025-10-15 00:15:22,527 | INFO | Training epoch 872, Batch 1000/1000: LR=6.05e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:15:22,596 | INFO | Epoch 872 Train Time 31.641135931015015s

2025-10-15 00:15:54,111 | INFO | Training epoch 873, Batch 1000/1000: LR=6.04e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:15:54,163 | INFO | Epoch 873 Train Time 31.56540822982788s

2025-10-15 00:16:25,296 | INFO | Training epoch 874, Batch 1000/1000: LR=6.03e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:16:25,343 | INFO | Epoch 874 Train Time 31.178995847702026s

2025-10-15 00:16:56,593 | INFO | Training epoch 875, Batch 1000/1000: LR=6.02e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:16:56,668 | INFO | Epoch 875 Train Time 31.323731660842896s

2025-10-15 00:17:28,303 | INFO | Training epoch 876, Batch 1000/1000: LR=6.02e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:17:28,363 | INFO | Epoch 876 Train Time 31.694671154022217s

2025-10-15 00:18:00,021 | INFO | Training epoch 877, Batch 1000/1000: LR=6.01e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:18:00,074 | INFO | Epoch 877 Train Time 31.710054874420166s

2025-10-15 00:18:31,592 | INFO | Training epoch 878, Batch 1000/1000: LR=6.00e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:18:31,644 | INFO | Epoch 878 Train Time 31.569209814071655s

2025-10-15 00:19:01,900 | INFO | Training epoch 879, Batch 1000/1000: LR=5.99e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 00:19:01,977 | INFO | Epoch 879 Train Time 30.33167028427124s

2025-10-15 00:19:01,978 | INFO | [P2] saving best_model (QAT) with loss 0.027916 at epoch 879
2025-10-15 00:19:33,506 | INFO | Training epoch 880, Batch 1000/1000: LR=5.99e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:19:33,560 | INFO | Epoch 880 Train Time 31.565184593200684s

2025-10-15 00:20:04,589 | INFO | Training epoch 881, Batch 1000/1000: LR=5.98e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 00:20:04,649 | INFO | Epoch 881 Train Time 31.08851432800293s

2025-10-15 00:20:36,697 | INFO | Training epoch 882, Batch 1000/1000: LR=5.97e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:20:36,749 | INFO | Epoch 882 Train Time 32.09836483001709s

2025-10-15 00:21:08,109 | INFO | Training epoch 883, Batch 1000/1000: LR=5.96e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 00:21:08,152 | INFO | Epoch 883 Train Time 31.40230083465576s

2025-10-15 00:21:40,421 | INFO | Training epoch 884, Batch 1000/1000: LR=5.95e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 00:21:40,478 | INFO | Epoch 884 Train Time 32.32537817955017s

2025-10-15 00:22:11,199 | INFO | Training epoch 885, Batch 1000/1000: LR=5.95e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 00:22:11,245 | INFO | Epoch 885 Train Time 30.76608657836914s

2025-10-15 00:22:43,009 | INFO | Training epoch 886, Batch 1000/1000: LR=5.94e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:22:43,062 | INFO | Epoch 886 Train Time 31.816601276397705s

2025-10-15 00:23:14,038 | INFO | Training epoch 887, Batch 1000/1000: LR=5.93e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:23:14,086 | INFO | Epoch 887 Train Time 31.023369312286377s

2025-10-15 00:23:45,713 | INFO | Training epoch 888, Batch 1000/1000: LR=5.92e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:23:45,766 | INFO | Epoch 888 Train Time 31.6789071559906s

2025-10-15 00:24:17,092 | INFO | Training epoch 889, Batch 1000/1000: LR=5.92e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:24:17,149 | INFO | Epoch 889 Train Time 31.381856203079224s

2025-10-15 00:24:48,411 | INFO | Training epoch 890, Batch 1000/1000: LR=5.91e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:24:48,468 | INFO | Epoch 890 Train Time 31.31706953048706s

2025-10-15 00:25:19,289 | INFO | Training epoch 891, Batch 1000/1000: LR=5.90e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 00:25:19,345 | INFO | Epoch 891 Train Time 30.87673282623291s

2025-10-15 00:25:50,393 | INFO | Training epoch 892, Batch 1000/1000: LR=5.89e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:25:50,446 | INFO | Epoch 892 Train Time 31.099729776382446s

2025-10-15 00:26:22,131 | INFO | Training epoch 893, Batch 1000/1000: LR=5.89e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:26:22,183 | INFO | Epoch 893 Train Time 31.736127853393555s

2025-10-15 00:26:53,503 | INFO | Training epoch 894, Batch 1000/1000: LR=5.88e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:26:53,551 | INFO | Epoch 894 Train Time 31.36789083480835s

2025-10-15 00:27:25,316 | INFO | Training epoch 895, Batch 1000/1000: LR=5.87e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 00:27:25,369 | INFO | Epoch 895 Train Time 31.81662082672119s

2025-10-15 00:27:56,715 | INFO | Training epoch 896, Batch 1000/1000: LR=5.86e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:27:56,761 | INFO | Epoch 896 Train Time 31.390929460525513s

2025-10-15 00:28:27,813 | INFO | Training epoch 897, Batch 1000/1000: LR=5.86e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:28:27,870 | INFO | Epoch 897 Train Time 31.108043909072876s

2025-10-15 00:28:59,289 | INFO | Training epoch 898, Batch 1000/1000: LR=5.85e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 00:28:59,347 | INFO | Epoch 898 Train Time 31.47584319114685s

2025-10-15 00:29:31,057 | INFO | Training epoch 899, Batch 1000/1000: LR=5.84e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:29:31,115 | INFO | Epoch 899 Train Time 31.767837285995483s

2025-10-15 00:30:02,399 | INFO | Training epoch 900, Batch 1000/1000: LR=5.83e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:30:02,451 | INFO | Epoch 900 Train Time 31.333892107009888s

2025-10-15 00:30:33,792 | INFO | Training epoch 901, Batch 1000/1000: LR=5.82e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:30:33,841 | INFO | Epoch 901 Train Time 31.389517307281494s

2025-10-15 00:31:05,001 | INFO | Training epoch 902, Batch 1000/1000: LR=5.82e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:31:05,045 | INFO | Epoch 902 Train Time 31.20300531387329s

2025-10-15 00:31:36,820 | INFO | Training epoch 903, Batch 1000/1000: LR=5.81e-05, Loss=2.90e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 00:31:36,866 | INFO | Epoch 903 Train Time 31.819256067276s

2025-10-15 00:32:08,202 | INFO | Training epoch 904, Batch 1000/1000: LR=5.80e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:32:08,256 | INFO | Epoch 904 Train Time 31.38908076286316s

2025-10-15 00:32:38,893 | INFO | Training epoch 905, Batch 1000/1000: LR=5.79e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:32:38,949 | INFO | Epoch 905 Train Time 30.691461086273193s

2025-10-15 00:33:10,405 | INFO | Training epoch 906, Batch 1000/1000: LR=5.79e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:33:10,463 | INFO | Epoch 906 Train Time 31.51282238960266s

2025-10-15 00:33:41,791 | INFO | Training epoch 907, Batch 1000/1000: LR=5.78e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:33:41,845 | INFO | Epoch 907 Train Time 31.381834268569946s

2025-10-15 00:34:13,102 | INFO | Training epoch 908, Batch 1000/1000: LR=5.77e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 00:34:13,153 | INFO | Epoch 908 Train Time 31.30638074874878s

2025-10-15 00:34:44,415 | INFO | Training epoch 909, Batch 1000/1000: LR=5.76e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:34:44,468 | INFO | Epoch 909 Train Time 31.313945055007935s

2025-10-15 00:35:15,296 | INFO | Training epoch 910, Batch 1000/1000: LR=5.76e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:35:15,356 | INFO | Epoch 910 Train Time 30.886502265930176s

2025-10-15 00:35:46,813 | INFO | Training epoch 911, Batch 1000/1000: LR=5.75e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:35:46,870 | INFO | Epoch 911 Train Time 31.51391887664795s

2025-10-15 00:36:17,795 | INFO | Training epoch 912, Batch 1000/1000: LR=5.74e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 00:36:17,845 | INFO | Epoch 912 Train Time 30.97268509864807s

2025-10-15 00:36:17,845 | INFO | [P2] saving best_model (QAT) with loss 0.027889 at epoch 912
2025-10-15 00:36:49,698 | INFO | Training epoch 913, Batch 1000/1000: LR=5.73e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:36:49,744 | INFO | Epoch 913 Train Time 31.883143663406372s

2025-10-15 00:37:20,606 | INFO | Training epoch 914, Batch 1000/1000: LR=5.72e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:37:20,655 | INFO | Epoch 914 Train Time 30.91062569618225s

2025-10-15 00:37:51,417 | INFO | Training epoch 915, Batch 1000/1000: LR=5.72e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:37:51,464 | INFO | Epoch 915 Train Time 30.808281421661377s

2025-10-15 00:38:22,700 | INFO | Training epoch 916, Batch 1000/1000: LR=5.71e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 00:38:22,747 | INFO | Epoch 916 Train Time 31.281015634536743s

2025-10-15 00:38:22,747 | INFO | [P2] saving best_model (QAT) with loss 0.027745 at epoch 916
2025-10-15 00:38:54,218 | INFO | Training epoch 917, Batch 1000/1000: LR=5.70e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:38:54,273 | INFO | Epoch 917 Train Time 31.511626720428467s

2025-10-15 00:39:25,822 | INFO | Training epoch 918, Batch 1000/1000: LR=5.69e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:39:25,876 | INFO | Epoch 918 Train Time 31.602458477020264s

2025-10-15 00:39:57,207 | INFO | Training epoch 919, Batch 1000/1000: LR=5.69e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:39:57,271 | INFO | Epoch 919 Train Time 31.393556594848633s

2025-10-15 00:40:28,747 | INFO | Training epoch 920, Batch 1000/1000: LR=5.68e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:40:28,805 | INFO | Epoch 920 Train Time 31.532989501953125s

2025-10-15 00:41:00,305 | INFO | Training epoch 921, Batch 1000/1000: LR=5.67e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:41:00,366 | INFO | Epoch 921 Train Time 31.560546159744263s

2025-10-15 00:41:32,117 | INFO | Training epoch 922, Batch 1000/1000: LR=5.66e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:41:32,164 | INFO | Epoch 922 Train Time 31.796534061431885s

2025-10-15 00:42:03,114 | INFO | Training epoch 923, Batch 1000/1000: LR=5.65e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 00:42:03,166 | INFO | Epoch 923 Train Time 31.001381874084473s

2025-10-15 00:42:34,400 | INFO | Training epoch 924, Batch 1000/1000: LR=5.65e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:42:34,449 | INFO | Epoch 924 Train Time 31.281219482421875s

2025-10-15 00:43:05,104 | INFO | Training epoch 925, Batch 1000/1000: LR=5.64e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 00:43:05,160 | INFO | Epoch 925 Train Time 30.7099347114563s

2025-10-15 00:43:36,914 | INFO | Training epoch 926, Batch 1000/1000: LR=5.63e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:43:36,964 | INFO | Epoch 926 Train Time 31.803377866744995s

2025-10-15 00:44:07,909 | INFO | Training epoch 927, Batch 1000/1000: LR=5.62e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:44:07,951 | INFO | Epoch 927 Train Time 30.985553741455078s

2025-10-15 00:44:39,201 | INFO | Training epoch 928, Batch 1000/1000: LR=5.62e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 00:44:39,254 | INFO | Epoch 928 Train Time 31.30193328857422s

2025-10-15 00:45:10,500 | INFO | Training epoch 929, Batch 1000/1000: LR=5.61e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:45:10,555 | INFO | Epoch 929 Train Time 31.300199508666992s

2025-10-15 00:45:41,601 | INFO | Training epoch 930, Batch 1000/1000: LR=5.60e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:45:41,666 | INFO | Epoch 930 Train Time 31.109949588775635s

2025-10-15 00:46:12,602 | INFO | Training epoch 931, Batch 1000/1000: LR=5.59e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:46:12,653 | INFO | Epoch 931 Train Time 30.985790252685547s

2025-10-15 00:46:43,736 | INFO | Training epoch 932, Batch 1000/1000: LR=5.59e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:46:43,784 | INFO | Epoch 932 Train Time 31.12874126434326s

2025-10-15 00:47:15,198 | INFO | Training epoch 933, Batch 1000/1000: LR=5.58e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:47:15,260 | INFO | Epoch 933 Train Time 31.475132703781128s

2025-10-15 00:47:46,523 | INFO | Training epoch 934, Batch 1000/1000: LR=5.57e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 00:47:46,577 | INFO | Epoch 934 Train Time 31.315449714660645s

2025-10-15 00:48:17,304 | INFO | Training epoch 935, Batch 1000/1000: LR=5.56e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:48:17,361 | INFO | Epoch 935 Train Time 30.782947778701782s

2025-10-15 00:48:49,118 | INFO | Training epoch 936, Batch 1000/1000: LR=5.55e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:48:49,167 | INFO | Epoch 936 Train Time 31.805399656295776s

2025-10-15 00:49:20,402 | INFO | Training epoch 937, Batch 1000/1000: LR=5.55e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:49:20,457 | INFO | Epoch 937 Train Time 31.288910388946533s

2025-10-15 00:49:51,198 | INFO | Training epoch 938, Batch 1000/1000: LR=5.54e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:49:51,241 | INFO | Epoch 938 Train Time 30.783490419387817s

2025-10-15 00:50:22,398 | INFO | Training epoch 939, Batch 1000/1000: LR=5.53e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:50:22,442 | INFO | Epoch 939 Train Time 31.199987411499023s

2025-10-15 00:50:53,215 | INFO | Training epoch 940, Batch 1000/1000: LR=5.52e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:50:53,269 | INFO | Epoch 940 Train Time 30.826059103012085s

2025-10-15 00:51:24,503 | INFO | Training epoch 941, Batch 1000/1000: LR=5.52e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:51:24,550 | INFO | Epoch 941 Train Time 31.280111074447632s

2025-10-15 00:51:56,309 | INFO | Training epoch 942, Batch 1000/1000: LR=5.51e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:51:56,353 | INFO | Epoch 942 Train Time 31.802489042282104s

2025-10-15 00:52:28,201 | INFO | Training epoch 943, Batch 1000/1000: LR=5.50e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:52:28,252 | INFO | Epoch 943 Train Time 31.898377418518066s

2025-10-15 00:52:59,405 | INFO | Training epoch 944, Batch 1000/1000: LR=5.49e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:52:59,453 | INFO | Epoch 944 Train Time 31.19903874397278s

2025-10-15 00:53:31,199 | INFO | Training epoch 945, Batch 1000/1000: LR=5.48e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:53:31,247 | INFO | Epoch 945 Train Time 31.79360318183899s

2025-10-15 00:54:02,808 | INFO | Training epoch 946, Batch 1000/1000: LR=5.48e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 00:54:02,859 | INFO | Epoch 946 Train Time 31.610907793045044s

2025-10-15 00:54:33,904 | INFO | Training epoch 947, Batch 1000/1000: LR=5.47e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:54:33,973 | INFO | Epoch 947 Train Time 31.113506078720093s

2025-10-15 00:55:04,889 | INFO | Training epoch 948, Batch 1000/1000: LR=5.46e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 00:55:04,946 | INFO | Epoch 948 Train Time 30.971595287322998s

2025-10-15 00:55:36,203 | INFO | Training epoch 949, Batch 1000/1000: LR=5.45e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:55:36,262 | INFO | Epoch 949 Train Time 31.315023183822632s

2025-10-15 00:56:07,593 | INFO | Training epoch 950, Batch 1000/1000: LR=5.45e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 00:56:07,644 | INFO | Epoch 950 Train Time 31.381208419799805s

2025-10-15 00:56:38,391 | INFO | Training epoch 951, Batch 1000/1000: LR=5.44e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 00:56:38,435 | INFO | Epoch 951 Train Time 30.78976273536682s

2025-10-15 00:57:09,487 | INFO | Training epoch 952, Batch 1000/1000: LR=5.43e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:57:09,535 | INFO | Epoch 952 Train Time 31.097748517990112s

2025-10-15 00:57:40,399 | INFO | Training epoch 953, Batch 1000/1000: LR=5.42e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:57:40,452 | INFO | Epoch 953 Train Time 30.916232109069824s

2025-10-15 00:58:11,606 | INFO | Training epoch 954, Batch 1000/1000: LR=5.42e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 00:58:11,657 | INFO | Epoch 954 Train Time 31.202775239944458s

2025-10-15 00:58:42,406 | INFO | Training epoch 955, Batch 1000/1000: LR=5.41e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 00:58:42,458 | INFO | Epoch 955 Train Time 30.800488471984863s

2025-10-15 00:59:13,793 | INFO | Training epoch 956, Batch 1000/1000: LR=5.40e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:59:13,854 | INFO | Epoch 956 Train Time 31.39447259902954s

2025-10-15 00:59:45,218 | INFO | Training epoch 957, Batch 1000/1000: LR=5.39e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 00:59:45,271 | INFO | Epoch 957 Train Time 31.41617512702942s

2025-10-15 01:00:16,207 | INFO | Training epoch 958, Batch 1000/1000: LR=5.38e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:00:16,250 | INFO | Epoch 958 Train Time 30.977976083755493s

2025-10-15 01:00:47,298 | INFO | Training epoch 959, Batch 1000/1000: LR=5.38e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:00:47,345 | INFO | Epoch 959 Train Time 31.094504594802856s

2025-10-15 01:01:19,110 | INFO | Training epoch 960, Batch 1000/1000: LR=5.37e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:01:19,159 | INFO | Epoch 960 Train Time 31.81269598007202s

2025-10-15 01:01:50,913 | INFO | Training epoch 961, Batch 1000/1000: LR=5.36e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:01:50,960 | INFO | Epoch 961 Train Time 31.799954175949097s

2025-10-15 01:02:22,303 | INFO | Training epoch 962, Batch 1000/1000: LR=5.35e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:02:22,362 | INFO | Epoch 962 Train Time 31.401100873947144s

2025-10-15 01:02:53,702 | INFO | Training epoch 963, Batch 1000/1000: LR=5.35e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:02:53,754 | INFO | Epoch 963 Train Time 31.391245365142822s

2025-10-15 01:03:25,033 | INFO | Training epoch 964, Batch 1000/1000: LR=5.34e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:03:25,088 | INFO | Epoch 964 Train Time 31.3318350315094s

2025-10-15 01:03:56,110 | INFO | Training epoch 965, Batch 1000/1000: LR=5.33e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:03:56,161 | INFO | Epoch 965 Train Time 31.072551488876343s

2025-10-15 01:04:27,809 | INFO | Training epoch 966, Batch 1000/1000: LR=5.32e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:04:27,856 | INFO | Epoch 966 Train Time 31.694700956344604s

2025-10-15 01:04:59,197 | INFO | Training epoch 967, Batch 1000/1000: LR=5.31e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:04:59,263 | INFO | Epoch 967 Train Time 31.40554714202881s

2025-10-15 01:05:31,091 | INFO | Training epoch 968, Batch 1000/1000: LR=5.31e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:05:31,142 | INFO | Epoch 968 Train Time 31.87774133682251s

2025-10-15 01:06:02,417 | INFO | Training epoch 969, Batch 1000/1000: LR=5.30e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:06:02,466 | INFO | Epoch 969 Train Time 31.32276701927185s

2025-10-15 01:06:34,505 | INFO | Training epoch 970, Batch 1000/1000: LR=5.29e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:06:34,571 | INFO | Epoch 970 Train Time 32.10331416130066s

2025-10-15 01:07:05,699 | INFO | Training epoch 971, Batch 1000/1000: LR=5.28e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 01:07:05,761 | INFO | Epoch 971 Train Time 31.187933444976807s

2025-10-15 01:07:37,597 | INFO | Training epoch 972, Batch 1000/1000: LR=5.28e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:07:37,653 | INFO | Epoch 972 Train Time 31.89055562019348s

2025-10-15 01:08:08,916 | INFO | Training epoch 973, Batch 1000/1000: LR=5.27e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:08:08,966 | INFO | Epoch 973 Train Time 31.31155562400818s

2025-10-15 01:08:40,707 | INFO | Training epoch 974, Batch 1000/1000: LR=5.26e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:08:40,766 | INFO | Epoch 974 Train Time 31.79893183708191s

2025-10-15 01:09:11,591 | INFO | Training epoch 975, Batch 1000/1000: LR=5.25e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:09:11,649 | INFO | Epoch 975 Train Time 30.883039712905884s

2025-10-15 01:09:43,124 | INFO | Training epoch 976, Batch 1000/1000: LR=5.24e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:09:43,172 | INFO | Epoch 976 Train Time 31.521470308303833s

2025-10-15 01:10:14,595 | INFO | Training epoch 977, Batch 1000/1000: LR=5.24e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:10:14,643 | INFO | Epoch 977 Train Time 31.471010208129883s

2025-10-15 01:10:46,303 | INFO | Training epoch 978, Batch 1000/1000: LR=5.23e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:10:46,358 | INFO | Epoch 978 Train Time 31.713351011276245s

2025-10-15 01:11:18,197 | INFO | Training epoch 979, Batch 1000/1000: LR=5.22e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:11:18,254 | INFO | Epoch 979 Train Time 31.895102739334106s

2025-10-15 01:11:49,809 | INFO | Training epoch 980, Batch 1000/1000: LR=5.21e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:11:49,858 | INFO | Epoch 980 Train Time 31.602141618728638s

2025-10-15 01:12:20,724 | INFO | Training epoch 981, Batch 1000/1000: LR=5.21e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:12:20,770 | INFO | Epoch 981 Train Time 30.911909103393555s

2025-10-15 01:12:52,106 | INFO | Training epoch 982, Batch 1000/1000: LR=5.20e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:12:52,150 | INFO | Epoch 982 Train Time 31.37907600402832s

2025-10-15 01:13:23,514 | INFO | Training epoch 983, Batch 1000/1000: LR=5.19e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.40e-01
2025-10-15 01:13:23,571 | INFO | Epoch 983 Train Time 31.41978907585144s

2025-10-15 01:13:54,202 | INFO | Training epoch 984, Batch 1000/1000: LR=5.18e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:13:54,247 | INFO | Epoch 984 Train Time 30.675706148147583s

2025-10-15 01:14:25,423 | INFO | Training epoch 985, Batch 1000/1000: LR=5.17e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:14:25,473 | INFO | Epoch 985 Train Time 31.224650382995605s

2025-10-15 01:14:57,306 | INFO | Training epoch 986, Batch 1000/1000: LR=5.17e-05, Loss=2.86e-02 BER=1.08e-02 FER=1.37e-01
2025-10-15 01:14:57,374 | INFO | Epoch 986 Train Time 31.899906396865845s

2025-10-15 01:15:28,505 | INFO | Training epoch 987, Batch 1000/1000: LR=5.16e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:15:28,554 | INFO | Epoch 987 Train Time 31.179914236068726s

2025-10-15 01:16:00,003 | INFO | Training epoch 988, Batch 1000/1000: LR=5.15e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:16:00,059 | INFO | Epoch 988 Train Time 31.50394630432129s

2025-10-15 01:16:31,417 | INFO | Training epoch 989, Batch 1000/1000: LR=5.14e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:16:31,465 | INFO | Epoch 989 Train Time 31.404505252838135s

2025-10-15 01:17:02,720 | INFO | Training epoch 990, Batch 1000/1000: LR=5.14e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.37e-01
2025-10-15 01:17:02,779 | INFO | Epoch 990 Train Time 31.31275463104248s

2025-10-15 01:17:34,310 | INFO | Training epoch 991, Batch 1000/1000: LR=5.13e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:17:34,374 | INFO | Epoch 991 Train Time 31.59406065940857s

2025-10-15 01:18:05,403 | INFO | Training epoch 992, Batch 1000/1000: LR=5.12e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:18:05,455 | INFO | Epoch 992 Train Time 31.079615354537964s

2025-10-15 01:18:36,703 | INFO | Training epoch 993, Batch 1000/1000: LR=5.11e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:18:36,752 | INFO | Epoch 993 Train Time 31.29655933380127s

2025-10-15 01:19:08,120 | INFO | Training epoch 994, Batch 1000/1000: LR=5.10e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:19:08,176 | INFO | Epoch 994 Train Time 31.42300534248352s

2025-10-15 01:19:39,203 | INFO | Training epoch 995, Batch 1000/1000: LR=5.10e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:19:39,259 | INFO | Epoch 995 Train Time 31.082501888275146s

2025-10-15 01:20:10,605 | INFO | Training epoch 996, Batch 1000/1000: LR=5.09e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:20:10,654 | INFO | Epoch 996 Train Time 31.393893003463745s

2025-10-15 01:20:41,717 | INFO | Training epoch 997, Batch 1000/1000: LR=5.08e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:20:41,765 | INFO | Epoch 997 Train Time 31.10892653465271s

2025-10-15 01:21:13,498 | INFO | Training epoch 998, Batch 1000/1000: LR=5.07e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:21:13,571 | INFO | Epoch 998 Train Time 31.804919242858887s

2025-10-15 01:21:45,026 | INFO | Training epoch 999, Batch 1000/1000: LR=5.07e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:21:45,081 | INFO | Epoch 999 Train Time 31.508870363235474s

2025-10-15 01:22:16,509 | INFO | Training epoch 1000, Batch 1000/1000: LR=5.06e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:22:16,556 | INFO | Epoch 1000 Train Time 31.473701000213623s

2025-10-15 01:22:47,697 | INFO | Training epoch 1001, Batch 1000/1000: LR=5.05e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:22:47,767 | INFO | Epoch 1001 Train Time 31.211103200912476s

2025-10-15 01:23:19,309 | INFO | Training epoch 1002, Batch 1000/1000: LR=5.04e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:23:19,360 | INFO | Epoch 1002 Train Time 31.591726541519165s

2025-10-15 01:23:50,704 | INFO | Training epoch 1003, Batch 1000/1000: LR=5.03e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:23:50,760 | INFO | Epoch 1003 Train Time 31.39940118789673s

2025-10-15 01:24:21,400 | INFO | Training epoch 1004, Batch 1000/1000: LR=5.03e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:24:21,450 | INFO | Epoch 1004 Train Time 30.68828010559082s

2025-10-15 01:24:53,411 | INFO | Training epoch 1005, Batch 1000/1000: LR=5.02e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:24:53,454 | INFO | Epoch 1005 Train Time 32.00317645072937s

2025-10-15 01:25:24,006 | INFO | Training epoch 1006, Batch 1000/1000: LR=5.01e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:25:24,055 | INFO | Epoch 1006 Train Time 30.599609851837158s

2025-10-15 01:25:56,000 | INFO | Training epoch 1007, Batch 1000/1000: LR=5.00e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:25:56,043 | INFO | Epoch 1007 Train Time 31.98807120323181s

2025-10-15 01:26:27,910 | INFO | Training epoch 1008, Batch 1000/1000: LR=5.00e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:26:27,972 | INFO | Epoch 1008 Train Time 31.928367853164673s

2025-10-15 01:26:59,405 | INFO | Training epoch 1009, Batch 1000/1000: LR=4.99e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:26:59,450 | INFO | Epoch 1009 Train Time 31.477036952972412s

2025-10-15 01:27:31,099 | INFO | Training epoch 1010, Batch 1000/1000: LR=4.98e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:27:31,153 | INFO | Epoch 1010 Train Time 31.70206356048584s

2025-10-15 01:28:02,529 | INFO | Training epoch 1011, Batch 1000/1000: LR=4.97e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 01:28:02,577 | INFO | Epoch 1011 Train Time 31.42304301261902s

2025-10-15 01:28:34,405 | INFO | Training epoch 1012, Batch 1000/1000: LR=4.96e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:28:34,456 | INFO | Epoch 1012 Train Time 31.876582622528076s

2025-10-15 01:29:06,095 | INFO | Training epoch 1013, Batch 1000/1000: LR=4.96e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:29:06,174 | INFO | Epoch 1013 Train Time 31.71738839149475s

2025-10-15 01:29:37,591 | INFO | Training epoch 1014, Batch 1000/1000: LR=4.95e-05, Loss=2.88e-02 BER=1.09e-02 FER=1.38e-01
2025-10-15 01:29:37,641 | INFO | Epoch 1014 Train Time 31.466575622558594s

2025-10-15 01:30:08,715 | INFO | Training epoch 1015, Batch 1000/1000: LR=4.94e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:30:08,758 | INFO | Epoch 1015 Train Time 31.116705179214478s

2025-10-15 01:30:40,200 | INFO | Training epoch 1016, Batch 1000/1000: LR=4.93e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 01:30:40,251 | INFO | Epoch 1016 Train Time 31.49245810508728s

2025-10-15 01:31:11,403 | INFO | Training epoch 1017, Batch 1000/1000: LR=4.93e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:31:11,454 | INFO | Epoch 1017 Train Time 31.201339721679688s

2025-10-15 01:31:43,202 | INFO | Training epoch 1018, Batch 1000/1000: LR=4.92e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:31:43,250 | INFO | Epoch 1018 Train Time 31.795257806777954s

2025-10-15 01:32:15,192 | INFO | Training epoch 1019, Batch 1000/1000: LR=4.91e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:32:15,234 | INFO | Epoch 1019 Train Time 31.982423067092896s

2025-10-15 01:32:46,793 | INFO | Training epoch 1020, Batch 1000/1000: LR=4.90e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:32:46,845 | INFO | Epoch 1020 Train Time 31.610742568969727s

2025-10-15 01:33:18,019 | INFO | Training epoch 1021, Batch 1000/1000: LR=4.89e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:33:18,068 | INFO | Epoch 1021 Train Time 31.22104263305664s

2025-10-15 01:33:49,395 | INFO | Training epoch 1022, Batch 1000/1000: LR=4.89e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:33:49,454 | INFO | Epoch 1022 Train Time 31.38457489013672s

2025-10-15 01:34:21,096 | INFO | Training epoch 1023, Batch 1000/1000: LR=4.88e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:34:21,149 | INFO | Epoch 1023 Train Time 31.69492244720459s

2025-10-15 01:34:53,003 | INFO | Training epoch 1024, Batch 1000/1000: LR=4.87e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:34:53,069 | INFO | Epoch 1024 Train Time 31.91937756538391s

2025-10-15 01:35:24,251 | INFO | Training epoch 1025, Batch 1000/1000: LR=4.86e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:35:24,302 | INFO | Epoch 1025 Train Time 31.23151421546936s

2025-10-15 01:35:55,351 | INFO | Training epoch 1026, Batch 1000/1000: LR=4.86e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:35:55,401 | INFO | Epoch 1026 Train Time 31.09787917137146s

2025-10-15 01:36:26,390 | INFO | Training epoch 1027, Batch 1000/1000: LR=4.85e-05, Loss=2.85e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 01:36:26,439 | INFO | Epoch 1027 Train Time 31.036762237548828s

2025-10-15 01:36:57,205 | INFO | Training epoch 1028, Batch 1000/1000: LR=4.84e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:36:57,258 | INFO | Epoch 1028 Train Time 30.817281484603882s

2025-10-15 01:37:28,713 | INFO | Training epoch 1029, Batch 1000/1000: LR=4.83e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:37:28,763 | INFO | Epoch 1029 Train Time 31.505061388015747s

2025-10-15 01:38:00,108 | INFO | Training epoch 1030, Batch 1000/1000: LR=4.82e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:38:00,168 | INFO | Epoch 1030 Train Time 31.40311312675476s

2025-10-15 01:38:31,004 | INFO | Training epoch 1031, Batch 1000/1000: LR=4.82e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:38:31,052 | INFO | Epoch 1031 Train Time 30.88276982307434s

2025-10-15 01:39:02,608 | INFO | Training epoch 1032, Batch 1000/1000: LR=4.81e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:39:02,654 | INFO | Epoch 1032 Train Time 31.601235151290894s

2025-10-15 01:39:34,313 | INFO | Training epoch 1033, Batch 1000/1000: LR=4.80e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:39:34,401 | INFO | Epoch 1033 Train Time 31.746940851211548s

2025-10-15 01:40:05,293 | INFO | Training epoch 1034, Batch 1000/1000: LR=4.79e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:40:05,341 | INFO | Epoch 1034 Train Time 30.93851351737976s

2025-10-15 01:40:36,695 | INFO | Training epoch 1035, Batch 1000/1000: LR=4.79e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:40:36,746 | INFO | Epoch 1035 Train Time 31.404718160629272s

2025-10-15 01:41:08,490 | INFO | Training epoch 1036, Batch 1000/1000: LR=4.78e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 01:41:08,542 | INFO | Epoch 1036 Train Time 31.79457116127014s

2025-10-15 01:41:40,417 | INFO | Training epoch 1037, Batch 1000/1000: LR=4.77e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:41:40,466 | INFO | Epoch 1037 Train Time 31.923429250717163s

2025-10-15 01:42:11,800 | INFO | Training epoch 1038, Batch 1000/1000: LR=4.76e-05, Loss=2.82e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:42:11,845 | INFO | Epoch 1038 Train Time 31.377955675125122s

2025-10-15 01:42:43,519 | INFO | Training epoch 1039, Batch 1000/1000: LR=4.75e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:42:43,578 | INFO | Epoch 1039 Train Time 31.731412172317505s

2025-10-15 01:43:14,095 | INFO | Training epoch 1040, Batch 1000/1000: LR=4.75e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:43:14,145 | INFO | Epoch 1040 Train Time 30.565897226333618s

2025-10-15 01:43:45,605 | INFO | Training epoch 1041, Batch 1000/1000: LR=4.74e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:43:45,667 | INFO | Epoch 1041 Train Time 31.520883560180664s

2025-10-15 01:44:17,421 | INFO | Training epoch 1042, Batch 1000/1000: LR=4.73e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:44:17,472 | INFO | Epoch 1042 Train Time 31.804442644119263s

2025-10-15 01:44:48,801 | INFO | Training epoch 1043, Batch 1000/1000: LR=4.72e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:44:48,861 | INFO | Epoch 1043 Train Time 31.387789726257324s

2025-10-15 01:45:20,210 | INFO | Training epoch 1044, Batch 1000/1000: LR=4.72e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:45:20,256 | INFO | Epoch 1044 Train Time 31.394575357437134s

2025-10-15 01:45:51,599 | INFO | Training epoch 1045, Batch 1000/1000: LR=4.71e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:45:51,656 | INFO | Epoch 1045 Train Time 31.397951364517212s

2025-10-15 01:46:22,798 | INFO | Training epoch 1046, Batch 1000/1000: LR=4.70e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:46:22,848 | INFO | Epoch 1046 Train Time 31.191123247146606s

2025-10-15 01:46:53,710 | INFO | Training epoch 1047, Batch 1000/1000: LR=4.69e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 01:46:53,765 | INFO | Epoch 1047 Train Time 30.915475606918335s

2025-10-15 01:46:53,765 | INFO | [P2] saving best_model (QAT) with loss 0.027711 at epoch 1047
2025-10-15 01:47:25,703 | INFO | Training epoch 1048, Batch 1000/1000: LR=4.68e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:47:25,766 | INFO | Epoch 1048 Train Time 31.98486852645874s

2025-10-15 01:47:57,222 | INFO | Training epoch 1049, Batch 1000/1000: LR=4.68e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.39e-01
2025-10-15 01:47:57,273 | INFO | Epoch 1049 Train Time 31.50641417503357s

2025-10-15 01:48:28,925 | INFO | Training epoch 1050, Batch 1000/1000: LR=4.67e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:48:28,978 | INFO | Epoch 1050 Train Time 31.70460081100464s

2025-10-15 01:48:58,705 | INFO | Training epoch 1051, Batch 1000/1000: LR=4.66e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:48:58,756 | INFO | Epoch 1051 Train Time 29.776758432388306s

2025-10-15 01:49:30,391 | INFO | Training epoch 1052, Batch 1000/1000: LR=4.65e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 01:49:30,439 | INFO | Epoch 1052 Train Time 31.68269443511963s

2025-10-15 01:50:02,001 | INFO | Training epoch 1053, Batch 1000/1000: LR=4.65e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 01:50:02,059 | INFO | Epoch 1053 Train Time 31.619346618652344s

2025-10-15 01:50:33,603 | INFO | Training epoch 1054, Batch 1000/1000: LR=4.64e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 01:50:33,652 | INFO | Epoch 1054 Train Time 31.59121322631836s

2025-10-15 01:51:04,727 | INFO | Training epoch 1055, Batch 1000/1000: LR=4.63e-05, Loss=2.81e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 01:51:04,778 | INFO | Epoch 1055 Train Time 31.123961925506592s

2025-10-15 01:51:35,698 | INFO | Training epoch 1056, Batch 1000/1000: LR=4.62e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:51:35,751 | INFO | Epoch 1056 Train Time 30.971854209899902s

2025-10-15 01:52:06,844 | INFO | Training epoch 1057, Batch 1000/1000: LR=4.62e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:52:06,897 | INFO | Epoch 1057 Train Time 31.144811868667603s

2025-10-15 01:52:37,897 | INFO | Training epoch 1058, Batch 1000/1000: LR=4.61e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:52:37,950 | INFO | Epoch 1058 Train Time 31.052567720413208s

2025-10-15 01:53:09,673 | INFO | Training epoch 1059, Batch 1000/1000: LR=4.60e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:53:09,734 | INFO | Epoch 1059 Train Time 31.782501697540283s

2025-10-15 01:53:41,405 | INFO | Training epoch 1060, Batch 1000/1000: LR=4.59e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:53:41,451 | INFO | Epoch 1060 Train Time 31.7163724899292s

2025-10-15 01:54:12,107 | INFO | Training epoch 1061, Batch 1000/1000: LR=4.58e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 01:54:12,155 | INFO | Epoch 1061 Train Time 30.70389699935913s

2025-10-15 01:54:43,802 | INFO | Training epoch 1062, Batch 1000/1000: LR=4.58e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 01:54:43,871 | INFO | Epoch 1062 Train Time 31.713752508163452s

2025-10-15 01:55:14,825 | INFO | Training epoch 1063, Batch 1000/1000: LR=4.57e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:55:14,876 | INFO | Epoch 1063 Train Time 31.00451135635376s

2025-10-15 01:55:45,905 | INFO | Training epoch 1064, Batch 1000/1000: LR=4.56e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:55:45,952 | INFO | Epoch 1064 Train Time 31.07485318183899s

2025-10-15 01:56:16,933 | INFO | Training epoch 1065, Batch 1000/1000: LR=4.55e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:56:17,007 | INFO | Epoch 1065 Train Time 31.05426335334778s

2025-10-15 01:56:48,102 | INFO | Training epoch 1066, Batch 1000/1000: LR=4.55e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.39e-01
2025-10-15 01:56:48,155 | INFO | Epoch 1066 Train Time 31.146114587783813s

2025-10-15 01:57:19,796 | INFO | Training epoch 1067, Batch 1000/1000: LR=4.54e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 01:57:19,847 | INFO | Epoch 1067 Train Time 31.69149136543274s

2025-10-15 01:57:51,509 | INFO | Training epoch 1068, Batch 1000/1000: LR=4.53e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 01:57:51,568 | INFO | Epoch 1068 Train Time 31.72013831138611s

2025-10-15 01:58:22,691 | INFO | Training epoch 1069, Batch 1000/1000: LR=4.52e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 01:58:22,752 | INFO | Epoch 1069 Train Time 31.181878566741943s

2025-10-15 01:58:53,935 | INFO | Training epoch 1070, Batch 1000/1000: LR=4.51e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 01:58:53,986 | INFO | Epoch 1070 Train Time 31.233423709869385s

2025-10-15 01:59:25,097 | INFO | Training epoch 1071, Batch 1000/1000: LR=4.51e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 01:59:25,145 | INFO | Epoch 1071 Train Time 31.15770697593689s

2025-10-15 01:59:56,605 | INFO | Training epoch 1072, Batch 1000/1000: LR=4.50e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 01:59:56,655 | INFO | Epoch 1072 Train Time 31.50964593887329s

2025-10-15 02:00:27,504 | INFO | Training epoch 1073, Batch 1000/1000: LR=4.49e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:00:27,558 | INFO | Epoch 1073 Train Time 30.90122628211975s

2025-10-15 02:00:58,496 | INFO | Training epoch 1074, Batch 1000/1000: LR=4.48e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:00:58,544 | INFO | Epoch 1074 Train Time 30.98519206047058s

2025-10-15 02:01:29,994 | INFO | Training epoch 1075, Batch 1000/1000: LR=4.48e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:01:30,049 | INFO | Epoch 1075 Train Time 31.504990577697754s

2025-10-15 02:02:00,594 | INFO | Training epoch 1076, Batch 1000/1000: LR=4.47e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 02:02:00,641 | INFO | Epoch 1076 Train Time 30.590753316879272s

2025-10-15 02:02:31,308 | INFO | Training epoch 1077, Batch 1000/1000: LR=4.46e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:02:31,358 | INFO | Epoch 1077 Train Time 30.715333700180054s

2025-10-15 02:03:03,306 | INFO | Training epoch 1078, Batch 1000/1000: LR=4.45e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:03:03,360 | INFO | Epoch 1078 Train Time 32.00056743621826s

2025-10-15 02:03:34,002 | INFO | Training epoch 1079, Batch 1000/1000: LR=4.45e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 02:03:34,044 | INFO | Epoch 1079 Train Time 30.68173837661743s

2025-10-15 02:04:05,601 | INFO | Training epoch 1080, Batch 1000/1000: LR=4.44e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:04:05,653 | INFO | Epoch 1080 Train Time 31.60800290107727s

2025-10-15 02:04:36,813 | INFO | Training epoch 1081, Batch 1000/1000: LR=4.43e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:04:36,868 | INFO | Epoch 1081 Train Time 31.214972257614136s

2025-10-15 02:05:08,407 | INFO | Training epoch 1082, Batch 1000/1000: LR=4.42e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:05:08,473 | INFO | Epoch 1082 Train Time 31.603405714035034s

2025-10-15 02:05:39,505 | INFO | Training epoch 1083, Batch 1000/1000: LR=4.41e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:05:39,551 | INFO | Epoch 1083 Train Time 31.07692289352417s

2025-10-15 02:06:10,804 | INFO | Training epoch 1084, Batch 1000/1000: LR=4.41e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:06:10,851 | INFO | Epoch 1084 Train Time 31.299693822860718s

2025-10-15 02:06:42,560 | INFO | Training epoch 1085, Batch 1000/1000: LR=4.40e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:06:42,623 | INFO | Epoch 1085 Train Time 31.77061414718628s

2025-10-15 02:07:14,306 | INFO | Training epoch 1086, Batch 1000/1000: LR=4.39e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:07:14,357 | INFO | Epoch 1086 Train Time 31.73298406600952s

2025-10-15 02:07:46,109 | INFO | Training epoch 1087, Batch 1000/1000: LR=4.38e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:07:46,167 | INFO | Epoch 1087 Train Time 31.80856466293335s

2025-10-15 02:08:18,009 | INFO | Training epoch 1088, Batch 1000/1000: LR=4.38e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:08:18,065 | INFO | Epoch 1088 Train Time 31.896861791610718s

2025-10-15 02:08:49,493 | INFO | Training epoch 1089, Batch 1000/1000: LR=4.37e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:08:49,537 | INFO | Epoch 1089 Train Time 31.471869945526123s

2025-10-15 02:09:20,994 | INFO | Training epoch 1090, Batch 1000/1000: LR=4.36e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:09:21,045 | INFO | Epoch 1090 Train Time 31.50564980506897s

2025-10-15 02:09:52,804 | INFO | Training epoch 1091, Batch 1000/1000: LR=4.35e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:09:52,865 | INFO | Epoch 1091 Train Time 31.819504737854004s

2025-10-15 02:10:24,513 | INFO | Training epoch 1092, Batch 1000/1000: LR=4.34e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:10:24,557 | INFO | Epoch 1092 Train Time 31.69118595123291s

2025-10-15 02:10:56,405 | INFO | Training epoch 1093, Batch 1000/1000: LR=4.34e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:10:56,455 | INFO | Epoch 1093 Train Time 31.897989511489868s

2025-10-15 02:11:28,207 | INFO | Training epoch 1094, Batch 1000/1000: LR=4.33e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:11:28,273 | INFO | Epoch 1094 Train Time 31.81644105911255s

2025-10-15 02:11:59,805 | INFO | Training epoch 1095, Batch 1000/1000: LR=4.32e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:11:59,866 | INFO | Epoch 1095 Train Time 31.592167139053345s

2025-10-15 02:12:30,907 | INFO | Training epoch 1096, Batch 1000/1000: LR=4.31e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:12:30,953 | INFO | Epoch 1096 Train Time 31.087105989456177s

2025-10-15 02:13:02,797 | INFO | Training epoch 1097, Batch 1000/1000: LR=4.31e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:13:02,851 | INFO | Epoch 1097 Train Time 31.896888494491577s

2025-10-15 02:13:34,002 | INFO | Training epoch 1098, Batch 1000/1000: LR=4.30e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:13:34,052 | INFO | Epoch 1098 Train Time 31.199891090393066s

2025-10-15 02:14:05,996 | INFO | Training epoch 1099, Batch 1000/1000: LR=4.29e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:14:06,073 | INFO | Epoch 1099 Train Time 32.02095293998718s

2025-10-15 02:14:37,403 | INFO | Training epoch 1100, Batch 1000/1000: LR=4.28e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:14:37,464 | INFO | Epoch 1100 Train Time 31.38925790786743s

2025-10-15 02:15:09,030 | INFO | Training epoch 1101, Batch 1000/1000: LR=4.28e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:15:09,094 | INFO | Epoch 1101 Train Time 31.62860941886902s

2025-10-15 02:15:39,895 | INFO | Training epoch 1102, Batch 1000/1000: LR=4.27e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 02:15:39,949 | INFO | Epoch 1102 Train Time 30.854761838912964s

2025-10-15 02:16:11,600 | INFO | Training epoch 1103, Batch 1000/1000: LR=4.26e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:16:11,658 | INFO | Epoch 1103 Train Time 31.708033323287964s

2025-10-15 02:16:42,793 | INFO | Training epoch 1104, Batch 1000/1000: LR=4.25e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:16:42,846 | INFO | Epoch 1104 Train Time 31.186418771743774s

2025-10-15 02:17:14,530 | INFO | Training epoch 1105, Batch 1000/1000: LR=4.24e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.39e-01
2025-10-15 02:17:14,581 | INFO | Epoch 1105 Train Time 31.734819412231445s

2025-10-15 02:17:45,505 | INFO | Training epoch 1106, Batch 1000/1000: LR=4.24e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 02:17:45,571 | INFO | Epoch 1106 Train Time 30.9893319606781s

2025-10-15 02:18:16,507 | INFO | Training epoch 1107, Batch 1000/1000: LR=4.23e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:18:16,569 | INFO | Epoch 1107 Train Time 30.996017932891846s

2025-10-15 02:18:48,109 | INFO | Training epoch 1108, Batch 1000/1000: LR=4.22e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:18:48,157 | INFO | Epoch 1108 Train Time 31.587584257125854s

2025-10-15 02:19:19,400 | INFO | Training epoch 1109, Batch 1000/1000: LR=4.21e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:19:19,455 | INFO | Epoch 1109 Train Time 31.297072649002075s

2025-10-15 02:19:50,409 | INFO | Training epoch 1110, Batch 1000/1000: LR=4.21e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:19:50,469 | INFO | Epoch 1110 Train Time 31.012699842453003s

2025-10-15 02:20:22,205 | INFO | Training epoch 1111, Batch 1000/1000: LR=4.20e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:20:22,253 | INFO | Epoch 1111 Train Time 31.78296446800232s

2025-10-15 02:20:52,999 | INFO | Training epoch 1112, Batch 1000/1000: LR=4.19e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:20:53,066 | INFO | Epoch 1112 Train Time 30.813011646270752s

2025-10-15 02:21:23,902 | INFO | Training epoch 1113, Batch 1000/1000: LR=4.18e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:21:23,946 | INFO | Epoch 1113 Train Time 30.878334522247314s

2025-10-15 02:21:55,710 | INFO | Training epoch 1114, Batch 1000/1000: LR=4.18e-05, Loss=2.81e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 02:21:55,755 | INFO | Epoch 1114 Train Time 31.809049129486084s

2025-10-15 02:22:27,516 | INFO | Training epoch 1115, Batch 1000/1000: LR=4.17e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:22:27,565 | INFO | Epoch 1115 Train Time 31.807607412338257s

2025-10-15 02:22:58,208 | INFO | Training epoch 1116, Batch 1000/1000: LR=4.16e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:22:58,257 | INFO | Epoch 1116 Train Time 30.69163751602173s

2025-10-15 02:23:30,201 | INFO | Training epoch 1117, Batch 1000/1000: LR=4.15e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:23:30,254 | INFO | Epoch 1117 Train Time 31.995952129364014s

2025-10-15 02:24:01,992 | INFO | Training epoch 1118, Batch 1000/1000: LR=4.15e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:24:02,037 | INFO | Epoch 1118 Train Time 31.782482385635376s

2025-10-15 02:24:33,988 | INFO | Training epoch 1119, Batch 1000/1000: LR=4.14e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:24:34,036 | INFO | Epoch 1119 Train Time 31.99787449836731s

2025-10-15 02:25:04,711 | INFO | Training epoch 1120, Batch 1000/1000: LR=4.13e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:25:04,768 | INFO | Epoch 1120 Train Time 30.731863021850586s

2025-10-15 02:25:36,014 | INFO | Training epoch 1121, Batch 1000/1000: LR=4.12e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:25:36,063 | INFO | Epoch 1121 Train Time 31.29282546043396s

2025-10-15 02:26:07,095 | INFO | Training epoch 1122, Batch 1000/1000: LR=4.11e-05, Loss=2.89e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 02:26:07,144 | INFO | Epoch 1122 Train Time 31.07859230041504s

2025-10-15 02:26:38,634 | INFO | Training epoch 1123, Batch 1000/1000: LR=4.11e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:26:38,707 | INFO | Epoch 1123 Train Time 31.562458753585815s

2025-10-15 02:27:08,993 | INFO | Training epoch 1124, Batch 1000/1000: LR=4.10e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:27:09,059 | INFO | Epoch 1124 Train Time 30.350141525268555s

2025-10-15 02:27:40,191 | INFO | Training epoch 1125, Batch 1000/1000: LR=4.09e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:27:40,242 | INFO | Epoch 1125 Train Time 31.18251085281372s

2025-10-15 02:28:11,902 | INFO | Training epoch 1126, Batch 1000/1000: LR=4.08e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:28:11,949 | INFO | Epoch 1126 Train Time 31.70707392692566s

2025-10-15 02:28:43,429 | INFO | Training epoch 1127, Batch 1000/1000: LR=4.08e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:28:43,477 | INFO | Epoch 1127 Train Time 31.525927305221558s

2025-10-15 02:29:15,604 | INFO | Training epoch 1128, Batch 1000/1000: LR=4.07e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:29:15,657 | INFO | Epoch 1128 Train Time 32.17866826057434s

2025-10-15 02:29:47,504 | INFO | Training epoch 1129, Batch 1000/1000: LR=4.06e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:29:47,556 | INFO | Epoch 1129 Train Time 31.89771866798401s

2025-10-15 02:30:19,225 | INFO | Training epoch 1130, Batch 1000/1000: LR=4.05e-05, Loss=2.86e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:30:19,273 | INFO | Epoch 1130 Train Time 31.71562170982361s

2025-10-15 02:30:50,805 | INFO | Training epoch 1131, Batch 1000/1000: LR=4.05e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:30:50,846 | INFO | Epoch 1131 Train Time 31.572178602218628s

2025-10-15 02:31:21,398 | INFO | Training epoch 1132, Batch 1000/1000: LR=4.04e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:31:21,453 | INFO | Epoch 1132 Train Time 30.607113361358643s

2025-10-15 02:31:53,009 | INFO | Training epoch 1133, Batch 1000/1000: LR=4.03e-05, Loss=2.83e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:31:53,054 | INFO | Epoch 1133 Train Time 31.599785089492798s

2025-10-15 02:32:24,002 | INFO | Training epoch 1134, Batch 1000/1000: LR=4.02e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:32:24,049 | INFO | Epoch 1134 Train Time 30.994343280792236s

2025-10-15 02:32:54,819 | INFO | Training epoch 1135, Batch 1000/1000: LR=4.02e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:32:54,866 | INFO | Epoch 1135 Train Time 30.81583523750305s

2025-10-15 02:33:26,406 | INFO | Training epoch 1136, Batch 1000/1000: LR=4.01e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:33:26,462 | INFO | Epoch 1136 Train Time 31.595523357391357s

2025-10-15 02:33:57,705 | INFO | Training epoch 1137, Batch 1000/1000: LR=4.00e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:33:57,764 | INFO | Epoch 1137 Train Time 31.299785375595093s

2025-10-15 02:34:29,592 | INFO | Training epoch 1138, Batch 1000/1000: LR=3.99e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:34:29,644 | INFO | Epoch 1138 Train Time 31.878618240356445s

2025-10-15 02:35:00,707 | INFO | Training epoch 1139, Batch 1000/1000: LR=3.99e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:35:00,765 | INFO | Epoch 1139 Train Time 31.119220972061157s

2025-10-15 02:35:30,911 | INFO | Training epoch 1140, Batch 1000/1000: LR=3.98e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:35:30,962 | INFO | Epoch 1140 Train Time 30.1948504447937s

2025-10-15 02:36:02,512 | INFO | Training epoch 1141, Batch 1000/1000: LR=3.97e-05, Loss=2.81e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 02:36:02,571 | INFO | Epoch 1141 Train Time 31.60830020904541s

2025-10-15 02:36:34,104 | INFO | Training epoch 1142, Batch 1000/1000: LR=3.96e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:36:34,156 | INFO | Epoch 1142 Train Time 31.58345675468445s

2025-10-15 02:37:06,005 | INFO | Training epoch 1143, Batch 1000/1000: LR=3.96e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:37:06,064 | INFO | Epoch 1143 Train Time 31.90665078163147s

2025-10-15 02:37:36,709 | INFO | Training epoch 1144, Batch 1000/1000: LR=3.95e-05, Loss=2.83e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:37:36,759 | INFO | Epoch 1144 Train Time 30.693546056747437s

2025-10-15 02:38:07,815 | INFO | Training epoch 1145, Batch 1000/1000: LR=3.94e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:38:07,863 | INFO | Epoch 1145 Train Time 31.104067087173462s

2025-10-15 02:38:38,694 | INFO | Training epoch 1146, Batch 1000/1000: LR=3.93e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:38:38,745 | INFO | Epoch 1146 Train Time 30.88155484199524s

2025-10-15 02:39:10,620 | INFO | Training epoch 1147, Batch 1000/1000: LR=3.92e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:39:10,668 | INFO | Epoch 1147 Train Time 31.920655012130737s

2025-10-15 02:39:41,791 | INFO | Training epoch 1148, Batch 1000/1000: LR=3.92e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:39:41,850 | INFO | Epoch 1148 Train Time 31.180684328079224s

2025-10-15 02:40:13,194 | INFO | Training epoch 1149, Batch 1000/1000: LR=3.91e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 02:40:13,250 | INFO | Epoch 1149 Train Time 31.399463891983032s

2025-10-15 02:40:44,803 | INFO | Training epoch 1150, Batch 1000/1000: LR=3.90e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:40:44,859 | INFO | Epoch 1150 Train Time 31.608130931854248s

2025-10-15 02:41:15,818 | INFO | Training epoch 1151, Batch 1000/1000: LR=3.89e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:41:15,869 | INFO | Epoch 1151 Train Time 31.00951075553894s

2025-10-15 02:41:47,511 | INFO | Training epoch 1152, Batch 1000/1000: LR=3.89e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.39e-01
2025-10-15 02:41:47,561 | INFO | Epoch 1152 Train Time 31.69086480140686s

2025-10-15 02:42:17,999 | INFO | Training epoch 1153, Batch 1000/1000: LR=3.88e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:42:18,049 | INFO | Epoch 1153 Train Time 30.488003253936768s

2025-10-15 02:42:48,832 | INFO | Training epoch 1154, Batch 1000/1000: LR=3.87e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:42:48,876 | INFO | Epoch 1154 Train Time 30.825992822647095s

2025-10-15 02:43:20,298 | INFO | Training epoch 1155, Batch 1000/1000: LR=3.86e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:43:20,359 | INFO | Epoch 1155 Train Time 31.481407403945923s

2025-10-15 02:43:52,003 | INFO | Training epoch 1156, Batch 1000/1000: LR=3.86e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:43:52,055 | INFO | Epoch 1156 Train Time 31.695578575134277s

2025-10-15 02:44:23,225 | INFO | Training epoch 1157, Batch 1000/1000: LR=3.85e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:44:23,280 | INFO | Epoch 1157 Train Time 31.223371267318726s

2025-10-15 02:44:54,705 | INFO | Training epoch 1158, Batch 1000/1000: LR=3.84e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:44:54,763 | INFO | Epoch 1158 Train Time 31.481985330581665s

2025-10-15 02:45:25,519 | INFO | Training epoch 1159, Batch 1000/1000: LR=3.83e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:45:25,581 | INFO | Epoch 1159 Train Time 30.816181421279907s

2025-10-15 02:45:56,500 | INFO | Training epoch 1160, Batch 1000/1000: LR=3.83e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:45:56,552 | INFO | Epoch 1160 Train Time 30.970378637313843s

2025-10-15 02:46:28,108 | INFO | Training epoch 1161, Batch 1000/1000: LR=3.82e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:46:28,157 | INFO | Epoch 1161 Train Time 31.604578256607056s

2025-10-15 02:46:59,565 | INFO | Training epoch 1162, Batch 1000/1000: LR=3.81e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:46:59,618 | INFO | Epoch 1162 Train Time 31.459381103515625s

2025-10-15 02:47:30,913 | INFO | Training epoch 1163, Batch 1000/1000: LR=3.80e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:47:30,962 | INFO | Epoch 1163 Train Time 31.34366726875305s

2025-10-15 02:48:02,517 | INFO | Training epoch 1164, Batch 1000/1000: LR=3.80e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:48:02,579 | INFO | Epoch 1164 Train Time 31.61571741104126s

2025-10-15 02:48:34,393 | INFO | Training epoch 1165, Batch 1000/1000: LR=3.79e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:48:34,443 | INFO | Epoch 1165 Train Time 31.863861083984375s

2025-10-15 02:49:06,002 | INFO | Training epoch 1166, Batch 1000/1000: LR=3.78e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:49:06,064 | INFO | Epoch 1166 Train Time 31.619490385055542s

2025-10-15 02:49:37,319 | INFO | Training epoch 1167, Batch 1000/1000: LR=3.77e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:49:37,367 | INFO | Epoch 1167 Train Time 31.301636695861816s

2025-10-15 02:50:09,109 | INFO | Training epoch 1168, Batch 1000/1000: LR=3.77e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 02:50:09,160 | INFO | Epoch 1168 Train Time 31.792179822921753s

2025-10-15 02:50:40,706 | INFO | Training epoch 1169, Batch 1000/1000: LR=3.76e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:50:40,753 | INFO | Epoch 1169 Train Time 31.592294931411743s

2025-10-15 02:51:12,315 | INFO | Training epoch 1170, Batch 1000/1000: LR=3.75e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 02:51:12,369 | INFO | Epoch 1170 Train Time 31.614135265350342s

2025-10-15 02:51:43,401 | INFO | Training epoch 1171, Batch 1000/1000: LR=3.74e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:51:43,482 | INFO | Epoch 1171 Train Time 31.11090350151062s

2025-10-15 02:52:14,609 | INFO | Training epoch 1172, Batch 1000/1000: LR=3.74e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:52:14,672 | INFO | Epoch 1172 Train Time 31.189974308013916s

2025-10-15 02:52:46,002 | INFO | Training epoch 1173, Batch 1000/1000: LR=3.73e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 02:52:46,050 | INFO | Epoch 1173 Train Time 31.377264738082886s

2025-10-15 02:53:16,995 | INFO | Training epoch 1174, Batch 1000/1000: LR=3.72e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:53:17,039 | INFO | Epoch 1174 Train Time 30.987905025482178s

2025-10-15 02:53:48,299 | INFO | Training epoch 1175, Batch 1000/1000: LR=3.71e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 02:53:48,371 | INFO | Epoch 1175 Train Time 31.331233501434326s

2025-10-15 02:54:20,224 | INFO | Training epoch 1176, Batch 1000/1000: LR=3.71e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 02:54:20,281 | INFO | Epoch 1176 Train Time 31.909400939941406s

2025-10-15 02:54:51,410 | INFO | Training epoch 1177, Batch 1000/1000: LR=3.70e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:54:51,459 | INFO | Epoch 1177 Train Time 31.176685571670532s

2025-10-15 02:55:23,194 | INFO | Training epoch 1178, Batch 1000/1000: LR=3.69e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.38e-01
2025-10-15 02:55:23,247 | INFO | Epoch 1178 Train Time 31.787755966186523s

2025-10-15 02:55:54,411 | INFO | Training epoch 1179, Batch 1000/1000: LR=3.68e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 02:55:54,460 | INFO | Epoch 1179 Train Time 31.211843967437744s

2025-10-15 02:56:25,207 | INFO | Training epoch 1180, Batch 1000/1000: LR=3.68e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:56:25,258 | INFO | Epoch 1180 Train Time 30.796646118164062s

2025-10-15 02:56:57,085 | INFO | Training epoch 1181, Batch 1000/1000: LR=3.67e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:56:57,148 | INFO | Epoch 1181 Train Time 31.889726877212524s

2025-10-15 02:57:28,299 | INFO | Training epoch 1182, Batch 1000/1000: LR=3.66e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:57:28,346 | INFO | Epoch 1182 Train Time 31.196022510528564s

2025-10-15 02:57:59,395 | INFO | Training epoch 1183, Batch 1000/1000: LR=3.65e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 02:57:59,443 | INFO | Epoch 1183 Train Time 31.095539093017578s

2025-10-15 02:58:31,001 | INFO | Training epoch 1184, Batch 1000/1000: LR=3.65e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 02:58:31,054 | INFO | Epoch 1184 Train Time 31.61063289642334s

2025-10-15 02:59:02,703 | INFO | Training epoch 1185, Batch 1000/1000: LR=3.64e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 02:59:02,754 | INFO | Epoch 1185 Train Time 31.69888401031494s

2025-10-15 02:59:34,235 | INFO | Training epoch 1186, Batch 1000/1000: LR=3.63e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 02:59:34,298 | INFO | Epoch 1186 Train Time 31.54379439353943s

2025-10-15 03:00:05,300 | INFO | Training epoch 1187, Batch 1000/1000: LR=3.62e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:00:05,359 | INFO | Epoch 1187 Train Time 31.059356927871704s

2025-10-15 03:00:36,399 | INFO | Training epoch 1188, Batch 1000/1000: LR=3.62e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:00:36,460 | INFO | Epoch 1188 Train Time 31.100355863571167s

2025-10-15 03:01:07,890 | INFO | Training epoch 1189, Batch 1000/1000: LR=3.61e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 03:01:07,937 | INFO | Epoch 1189 Train Time 31.476892471313477s

2025-10-15 03:01:38,610 | INFO | Training epoch 1190, Batch 1000/1000: LR=3.60e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:01:38,661 | INFO | Epoch 1190 Train Time 30.722843170166016s

2025-10-15 03:02:09,720 | INFO | Training epoch 1191, Batch 1000/1000: LR=3.59e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:02:09,774 | INFO | Epoch 1191 Train Time 31.11175537109375s

2025-10-15 03:02:41,608 | INFO | Training epoch 1192, Batch 1000/1000: LR=3.59e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:02:41,662 | INFO | Epoch 1192 Train Time 31.88709545135498s

2025-10-15 03:03:12,914 | INFO | Training epoch 1193, Batch 1000/1000: LR=3.58e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:03:12,961 | INFO | Epoch 1193 Train Time 31.299057245254517s

2025-10-15 03:03:45,113 | INFO | Training epoch 1194, Batch 1000/1000: LR=3.57e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:03:45,161 | INFO | Epoch 1194 Train Time 32.19905424118042s

2025-10-15 03:04:16,915 | INFO | Training epoch 1195, Batch 1000/1000: LR=3.56e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:04:16,968 | INFO | Epoch 1195 Train Time 31.805864334106445s

2025-10-15 03:04:47,904 | INFO | Training epoch 1196, Batch 1000/1000: LR=3.56e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:04:47,958 | INFO | Epoch 1196 Train Time 30.989265203475952s

2025-10-15 03:05:20,202 | INFO | Training epoch 1197, Batch 1000/1000: LR=3.55e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:05:20,258 | INFO | Epoch 1197 Train Time 32.29871082305908s

2025-10-15 03:05:51,030 | INFO | Training epoch 1198, Batch 1000/1000: LR=3.54e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 03:05:51,082 | INFO | Epoch 1198 Train Time 30.823150157928467s

2025-10-15 03:06:22,695 | INFO | Training epoch 1199, Batch 1000/1000: LR=3.54e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:06:22,746 | INFO | Epoch 1199 Train Time 31.663126230239868s

2025-10-15 03:06:53,917 | INFO | Training epoch 1200, Batch 1000/1000: LR=3.53e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:06:53,967 | INFO | Epoch 1200 Train Time 31.219826698303223s

2025-10-15 03:07:25,507 | INFO | Training epoch 1201, Batch 1000/1000: LR=3.52e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:07:25,559 | INFO | Epoch 1201 Train Time 31.59058690071106s

2025-10-15 03:07:57,019 | INFO | Training epoch 1202, Batch 1000/1000: LR=3.51e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:07:57,091 | INFO | Epoch 1202 Train Time 31.531704902648926s

2025-10-15 03:08:29,110 | INFO | Training epoch 1203, Batch 1000/1000: LR=3.51e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:08:29,163 | INFO | Epoch 1203 Train Time 32.06967639923096s

2025-10-15 03:09:00,006 | INFO | Training epoch 1204, Batch 1000/1000: LR=3.50e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:09:00,064 | INFO | Epoch 1204 Train Time 30.900208234786987s

2025-10-15 03:09:30,201 | INFO | Training epoch 1205, Batch 1000/1000: LR=3.49e-05, Loss=2.81e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:09:30,254 | INFO | Epoch 1205 Train Time 30.188318252563477s

2025-10-15 03:10:01,396 | INFO | Training epoch 1206, Batch 1000/1000: LR=3.48e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:10:01,445 | INFO | Epoch 1206 Train Time 31.19014048576355s

2025-10-15 03:10:32,613 | INFO | Training epoch 1207, Batch 1000/1000: LR=3.48e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:10:32,670 | INFO | Epoch 1207 Train Time 31.223875284194946s

2025-10-15 03:11:03,397 | INFO | Training epoch 1208, Batch 1000/1000: LR=3.47e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 03:11:03,445 | INFO | Epoch 1208 Train Time 30.77465796470642s

2025-10-15 03:11:35,104 | INFO | Training epoch 1209, Batch 1000/1000: LR=3.46e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 03:11:35,155 | INFO | Epoch 1209 Train Time 31.70870327949524s

2025-10-15 03:12:06,510 | INFO | Training epoch 1210, Batch 1000/1000: LR=3.45e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:12:06,561 | INFO | Epoch 1210 Train Time 31.40587544441223s

2025-10-15 03:12:37,706 | INFO | Training epoch 1211, Batch 1000/1000: LR=3.45e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:12:37,764 | INFO | Epoch 1211 Train Time 31.20164656639099s

2025-10-15 03:13:08,924 | INFO | Training epoch 1212, Batch 1000/1000: LR=3.44e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:13:08,981 | INFO | Epoch 1212 Train Time 31.21636462211609s

2025-10-15 03:13:40,305 | INFO | Training epoch 1213, Batch 1000/1000: LR=3.43e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 03:13:40,358 | INFO | Epoch 1213 Train Time 31.376391410827637s

2025-10-15 03:14:11,304 | INFO | Training epoch 1214, Batch 1000/1000: LR=3.42e-05, Loss=2.81e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 03:14:11,358 | INFO | Epoch 1214 Train Time 30.99936819076538s

2025-10-15 03:14:43,208 | INFO | Training epoch 1215, Batch 1000/1000: LR=3.42e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 03:14:43,266 | INFO | Epoch 1215 Train Time 31.90665054321289s

2025-10-15 03:15:14,200 | INFO | Training epoch 1216, Batch 1000/1000: LR=3.41e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:15:14,249 | INFO | Epoch 1216 Train Time 30.981687784194946s

2025-10-15 03:15:45,630 | INFO | Training epoch 1217, Batch 1000/1000: LR=3.40e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:15:45,711 | INFO | Epoch 1217 Train Time 31.462144136428833s

2025-10-15 03:16:17,593 | INFO | Training epoch 1218, Batch 1000/1000: LR=3.40e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:16:17,668 | INFO | Epoch 1218 Train Time 31.95579242706299s

2025-10-15 03:16:49,503 | INFO | Training epoch 1219, Batch 1000/1000: LR=3.39e-05, Loss=2.90e-02 BER=1.09e-02 FER=1.40e-01
2025-10-15 03:16:49,558 | INFO | Epoch 1219 Train Time 31.889655828475952s

2025-10-15 03:17:20,799 | INFO | Training epoch 1220, Batch 1000/1000: LR=3.38e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.36e-01
2025-10-15 03:17:20,851 | INFO | Epoch 1220 Train Time 31.291316509246826s

2025-10-15 03:17:52,217 | INFO | Training epoch 1221, Batch 1000/1000: LR=3.37e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:17:52,269 | INFO | Epoch 1221 Train Time 31.417313814163208s

2025-10-15 03:18:24,110 | INFO | Training epoch 1222, Batch 1000/1000: LR=3.37e-05, Loss=2.88e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:18:24,166 | INFO | Epoch 1222 Train Time 31.89634680747986s

2025-10-15 03:18:55,699 | INFO | Training epoch 1223, Batch 1000/1000: LR=3.36e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:18:55,753 | INFO | Epoch 1223 Train Time 31.586426734924316s

2025-10-15 03:19:26,495 | INFO | Training epoch 1224, Batch 1000/1000: LR=3.35e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:19:26,557 | INFO | Epoch 1224 Train Time 30.802629470825195s

2025-10-15 03:19:57,707 | INFO | Training epoch 1225, Batch 1000/1000: LR=3.34e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:19:57,766 | INFO | Epoch 1225 Train Time 31.207937717437744s

2025-10-15 03:20:29,728 | INFO | Training epoch 1226, Batch 1000/1000: LR=3.34e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:20:29,793 | INFO | Epoch 1226 Train Time 32.02612376213074s

2025-10-15 03:21:01,793 | INFO | Training epoch 1227, Batch 1000/1000: LR=3.33e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:21:01,866 | INFO | Epoch 1227 Train Time 32.071720361709595s

2025-10-15 03:21:36,039 | INFO | Training epoch 1228, Batch 1000/1000: LR=3.32e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:21:36,109 | INFO | Epoch 1228 Train Time 34.24123668670654s

2025-10-15 03:22:11,202 | INFO | Training epoch 1229, Batch 1000/1000: LR=3.31e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 03:22:11,260 | INFO | Epoch 1229 Train Time 35.1492121219635s

2025-10-15 03:22:44,588 | INFO | Training epoch 1230, Batch 1000/1000: LR=3.31e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:22:44,645 | INFO | Epoch 1230 Train Time 33.384554862976074s

2025-10-15 03:23:16,297 | INFO | Training epoch 1231, Batch 1000/1000: LR=3.30e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 03:23:16,355 | INFO | Epoch 1231 Train Time 31.708770751953125s

2025-10-15 03:23:49,216 | INFO | Training epoch 1232, Batch 1000/1000: LR=3.29e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 03:23:49,264 | INFO | Epoch 1232 Train Time 32.907963275909424s

2025-10-15 03:24:21,543 | INFO | Training epoch 1233, Batch 1000/1000: LR=3.29e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:24:21,615 | INFO | Epoch 1233 Train Time 32.35029697418213s

2025-10-15 03:24:54,393 | INFO | Training epoch 1234, Batch 1000/1000: LR=3.28e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:24:54,441 | INFO | Epoch 1234 Train Time 32.82482433319092s

2025-10-15 03:25:26,241 | INFO | Training epoch 1235, Batch 1000/1000: LR=3.27e-05, Loss=2.88e-02 BER=1.08e-02 FER=1.38e-01
2025-10-15 03:25:26,310 | INFO | Epoch 1235 Train Time 31.86888098716736s

2025-10-15 03:25:58,609 | INFO | Training epoch 1236, Batch 1000/1000: LR=3.26e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:25:58,680 | INFO | Epoch 1236 Train Time 32.368589639663696s

2025-10-15 03:26:31,000 | INFO | Training epoch 1237, Batch 1000/1000: LR=3.26e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:26:31,069 | INFO | Epoch 1237 Train Time 32.38819932937622s

2025-10-15 03:27:02,016 | INFO | Training epoch 1238, Batch 1000/1000: LR=3.25e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:27:02,074 | INFO | Epoch 1238 Train Time 31.004233837127686s

2025-10-15 03:27:34,004 | INFO | Training epoch 1239, Batch 1000/1000: LR=3.24e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:27:34,063 | INFO | Epoch 1239 Train Time 31.98798894882202s

2025-10-15 03:28:06,332 | INFO | Training epoch 1240, Batch 1000/1000: LR=3.24e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:28:06,402 | INFO | Epoch 1240 Train Time 32.3368444442749s

2025-10-15 03:28:38,709 | INFO | Training epoch 1241, Batch 1000/1000: LR=3.23e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:28:38,777 | INFO | Epoch 1241 Train Time 32.37446355819702s

2025-10-15 03:29:10,496 | INFO | Training epoch 1242, Batch 1000/1000: LR=3.22e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:29:10,562 | INFO | Epoch 1242 Train Time 31.78373956680298s

2025-10-15 03:29:42,506 | INFO | Training epoch 1243, Batch 1000/1000: LR=3.21e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:29:42,559 | INFO | Epoch 1243 Train Time 31.995722770690918s

2025-10-15 03:30:15,199 | INFO | Training epoch 1244, Batch 1000/1000: LR=3.21e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:30:15,245 | INFO | Epoch 1244 Train Time 32.68609952926636s

2025-10-15 03:30:48,016 | INFO | Training epoch 1245, Batch 1000/1000: LR=3.20e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:30:48,085 | INFO | Epoch 1245 Train Time 32.83865237236023s

2025-10-15 03:31:19,793 | INFO | Training epoch 1246, Batch 1000/1000: LR=3.19e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:31:19,843 | INFO | Epoch 1246 Train Time 31.755640983581543s

2025-10-15 03:31:52,105 | INFO | Training epoch 1247, Batch 1000/1000: LR=3.18e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:31:52,171 | INFO | Epoch 1247 Train Time 32.32707405090332s

2025-10-15 03:32:24,310 | INFO | Training epoch 1248, Batch 1000/1000: LR=3.18e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:32:24,369 | INFO | Epoch 1248 Train Time 32.197144508361816s

2025-10-15 03:32:56,597 | INFO | Training epoch 1249, Batch 1000/1000: LR=3.17e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:32:56,653 | INFO | Epoch 1249 Train Time 32.28335189819336s

2025-10-15 03:33:29,294 | INFO | Training epoch 1250, Batch 1000/1000: LR=3.16e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:33:29,361 | INFO | Epoch 1250 Train Time 32.707457542419434s

2025-10-15 03:34:02,500 | INFO | Training epoch 1251, Batch 1000/1000: LR=3.16e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:34:02,555 | INFO | Epoch 1251 Train Time 33.19287848472595s

2025-10-15 03:34:34,297 | INFO | Training epoch 1252, Batch 1000/1000: LR=3.15e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:34:34,341 | INFO | Epoch 1252 Train Time 31.785888671875s

2025-10-15 03:35:07,002 | INFO | Training epoch 1253, Batch 1000/1000: LR=3.14e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:35:07,054 | INFO | Epoch 1253 Train Time 32.71091675758362s

2025-10-15 03:35:39,826 | INFO | Training epoch 1254, Batch 1000/1000: LR=3.13e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:35:39,900 | INFO | Epoch 1254 Train Time 32.84577298164368s

2025-10-15 03:36:12,199 | INFO | Training epoch 1255, Batch 1000/1000: LR=3.13e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:36:12,251 | INFO | Epoch 1255 Train Time 32.34945273399353s

2025-10-15 03:36:44,217 | INFO | Training epoch 1256, Batch 1000/1000: LR=3.12e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:36:44,271 | INFO | Epoch 1256 Train Time 32.01953125s

2025-10-15 03:37:16,503 | INFO | Training epoch 1257, Batch 1000/1000: LR=3.11e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:37:16,552 | INFO | Epoch 1257 Train Time 32.280657052993774s

2025-10-15 03:37:48,523 | INFO | Training epoch 1258, Batch 1000/1000: LR=3.11e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:37:48,586 | INFO | Epoch 1258 Train Time 32.03241157531738s

2025-10-15 03:38:20,423 | INFO | Training epoch 1259, Batch 1000/1000: LR=3.10e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:38:20,485 | INFO | Epoch 1259 Train Time 31.898000240325928s

2025-10-15 03:38:52,001 | INFO | Training epoch 1260, Batch 1000/1000: LR=3.09e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:38:52,051 | INFO | Epoch 1260 Train Time 31.565314531326294s

2025-10-15 03:39:24,004 | INFO | Training epoch 1261, Batch 1000/1000: LR=3.08e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:39:24,065 | INFO | Epoch 1261 Train Time 32.01371741294861s

2025-10-15 03:39:56,116 | INFO | Training epoch 1262, Batch 1000/1000: LR=3.08e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:39:56,184 | INFO | Epoch 1262 Train Time 32.117090940475464s

2025-10-15 03:40:28,308 | INFO | Training epoch 1263, Batch 1000/1000: LR=3.07e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:40:28,371 | INFO | Epoch 1263 Train Time 32.185999155044556s

2025-10-15 03:41:00,311 | INFO | Training epoch 1264, Batch 1000/1000: LR=3.06e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:41:00,377 | INFO | Epoch 1264 Train Time 32.005115032196045s

2025-10-15 03:41:32,594 | INFO | Training epoch 1265, Batch 1000/1000: LR=3.06e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:41:32,636 | INFO | Epoch 1265 Train Time 32.25690293312073s

2025-10-15 03:42:05,004 | INFO | Training epoch 1266, Batch 1000/1000: LR=3.05e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:42:05,057 | INFO | Epoch 1266 Train Time 32.419023513793945s

2025-10-15 03:42:36,914 | INFO | Training epoch 1267, Batch 1000/1000: LR=3.04e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:42:36,962 | INFO | Epoch 1267 Train Time 31.904213666915894s

2025-10-15 03:43:08,315 | INFO | Training epoch 1268, Batch 1000/1000: LR=3.03e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:43:08,369 | INFO | Epoch 1268 Train Time 31.405742168426514s

2025-10-15 03:43:40,598 | INFO | Training epoch 1269, Batch 1000/1000: LR=3.03e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 03:43:40,655 | INFO | Epoch 1269 Train Time 32.28542923927307s

2025-10-15 03:44:13,001 | INFO | Training epoch 1270, Batch 1000/1000: LR=3.02e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:44:13,056 | INFO | Epoch 1270 Train Time 32.400036334991455s

2025-10-15 03:44:45,488 | INFO | Training epoch 1271, Batch 1000/1000: LR=3.01e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:44:45,545 | INFO | Epoch 1271 Train Time 32.48770761489868s

2025-10-15 03:45:18,300 | INFO | Training epoch 1272, Batch 1000/1000: LR=3.01e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:45:18,348 | INFO | Epoch 1272 Train Time 32.80222749710083s

2025-10-15 03:45:49,323 | INFO | Training epoch 1273, Batch 1000/1000: LR=3.00e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 03:45:49,366 | INFO | Epoch 1273 Train Time 31.017732858657837s

2025-10-15 03:46:21,089 | INFO | Training epoch 1274, Batch 1000/1000: LR=2.99e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:46:21,162 | INFO | Epoch 1274 Train Time 31.79497003555298s

2025-10-15 03:46:53,396 | INFO | Training epoch 1275, Batch 1000/1000: LR=2.98e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:46:53,455 | INFO | Epoch 1275 Train Time 32.29167032241821s

2025-10-15 03:47:26,591 | INFO | Training epoch 1276, Batch 1000/1000: LR=2.98e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:47:26,644 | INFO | Epoch 1276 Train Time 33.18730974197388s

2025-10-15 03:47:59,212 | INFO | Training epoch 1277, Batch 1000/1000: LR=2.97e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:47:59,265 | INFO | Epoch 1277 Train Time 32.62004637718201s

2025-10-15 03:48:31,315 | INFO | Training epoch 1278, Batch 1000/1000: LR=2.96e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 03:48:31,383 | INFO | Epoch 1278 Train Time 32.117106437683105s

2025-10-15 03:49:04,863 | INFO | Training epoch 1279, Batch 1000/1000: LR=2.96e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:49:04,935 | INFO | Epoch 1279 Train Time 33.55145716667175s

2025-10-15 03:49:41,281 | INFO | Training epoch 1280, Batch 1000/1000: LR=2.95e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:49:41,351 | INFO | Epoch 1280 Train Time 36.41418647766113s

2025-10-15 03:50:17,437 | INFO | Training epoch 1281, Batch 1000/1000: LR=2.94e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:50:17,524 | INFO | Epoch 1281 Train Time 36.17183256149292s

2025-10-15 03:50:54,040 | INFO | Training epoch 1282, Batch 1000/1000: LR=2.94e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 03:50:54,127 | INFO | Epoch 1282 Train Time 36.60225439071655s

2025-10-15 03:51:31,158 | INFO | Training epoch 1283, Batch 1000/1000: LR=2.93e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:51:31,227 | INFO | Epoch 1283 Train Time 37.097591161727905s

2025-10-15 03:52:08,230 | INFO | Training epoch 1284, Batch 1000/1000: LR=2.92e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:52:08,297 | INFO | Epoch 1284 Train Time 37.067567110061646s

2025-10-15 03:52:44,745 | INFO | Training epoch 1285, Batch 1000/1000: LR=2.91e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:52:44,810 | INFO | Epoch 1285 Train Time 36.511929512023926s

2025-10-15 03:53:21,038 | INFO | Training epoch 1286, Batch 1000/1000: LR=2.91e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 03:53:21,112 | INFO | Epoch 1286 Train Time 36.30140948295593s

2025-10-15 03:53:21,113 | INFO | [P2] saving best_model (QAT) with loss 0.027643 at epoch 1286
2025-10-15 03:53:58,128 | INFO | Training epoch 1287, Batch 1000/1000: LR=2.90e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 03:53:58,216 | INFO | Epoch 1287 Train Time 37.076653242111206s

2025-10-15 03:54:35,251 | INFO | Training epoch 1288, Batch 1000/1000: LR=2.89e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:54:35,316 | INFO | Epoch 1288 Train Time 37.09893846511841s

2025-10-15 03:55:11,931 | INFO | Training epoch 1289, Batch 1000/1000: LR=2.89e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 03:55:11,997 | INFO | Epoch 1289 Train Time 36.68051719665527s

2025-10-15 03:55:47,846 | INFO | Training epoch 1290, Batch 1000/1000: LR=2.88e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 03:55:47,920 | INFO | Epoch 1290 Train Time 35.922144651412964s

2025-10-15 03:56:24,234 | INFO | Training epoch 1291, Batch 1000/1000: LR=2.87e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 03:56:24,308 | INFO | Epoch 1291 Train Time 36.38656711578369s

2025-10-15 03:57:00,440 | INFO | Training epoch 1292, Batch 1000/1000: LR=2.87e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:57:00,523 | INFO | Epoch 1292 Train Time 36.21365427970886s

2025-10-15 03:57:37,348 | INFO | Training epoch 1293, Batch 1000/1000: LR=2.86e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 03:57:37,439 | INFO | Epoch 1293 Train Time 36.91600203514099s

2025-10-15 03:58:14,337 | INFO | Training epoch 1294, Batch 1000/1000: LR=2.85e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:58:14,424 | INFO | Epoch 1294 Train Time 36.98360776901245s

2025-10-15 03:58:51,086 | INFO | Training epoch 1295, Batch 1000/1000: LR=2.84e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 03:58:51,175 | INFO | Epoch 1295 Train Time 36.74974751472473s

2025-10-15 03:59:27,152 | INFO | Training epoch 1296, Batch 1000/1000: LR=2.84e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 03:59:27,220 | INFO | Epoch 1296 Train Time 36.04346561431885s

2025-10-15 04:00:03,841 | INFO | Training epoch 1297, Batch 1000/1000: LR=2.83e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:00:03,908 | INFO | Epoch 1297 Train Time 36.686925411224365s

2025-10-15 04:00:40,735 | INFO | Training epoch 1298, Batch 1000/1000: LR=2.82e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:00:40,798 | INFO | Epoch 1298 Train Time 36.889086961746216s

2025-10-15 04:01:17,638 | INFO | Training epoch 1299, Batch 1000/1000: LR=2.82e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:01:17,711 | INFO | Epoch 1299 Train Time 36.911351442337036s

2025-10-15 04:01:54,538 | INFO | Training epoch 1300, Batch 1000/1000: LR=2.81e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:01:54,609 | INFO | Epoch 1300 Train Time 36.89699840545654s

2025-10-15 04:02:31,452 | INFO | Training epoch 1301, Batch 1000/1000: LR=2.80e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:02:31,518 | INFO | Epoch 1301 Train Time 36.9078586101532s

2025-10-15 04:03:08,060 | INFO | Training epoch 1302, Batch 1000/1000: LR=2.80e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 04:03:08,128 | INFO | Epoch 1302 Train Time 36.608776807785034s

2025-10-15 04:03:43,536 | INFO | Training epoch 1303, Batch 1000/1000: LR=2.79e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:03:43,619 | INFO | Epoch 1303 Train Time 35.489049434661865s

2025-10-15 04:04:20,436 | INFO | Training epoch 1304, Batch 1000/1000: LR=2.78e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:04:20,520 | INFO | Epoch 1304 Train Time 36.90029835700989s

2025-10-15 04:04:57,725 | INFO | Training epoch 1305, Batch 1000/1000: LR=2.78e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:04:57,793 | INFO | Epoch 1305 Train Time 37.27195978164673s

2025-10-15 04:05:35,340 | INFO | Training epoch 1306, Batch 1000/1000: LR=2.77e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:05:35,432 | INFO | Epoch 1306 Train Time 37.637951374053955s

2025-10-15 04:06:12,045 | INFO | Training epoch 1307, Batch 1000/1000: LR=2.76e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:06:12,117 | INFO | Epoch 1307 Train Time 36.6841344833374s

2025-10-15 04:06:48,941 | INFO | Training epoch 1308, Batch 1000/1000: LR=2.75e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 04:06:49,030 | INFO | Epoch 1308 Train Time 36.910993576049805s

2025-10-15 04:07:25,922 | INFO | Training epoch 1309, Batch 1000/1000: LR=2.75e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:07:26,006 | INFO | Epoch 1309 Train Time 36.97393202781677s

2025-10-15 04:08:03,152 | INFO | Training epoch 1310, Batch 1000/1000: LR=2.74e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:08:03,221 | INFO | Epoch 1310 Train Time 37.21427893638611s

2025-10-15 04:08:40,162 | INFO | Training epoch 1311, Batch 1000/1000: LR=2.73e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:08:40,242 | INFO | Epoch 1311 Train Time 37.02049279212952s

2025-10-15 04:09:16,341 | INFO | Training epoch 1312, Batch 1000/1000: LR=2.73e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:09:16,429 | INFO | Epoch 1312 Train Time 36.18490982055664s

2025-10-15 04:09:53,055 | INFO | Training epoch 1313, Batch 1000/1000: LR=2.72e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:09:53,145 | INFO | Epoch 1313 Train Time 36.714436054229736s

2025-10-15 04:10:29,704 | INFO | Training epoch 1314, Batch 1000/1000: LR=2.71e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.36e-01
2025-10-15 04:10:29,788 | INFO | Epoch 1314 Train Time 36.64162611961365s

2025-10-15 04:11:06,165 | INFO | Training epoch 1315, Batch 1000/1000: LR=2.71e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 04:11:06,234 | INFO | Epoch 1315 Train Time 36.44445776939392s

2025-10-15 04:11:43,134 | INFO | Training epoch 1316, Batch 1000/1000: LR=2.70e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 04:11:43,202 | INFO | Epoch 1316 Train Time 36.96616864204407s

2025-10-15 04:12:20,335 | INFO | Training epoch 1317, Batch 1000/1000: LR=2.69e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:12:20,428 | INFO | Epoch 1317 Train Time 37.22606897354126s

2025-10-15 04:12:56,933 | INFO | Training epoch 1318, Batch 1000/1000: LR=2.69e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:12:57,016 | INFO | Epoch 1318 Train Time 36.585740089416504s

2025-10-15 04:13:34,035 | INFO | Training epoch 1319, Batch 1000/1000: LR=2.68e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 04:13:34,108 | INFO | Epoch 1319 Train Time 37.08947539329529s

2025-10-15 04:14:10,948 | INFO | Training epoch 1320, Batch 1000/1000: LR=2.67e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:14:11,039 | INFO | Epoch 1320 Train Time 36.931116580963135s

2025-10-15 04:14:47,632 | INFO | Training epoch 1321, Batch 1000/1000: LR=2.67e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 04:14:47,709 | INFO | Epoch 1321 Train Time 36.66846942901611s

2025-10-15 04:15:24,698 | INFO | Training epoch 1322, Batch 1000/1000: LR=2.66e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:15:24,766 | INFO | Epoch 1322 Train Time 37.05540490150452s

2025-10-15 04:16:01,484 | INFO | Training epoch 1323, Batch 1000/1000: LR=2.65e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:16:01,582 | INFO | Epoch 1323 Train Time 36.81421136856079s

2025-10-15 04:16:38,337 | INFO | Training epoch 1324, Batch 1000/1000: LR=2.64e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:16:38,417 | INFO | Epoch 1324 Train Time 36.83303141593933s

2025-10-15 04:17:15,332 | INFO | Training epoch 1325, Batch 1000/1000: LR=2.64e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:17:15,427 | INFO | Epoch 1325 Train Time 37.00852870941162s

2025-10-15 04:17:52,232 | INFO | Training epoch 1326, Batch 1000/1000: LR=2.63e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 04:17:52,321 | INFO | Epoch 1326 Train Time 36.893447160720825s

2025-10-15 04:18:29,252 | INFO | Training epoch 1327, Batch 1000/1000: LR=2.62e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:18:29,318 | INFO | Epoch 1327 Train Time 36.995707511901855s

2025-10-15 04:19:06,028 | INFO | Training epoch 1328, Batch 1000/1000: LR=2.62e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:19:06,120 | INFO | Epoch 1328 Train Time 36.800440311431885s

2025-10-15 04:19:42,629 | INFO | Training epoch 1329, Batch 1000/1000: LR=2.61e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:19:42,725 | INFO | Epoch 1329 Train Time 36.60285234451294s

2025-10-15 04:20:19,557 | INFO | Training epoch 1330, Batch 1000/1000: LR=2.60e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:20:19,639 | INFO | Epoch 1330 Train Time 36.91297912597656s

2025-10-15 04:20:57,190 | INFO | Training epoch 1331, Batch 1000/1000: LR=2.60e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 04:20:57,262 | INFO | Epoch 1331 Train Time 37.62171769142151s

2025-10-15 04:21:34,637 | INFO | Training epoch 1332, Batch 1000/1000: LR=2.59e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:21:34,732 | INFO | Epoch 1332 Train Time 37.46822643280029s

2025-10-15 04:22:11,535 | INFO | Training epoch 1333, Batch 1000/1000: LR=2.58e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 04:22:11,625 | INFO | Epoch 1333 Train Time 36.89242100715637s

2025-10-15 04:22:48,938 | INFO | Training epoch 1334, Batch 1000/1000: LR=2.58e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:22:49,009 | INFO | Epoch 1334 Train Time 37.382811307907104s

2025-10-15 04:23:25,931 | INFO | Training epoch 1335, Batch 1000/1000: LR=2.57e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:23:26,023 | INFO | Epoch 1335 Train Time 37.01328659057617s

2025-10-15 04:24:02,951 | INFO | Training epoch 1336, Batch 1000/1000: LR=2.56e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:24:03,042 | INFO | Epoch 1336 Train Time 37.0179340839386s

2025-10-15 04:24:39,440 | INFO | Training epoch 1337, Batch 1000/1000: LR=2.56e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:24:39,510 | INFO | Epoch 1337 Train Time 36.46576762199402s

2025-10-15 04:25:16,141 | INFO | Training epoch 1338, Batch 1000/1000: LR=2.55e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 04:25:16,219 | INFO | Epoch 1338 Train Time 36.706252336502075s

2025-10-15 04:25:52,420 | INFO | Training epoch 1339, Batch 1000/1000: LR=2.54e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:25:52,492 | INFO | Epoch 1339 Train Time 36.27159929275513s

2025-10-15 04:26:29,233 | INFO | Training epoch 1340, Batch 1000/1000: LR=2.54e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:26:29,302 | INFO | Epoch 1340 Train Time 36.80923056602478s

2025-10-15 04:27:05,948 | INFO | Training epoch 1341, Batch 1000/1000: LR=2.53e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:27:06,030 | INFO | Epoch 1341 Train Time 36.727774143218994s

2025-10-15 04:27:42,336 | INFO | Training epoch 1342, Batch 1000/1000: LR=2.52e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:27:42,432 | INFO | Epoch 1342 Train Time 36.40000295639038s

2025-10-15 04:28:18,649 | INFO | Training epoch 1343, Batch 1000/1000: LR=2.52e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 04:28:18,726 | INFO | Epoch 1343 Train Time 36.293617725372314s

2025-10-15 04:28:55,833 | INFO | Training epoch 1344, Batch 1000/1000: LR=2.51e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:28:55,911 | INFO | Epoch 1344 Train Time 37.182427406311035s

2025-10-15 04:29:32,433 | INFO | Training epoch 1345, Batch 1000/1000: LR=2.50e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 04:29:32,518 | INFO | Epoch 1345 Train Time 36.60549306869507s

2025-10-15 04:30:09,240 | INFO | Training epoch 1346, Batch 1000/1000: LR=2.50e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 04:30:09,325 | INFO | Epoch 1346 Train Time 36.80523347854614s

2025-10-15 04:30:46,131 | INFO | Training epoch 1347, Batch 1000/1000: LR=2.49e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:30:46,208 | INFO | Epoch 1347 Train Time 36.8808274269104s

2025-10-15 04:31:22,632 | INFO | Training epoch 1348, Batch 1000/1000: LR=2.48e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:31:22,724 | INFO | Epoch 1348 Train Time 36.514639139175415s

2025-10-15 04:31:57,957 | INFO | Training epoch 1349, Batch 1000/1000: LR=2.48e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:31:58,042 | INFO | Epoch 1349 Train Time 35.3170268535614s

2025-10-15 04:32:34,853 | INFO | Training epoch 1350, Batch 1000/1000: LR=2.47e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:32:34,928 | INFO | Epoch 1350 Train Time 36.8842499256134s

2025-10-15 04:33:11,661 | INFO | Training epoch 1351, Batch 1000/1000: LR=2.46e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 04:33:11,732 | INFO | Epoch 1351 Train Time 36.80334806442261s

2025-10-15 04:33:48,037 | INFO | Training epoch 1352, Batch 1000/1000: LR=2.46e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 04:33:48,112 | INFO | Epoch 1352 Train Time 36.37816524505615s

2025-10-15 04:34:24,800 | INFO | Training epoch 1353, Batch 1000/1000: LR=2.45e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 04:34:24,898 | INFO | Epoch 1353 Train Time 36.78441524505615s

2025-10-15 04:35:01,535 | INFO | Training epoch 1354, Batch 1000/1000: LR=2.44e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:35:01,609 | INFO | Epoch 1354 Train Time 36.70981001853943s

2025-10-15 04:35:38,232 | INFO | Training epoch 1355, Batch 1000/1000: LR=2.44e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:35:38,316 | INFO | Epoch 1355 Train Time 36.70620918273926s

2025-10-15 04:36:15,335 | INFO | Training epoch 1356, Batch 1000/1000: LR=2.43e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.36e-01
2025-10-15 04:36:15,418 | INFO | Epoch 1356 Train Time 37.101643323898315s

2025-10-15 04:36:52,456 | INFO | Training epoch 1357, Batch 1000/1000: LR=2.42e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 04:36:52,534 | INFO | Epoch 1357 Train Time 37.113380432128906s

2025-10-15 04:37:29,254 | INFO | Training epoch 1358, Batch 1000/1000: LR=2.42e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:37:29,324 | INFO | Epoch 1358 Train Time 36.78859567642212s

2025-10-15 04:38:06,238 | INFO | Training epoch 1359, Batch 1000/1000: LR=2.41e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 04:38:06,325 | INFO | Epoch 1359 Train Time 36.999059438705444s

2025-10-15 04:38:42,633 | INFO | Training epoch 1360, Batch 1000/1000: LR=2.40e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:38:42,698 | INFO | Epoch 1360 Train Time 36.37207746505737s

2025-10-15 04:39:19,188 | INFO | Training epoch 1361, Batch 1000/1000: LR=2.40e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 04:39:19,284 | INFO | Epoch 1361 Train Time 36.58583736419678s

2025-10-15 04:39:55,735 | INFO | Training epoch 1362, Batch 1000/1000: LR=2.39e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:39:55,830 | INFO | Epoch 1362 Train Time 36.54506039619446s

2025-10-15 04:40:32,689 | INFO | Training epoch 1363, Batch 1000/1000: LR=2.38e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:40:32,783 | INFO | Epoch 1363 Train Time 36.952237606048584s

2025-10-15 04:41:09,537 | INFO | Training epoch 1364, Batch 1000/1000: LR=2.38e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:41:09,609 | INFO | Epoch 1364 Train Time 36.82444405555725s

2025-10-15 04:41:46,435 | INFO | Training epoch 1365, Batch 1000/1000: LR=2.37e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 04:41:46,523 | INFO | Epoch 1365 Train Time 36.91232919692993s

2025-10-15 04:41:46,523 | INFO | [P2] saving best_model (QAT) with loss 0.027632 at epoch 1365
2025-10-15 04:42:22,137 | INFO | Training epoch 1366, Batch 1000/1000: LR=2.36e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:42:22,227 | INFO | Epoch 1366 Train Time 35.68269920349121s

2025-10-15 04:42:59,197 | INFO | Training epoch 1367, Batch 1000/1000: LR=2.36e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:42:59,276 | INFO | Epoch 1367 Train Time 37.04762387275696s

2025-10-15 04:43:35,728 | INFO | Training epoch 1368, Batch 1000/1000: LR=2.35e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.35e-01
2025-10-15 04:43:35,812 | INFO | Epoch 1368 Train Time 36.535070180892944s

2025-10-15 04:44:12,235 | INFO | Training epoch 1369, Batch 1000/1000: LR=2.35e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:44:12,328 | INFO | Epoch 1369 Train Time 36.51395082473755s

2025-10-15 04:44:49,190 | INFO | Training epoch 1370, Batch 1000/1000: LR=2.34e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 04:44:49,287 | INFO | Epoch 1370 Train Time 36.95665383338928s

2025-10-15 04:44:49,288 | INFO | [P2] saving best_model (QAT) with loss 0.027429 at epoch 1370
2025-10-15 04:45:26,351 | INFO | Training epoch 1371, Batch 1000/1000: LR=2.33e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:45:26,435 | INFO | Epoch 1371 Train Time 37.12984347343445s

2025-10-15 04:46:03,135 | INFO | Training epoch 1372, Batch 1000/1000: LR=2.33e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 04:46:03,217 | INFO | Epoch 1372 Train Time 36.780439138412476s

2025-10-15 04:46:39,621 | INFO | Training epoch 1373, Batch 1000/1000: LR=2.32e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:46:39,699 | INFO | Epoch 1373 Train Time 36.48070955276489s

2025-10-15 04:47:16,523 | INFO | Training epoch 1374, Batch 1000/1000: LR=2.31e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:47:16,610 | INFO | Epoch 1374 Train Time 36.9101448059082s

2025-10-15 04:47:53,026 | INFO | Training epoch 1375, Batch 1000/1000: LR=2.31e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 04:47:53,102 | INFO | Epoch 1375 Train Time 36.49082922935486s

2025-10-15 04:48:29,820 | INFO | Training epoch 1376, Batch 1000/1000: LR=2.30e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.36e-01
2025-10-15 04:48:29,885 | INFO | Epoch 1376 Train Time 36.781822204589844s

2025-10-15 04:49:06,643 | INFO | Training epoch 1377, Batch 1000/1000: LR=2.29e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:49:06,723 | INFO | Epoch 1377 Train Time 36.83684825897217s

2025-10-15 04:49:43,425 | INFO | Training epoch 1378, Batch 1000/1000: LR=2.29e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:49:43,509 | INFO | Epoch 1378 Train Time 36.785459995269775s

2025-10-15 04:50:20,441 | INFO | Training epoch 1379, Batch 1000/1000: LR=2.28e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 04:50:20,539 | INFO | Epoch 1379 Train Time 37.0280282497406s

2025-10-15 04:50:56,439 | INFO | Training epoch 1380, Batch 1000/1000: LR=2.27e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 04:50:56,523 | INFO | Epoch 1380 Train Time 35.983439445495605s

2025-10-15 04:51:33,531 | INFO | Training epoch 1381, Batch 1000/1000: LR=2.27e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:51:33,620 | INFO | Epoch 1381 Train Time 37.09536027908325s

2025-10-15 04:52:10,262 | INFO | Training epoch 1382, Batch 1000/1000: LR=2.26e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.35e-01
2025-10-15 04:52:10,348 | INFO | Epoch 1382 Train Time 36.727396965026855s

2025-10-15 04:52:46,141 | INFO | Training epoch 1383, Batch 1000/1000: LR=2.25e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 04:52:46,225 | INFO | Epoch 1383 Train Time 35.87586736679077s

2025-10-15 04:53:22,440 | INFO | Training epoch 1384, Batch 1000/1000: LR=2.25e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:53:22,525 | INFO | Epoch 1384 Train Time 36.29928779602051s

2025-10-15 04:53:58,934 | INFO | Training epoch 1385, Batch 1000/1000: LR=2.24e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 04:53:59,007 | INFO | Epoch 1385 Train Time 36.48034453392029s

2025-10-15 04:54:35,334 | INFO | Training epoch 1386, Batch 1000/1000: LR=2.24e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:54:35,423 | INFO | Epoch 1386 Train Time 36.414265155792236s

2025-10-15 04:55:12,235 | INFO | Training epoch 1387, Batch 1000/1000: LR=2.23e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 04:55:12,314 | INFO | Epoch 1387 Train Time 36.889108180999756s

2025-10-15 04:55:49,169 | INFO | Training epoch 1388, Batch 1000/1000: LR=2.22e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:55:49,251 | INFO | Epoch 1388 Train Time 36.935617446899414s

2025-10-15 04:56:25,969 | INFO | Training epoch 1389, Batch 1000/1000: LR=2.22e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:56:26,043 | INFO | Epoch 1389 Train Time 36.79145932197571s

2025-10-15 04:57:02,738 | INFO | Training epoch 1390, Batch 1000/1000: LR=2.21e-05, Loss=2.85e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 04:57:02,833 | INFO | Epoch 1390 Train Time 36.788647174835205s

2025-10-15 04:57:39,531 | INFO | Training epoch 1391, Batch 1000/1000: LR=2.20e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 04:57:39,616 | INFO | Epoch 1391 Train Time 36.78223252296448s

2025-10-15 04:58:17,144 | INFO | Training epoch 1392, Batch 1000/1000: LR=2.20e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 04:58:17,228 | INFO | Epoch 1392 Train Time 37.609519481658936s

2025-10-15 04:58:53,532 | INFO | Training epoch 1393, Batch 1000/1000: LR=2.19e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:58:53,600 | INFO | Epoch 1393 Train Time 36.37045454978943s

2025-10-15 04:59:30,724 | INFO | Training epoch 1394, Batch 1000/1000: LR=2.18e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 04:59:30,806 | INFO | Epoch 1394 Train Time 37.2054181098938s

2025-10-15 05:00:07,440 | INFO | Training epoch 1395, Batch 1000/1000: LR=2.18e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:00:07,531 | INFO | Epoch 1395 Train Time 36.723150968551636s

2025-10-15 05:00:44,355 | INFO | Training epoch 1396, Batch 1000/1000: LR=2.17e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:00:44,437 | INFO | Epoch 1396 Train Time 36.90488934516907s

2025-10-15 05:01:21,329 | INFO | Training epoch 1397, Batch 1000/1000: LR=2.17e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:01:21,425 | INFO | Epoch 1397 Train Time 36.986305236816406s

2025-10-15 05:01:57,531 | INFO | Training epoch 1398, Batch 1000/1000: LR=2.16e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:01:57,622 | INFO | Epoch 1398 Train Time 36.194966077804565s

2025-10-15 05:02:34,448 | INFO | Training epoch 1399, Batch 1000/1000: LR=2.15e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:02:34,525 | INFO | Epoch 1399 Train Time 36.90245223045349s

2025-10-15 05:03:11,034 | INFO | Training epoch 1400, Batch 1000/1000: LR=2.15e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:03:11,123 | INFO | Epoch 1400 Train Time 36.59667253494263s

2025-10-15 05:03:47,534 | INFO | Training epoch 1401, Batch 1000/1000: LR=2.14e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:03:47,612 | INFO | Epoch 1401 Train Time 36.48723340034485s

2025-10-15 05:04:23,942 | INFO | Training epoch 1402, Batch 1000/1000: LR=2.13e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:04:24,027 | INFO | Epoch 1402 Train Time 36.41436147689819s

2025-10-15 05:05:00,630 | INFO | Training epoch 1403, Batch 1000/1000: LR=2.13e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 05:05:00,702 | INFO | Epoch 1403 Train Time 36.6741156578064s

2025-10-15 05:05:37,380 | INFO | Training epoch 1404, Batch 1000/1000: LR=2.12e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:05:37,452 | INFO | Epoch 1404 Train Time 36.74896430969238s

2025-10-15 05:06:14,359 | INFO | Training epoch 1405, Batch 1000/1000: LR=2.12e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 05:06:14,452 | INFO | Epoch 1405 Train Time 36.997209310531616s

2025-10-15 05:06:50,940 | INFO | Training epoch 1406, Batch 1000/1000: LR=2.11e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 05:06:51,023 | INFO | Epoch 1406 Train Time 36.57015037536621s

2025-10-15 05:07:28,155 | INFO | Training epoch 1407, Batch 1000/1000: LR=2.10e-05, Loss=2.81e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 05:07:28,238 | INFO | Epoch 1407 Train Time 37.21317672729492s

2025-10-15 05:08:05,281 | INFO | Training epoch 1408, Batch 1000/1000: LR=2.10e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 05:08:05,359 | INFO | Epoch 1408 Train Time 37.11901903152466s

2025-10-15 05:08:41,231 | INFO | Training epoch 1409, Batch 1000/1000: LR=2.09e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:08:41,304 | INFO | Epoch 1409 Train Time 35.94235110282898s

2025-10-15 05:09:17,821 | INFO | Training epoch 1410, Batch 1000/1000: LR=2.08e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:09:17,898 | INFO | Epoch 1410 Train Time 36.591108322143555s

2025-10-15 05:09:54,833 | INFO | Training epoch 1411, Batch 1000/1000: LR=2.08e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.35e-01
2025-10-15 05:09:54,920 | INFO | Epoch 1411 Train Time 37.020710945129395s

2025-10-15 05:10:31,734 | INFO | Training epoch 1412, Batch 1000/1000: LR=2.07e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:10:31,824 | INFO | Epoch 1412 Train Time 36.90369749069214s

2025-10-15 05:11:08,482 | INFO | Training epoch 1413, Batch 1000/1000: LR=2.07e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 05:11:08,573 | INFO | Epoch 1413 Train Time 36.747450828552246s

2025-10-15 05:11:45,439 | INFO | Training epoch 1414, Batch 1000/1000: LR=2.06e-05, Loss=2.87e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 05:11:45,538 | INFO | Epoch 1414 Train Time 36.96431827545166s

2025-10-15 05:12:22,053 | INFO | Training epoch 1415, Batch 1000/1000: LR=2.05e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:12:22,123 | INFO | Epoch 1415 Train Time 36.583149671554565s

2025-10-15 05:12:57,843 | INFO | Training epoch 1416, Batch 1000/1000: LR=2.05e-05, Loss=2.84e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:12:57,920 | INFO | Epoch 1416 Train Time 35.79480791091919s

2025-10-15 05:13:34,355 | INFO | Training epoch 1417, Batch 1000/1000: LR=2.04e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:13:34,426 | INFO | Epoch 1417 Train Time 36.50461435317993s

2025-10-15 05:14:10,863 | INFO | Training epoch 1418, Batch 1000/1000: LR=2.03e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:14:10,955 | INFO | Epoch 1418 Train Time 36.52791905403137s

2025-10-15 05:14:47,479 | INFO | Training epoch 1419, Batch 1000/1000: LR=2.03e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 05:14:47,561 | INFO | Epoch 1419 Train Time 36.60481071472168s

2025-10-15 05:15:25,641 | INFO | Training epoch 1420, Batch 1000/1000: LR=2.02e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:15:25,732 | INFO | Epoch 1420 Train Time 38.16989755630493s

2025-10-15 05:16:02,437 | INFO | Training epoch 1421, Batch 1000/1000: LR=2.02e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:16:02,522 | INFO | Epoch 1421 Train Time 36.787434816360474s

2025-10-15 05:16:38,737 | INFO | Training epoch 1422, Batch 1000/1000: LR=2.01e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 05:16:38,837 | INFO | Epoch 1422 Train Time 36.31381154060364s

2025-10-15 05:17:15,565 | INFO | Training epoch 1423, Batch 1000/1000: LR=2.00e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:17:15,656 | INFO | Epoch 1423 Train Time 36.81775736808777s

2025-10-15 05:17:52,652 | INFO | Training epoch 1424, Batch 1000/1000: LR=2.00e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 05:17:52,734 | INFO | Epoch 1424 Train Time 37.07699513435364s

2025-10-15 05:18:29,138 | INFO | Training epoch 1425, Batch 1000/1000: LR=1.99e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:18:29,209 | INFO | Epoch 1425 Train Time 36.4731822013855s

2025-10-15 05:19:05,526 | INFO | Training epoch 1426, Batch 1000/1000: LR=1.99e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:19:05,600 | INFO | Epoch 1426 Train Time 36.38881492614746s

2025-10-15 05:19:41,989 | INFO | Training epoch 1427, Batch 1000/1000: LR=1.98e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:19:42,081 | INFO | Epoch 1427 Train Time 36.4803900718689s

2025-10-15 05:20:19,040 | INFO | Training epoch 1428, Batch 1000/1000: LR=1.97e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.35e-01
2025-10-15 05:20:19,120 | INFO | Epoch 1428 Train Time 37.037811279296875s

2025-10-15 05:20:55,436 | INFO | Training epoch 1429, Batch 1000/1000: LR=1.97e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:20:55,507 | INFO | Epoch 1429 Train Time 36.385122537612915s

2025-10-15 05:21:31,732 | INFO | Training epoch 1430, Batch 1000/1000: LR=1.96e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:21:31,830 | INFO | Epoch 1430 Train Time 36.32108211517334s

2025-10-15 05:22:08,056 | INFO | Training epoch 1431, Batch 1000/1000: LR=1.96e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:22:08,153 | INFO | Epoch 1431 Train Time 36.32173800468445s

2025-10-15 05:22:44,837 | INFO | Training epoch 1432, Batch 1000/1000: LR=1.95e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 05:22:44,918 | INFO | Epoch 1432 Train Time 36.763330936431885s

2025-10-15 05:23:21,464 | INFO | Training epoch 1433, Batch 1000/1000: LR=1.94e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:23:21,535 | INFO | Epoch 1433 Train Time 36.61685228347778s

2025-10-15 05:23:57,943 | INFO | Training epoch 1434, Batch 1000/1000: LR=1.94e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 05:23:58,049 | INFO | Epoch 1434 Train Time 36.51211452484131s

2025-10-15 05:24:34,135 | INFO | Training epoch 1435, Batch 1000/1000: LR=1.93e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:24:34,229 | INFO | Epoch 1435 Train Time 36.17843294143677s

2025-10-15 05:25:10,635 | INFO | Training epoch 1436, Batch 1000/1000: LR=1.92e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:25:10,714 | INFO | Epoch 1436 Train Time 36.48327684402466s

2025-10-15 05:25:47,131 | INFO | Training epoch 1437, Batch 1000/1000: LR=1.92e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:25:47,196 | INFO | Epoch 1437 Train Time 36.48021197319031s

2025-10-15 05:26:23,827 | INFO | Training epoch 1438, Batch 1000/1000: LR=1.91e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 05:26:23,918 | INFO | Epoch 1438 Train Time 36.721407413482666s

2025-10-15 05:27:00,238 | INFO | Training epoch 1439, Batch 1000/1000: LR=1.91e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:27:00,331 | INFO | Epoch 1439 Train Time 36.41215419769287s

2025-10-15 05:27:37,159 | INFO | Training epoch 1440, Batch 1000/1000: LR=1.90e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:27:37,247 | INFO | Epoch 1440 Train Time 36.913471937179565s

2025-10-15 05:28:14,136 | INFO | Training epoch 1441, Batch 1000/1000: LR=1.89e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:28:14,214 | INFO | Epoch 1441 Train Time 36.96488332748413s

2025-10-15 05:28:50,140 | INFO | Training epoch 1442, Batch 1000/1000: LR=1.89e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:28:50,214 | INFO | Epoch 1442 Train Time 35.99925684928894s

2025-10-15 05:29:27,534 | INFO | Training epoch 1443, Batch 1000/1000: LR=1.88e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:29:27,632 | INFO | Epoch 1443 Train Time 37.416707277297974s

2025-10-15 05:30:04,244 | INFO | Training epoch 1444, Batch 1000/1000: LR=1.88e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:30:04,336 | INFO | Epoch 1444 Train Time 36.70259642601013s

2025-10-15 05:30:41,331 | INFO | Training epoch 1445, Batch 1000/1000: LR=1.87e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:30:41,404 | INFO | Epoch 1445 Train Time 37.06599140167236s

2025-10-15 05:31:18,536 | INFO | Training epoch 1446, Batch 1000/1000: LR=1.86e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:31:18,612 | INFO | Epoch 1446 Train Time 37.207558393478394s

2025-10-15 05:31:55,855 | INFO | Training epoch 1447, Batch 1000/1000: LR=1.86e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:31:55,933 | INFO | Epoch 1447 Train Time 37.31947302818298s

2025-10-15 05:32:32,868 | INFO | Training epoch 1448, Batch 1000/1000: LR=1.85e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:32:32,955 | INFO | Epoch 1448 Train Time 37.021629333496094s

2025-10-15 05:33:10,039 | INFO | Training epoch 1449, Batch 1000/1000: LR=1.85e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:33:10,139 | INFO | Epoch 1449 Train Time 37.18311882019043s

2025-10-15 05:33:47,038 | INFO | Training epoch 1450, Batch 1000/1000: LR=1.84e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:33:47,122 | INFO | Epoch 1450 Train Time 36.981725454330444s

2025-10-15 05:34:24,343 | INFO | Training epoch 1451, Batch 1000/1000: LR=1.84e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:34:24,430 | INFO | Epoch 1451 Train Time 37.304829835891724s

2025-10-15 05:34:59,939 | INFO | Training epoch 1452, Batch 1000/1000: LR=1.83e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:35:00,009 | INFO | Epoch 1452 Train Time 35.578161001205444s

2025-10-15 05:35:37,044 | INFO | Training epoch 1453, Batch 1000/1000: LR=1.82e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:35:37,123 | INFO | Epoch 1453 Train Time 37.113799810409546s

2025-10-15 05:36:14,233 | INFO | Training epoch 1454, Batch 1000/1000: LR=1.82e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 05:36:14,320 | INFO | Epoch 1454 Train Time 37.19633436203003s

2025-10-15 05:36:51,133 | INFO | Training epoch 1455, Batch 1000/1000: LR=1.81e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 05:36:51,199 | INFO | Epoch 1455 Train Time 36.87644624710083s

2025-10-15 05:37:28,238 | INFO | Training epoch 1456, Batch 1000/1000: LR=1.81e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 05:37:28,309 | INFO | Epoch 1456 Train Time 37.10727047920227s

2025-10-15 05:38:04,337 | INFO | Training epoch 1457, Batch 1000/1000: LR=1.80e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:38:04,415 | INFO | Epoch 1457 Train Time 36.104536056518555s

2025-10-15 05:38:41,021 | INFO | Training epoch 1458, Batch 1000/1000: LR=1.79e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:38:41,106 | INFO | Epoch 1458 Train Time 36.6907000541687s

2025-10-15 05:39:17,838 | INFO | Training epoch 1459, Batch 1000/1000: LR=1.79e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:39:17,912 | INFO | Epoch 1459 Train Time 36.804136991500854s

2025-10-15 05:39:54,164 | INFO | Training epoch 1460, Batch 1000/1000: LR=1.78e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:39:54,251 | INFO | Epoch 1460 Train Time 36.33746528625488s

2025-10-15 05:40:31,030 | INFO | Training epoch 1461, Batch 1000/1000: LR=1.78e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:40:31,109 | INFO | Epoch 1461 Train Time 36.856520891189575s

2025-10-15 05:41:08,034 | INFO | Training epoch 1462, Batch 1000/1000: LR=1.77e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:41:08,115 | INFO | Epoch 1462 Train Time 37.00431275367737s

2025-10-15 05:41:44,156 | INFO | Training epoch 1463, Batch 1000/1000: LR=1.76e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:41:44,231 | INFO | Epoch 1463 Train Time 36.11540174484253s

2025-10-15 05:42:20,539 | INFO | Training epoch 1464, Batch 1000/1000: LR=1.76e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:42:20,614 | INFO | Epoch 1464 Train Time 36.38104009628296s

2025-10-15 05:42:57,458 | INFO | Training epoch 1465, Batch 1000/1000: LR=1.75e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 05:42:57,539 | INFO | Epoch 1465 Train Time 36.923890829086304s

2025-10-15 05:43:33,939 | INFO | Training epoch 1466, Batch 1000/1000: LR=1.75e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:43:34,025 | INFO | Epoch 1466 Train Time 36.48506760597229s

2025-10-15 05:44:11,605 | INFO | Training epoch 1467, Batch 1000/1000: LR=1.74e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:44:11,693 | INFO | Epoch 1467 Train Time 37.66666555404663s

2025-10-15 05:44:48,742 | INFO | Training epoch 1468, Batch 1000/1000: LR=1.74e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 05:44:48,820 | INFO | Epoch 1468 Train Time 37.125954151153564s

2025-10-15 05:45:25,259 | INFO | Training epoch 1469, Batch 1000/1000: LR=1.73e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:45:25,342 | INFO | Epoch 1469 Train Time 36.521169900894165s

2025-10-15 05:46:01,028 | INFO | Training epoch 1470, Batch 1000/1000: LR=1.72e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 05:46:01,095 | INFO | Epoch 1470 Train Time 35.75181245803833s

2025-10-15 05:46:37,435 | INFO | Training epoch 1471, Batch 1000/1000: LR=1.72e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:46:37,499 | INFO | Epoch 1471 Train Time 36.403252601623535s

2025-10-15 05:47:14,029 | INFO | Training epoch 1472, Batch 1000/1000: LR=1.71e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:47:14,119 | INFO | Epoch 1472 Train Time 36.61922788619995s

2025-10-15 05:47:50,439 | INFO | Training epoch 1473, Batch 1000/1000: LR=1.71e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.38e-01
2025-10-15 05:47:50,548 | INFO | Epoch 1473 Train Time 36.42663931846619s

2025-10-15 05:48:27,521 | INFO | Training epoch 1474, Batch 1000/1000: LR=1.70e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 05:48:27,595 | INFO | Epoch 1474 Train Time 37.044591665267944s

2025-10-15 05:49:03,939 | INFO | Training epoch 1475, Batch 1000/1000: LR=1.70e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 05:49:04,027 | INFO | Epoch 1475 Train Time 36.43168592453003s

2025-10-15 05:49:40,368 | INFO | Training epoch 1476, Batch 1000/1000: LR=1.69e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 05:49:40,462 | INFO | Epoch 1476 Train Time 36.4321084022522s

2025-10-15 05:50:16,982 | INFO | Training epoch 1477, Batch 1000/1000: LR=1.68e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:50:17,055 | INFO | Epoch 1477 Train Time 36.592962980270386s

2025-10-15 05:50:53,532 | INFO | Training epoch 1478, Batch 1000/1000: LR=1.68e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:50:53,604 | INFO | Epoch 1478 Train Time 36.54770755767822s

2025-10-15 05:51:29,728 | INFO | Training epoch 1479, Batch 1000/1000: LR=1.67e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 05:51:29,804 | INFO | Epoch 1479 Train Time 36.19733190536499s

2025-10-15 05:52:06,422 | INFO | Training epoch 1480, Batch 1000/1000: LR=1.67e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:52:06,495 | INFO | Epoch 1480 Train Time 36.69010305404663s

2025-10-15 05:52:43,250 | INFO | Training epoch 1481, Batch 1000/1000: LR=1.66e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 05:52:43,334 | INFO | Epoch 1481 Train Time 36.838560581207275s

2025-10-15 05:53:19,721 | INFO | Training epoch 1482, Batch 1000/1000: LR=1.66e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:53:19,787 | INFO | Epoch 1482 Train Time 36.45177149772644s

2025-10-15 05:53:56,450 | INFO | Training epoch 1483, Batch 1000/1000: LR=1.65e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:53:56,536 | INFO | Epoch 1483 Train Time 36.74686908721924s

2025-10-15 05:54:32,963 | INFO | Training epoch 1484, Batch 1000/1000: LR=1.64e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:54:33,054 | INFO | Epoch 1484 Train Time 36.51687574386597s

2025-10-15 05:55:08,492 | INFO | Training epoch 1485, Batch 1000/1000: LR=1.64e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 05:55:08,591 | INFO | Epoch 1485 Train Time 35.534703969955444s

2025-10-15 05:55:44,650 | INFO | Training epoch 1486, Batch 1000/1000: LR=1.63e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:55:44,734 | INFO | Epoch 1486 Train Time 36.142096757888794s

2025-10-15 05:56:21,144 | INFO | Training epoch 1487, Batch 1000/1000: LR=1.63e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 05:56:21,248 | INFO | Epoch 1487 Train Time 36.51191234588623s

2025-10-15 05:56:57,733 | INFO | Training epoch 1488, Batch 1000/1000: LR=1.62e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 05:56:57,835 | INFO | Epoch 1488 Train Time 36.585750341415405s

2025-10-15 05:57:34,432 | INFO | Training epoch 1489, Batch 1000/1000: LR=1.62e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 05:57:34,500 | INFO | Epoch 1489 Train Time 36.664400577545166s

2025-10-15 05:58:11,051 | INFO | Training epoch 1490, Batch 1000/1000: LR=1.61e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 05:58:11,122 | INFO | Epoch 1490 Train Time 36.61935305595398s

2025-10-15 05:58:47,905 | INFO | Training epoch 1491, Batch 1000/1000: LR=1.61e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.36e-01
2025-10-15 05:58:47,985 | INFO | Epoch 1491 Train Time 36.861915826797485s

2025-10-15 05:59:24,865 | INFO | Training epoch 1492, Batch 1000/1000: LR=1.60e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 05:59:24,937 | INFO | Epoch 1492 Train Time 36.949901819229126s

2025-10-15 06:00:01,243 | INFO | Training epoch 1493, Batch 1000/1000: LR=1.59e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:00:01,318 | INFO | Epoch 1493 Train Time 36.380704402923584s

2025-10-15 06:00:37,525 | INFO | Training epoch 1494, Batch 1000/1000: LR=1.59e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 06:00:37,609 | INFO | Epoch 1494 Train Time 36.290353536605835s

2025-10-15 06:01:14,956 | INFO | Training epoch 1495, Batch 1000/1000: LR=1.58e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:01:15,042 | INFO | Epoch 1495 Train Time 37.43100595474243s

2025-10-15 06:01:52,737 | INFO | Training epoch 1496, Batch 1000/1000: LR=1.58e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:01:52,826 | INFO | Epoch 1496 Train Time 37.782570362091064s

2025-10-15 06:02:29,731 | INFO | Training epoch 1497, Batch 1000/1000: LR=1.57e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:02:29,824 | INFO | Epoch 1497 Train Time 36.99683976173401s

2025-10-15 06:03:06,759 | INFO | Training epoch 1498, Batch 1000/1000: LR=1.57e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:03:06,834 | INFO | Epoch 1498 Train Time 37.007563829422s

2025-10-15 06:03:42,837 | INFO | Training epoch 1499, Batch 1000/1000: LR=1.56e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:03:42,919 | INFO | Epoch 1499 Train Time 36.08311223983765s

2025-10-15 06:04:19,935 | INFO | Training epoch 1500, Batch 1000/1000: LR=1.56e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:04:20,015 | INFO | Epoch 1500 Train Time 37.095030069351196s

2025-10-15 06:04:57,541 | INFO | Training epoch 1501, Batch 1000/1000: LR=1.55e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:04:57,619 | INFO | Epoch 1501 Train Time 37.60335302352905s

2025-10-15 06:05:34,038 | INFO | Training epoch 1502, Batch 1000/1000: LR=1.54e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 06:05:34,125 | INFO | Epoch 1502 Train Time 36.50456619262695s

2025-10-15 06:06:10,993 | INFO | Training epoch 1503, Batch 1000/1000: LR=1.54e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 06:06:11,074 | INFO | Epoch 1503 Train Time 36.94723558425903s

2025-10-15 06:06:46,760 | INFO | Training epoch 1504, Batch 1000/1000: LR=1.53e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:06:46,832 | INFO | Epoch 1504 Train Time 35.7561137676239s

2025-10-15 06:07:23,325 | INFO | Training epoch 1505, Batch 1000/1000: LR=1.53e-05, Loss=2.84e-02 BER=1.07e-02 FER=1.38e-01
2025-10-15 06:07:23,392 | INFO | Epoch 1505 Train Time 36.558788776397705s

2025-10-15 06:08:00,139 | INFO | Training epoch 1506, Batch 1000/1000: LR=1.52e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:08:00,225 | INFO | Epoch 1506 Train Time 36.83213710784912s

2025-10-15 06:08:36,659 | INFO | Training epoch 1507, Batch 1000/1000: LR=1.52e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:08:36,749 | INFO | Epoch 1507 Train Time 36.522555351257324s

2025-10-15 06:09:13,475 | INFO | Training epoch 1508, Batch 1000/1000: LR=1.51e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:09:13,565 | INFO | Epoch 1508 Train Time 36.814833641052246s

2025-10-15 06:09:50,061 | INFO | Training epoch 1509, Batch 1000/1000: LR=1.51e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:09:50,156 | INFO | Epoch 1509 Train Time 36.58933687210083s

2025-10-15 06:10:26,342 | INFO | Training epoch 1510, Batch 1000/1000: LR=1.50e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:10:26,414 | INFO | Epoch 1510 Train Time 36.256508350372314s

2025-10-15 06:11:02,930 | INFO | Training epoch 1511, Batch 1000/1000: LR=1.50e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:11:03,020 | INFO | Epoch 1511 Train Time 36.60490012168884s

2025-10-15 06:11:39,338 | INFO | Training epoch 1512, Batch 1000/1000: LR=1.49e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 06:11:39,432 | INFO | Epoch 1512 Train Time 36.411917209625244s

2025-10-15 06:12:16,241 | INFO | Training epoch 1513, Batch 1000/1000: LR=1.48e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:12:16,336 | INFO | Epoch 1513 Train Time 36.90273904800415s

2025-10-15 06:12:53,061 | INFO | Training epoch 1514, Batch 1000/1000: LR=1.48e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 06:12:53,152 | INFO | Epoch 1514 Train Time 36.81382417678833s

2025-10-15 06:13:29,237 | INFO | Training epoch 1515, Batch 1000/1000: LR=1.47e-05, Loss=2.83e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:13:29,315 | INFO | Epoch 1515 Train Time 36.16277527809143s

2025-10-15 06:14:05,750 | INFO | Training epoch 1516, Batch 1000/1000: LR=1.47e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:14:05,818 | INFO | Epoch 1516 Train Time 36.501853466033936s

2025-10-15 06:14:43,058 | INFO | Training epoch 1517, Batch 1000/1000: LR=1.46e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:14:43,145 | INFO | Epoch 1517 Train Time 37.32559370994568s

2025-10-15 06:15:19,940 | INFO | Training epoch 1518, Batch 1000/1000: LR=1.46e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:15:20,036 | INFO | Epoch 1518 Train Time 36.889973878860474s

2025-10-15 06:15:56,235 | INFO | Training epoch 1519, Batch 1000/1000: LR=1.45e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 06:15:56,312 | INFO | Epoch 1519 Train Time 36.27415466308594s

2025-10-15 06:16:32,329 | INFO | Training epoch 1520, Batch 1000/1000: LR=1.45e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 06:16:32,405 | INFO | Epoch 1520 Train Time 36.091590881347656s

2025-10-15 06:17:09,729 | INFO | Training epoch 1521, Batch 1000/1000: LR=1.44e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:17:09,821 | INFO | Epoch 1521 Train Time 37.41508173942566s

2025-10-15 06:17:45,930 | INFO | Training epoch 1522, Batch 1000/1000: LR=1.44e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 06:17:46,033 | INFO | Epoch 1522 Train Time 36.210532426834106s

2025-10-15 06:18:22,434 | INFO | Training epoch 1523, Batch 1000/1000: LR=1.43e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:18:22,511 | INFO | Epoch 1523 Train Time 36.47655487060547s

2025-10-15 06:18:59,037 | INFO | Training epoch 1524, Batch 1000/1000: LR=1.43e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:18:59,113 | INFO | Epoch 1524 Train Time 36.60048866271973s

2025-10-15 06:19:34,961 | INFO | Training epoch 1525, Batch 1000/1000: LR=1.42e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:19:35,047 | INFO | Epoch 1525 Train Time 35.93266773223877s

2025-10-15 06:20:11,132 | INFO | Training epoch 1526, Batch 1000/1000: LR=1.42e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:20:11,204 | INFO | Epoch 1526 Train Time 36.15534949302673s

2025-10-15 06:20:47,031 | INFO | Training epoch 1527, Batch 1000/1000: LR=1.41e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:20:47,099 | INFO | Epoch 1527 Train Time 35.89472723007202s

2025-10-15 06:21:23,540 | INFO | Training epoch 1528, Batch 1000/1000: LR=1.40e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:21:23,639 | INFO | Epoch 1528 Train Time 36.53868293762207s

2025-10-15 06:21:59,135 | INFO | Training epoch 1529, Batch 1000/1000: LR=1.40e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 06:21:59,222 | INFO | Epoch 1529 Train Time 35.5830659866333s

2025-10-15 06:22:35,433 | INFO | Training epoch 1530, Batch 1000/1000: LR=1.39e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:22:35,526 | INFO | Epoch 1530 Train Time 36.30273938179016s

2025-10-15 06:23:12,147 | INFO | Training epoch 1531, Batch 1000/1000: LR=1.39e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:23:12,229 | INFO | Epoch 1531 Train Time 36.70143437385559s

2025-10-15 06:23:48,872 | INFO | Training epoch 1532, Batch 1000/1000: LR=1.38e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 06:23:48,946 | INFO | Epoch 1532 Train Time 36.71536612510681s

2025-10-15 06:24:26,238 | INFO | Training epoch 1533, Batch 1000/1000: LR=1.38e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:24:26,319 | INFO | Epoch 1533 Train Time 37.371127128601074s

2025-10-15 06:25:03,132 | INFO | Training epoch 1534, Batch 1000/1000: LR=1.37e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 06:25:03,217 | INFO | Epoch 1534 Train Time 36.896894454956055s

2025-10-15 06:25:39,624 | INFO | Training epoch 1535, Batch 1000/1000: LR=1.37e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:25:39,713 | INFO | Epoch 1535 Train Time 36.49421215057373s

2025-10-15 06:26:16,332 | INFO | Training epoch 1536, Batch 1000/1000: LR=1.36e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:26:16,435 | INFO | Epoch 1536 Train Time 36.72128772735596s

2025-10-15 06:26:53,539 | INFO | Training epoch 1537, Batch 1000/1000: LR=1.36e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:26:53,623 | INFO | Epoch 1537 Train Time 37.186511516571045s

2025-10-15 06:27:29,960 | INFO | Training epoch 1538, Batch 1000/1000: LR=1.35e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:27:30,050 | INFO | Epoch 1538 Train Time 36.426597356796265s

2025-10-15 06:28:06,944 | INFO | Training epoch 1539, Batch 1000/1000: LR=1.35e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:28:07,037 | INFO | Epoch 1539 Train Time 36.98564171791077s

2025-10-15 06:28:43,839 | INFO | Training epoch 1540, Batch 1000/1000: LR=1.34e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:28:43,910 | INFO | Epoch 1540 Train Time 36.872291803359985s

2025-10-15 06:29:20,639 | INFO | Training epoch 1541, Batch 1000/1000: LR=1.34e-05, Loss=2.75e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 06:29:20,727 | INFO | Epoch 1541 Train Time 36.81529402732849s

2025-10-15 06:29:57,645 | INFO | Training epoch 1542, Batch 1000/1000: LR=1.33e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 06:29:57,733 | INFO | Epoch 1542 Train Time 37.004934310913086s

2025-10-15 06:30:33,859 | INFO | Training epoch 1543, Batch 1000/1000: LR=1.33e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 06:30:33,939 | INFO | Epoch 1543 Train Time 36.20472955703735s

2025-10-15 06:31:11,536 | INFO | Training epoch 1544, Batch 1000/1000: LR=1.32e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 06:31:11,604 | INFO | Epoch 1544 Train Time 37.66452765464783s

2025-10-15 06:31:48,121 | INFO | Training epoch 1545, Batch 1000/1000: LR=1.32e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:31:48,195 | INFO | Epoch 1545 Train Time 36.58948040008545s

2025-10-15 06:32:24,728 | INFO | Training epoch 1546, Batch 1000/1000: LR=1.31e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:32:24,812 | INFO | Epoch 1546 Train Time 36.616042613983154s

2025-10-15 06:33:01,368 | INFO | Training epoch 1547, Batch 1000/1000: LR=1.31e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 06:33:01,450 | INFO | Epoch 1547 Train Time 36.63654065132141s

2025-10-15 06:33:38,433 | INFO | Training epoch 1548, Batch 1000/1000: LR=1.30e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:33:38,531 | INFO | Epoch 1548 Train Time 37.0790798664093s

2025-10-15 06:34:15,131 | INFO | Training epoch 1549, Batch 1000/1000: LR=1.30e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:34:15,202 | INFO | Epoch 1549 Train Time 36.67064428329468s

2025-10-15 06:34:52,145 | INFO | Training epoch 1550, Batch 1000/1000: LR=1.29e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:34:52,234 | INFO | Epoch 1550 Train Time 37.030601501464844s

2025-10-15 06:35:29,148 | INFO | Training epoch 1551, Batch 1000/1000: LR=1.29e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:35:29,229 | INFO | Epoch 1551 Train Time 36.994507789611816s

2025-10-15 06:36:05,340 | INFO | Training epoch 1552, Batch 1000/1000: LR=1.28e-05, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 06:36:05,427 | INFO | Epoch 1552 Train Time 36.19744038581848s

2025-10-15 06:36:42,238 | INFO | Training epoch 1553, Batch 1000/1000: LR=1.28e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:36:42,325 | INFO | Epoch 1553 Train Time 36.89445972442627s

2025-10-15 06:37:18,974 | INFO | Training epoch 1554, Batch 1000/1000: LR=1.27e-05, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:37:19,055 | INFO | Epoch 1554 Train Time 36.72925400733948s

2025-10-15 06:37:55,426 | INFO | Training epoch 1555, Batch 1000/1000: LR=1.27e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:37:55,500 | INFO | Epoch 1555 Train Time 36.44339346885681s

2025-10-15 06:38:32,030 | INFO | Training epoch 1556, Batch 1000/1000: LR=1.26e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 06:38:32,119 | INFO | Epoch 1556 Train Time 36.618491411209106s

2025-10-15 06:39:09,136 | INFO | Training epoch 1557, Batch 1000/1000: LR=1.26e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:39:09,218 | INFO | Epoch 1557 Train Time 37.098440170288086s

2025-10-15 06:39:45,842 | INFO | Training epoch 1558, Batch 1000/1000: LR=1.25e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:39:45,912 | INFO | Epoch 1558 Train Time 36.69271755218506s

2025-10-15 06:40:22,620 | INFO | Training epoch 1559, Batch 1000/1000: LR=1.25e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:40:22,704 | INFO | Epoch 1559 Train Time 36.79098868370056s

2025-10-15 06:40:59,440 | INFO | Training epoch 1560, Batch 1000/1000: LR=1.24e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 06:40:59,522 | INFO | Epoch 1560 Train Time 36.81626892089844s

2025-10-15 06:41:36,442 | INFO | Training epoch 1561, Batch 1000/1000: LR=1.24e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:41:36,525 | INFO | Epoch 1561 Train Time 37.00197076797485s

2025-10-15 06:42:12,867 | INFO | Training epoch 1562, Batch 1000/1000: LR=1.23e-05, Loss=2.86e-02 BER=1.07e-02 FER=1.36e-01
2025-10-15 06:42:12,951 | INFO | Epoch 1562 Train Time 36.42380356788635s

2025-10-15 06:42:49,722 | INFO | Training epoch 1563, Batch 1000/1000: LR=1.23e-05, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:42:49,794 | INFO | Epoch 1563 Train Time 36.84157943725586s

2025-10-15 06:43:26,160 | INFO | Training epoch 1564, Batch 1000/1000: LR=1.22e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:43:26,248 | INFO | Epoch 1564 Train Time 36.45294666290283s

2025-10-15 06:44:02,947 | INFO | Training epoch 1565, Batch 1000/1000: LR=1.22e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 06:44:03,034 | INFO | Epoch 1565 Train Time 36.78349757194519s

2025-10-15 06:44:39,597 | INFO | Training epoch 1566, Batch 1000/1000: LR=1.21e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:44:39,676 | INFO | Epoch 1566 Train Time 36.63972878456116s

2025-10-15 06:45:16,471 | INFO | Training epoch 1567, Batch 1000/1000: LR=1.21e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:45:16,541 | INFO | Epoch 1567 Train Time 36.864360094070435s

2025-10-15 06:45:53,039 | INFO | Training epoch 1568, Batch 1000/1000: LR=1.20e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:45:53,134 | INFO | Epoch 1568 Train Time 36.59116005897522s

2025-10-15 06:46:28,763 | INFO | Training epoch 1569, Batch 1000/1000: LR=1.20e-05, Loss=2.78e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:46:28,861 | INFO | Epoch 1569 Train Time 35.72635054588318s

2025-10-15 06:47:05,641 | INFO | Training epoch 1570, Batch 1000/1000: LR=1.19e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 06:47:05,710 | INFO | Epoch 1570 Train Time 36.84754705429077s

2025-10-15 06:47:42,834 | INFO | Training epoch 1571, Batch 1000/1000: LR=1.19e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:47:42,912 | INFO | Epoch 1571 Train Time 37.20062589645386s

2025-10-15 06:48:19,832 | INFO | Training epoch 1572, Batch 1000/1000: LR=1.18e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 06:48:19,928 | INFO | Epoch 1572 Train Time 37.01321625709534s

2025-10-15 06:48:57,276 | INFO | Training epoch 1573, Batch 1000/1000: LR=1.18e-05, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 06:48:57,352 | INFO | Epoch 1573 Train Time 37.42341470718384s

2025-10-15 06:49:34,241 | INFO | Training epoch 1574, Batch 1000/1000: LR=1.17e-05, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 06:49:34,314 | INFO | Epoch 1574 Train Time 36.960267066955566s

2025-10-15 06:50:10,720 | INFO | Training epoch 1575, Batch 1000/1000: LR=1.17e-05, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 06:50:10,785 | INFO | Epoch 1575 Train Time 36.468735694885254s

2025-10-15 06:50:10,786 | INFO | [P2] saving best_model (QAT) with loss 0.027426 at epoch 1575
2025-10-15 06:50:47,457 | INFO | Training epoch 1576, Batch 1000/1000: LR=1.16e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 06:50:47,523 | INFO | Epoch 1576 Train Time 36.721328020095825s

2025-10-15 06:51:24,459 | INFO | Training epoch 1577, Batch 1000/1000: LR=1.16e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 06:51:24,549 | INFO | Epoch 1577 Train Time 37.025390625s

2025-10-15 06:52:01,185 | INFO | Training epoch 1578, Batch 1000/1000: LR=1.15e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 06:52:01,258 | INFO | Epoch 1578 Train Time 36.70767307281494s

2025-10-15 06:52:37,536 | INFO | Training epoch 1579, Batch 1000/1000: LR=1.15e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:52:37,602 | INFO | Epoch 1579 Train Time 36.342084884643555s

2025-10-15 06:53:13,825 | INFO | Training epoch 1580, Batch 1000/1000: LR=1.14e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 06:53:13,903 | INFO | Epoch 1580 Train Time 36.29930567741394s

2025-10-15 06:53:50,736 | INFO | Training epoch 1581, Batch 1000/1000: LR=1.14e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 06:53:50,798 | INFO | Epoch 1581 Train Time 36.89398503303528s

2025-10-15 06:54:26,780 | INFO | Training epoch 1582, Batch 1000/1000: LR=1.13e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:54:26,843 | INFO | Epoch 1582 Train Time 36.04457068443298s

2025-10-15 06:55:03,740 | INFO | Training epoch 1583, Batch 1000/1000: LR=1.13e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:55:03,830 | INFO | Epoch 1583 Train Time 36.984840869903564s

2025-10-15 06:55:40,634 | INFO | Training epoch 1584, Batch 1000/1000: LR=1.12e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 06:55:40,719 | INFO | Epoch 1584 Train Time 36.887354612350464s

2025-10-15 06:56:16,236 | INFO | Training epoch 1585, Batch 1000/1000: LR=1.12e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:56:16,318 | INFO | Epoch 1585 Train Time 35.59671902656555s

2025-10-15 06:56:53,131 | INFO | Training epoch 1586, Batch 1000/1000: LR=1.12e-05, Loss=2.81e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 06:56:53,222 | INFO | Epoch 1586 Train Time 36.90314698219299s

2025-10-15 06:57:29,536 | INFO | Training epoch 1587, Batch 1000/1000: LR=1.11e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 06:57:29,601 | INFO | Epoch 1587 Train Time 36.378477334976196s

2025-10-15 06:58:06,123 | INFO | Training epoch 1588, Batch 1000/1000: LR=1.11e-05, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 06:58:06,191 | INFO | Epoch 1588 Train Time 36.5884211063385s

2025-10-15 06:58:43,945 | INFO | Training epoch 1589, Batch 1000/1000: LR=1.10e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 06:58:44,035 | INFO | Epoch 1589 Train Time 37.841768980026245s

2025-10-15 06:59:20,633 | INFO | Training epoch 1590, Batch 1000/1000: LR=1.10e-05, Loss=2.78e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 06:59:20,694 | INFO | Epoch 1590 Train Time 36.65832042694092s

2025-10-15 06:59:56,159 | INFO | Training epoch 1591, Batch 1000/1000: LR=1.09e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 06:59:56,245 | INFO | Epoch 1591 Train Time 35.54904079437256s

2025-10-15 07:00:32,840 | INFO | Training epoch 1592, Batch 1000/1000: LR=1.09e-05, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:00:32,930 | INFO | Epoch 1592 Train Time 36.68412661552429s

2025-10-15 07:01:09,147 | INFO | Training epoch 1593, Batch 1000/1000: LR=1.08e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:01:09,213 | INFO | Epoch 1593 Train Time 36.281272888183594s

2025-10-15 07:01:46,478 | INFO | Training epoch 1594, Batch 1000/1000: LR=1.08e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 07:01:46,540 | INFO | Epoch 1594 Train Time 37.325461626052856s

2025-10-15 07:02:25,376 | INFO | Training epoch 1595, Batch 1000/1000: LR=1.07e-05, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:02:25,459 | INFO | Epoch 1595 Train Time 38.916940689086914s

2025-10-15 07:03:02,461 | INFO | Training epoch 1596, Batch 1000/1000: LR=1.07e-05, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:03:02,530 | INFO | Epoch 1596 Train Time 37.06966280937195s

2025-10-15 07:03:40,526 | INFO | Training epoch 1597, Batch 1000/1000: LR=1.06e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:03:40,600 | INFO | Epoch 1597 Train Time 38.069112062454224s

2025-10-15 07:04:15,731 | INFO | Training epoch 1598, Batch 1000/1000: LR=1.06e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:04:15,796 | INFO | Epoch 1598 Train Time 35.19485831260681s

2025-10-15 07:04:52,729 | INFO | Training epoch 1599, Batch 1000/1000: LR=1.05e-05, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:04:52,808 | INFO | Epoch 1599 Train Time 37.01122498512268s

2025-10-15 07:05:29,636 | INFO | Training epoch 1600, Batch 1000/1000: LR=1.05e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:05:29,708 | INFO | Epoch 1600 Train Time 36.898818492889404s

2025-10-15 07:06:05,840 | INFO | Training epoch 1601, Batch 1000/1000: LR=1.05e-05, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:06:05,906 | INFO | Epoch 1601 Train Time 36.197097063064575s

2025-10-15 07:06:42,541 | INFO | Training epoch 1602, Batch 1000/1000: LR=1.04e-05, Loss=2.84e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:06:42,614 | INFO | Epoch 1602 Train Time 36.70648550987244s

2025-10-15 07:07:18,938 | INFO | Training epoch 1603, Batch 1000/1000: LR=1.04e-05, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:07:19,031 | INFO | Epoch 1603 Train Time 36.41614818572998s

2025-10-15 07:07:56,133 | INFO | Training epoch 1604, Batch 1000/1000: LR=1.03e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:07:56,214 | INFO | Epoch 1604 Train Time 37.18219828605652s

2025-10-15 07:08:32,772 | INFO | Training epoch 1605, Batch 1000/1000: LR=1.03e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 07:08:32,843 | INFO | Epoch 1605 Train Time 36.62801504135132s

2025-10-15 07:09:08,932 | INFO | Training epoch 1606, Batch 1000/1000: LR=1.02e-05, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:09:09,014 | INFO | Epoch 1606 Train Time 36.16944122314453s

2025-10-15 07:09:45,348 | INFO | Training epoch 1607, Batch 1000/1000: LR=1.02e-05, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:09:45,435 | INFO | Epoch 1607 Train Time 36.42008709907532s

2025-10-15 07:10:22,561 | INFO | Training epoch 1608, Batch 1000/1000: LR=1.01e-05, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:10:22,635 | INFO | Epoch 1608 Train Time 37.19910955429077s

2025-10-15 07:10:58,860 | INFO | Training epoch 1609, Batch 1000/1000: LR=1.01e-05, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:10:58,944 | INFO | Epoch 1609 Train Time 36.30615711212158s

2025-10-15 07:10:58,944 | INFO | [P2] saving best_model (QAT) with loss 0.027322 at epoch 1609
2025-10-15 07:11:35,739 | INFO | Training epoch 1610, Batch 1000/1000: LR=1.00e-05, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 07:11:35,828 | INFO | Epoch 1610 Train Time 36.86844062805176s

2025-10-15 07:12:12,533 | INFO | Training epoch 1611, Batch 1000/1000: LR=1.00e-05, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:12:12,623 | INFO | Epoch 1611 Train Time 36.79349327087402s

2025-10-15 07:12:49,515 | INFO | Training epoch 1612, Batch 1000/1000: LR=9.96e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:12:49,609 | INFO | Epoch 1612 Train Time 36.98481297492981s

2025-10-15 07:13:26,545 | INFO | Training epoch 1613, Batch 1000/1000: LR=9.91e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:13:26,621 | INFO | Epoch 1613 Train Time 37.011826515197754s

2025-10-15 07:14:03,760 | INFO | Training epoch 1614, Batch 1000/1000: LR=9.87e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:14:03,830 | INFO | Epoch 1614 Train Time 37.20710301399231s

2025-10-15 07:14:40,620 | INFO | Training epoch 1615, Batch 1000/1000: LR=9.82e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:14:40,706 | INFO | Epoch 1615 Train Time 36.87425136566162s

2025-10-15 07:15:17,535 | INFO | Training epoch 1616, Batch 1000/1000: LR=9.78e-06, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:15:17,623 | INFO | Epoch 1616 Train Time 36.916534423828125s

2025-10-15 07:15:54,634 | INFO | Training epoch 1617, Batch 1000/1000: LR=9.74e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 07:15:54,718 | INFO | Epoch 1617 Train Time 37.092963218688965s

2025-10-15 07:16:31,636 | INFO | Training epoch 1618, Batch 1000/1000: LR=9.69e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:16:31,704 | INFO | Epoch 1618 Train Time 36.984862327575684s

2025-10-15 07:17:08,535 | INFO | Training epoch 1619, Batch 1000/1000: LR=9.65e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:17:08,605 | INFO | Epoch 1619 Train Time 36.90018558502197s

2025-10-15 07:17:44,334 | INFO | Training epoch 1620, Batch 1000/1000: LR=9.60e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:17:44,409 | INFO | Epoch 1620 Train Time 35.801400661468506s

2025-10-15 07:18:21,431 | INFO | Training epoch 1621, Batch 1000/1000: LR=9.56e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:18:21,497 | INFO | Epoch 1621 Train Time 37.08673596382141s

2025-10-15 07:18:57,935 | INFO | Training epoch 1622, Batch 1000/1000: LR=9.52e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:18:58,036 | INFO | Epoch 1622 Train Time 36.53757667541504s

2025-10-15 07:19:34,693 | INFO | Training epoch 1623, Batch 1000/1000: LR=9.47e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:19:34,765 | INFO | Epoch 1623 Train Time 36.727909326553345s

2025-10-15 07:20:11,823 | INFO | Training epoch 1624, Batch 1000/1000: LR=9.43e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 07:20:11,908 | INFO | Epoch 1624 Train Time 37.14086723327637s

2025-10-15 07:20:48,844 | INFO | Training epoch 1625, Batch 1000/1000: LR=9.39e-06, Loss=2.81e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 07:20:48,927 | INFO | Epoch 1625 Train Time 37.017799377441406s

2025-10-15 07:21:25,849 | INFO | Training epoch 1626, Batch 1000/1000: LR=9.34e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 07:21:25,922 | INFO | Epoch 1626 Train Time 36.993282318115234s

2025-10-15 07:22:03,135 | INFO | Training epoch 1627, Batch 1000/1000: LR=9.30e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:22:03,222 | INFO | Epoch 1627 Train Time 37.298479080200195s

2025-10-15 07:22:40,858 | INFO | Training epoch 1628, Batch 1000/1000: LR=9.26e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:22:40,942 | INFO | Epoch 1628 Train Time 37.7199866771698s

2025-10-15 07:23:17,640 | INFO | Training epoch 1629, Batch 1000/1000: LR=9.21e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 07:23:17,698 | INFO | Epoch 1629 Train Time 36.754801750183105s

2025-10-15 07:23:54,141 | INFO | Training epoch 1630, Batch 1000/1000: LR=9.17e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:23:54,211 | INFO | Epoch 1630 Train Time 36.51212215423584s

2025-10-15 07:24:31,133 | INFO | Training epoch 1631, Batch 1000/1000: LR=9.13e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:24:31,204 | INFO | Epoch 1631 Train Time 36.99242806434631s

2025-10-15 07:25:08,438 | INFO | Training epoch 1632, Batch 1000/1000: LR=9.08e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:25:08,511 | INFO | Epoch 1632 Train Time 37.3061842918396s

2025-10-15 07:25:45,550 | INFO | Training epoch 1633, Batch 1000/1000: LR=9.04e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:25:45,638 | INFO | Epoch 1633 Train Time 37.12513208389282s

2025-10-15 07:26:22,534 | INFO | Training epoch 1634, Batch 1000/1000: LR=9.00e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:26:22,606 | INFO | Epoch 1634 Train Time 36.96668100357056s

2025-10-15 07:26:59,137 | INFO | Training epoch 1635, Batch 1000/1000: LR=8.96e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:26:59,212 | INFO | Epoch 1635 Train Time 36.604592084884644s

2025-10-15 07:27:36,341 | INFO | Training epoch 1636, Batch 1000/1000: LR=8.92e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:27:36,412 | INFO | Epoch 1636 Train Time 37.19875192642212s

2025-10-15 07:28:13,049 | INFO | Training epoch 1637, Batch 1000/1000: LR=8.87e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:28:13,135 | INFO | Epoch 1637 Train Time 36.72226524353027s

2025-10-15 07:28:49,546 | INFO | Training epoch 1638, Batch 1000/1000: LR=8.83e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:28:49,639 | INFO | Epoch 1638 Train Time 36.50293755531311s

2025-10-15 07:29:26,929 | INFO | Training epoch 1639, Batch 1000/1000: LR=8.79e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:29:26,996 | INFO | Epoch 1639 Train Time 37.356236696243286s

2025-10-15 07:30:03,740 | INFO | Training epoch 1640, Batch 1000/1000: LR=8.75e-06, Loss=2.81e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 07:30:03,826 | INFO | Epoch 1640 Train Time 36.82790541648865s

2025-10-15 07:30:40,843 | INFO | Training epoch 1641, Batch 1000/1000: LR=8.71e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:30:40,921 | INFO | Epoch 1641 Train Time 37.09395956993103s

2025-10-15 07:31:17,328 | INFO | Training epoch 1642, Batch 1000/1000: LR=8.66e-06, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:31:17,395 | INFO | Epoch 1642 Train Time 36.472858905792236s

2025-10-15 07:31:54,241 | INFO | Training epoch 1643, Batch 1000/1000: LR=8.62e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:31:54,321 | INFO | Epoch 1643 Train Time 36.9255154132843s

2025-10-15 07:32:31,123 | INFO | Training epoch 1644, Batch 1000/1000: LR=8.58e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:32:31,191 | INFO | Epoch 1644 Train Time 36.868733406066895s

2025-10-15 07:33:08,435 | INFO | Training epoch 1645, Batch 1000/1000: LR=8.54e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:33:08,517 | INFO | Epoch 1645 Train Time 37.32440543174744s

2025-10-15 07:33:44,872 | INFO | Training epoch 1646, Batch 1000/1000: LR=8.50e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:33:44,938 | INFO | Epoch 1646 Train Time 36.41997504234314s

2025-10-15 07:34:21,137 | INFO | Training epoch 1647, Batch 1000/1000: LR=8.46e-06, Loss=2.85e-02 BER=1.07e-02 FER=1.37e-01
2025-10-15 07:34:21,210 | INFO | Epoch 1647 Train Time 36.27101182937622s

2025-10-15 07:34:57,130 | INFO | Training epoch 1648, Batch 1000/1000: LR=8.42e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:34:57,198 | INFO | Epoch 1648 Train Time 35.986644983291626s

2025-10-15 07:35:33,435 | INFO | Training epoch 1649, Batch 1000/1000: LR=8.38e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:35:33,504 | INFO | Epoch 1649 Train Time 36.30557060241699s

2025-10-15 07:36:10,447 | INFO | Training epoch 1650, Batch 1000/1000: LR=8.33e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:36:10,512 | INFO | Epoch 1650 Train Time 37.00502038002014s

2025-10-15 07:36:46,925 | INFO | Training epoch 1651, Batch 1000/1000: LR=8.29e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:36:47,000 | INFO | Epoch 1651 Train Time 36.48687672615051s

2025-10-15 07:37:23,840 | INFO | Training epoch 1652, Batch 1000/1000: LR=8.25e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:37:23,908 | INFO | Epoch 1652 Train Time 36.90664076805115s

2025-10-15 07:38:00,558 | INFO | Training epoch 1653, Batch 1000/1000: LR=8.21e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:38:00,628 | INFO | Epoch 1653 Train Time 36.718321561813354s

2025-10-15 07:38:38,234 | INFO | Training epoch 1654, Batch 1000/1000: LR=8.17e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:38:38,317 | INFO | Epoch 1654 Train Time 37.68798327445984s

2025-10-15 07:39:15,425 | INFO | Training epoch 1655, Batch 1000/1000: LR=8.13e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 07:39:15,491 | INFO | Epoch 1655 Train Time 37.17367506027222s

2025-10-15 07:39:51,853 | INFO | Training epoch 1656, Batch 1000/1000: LR=8.09e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:39:51,940 | INFO | Epoch 1656 Train Time 36.44831466674805s

2025-10-15 07:40:28,135 | INFO | Training epoch 1657, Batch 1000/1000: LR=8.05e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:40:28,196 | INFO | Epoch 1657 Train Time 36.254433155059814s

2025-10-15 07:41:04,739 | INFO | Training epoch 1658, Batch 1000/1000: LR=8.01e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:41:04,814 | INFO | Epoch 1658 Train Time 36.617045402526855s

2025-10-15 07:41:41,941 | INFO | Training epoch 1659, Batch 1000/1000: LR=7.97e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:41:42,010 | INFO | Epoch 1659 Train Time 37.1953341960907s

2025-10-15 07:42:18,854 | INFO | Training epoch 1660, Batch 1000/1000: LR=7.93e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:42:18,919 | INFO | Epoch 1660 Train Time 36.907108306884766s

2025-10-15 07:42:55,433 | INFO | Training epoch 1661, Batch 1000/1000: LR=7.89e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:42:55,502 | INFO | Epoch 1661 Train Time 36.58151435852051s

2025-10-15 07:43:32,235 | INFO | Training epoch 1662, Batch 1000/1000: LR=7.85e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 07:43:32,333 | INFO | Epoch 1662 Train Time 36.83025503158569s

2025-10-15 07:44:09,365 | INFO | Training epoch 1663, Batch 1000/1000: LR=7.81e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:44:09,434 | INFO | Epoch 1663 Train Time 37.09950256347656s

2025-10-15 07:44:45,379 | INFO | Training epoch 1664, Batch 1000/1000: LR=7.78e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.37e-01
2025-10-15 07:44:45,457 | INFO | Epoch 1664 Train Time 36.02228760719299s

2025-10-15 07:45:21,821 | INFO | Training epoch 1665, Batch 1000/1000: LR=7.74e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:45:21,901 | INFO | Epoch 1665 Train Time 36.44259166717529s

2025-10-15 07:45:58,941 | INFO | Training epoch 1666, Batch 1000/1000: LR=7.70e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 07:45:59,028 | INFO | Epoch 1666 Train Time 37.124876499176025s

2025-10-15 07:46:36,235 | INFO | Training epoch 1667, Batch 1000/1000: LR=7.66e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:46:36,309 | INFO | Epoch 1667 Train Time 37.27946591377258s

2025-10-15 07:47:12,608 | INFO | Training epoch 1668, Batch 1000/1000: LR=7.62e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:47:12,678 | INFO | Epoch 1668 Train Time 36.36835265159607s

2025-10-15 07:47:49,172 | INFO | Training epoch 1669, Batch 1000/1000: LR=7.58e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:47:49,244 | INFO | Epoch 1669 Train Time 36.56480646133423s

2025-10-15 07:48:25,934 | INFO | Training epoch 1670, Batch 1000/1000: LR=7.54e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:48:26,003 | INFO | Epoch 1670 Train Time 36.758368492126465s

2025-10-15 07:49:02,939 | INFO | Training epoch 1671, Batch 1000/1000: LR=7.50e-06, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 07:49:03,001 | INFO | Epoch 1671 Train Time 36.99604654312134s

2025-10-15 07:49:39,624 | INFO | Training epoch 1672, Batch 1000/1000: LR=7.46e-06, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 07:49:39,697 | INFO | Epoch 1672 Train Time 36.6942834854126s

2025-10-15 07:50:16,533 | INFO | Training epoch 1673, Batch 1000/1000: LR=7.43e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:50:16,613 | INFO | Epoch 1673 Train Time 36.9154326915741s

2025-10-15 07:50:53,238 | INFO | Training epoch 1674, Batch 1000/1000: LR=7.39e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 07:50:53,319 | INFO | Epoch 1674 Train Time 36.70511531829834s

2025-10-15 07:51:30,237 | INFO | Training epoch 1675, Batch 1000/1000: LR=7.35e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:51:30,301 | INFO | Epoch 1675 Train Time 36.979294300079346s

2025-10-15 07:52:06,858 | INFO | Training epoch 1676, Batch 1000/1000: LR=7.31e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:52:06,921 | INFO | Epoch 1676 Train Time 36.61917972564697s

2025-10-15 07:52:43,825 | INFO | Training epoch 1677, Batch 1000/1000: LR=7.27e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:52:43,892 | INFO | Epoch 1677 Train Time 36.97028064727783s

2025-10-15 07:53:20,378 | INFO | Training epoch 1678, Batch 1000/1000: LR=7.24e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:53:20,462 | INFO | Epoch 1678 Train Time 36.56871724128723s

2025-10-15 07:53:57,057 | INFO | Training epoch 1679, Batch 1000/1000: LR=7.20e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:53:57,121 | INFO | Epoch 1679 Train Time 36.658116817474365s

2025-10-15 07:54:34,141 | INFO | Training epoch 1680, Batch 1000/1000: LR=7.16e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:54:34,217 | INFO | Epoch 1680 Train Time 37.09381651878357s

2025-10-15 07:55:10,537 | INFO | Training epoch 1681, Batch 1000/1000: LR=7.12e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 07:55:10,608 | INFO | Epoch 1681 Train Time 36.390087604522705s

2025-10-15 07:55:47,165 | INFO | Training epoch 1682, Batch 1000/1000: LR=7.09e-06, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:55:47,230 | INFO | Epoch 1682 Train Time 36.61966824531555s

2025-10-15 07:56:23,628 | INFO | Training epoch 1683, Batch 1000/1000: LR=7.05e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 07:56:23,716 | INFO | Epoch 1683 Train Time 36.48543453216553s

2025-10-15 07:56:59,800 | INFO | Training epoch 1684, Batch 1000/1000: LR=7.01e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 07:56:59,876 | INFO | Epoch 1684 Train Time 36.15791654586792s

2025-10-15 07:57:36,650 | INFO | Training epoch 1685, Batch 1000/1000: LR=6.97e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:57:36,724 | INFO | Epoch 1685 Train Time 36.84657096862793s

2025-10-15 07:58:13,728 | INFO | Training epoch 1686, Batch 1000/1000: LR=6.94e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 07:58:13,799 | INFO | Epoch 1686 Train Time 37.07144379615784s

2025-10-15 07:58:50,954 | INFO | Training epoch 1687, Batch 1000/1000: LR=6.90e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 07:58:51,025 | INFO | Epoch 1687 Train Time 37.22505187988281s

2025-10-15 07:59:27,634 | INFO | Training epoch 1688, Batch 1000/1000: LR=6.86e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 07:59:27,697 | INFO | Epoch 1688 Train Time 36.67101788520813s

2025-10-15 08:00:04,162 | INFO | Training epoch 1689, Batch 1000/1000: LR=6.83e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:00:04,235 | INFO | Epoch 1689 Train Time 36.5367648601532s

2025-10-15 08:00:41,361 | INFO | Training epoch 1690, Batch 1000/1000: LR=6.79e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:00:41,426 | INFO | Epoch 1690 Train Time 37.19035291671753s

2025-10-15 08:01:18,959 | INFO | Training epoch 1691, Batch 1000/1000: LR=6.75e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:01:19,045 | INFO | Epoch 1691 Train Time 37.6180899143219s

2025-10-15 08:01:55,734 | INFO | Training epoch 1692, Batch 1000/1000: LR=6.72e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 08:01:55,818 | INFO | Epoch 1692 Train Time 36.77238368988037s

2025-10-15 08:02:32,952 | INFO | Training epoch 1693, Batch 1000/1000: LR=6.68e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:02:33,012 | INFO | Epoch 1693 Train Time 37.192371129989624s

2025-10-15 08:03:10,160 | INFO | Training epoch 1694, Batch 1000/1000: LR=6.64e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:03:10,225 | INFO | Epoch 1694 Train Time 37.21092128753662s

2025-10-15 08:03:46,959 | INFO | Training epoch 1695, Batch 1000/1000: LR=6.61e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:03:47,020 | INFO | Epoch 1695 Train Time 36.793012619018555s

2025-10-15 08:04:23,937 | INFO | Training epoch 1696, Batch 1000/1000: LR=6.57e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:04:24,006 | INFO | Epoch 1696 Train Time 36.984506607055664s

2025-10-15 08:05:00,231 | INFO | Training epoch 1697, Batch 1000/1000: LR=6.54e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.36e-01
2025-10-15 08:05:00,297 | INFO | Epoch 1697 Train Time 36.29022169113159s

2025-10-15 08:05:37,347 | INFO | Training epoch 1698, Batch 1000/1000: LR=6.50e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:05:37,418 | INFO | Epoch 1698 Train Time 37.12064051628113s

2025-10-15 08:06:14,725 | INFO | Training epoch 1699, Batch 1000/1000: LR=6.47e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:06:14,799 | INFO | Epoch 1699 Train Time 37.37959551811218s

2025-10-15 08:06:51,460 | INFO | Training epoch 1700, Batch 1000/1000: LR=6.43e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:06:51,531 | INFO | Epoch 1700 Train Time 36.73124814033508s

2025-10-15 08:07:27,738 | INFO | Training epoch 1701, Batch 1000/1000: LR=6.40e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:07:27,824 | INFO | Epoch 1701 Train Time 36.292174100875854s

2025-10-15 08:08:04,873 | INFO | Training epoch 1702, Batch 1000/1000: LR=6.36e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 08:08:04,940 | INFO | Epoch 1702 Train Time 37.115156173706055s

2025-10-15 08:08:41,322 | INFO | Training epoch 1703, Batch 1000/1000: LR=6.32e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:08:41,396 | INFO | Epoch 1703 Train Time 36.455408811569214s

2025-10-15 08:09:18,440 | INFO | Training epoch 1704, Batch 1000/1000: LR=6.29e-06, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:09:18,543 | INFO | Epoch 1704 Train Time 37.14474058151245s

2025-10-15 08:09:55,641 | INFO | Training epoch 1705, Batch 1000/1000: LR=6.25e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 08:09:55,708 | INFO | Epoch 1705 Train Time 37.16414451599121s

2025-10-15 08:10:32,632 | INFO | Training epoch 1706, Batch 1000/1000: LR=6.22e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:10:32,728 | INFO | Epoch 1706 Train Time 37.017478942871094s

2025-10-15 08:11:09,470 | INFO | Training epoch 1707, Batch 1000/1000: LR=6.19e-06, Loss=2.84e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 08:11:09,547 | INFO | Epoch 1707 Train Time 36.816569328308105s

2025-10-15 08:11:46,137 | INFO | Training epoch 1708, Batch 1000/1000: LR=6.15e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:11:46,225 | INFO | Epoch 1708 Train Time 36.67705225944519s

2025-10-15 08:12:23,032 | INFO | Training epoch 1709, Batch 1000/1000: LR=6.12e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:12:23,105 | INFO | Epoch 1709 Train Time 36.87912845611572s

2025-10-15 08:12:59,634 | INFO | Training epoch 1710, Batch 1000/1000: LR=6.08e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:12:59,722 | INFO | Epoch 1710 Train Time 36.61552095413208s

2025-10-15 08:13:36,442 | INFO | Training epoch 1711, Batch 1000/1000: LR=6.05e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:13:36,513 | INFO | Epoch 1711 Train Time 36.78983211517334s

2025-10-15 08:14:13,540 | INFO | Training epoch 1712, Batch 1000/1000: LR=6.01e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:14:13,613 | INFO | Epoch 1712 Train Time 37.099502086639404s

2025-10-15 08:14:50,056 | INFO | Training epoch 1713, Batch 1000/1000: LR=5.98e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:14:50,151 | INFO | Epoch 1713 Train Time 36.53700590133667s

2025-10-15 08:15:26,626 | INFO | Training epoch 1714, Batch 1000/1000: LR=5.95e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:15:26,703 | INFO | Epoch 1714 Train Time 36.55143332481384s

2025-10-15 08:16:02,541 | INFO | Training epoch 1715, Batch 1000/1000: LR=5.91e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:16:02,624 | INFO | Epoch 1715 Train Time 35.91986346244812s

2025-10-15 08:16:39,220 | INFO | Training epoch 1716, Batch 1000/1000: LR=5.88e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:16:39,307 | INFO | Epoch 1716 Train Time 36.682560443878174s

2025-10-15 08:17:16,237 | INFO | Training epoch 1717, Batch 1000/1000: LR=5.84e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:17:16,324 | INFO | Epoch 1717 Train Time 37.01569175720215s

2025-10-15 08:17:53,350 | INFO | Training epoch 1718, Batch 1000/1000: LR=5.81e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:17:53,415 | INFO | Epoch 1718 Train Time 37.08985352516174s

2025-10-15 08:18:30,250 | INFO | Training epoch 1719, Batch 1000/1000: LR=5.78e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:18:30,349 | INFO | Epoch 1719 Train Time 36.93253684043884s

2025-10-15 08:19:07,056 | INFO | Training epoch 1720, Batch 1000/1000: LR=5.74e-06, Loss=2.78e-02 BER=1.03e-02 FER=1.35e-01
2025-10-15 08:19:07,136 | INFO | Epoch 1720 Train Time 36.78561592102051s

2025-10-15 08:19:44,015 | INFO | Training epoch 1721, Batch 1000/1000: LR=5.71e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:19:44,087 | INFO | Epoch 1721 Train Time 36.94994854927063s

2025-10-15 08:19:44,088 | INFO | [P2] saving best_model (QAT) with loss 0.027262 at epoch 1721
2025-10-15 08:20:19,933 | INFO | Training epoch 1722, Batch 1000/1000: LR=5.68e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:20:20,007 | INFO | Epoch 1722 Train Time 35.9015154838562s

2025-10-15 08:20:57,037 | INFO | Training epoch 1723, Batch 1000/1000: LR=5.65e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:20:57,105 | INFO | Epoch 1723 Train Time 37.097188234329224s

2025-10-15 08:21:33,540 | INFO | Training epoch 1724, Batch 1000/1000: LR=5.61e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:21:33,622 | INFO | Epoch 1724 Train Time 36.51598143577576s

2025-10-15 08:22:10,321 | INFO | Training epoch 1725, Batch 1000/1000: LR=5.58e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 08:22:10,412 | INFO | Epoch 1725 Train Time 36.788581132888794s

2025-10-15 08:22:46,936 | INFO | Training epoch 1726, Batch 1000/1000: LR=5.55e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:22:47,003 | INFO | Epoch 1726 Train Time 36.5901243686676s

2025-10-15 08:23:23,954 | INFO | Training epoch 1727, Batch 1000/1000: LR=5.51e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:23:24,034 | INFO | Epoch 1727 Train Time 37.029401540756226s

2025-10-15 08:24:01,258 | INFO | Training epoch 1728, Batch 1000/1000: LR=5.48e-06, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:24:01,323 | INFO | Epoch 1728 Train Time 37.28747773170471s

2025-10-15 08:24:38,372 | INFO | Training epoch 1729, Batch 1000/1000: LR=5.45e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:24:38,457 | INFO | Epoch 1729 Train Time 37.13372850418091s

2025-10-15 08:25:15,124 | INFO | Training epoch 1730, Batch 1000/1000: LR=5.42e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:25:15,184 | INFO | Epoch 1730 Train Time 36.72599124908447s

2025-10-15 08:25:51,884 | INFO | Training epoch 1731, Batch 1000/1000: LR=5.39e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:25:51,948 | INFO | Epoch 1731 Train Time 36.76267766952515s

2025-10-15 08:26:29,430 | INFO | Training epoch 1732, Batch 1000/1000: LR=5.35e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:26:29,521 | INFO | Epoch 1732 Train Time 37.57092308998108s

2025-10-15 08:27:04,978 | INFO | Training epoch 1733, Batch 1000/1000: LR=5.32e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:27:05,050 | INFO | Epoch 1733 Train Time 35.52639937400818s

2025-10-15 08:27:41,841 | INFO | Training epoch 1734, Batch 1000/1000: LR=5.29e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:27:41,931 | INFO | Epoch 1734 Train Time 36.87989926338196s

2025-10-15 08:28:17,855 | INFO | Training epoch 1735, Batch 1000/1000: LR=5.26e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:28:17,919 | INFO | Epoch 1735 Train Time 35.986499309539795s

2025-10-15 08:28:54,536 | INFO | Training epoch 1736, Batch 1000/1000: LR=5.23e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:28:54,610 | INFO | Epoch 1736 Train Time 36.689476013183594s

2025-10-15 08:29:31,434 | INFO | Training epoch 1737, Batch 1000/1000: LR=5.20e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:29:31,512 | INFO | Epoch 1737 Train Time 36.90090489387512s

2025-10-15 08:30:07,741 | INFO | Training epoch 1738, Batch 1000/1000: LR=5.16e-06, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:30:07,806 | INFO | Epoch 1738 Train Time 36.293150186538696s

2025-10-15 08:30:44,763 | INFO | Training epoch 1739, Batch 1000/1000: LR=5.13e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 08:30:44,837 | INFO | Epoch 1739 Train Time 37.02926063537598s

2025-10-15 08:31:21,737 | INFO | Training epoch 1740, Batch 1000/1000: LR=5.10e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 08:31:21,815 | INFO | Epoch 1740 Train Time 36.97719049453735s

2025-10-15 08:31:58,333 | INFO | Training epoch 1741, Batch 1000/1000: LR=5.07e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:31:58,412 | INFO | Epoch 1741 Train Time 36.59460496902466s

2025-10-15 08:32:34,130 | INFO | Training epoch 1742, Batch 1000/1000: LR=5.04e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:32:34,198 | INFO | Epoch 1742 Train Time 35.78433179855347s

2025-10-15 08:33:10,259 | INFO | Training epoch 1743, Batch 1000/1000: LR=5.01e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:33:10,325 | INFO | Epoch 1743 Train Time 36.12576603889465s

2025-10-15 08:33:47,042 | INFO | Training epoch 1744, Batch 1000/1000: LR=4.98e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:33:47,119 | INFO | Epoch 1744 Train Time 36.792033433914185s

2025-10-15 08:34:24,054 | INFO | Training epoch 1745, Batch 1000/1000: LR=4.95e-06, Loss=2.81e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 08:34:24,120 | INFO | Epoch 1745 Train Time 36.999664068222046s

2025-10-15 08:35:00,434 | INFO | Training epoch 1746, Batch 1000/1000: LR=4.92e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:35:00,514 | INFO | Epoch 1746 Train Time 36.39326500892639s

2025-10-15 08:35:37,637 | INFO | Training epoch 1747, Batch 1000/1000: LR=4.89e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:35:37,730 | INFO | Epoch 1747 Train Time 37.21465492248535s

2025-10-15 08:36:14,998 | INFO | Training epoch 1748, Batch 1000/1000: LR=4.86e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 08:36:15,078 | INFO | Epoch 1748 Train Time 37.346442461013794s

2025-10-15 08:36:51,684 | INFO | Training epoch 1749, Batch 1000/1000: LR=4.83e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:36:51,784 | INFO | Epoch 1749 Train Time 36.70392727851868s

2025-10-15 08:37:28,235 | INFO | Training epoch 1750, Batch 1000/1000: LR=4.80e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:37:28,325 | INFO | Epoch 1750 Train Time 36.53984189033508s

2025-10-15 08:38:04,646 | INFO | Training epoch 1751, Batch 1000/1000: LR=4.77e-06, Loss=2.82e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 08:38:04,707 | INFO | Epoch 1751 Train Time 36.380507469177246s

2025-10-15 08:38:41,273 | INFO | Training epoch 1752, Batch 1000/1000: LR=4.74e-06, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:38:41,341 | INFO | Epoch 1752 Train Time 36.631853103637695s

2025-10-15 08:39:17,731 | INFO | Training epoch 1753, Batch 1000/1000: LR=4.71e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 08:39:17,804 | INFO | Epoch 1753 Train Time 36.46178674697876s

2025-10-15 08:39:54,687 | INFO | Training epoch 1754, Batch 1000/1000: LR=4.68e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:39:54,764 | INFO | Epoch 1754 Train Time 36.95873165130615s

2025-10-15 08:40:31,635 | INFO | Training epoch 1755, Batch 1000/1000: LR=4.65e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:40:31,720 | INFO | Epoch 1755 Train Time 36.953426122665405s

2025-10-15 08:41:08,134 | INFO | Training epoch 1756, Batch 1000/1000: LR=4.62e-06, Loss=2.75e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 08:41:08,223 | INFO | Epoch 1756 Train Time 36.50153112411499s

2025-10-15 08:41:44,455 | INFO | Training epoch 1757, Batch 1000/1000: LR=4.59e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 08:41:44,520 | INFO | Epoch 1757 Train Time 36.29619812965393s

2025-10-15 08:42:20,760 | INFO | Training epoch 1758, Batch 1000/1000: LR=4.56e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:42:20,825 | INFO | Epoch 1758 Train Time 36.30392360687256s

2025-10-15 08:42:57,574 | INFO | Training epoch 1759, Batch 1000/1000: LR=4.53e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:42:57,646 | INFO | Epoch 1759 Train Time 36.819029569625854s

2025-10-15 08:43:34,131 | INFO | Training epoch 1760, Batch 1000/1000: LR=4.50e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:43:34,207 | INFO | Epoch 1760 Train Time 36.56018853187561s

2025-10-15 08:44:11,036 | INFO | Training epoch 1761, Batch 1000/1000: LR=4.48e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:44:11,115 | INFO | Epoch 1761 Train Time 36.90711951255798s

2025-10-15 08:44:48,153 | INFO | Training epoch 1762, Batch 1000/1000: LR=4.45e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:44:48,225 | INFO | Epoch 1762 Train Time 37.10906910896301s

2025-10-15 08:45:24,625 | INFO | Training epoch 1763, Batch 1000/1000: LR=4.42e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:45:24,711 | INFO | Epoch 1763 Train Time 36.48539733886719s

2025-10-15 08:46:01,530 | INFO | Training epoch 1764, Batch 1000/1000: LR=4.39e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:46:01,589 | INFO | Epoch 1764 Train Time 36.87670922279358s

2025-10-15 08:46:37,533 | INFO | Training epoch 1765, Batch 1000/1000: LR=4.36e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:46:37,601 | INFO | Epoch 1765 Train Time 36.01075029373169s

2025-10-15 08:47:13,934 | INFO | Training epoch 1766, Batch 1000/1000: LR=4.33e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 08:47:14,018 | INFO | Epoch 1766 Train Time 36.41529893875122s

2025-10-15 08:47:51,333 | INFO | Training epoch 1767, Batch 1000/1000: LR=4.31e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:47:51,414 | INFO | Epoch 1767 Train Time 37.39513826370239s

2025-10-15 08:48:28,252 | INFO | Training epoch 1768, Batch 1000/1000: LR=4.28e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 08:48:28,339 | INFO | Epoch 1768 Train Time 36.92374801635742s

2025-10-15 08:49:04,636 | INFO | Training epoch 1769, Batch 1000/1000: LR=4.25e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 08:49:04,723 | INFO | Epoch 1769 Train Time 36.382933616638184s

2025-10-15 08:49:42,044 | INFO | Training epoch 1770, Batch 1000/1000: LR=4.22e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:49:42,115 | INFO | Epoch 1770 Train Time 37.39142203330994s

2025-10-15 08:50:18,759 | INFO | Training epoch 1771, Batch 1000/1000: LR=4.20e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:50:18,839 | INFO | Epoch 1771 Train Time 36.72204637527466s

2025-10-15 08:50:55,321 | INFO | Training epoch 1772, Batch 1000/1000: LR=4.17e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:50:55,409 | INFO | Epoch 1772 Train Time 36.568238735198975s

2025-10-15 08:51:31,955 | INFO | Training epoch 1773, Batch 1000/1000: LR=4.14e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:51:32,034 | INFO | Epoch 1773 Train Time 36.623450756073s

2025-10-15 08:52:08,978 | INFO | Training epoch 1774, Batch 1000/1000: LR=4.11e-06, Loss=2.78e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 08:52:09,053 | INFO | Epoch 1774 Train Time 37.01830697059631s

2025-10-15 08:52:47,265 | INFO | Training epoch 1775, Batch 1000/1000: LR=4.09e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:52:47,336 | INFO | Epoch 1775 Train Time 38.28193950653076s

2025-10-15 08:53:24,437 | INFO | Training epoch 1776, Batch 1000/1000: LR=4.06e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 08:53:24,507 | INFO | Epoch 1776 Train Time 37.16982960700989s

2025-10-15 08:54:00,938 | INFO | Training epoch 1777, Batch 1000/1000: LR=4.03e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 08:54:01,018 | INFO | Epoch 1777 Train Time 36.51106643676758s

2025-10-15 08:54:36,662 | INFO | Training epoch 1778, Batch 1000/1000: LR=4.01e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:54:36,728 | INFO | Epoch 1778 Train Time 35.708311319351196s

2025-10-15 08:55:13,223 | INFO | Training epoch 1779, Batch 1000/1000: LR=3.98e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:55:13,311 | INFO | Epoch 1779 Train Time 36.58178186416626s

2025-10-15 08:55:50,040 | INFO | Training epoch 1780, Batch 1000/1000: LR=3.95e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 08:55:50,127 | INFO | Epoch 1780 Train Time 36.8144748210907s

2025-10-15 08:56:27,033 | INFO | Training epoch 1781, Batch 1000/1000: LR=3.93e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 08:56:27,100 | INFO | Epoch 1781 Train Time 36.9723846912384s

2025-10-15 08:57:03,638 | INFO | Training epoch 1782, Batch 1000/1000: LR=3.90e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 08:57:03,708 | INFO | Epoch 1782 Train Time 36.605998039245605s

2025-10-15 08:57:40,439 | INFO | Training epoch 1783, Batch 1000/1000: LR=3.87e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 08:57:40,515 | INFO | Epoch 1783 Train Time 36.8048460483551s

2025-10-15 08:58:17,257 | INFO | Training epoch 1784, Batch 1000/1000: LR=3.85e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 08:58:17,344 | INFO | Epoch 1784 Train Time 36.82651662826538s

2025-10-15 08:58:17,345 | INFO | [P2] saving best_model (QAT) with loss 0.027151 at epoch 1784
2025-10-15 08:58:53,938 | INFO | Training epoch 1785, Batch 1000/1000: LR=3.82e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 08:58:54,018 | INFO | Epoch 1785 Train Time 36.64307999610901s

2025-10-15 08:59:31,307 | INFO | Training epoch 1786, Batch 1000/1000: LR=3.80e-06, Loss=2.78e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 08:59:31,378 | INFO | Epoch 1786 Train Time 37.359503984451294s

2025-10-15 09:00:07,241 | INFO | Training epoch 1787, Batch 1000/1000: LR=3.77e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:00:07,320 | INFO | Epoch 1787 Train Time 35.940210580825806s

2025-10-15 09:00:44,118 | INFO | Training epoch 1788, Batch 1000/1000: LR=3.74e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:00:44,206 | INFO | Epoch 1788 Train Time 36.884034395217896s

2025-10-15 09:01:21,070 | INFO | Training epoch 1789, Batch 1000/1000: LR=3.72e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 09:01:21,167 | INFO | Epoch 1789 Train Time 36.96027684211731s

2025-10-15 09:01:57,620 | INFO | Training epoch 1790, Batch 1000/1000: LR=3.69e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:01:57,677 | INFO | Epoch 1790 Train Time 36.50885486602783s

2025-10-15 09:02:34,130 | INFO | Training epoch 1791, Batch 1000/1000: LR=3.67e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:02:34,201 | INFO | Epoch 1791 Train Time 36.52258253097534s

2025-10-15 09:03:10,837 | INFO | Training epoch 1792, Batch 1000/1000: LR=3.64e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:03:10,918 | INFO | Epoch 1792 Train Time 36.71639108657837s

2025-10-15 09:03:47,266 | INFO | Training epoch 1793, Batch 1000/1000: LR=3.62e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 09:03:47,349 | INFO | Epoch 1793 Train Time 36.428699254989624s

2025-10-15 09:04:23,633 | INFO | Training epoch 1794, Batch 1000/1000: LR=3.59e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 09:04:23,711 | INFO | Epoch 1794 Train Time 36.361409425735474s

2025-10-15 09:04:59,929 | INFO | Training epoch 1795, Batch 1000/1000: LR=3.57e-06, Loss=2.80e-02 BER=1.06e-02 FER=1.35e-01
2025-10-15 09:05:00,019 | INFO | Epoch 1795 Train Time 36.30660533905029s

2025-10-15 09:05:36,825 | INFO | Training epoch 1796, Batch 1000/1000: LR=3.54e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 09:05:36,912 | INFO | Epoch 1796 Train Time 36.8916175365448s

2025-10-15 09:06:13,633 | INFO | Training epoch 1797, Batch 1000/1000: LR=3.52e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:06:13,700 | INFO | Epoch 1797 Train Time 36.78700637817383s

2025-10-15 09:06:50,233 | INFO | Training epoch 1798, Batch 1000/1000: LR=3.50e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:06:50,324 | INFO | Epoch 1798 Train Time 36.623074531555176s

2025-10-15 09:07:26,863 | INFO | Training epoch 1799, Batch 1000/1000: LR=3.47e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:07:26,928 | INFO | Epoch 1799 Train Time 36.60387301445007s

2025-10-15 09:08:03,826 | INFO | Training epoch 1800, Batch 1000/1000: LR=3.45e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:08:03,907 | INFO | Epoch 1800 Train Time 36.97827672958374s

2025-10-15 09:08:40,235 | INFO | Training epoch 1801, Batch 1000/1000: LR=3.42e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:08:40,302 | INFO | Epoch 1801 Train Time 36.39365243911743s

2025-10-15 09:09:17,153 | INFO | Training epoch 1802, Batch 1000/1000: LR=3.40e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:09:17,233 | INFO | Epoch 1802 Train Time 36.92884039878845s

2025-10-15 09:09:53,841 | INFO | Training epoch 1803, Batch 1000/1000: LR=3.37e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:09:53,925 | INFO | Epoch 1803 Train Time 36.690940141677856s

2025-10-15 09:10:30,379 | INFO | Training epoch 1804, Batch 1000/1000: LR=3.35e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:10:30,441 | INFO | Epoch 1804 Train Time 36.51545858383179s

2025-10-15 09:11:07,339 | INFO | Training epoch 1805, Batch 1000/1000: LR=3.33e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:11:07,403 | INFO | Epoch 1805 Train Time 36.95996046066284s

2025-10-15 09:11:43,738 | INFO | Training epoch 1806, Batch 1000/1000: LR=3.30e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:11:43,826 | INFO | Epoch 1806 Train Time 36.42072248458862s

2025-10-15 09:12:20,779 | INFO | Training epoch 1807, Batch 1000/1000: LR=3.28e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:12:20,859 | INFO | Epoch 1807 Train Time 37.03212213516235s

2025-10-15 09:12:57,332 | INFO | Training epoch 1808, Batch 1000/1000: LR=3.26e-06, Loss=2.83e-02 BER=1.06e-02 FER=1.37e-01
2025-10-15 09:12:57,413 | INFO | Epoch 1808 Train Time 36.55258250236511s

2025-10-15 09:13:33,940 | INFO | Training epoch 1809, Batch 1000/1000: LR=3.23e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:13:34,020 | INFO | Epoch 1809 Train Time 36.605727434158325s

2025-10-15 09:14:10,733 | INFO | Training epoch 1810, Batch 1000/1000: LR=3.21e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:14:10,804 | INFO | Epoch 1810 Train Time 36.782527923583984s

2025-10-15 09:14:47,227 | INFO | Training epoch 1811, Batch 1000/1000: LR=3.19e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 09:14:47,293 | INFO | Epoch 1811 Train Time 36.487380504608154s

2025-10-15 09:15:24,234 | INFO | Training epoch 1812, Batch 1000/1000: LR=3.17e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 09:15:24,324 | INFO | Epoch 1812 Train Time 37.02978587150574s

2025-10-15 09:16:00,967 | INFO | Training epoch 1813, Batch 1000/1000: LR=3.14e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:16:01,053 | INFO | Epoch 1813 Train Time 36.72771191596985s

2025-10-15 09:16:37,436 | INFO | Training epoch 1814, Batch 1000/1000: LR=3.12e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 09:16:37,503 | INFO | Epoch 1814 Train Time 36.44996929168701s

2025-10-15 09:17:14,350 | INFO | Training epoch 1815, Batch 1000/1000: LR=3.10e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 09:17:14,421 | INFO | Epoch 1815 Train Time 36.916789531707764s

2025-10-15 09:17:51,136 | INFO | Training epoch 1816, Batch 1000/1000: LR=3.08e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 09:17:51,226 | INFO | Epoch 1816 Train Time 36.80338263511658s

2025-10-15 09:18:27,341 | INFO | Training epoch 1817, Batch 1000/1000: LR=3.05e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 09:18:27,417 | INFO | Epoch 1817 Train Time 36.189186811447144s

2025-10-15 09:19:03,734 | INFO | Training epoch 1818, Batch 1000/1000: LR=3.03e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:19:03,807 | INFO | Epoch 1818 Train Time 36.38872790336609s

2025-10-15 09:19:41,551 | INFO | Training epoch 1819, Batch 1000/1000: LR=3.01e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:19:41,616 | INFO | Epoch 1819 Train Time 37.80678033828735s

2025-10-15 09:20:18,341 | INFO | Training epoch 1820, Batch 1000/1000: LR=2.99e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:20:18,417 | INFO | Epoch 1820 Train Time 36.80016016960144s

2025-10-15 09:20:55,449 | INFO | Training epoch 1821, Batch 1000/1000: LR=2.97e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:20:55,526 | INFO | Epoch 1821 Train Time 37.10831904411316s

2025-10-15 09:21:32,662 | INFO | Training epoch 1822, Batch 1000/1000: LR=2.94e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:21:32,731 | INFO | Epoch 1822 Train Time 37.20321011543274s

2025-10-15 09:22:09,045 | INFO | Training epoch 1823, Batch 1000/1000: LR=2.92e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:22:09,113 | INFO | Epoch 1823 Train Time 36.3817458152771s

2025-10-15 09:22:45,530 | INFO | Training epoch 1824, Batch 1000/1000: LR=2.90e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 09:22:45,614 | INFO | Epoch 1824 Train Time 36.498331785202026s

2025-10-15 09:23:24,269 | INFO | Training epoch 1825, Batch 1000/1000: LR=2.88e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:23:24,354 | INFO | Epoch 1825 Train Time 38.73928999900818s

2025-10-15 09:24:01,633 | INFO | Training epoch 1826, Batch 1000/1000: LR=2.86e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:24:01,701 | INFO | Epoch 1826 Train Time 37.3452308177948s

2025-10-15 09:24:38,226 | INFO | Training epoch 1827, Batch 1000/1000: LR=2.84e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:24:38,298 | INFO | Epoch 1827 Train Time 36.5950825214386s

2025-10-15 09:25:14,839 | INFO | Training epoch 1828, Batch 1000/1000: LR=2.82e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:25:14,921 | INFO | Epoch 1828 Train Time 36.62215209007263s

2025-10-15 09:25:51,633 | INFO | Training epoch 1829, Batch 1000/1000: LR=2.80e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 09:25:51,718 | INFO | Epoch 1829 Train Time 36.79579567909241s

2025-10-15 09:26:28,444 | INFO | Training epoch 1830, Batch 1000/1000: LR=2.77e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:26:28,515 | INFO | Epoch 1830 Train Time 36.79532241821289s

2025-10-15 09:27:04,769 | INFO | Training epoch 1831, Batch 1000/1000: LR=2.75e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 09:27:04,855 | INFO | Epoch 1831 Train Time 36.33907771110535s

2025-10-15 09:27:41,730 | INFO | Training epoch 1832, Batch 1000/1000: LR=2.73e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:27:41,814 | INFO | Epoch 1832 Train Time 36.9574990272522s

2025-10-15 09:28:18,038 | INFO | Training epoch 1833, Batch 1000/1000: LR=2.71e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 09:28:18,111 | INFO | Epoch 1833 Train Time 36.296605825424194s

2025-10-15 09:28:54,330 | INFO | Training epoch 1834, Batch 1000/1000: LR=2.69e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:28:54,396 | INFO | Epoch 1834 Train Time 36.28321361541748s

2025-10-15 09:29:30,769 | INFO | Training epoch 1835, Batch 1000/1000: LR=2.67e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:29:30,840 | INFO | Epoch 1835 Train Time 36.44291663169861s

2025-10-15 09:30:06,725 | INFO | Training epoch 1836, Batch 1000/1000: LR=2.65e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:30:06,793 | INFO | Epoch 1836 Train Time 35.95069169998169s

2025-10-15 09:30:43,642 | INFO | Training epoch 1837, Batch 1000/1000: LR=2.63e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 09:30:43,705 | INFO | Epoch 1837 Train Time 36.91117286682129s

2025-10-15 09:31:20,433 | INFO | Training epoch 1838, Batch 1000/1000: LR=2.61e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 09:31:20,523 | INFO | Epoch 1838 Train Time 36.81725525856018s

2025-10-15 09:31:57,070 | INFO | Training epoch 1839, Batch 1000/1000: LR=2.59e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:31:57,142 | INFO | Epoch 1839 Train Time 36.618242263793945s

2025-10-15 09:32:33,838 | INFO | Training epoch 1840, Batch 1000/1000: LR=2.57e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 09:32:33,913 | INFO | Epoch 1840 Train Time 36.77017068862915s

2025-10-15 09:33:10,628 | INFO | Training epoch 1841, Batch 1000/1000: LR=2.56e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:33:10,699 | INFO | Epoch 1841 Train Time 36.78428339958191s

2025-10-15 09:33:47,235 | INFO | Training epoch 1842, Batch 1000/1000: LR=2.54e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:33:47,328 | INFO | Epoch 1842 Train Time 36.627872705459595s

2025-10-15 09:34:23,937 | INFO | Training epoch 1843, Batch 1000/1000: LR=2.52e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:34:24,013 | INFO | Epoch 1843 Train Time 36.684038162231445s

2025-10-15 09:35:00,443 | INFO | Training epoch 1844, Batch 1000/1000: LR=2.50e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 09:35:00,502 | INFO | Epoch 1844 Train Time 36.488264322280884s

2025-10-15 09:35:37,063 | INFO | Training epoch 1845, Batch 1000/1000: LR=2.48e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 09:35:37,133 | INFO | Epoch 1845 Train Time 36.62989282608032s

2025-10-15 09:36:14,342 | INFO | Training epoch 1846, Batch 1000/1000: LR=2.46e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:36:14,421 | INFO | Epoch 1846 Train Time 37.28648567199707s

2025-10-15 09:36:51,070 | INFO | Training epoch 1847, Batch 1000/1000: LR=2.44e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:36:51,137 | INFO | Epoch 1847 Train Time 36.71520495414734s

2025-10-15 09:37:28,141 | INFO | Training epoch 1848, Batch 1000/1000: LR=2.42e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:37:28,206 | INFO | Epoch 1848 Train Time 37.06584620475769s

2025-10-15 09:38:04,955 | INFO | Training epoch 1849, Batch 1000/1000: LR=2.40e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:38:05,027 | INFO | Epoch 1849 Train Time 36.81911301612854s

2025-10-15 09:38:42,133 | INFO | Training epoch 1850, Batch 1000/1000: LR=2.39e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:38:42,221 | INFO | Epoch 1850 Train Time 37.19277596473694s

2025-10-15 09:39:19,447 | INFO | Training epoch 1851, Batch 1000/1000: LR=2.37e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:39:19,523 | INFO | Epoch 1851 Train Time 37.30078721046448s

2025-10-15 09:39:55,853 | INFO | Training epoch 1852, Batch 1000/1000: LR=2.35e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:39:55,923 | INFO | Epoch 1852 Train Time 36.39797782897949s

2025-10-15 09:40:32,437 | INFO | Training epoch 1853, Batch 1000/1000: LR=2.33e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:40:32,505 | INFO | Epoch 1853 Train Time 36.58069729804993s

2025-10-15 09:41:08,559 | INFO | Training epoch 1854, Batch 1000/1000: LR=2.31e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:41:08,629 | INFO | Epoch 1854 Train Time 36.12180042266846s

2025-10-15 09:41:45,433 | INFO | Training epoch 1855, Batch 1000/1000: LR=2.30e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:41:45,513 | INFO | Epoch 1855 Train Time 36.882426500320435s

2025-10-15 09:42:22,241 | INFO | Training epoch 1856, Batch 1000/1000: LR=2.28e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 09:42:22,313 | INFO | Epoch 1856 Train Time 36.79916787147522s

2025-10-15 09:42:59,179 | INFO | Training epoch 1857, Batch 1000/1000: LR=2.26e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:42:59,243 | INFO | Epoch 1857 Train Time 36.92898344993591s

2025-10-15 09:43:34,744 | INFO | Training epoch 1858, Batch 1000/1000: LR=2.24e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 09:43:34,818 | INFO | Epoch 1858 Train Time 35.57407307624817s

2025-10-15 09:44:11,736 | INFO | Training epoch 1859, Batch 1000/1000: LR=2.23e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:44:11,818 | INFO | Epoch 1859 Train Time 36.999234199523926s

2025-10-15 09:44:48,563 | INFO | Training epoch 1860, Batch 1000/1000: LR=2.21e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:44:48,632 | INFO | Epoch 1860 Train Time 36.812087059020996s

2025-10-15 09:45:25,229 | INFO | Training epoch 1861, Batch 1000/1000: LR=2.19e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 09:45:25,307 | INFO | Epoch 1861 Train Time 36.67417240142822s

2025-10-15 09:46:01,741 | INFO | Training epoch 1862, Batch 1000/1000: LR=2.18e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:46:01,818 | INFO | Epoch 1862 Train Time 36.50981426239014s

2025-10-15 09:46:38,442 | INFO | Training epoch 1863, Batch 1000/1000: LR=2.16e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:46:38,517 | INFO | Epoch 1863 Train Time 36.698179721832275s

2025-10-15 09:47:15,341 | INFO | Training epoch 1864, Batch 1000/1000: LR=2.14e-06, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:47:15,421 | INFO | Epoch 1864 Train Time 36.903642654418945s

2025-10-15 09:47:52,436 | INFO | Training epoch 1865, Batch 1000/1000: LR=2.13e-06, Loss=2.81e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 09:47:52,526 | INFO | Epoch 1865 Train Time 37.10304880142212s

2025-10-15 09:48:28,855 | INFO | Training epoch 1866, Batch 1000/1000: LR=2.11e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:48:28,932 | INFO | Epoch 1866 Train Time 36.404461145401s

2025-10-15 09:49:04,030 | INFO | Training epoch 1867, Batch 1000/1000: LR=2.09e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 09:49:04,097 | INFO | Epoch 1867 Train Time 35.163330078125s

2025-10-15 09:49:41,042 | INFO | Training epoch 1868, Batch 1000/1000: LR=2.08e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:49:41,107 | INFO | Epoch 1868 Train Time 37.008042335510254s

2025-10-15 09:50:17,937 | INFO | Training epoch 1869, Batch 1000/1000: LR=2.06e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:50:18,018 | INFO | Epoch 1869 Train Time 36.90956497192383s

2025-10-15 09:50:55,179 | INFO | Training epoch 1870, Batch 1000/1000: LR=2.04e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:50:55,265 | INFO | Epoch 1870 Train Time 37.24409770965576s

2025-10-15 09:51:32,340 | INFO | Training epoch 1871, Batch 1000/1000: LR=2.03e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 09:51:32,413 | INFO | Epoch 1871 Train Time 37.145570039749146s

2025-10-15 09:52:09,433 | INFO | Training epoch 1872, Batch 1000/1000: LR=2.01e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:52:09,504 | INFO | Epoch 1872 Train Time 37.08978295326233s

2025-10-15 09:52:46,032 | INFO | Training epoch 1873, Batch 1000/1000: LR=2.00e-06, Loss=2.81e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:52:46,111 | INFO | Epoch 1873 Train Time 36.60657072067261s

2025-10-15 09:53:22,556 | INFO | Training epoch 1874, Batch 1000/1000: LR=1.98e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:53:22,627 | INFO | Epoch 1874 Train Time 36.51493740081787s

2025-10-15 09:53:59,643 | INFO | Training epoch 1875, Batch 1000/1000: LR=1.97e-06, Loss=2.70e-02 BER=1.01e-02 FER=1.31e-01
2025-10-15 09:53:59,727 | INFO | Epoch 1875 Train Time 37.09811615943909s

2025-10-15 09:53:59,727 | INFO | [P2] saving best_model (QAT) with loss 0.027019 at epoch 1875
2025-10-15 09:54:36,644 | INFO | Training epoch 1876, Batch 1000/1000: LR=1.95e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:54:36,736 | INFO | Epoch 1876 Train Time 36.987773418426514s

2025-10-15 09:55:13,378 | INFO | Training epoch 1877, Batch 1000/1000: LR=1.94e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 09:55:13,445 | INFO | Epoch 1877 Train Time 36.7079381942749s

2025-10-15 09:55:50,235 | INFO | Training epoch 1878, Batch 1000/1000: LR=1.92e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 09:55:50,303 | INFO | Epoch 1878 Train Time 36.857298612594604s

2025-10-15 09:56:27,560 | INFO | Training epoch 1879, Batch 1000/1000: LR=1.91e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:56:27,639 | INFO | Epoch 1879 Train Time 37.33552432060242s

2025-10-15 09:57:04,530 | INFO | Training epoch 1880, Batch 1000/1000: LR=1.89e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:57:04,600 | INFO | Epoch 1880 Train Time 36.958269119262695s

2025-10-15 09:57:41,311 | INFO | Training epoch 1881, Batch 1000/1000: LR=1.88e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 09:57:41,401 | INFO | Epoch 1881 Train Time 36.79975867271423s

2025-10-15 09:58:18,366 | INFO | Training epoch 1882, Batch 1000/1000: LR=1.86e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 09:58:18,442 | INFO | Epoch 1882 Train Time 37.03771924972534s

2025-10-15 09:58:55,136 | INFO | Training epoch 1883, Batch 1000/1000: LR=1.85e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 09:58:55,203 | INFO | Epoch 1883 Train Time 36.76024627685547s

2025-10-15 09:59:32,184 | INFO | Training epoch 1884, Batch 1000/1000: LR=1.83e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 09:59:32,266 | INFO | Epoch 1884 Train Time 37.06289839744568s

2025-10-15 10:00:09,172 | INFO | Training epoch 1885, Batch 1000/1000: LR=1.82e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:00:09,236 | INFO | Epoch 1885 Train Time 36.96836519241333s

2025-10-15 10:00:45,350 | INFO | Training epoch 1886, Batch 1000/1000: LR=1.81e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:00:45,432 | INFO | Epoch 1886 Train Time 36.19507884979248s

2025-10-15 10:01:22,430 | INFO | Training epoch 1887, Batch 1000/1000: LR=1.79e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:01:22,507 | INFO | Epoch 1887 Train Time 37.073793172836304s

2025-10-15 10:01:59,532 | INFO | Training epoch 1888, Batch 1000/1000: LR=1.78e-06, Loss=2.78e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:01:59,603 | INFO | Epoch 1888 Train Time 37.09425210952759s

2025-10-15 10:02:36,128 | INFO | Training epoch 1889, Batch 1000/1000: LR=1.76e-06, Loss=2.83e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 10:02:36,209 | INFO | Epoch 1889 Train Time 36.605565786361694s

2025-10-15 10:03:13,132 | INFO | Training epoch 1890, Batch 1000/1000: LR=1.75e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 10:03:13,197 | INFO | Epoch 1890 Train Time 36.98574662208557s

2025-10-15 10:03:50,340 | INFO | Training epoch 1891, Batch 1000/1000: LR=1.74e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:03:50,408 | INFO | Epoch 1891 Train Time 37.20798969268799s

2025-10-15 10:04:27,235 | INFO | Training epoch 1892, Batch 1000/1000: LR=1.72e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:04:27,305 | INFO | Epoch 1892 Train Time 36.89591360092163s

2025-10-15 10:05:03,823 | INFO | Training epoch 1893, Batch 1000/1000: LR=1.71e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 10:05:03,892 | INFO | Epoch 1893 Train Time 36.58558011054993s

2025-10-15 10:05:40,756 | INFO | Training epoch 1894, Batch 1000/1000: LR=1.70e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 10:05:40,817 | INFO | Epoch 1894 Train Time 36.9244921207428s

2025-10-15 10:06:16,841 | INFO | Training epoch 1895, Batch 1000/1000: LR=1.68e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:06:16,910 | INFO | Epoch 1895 Train Time 36.09099721908569s

2025-10-15 10:06:54,035 | INFO | Training epoch 1896, Batch 1000/1000: LR=1.67e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:06:54,098 | INFO | Epoch 1896 Train Time 37.1875479221344s

2025-10-15 10:07:32,628 | INFO | Training epoch 1897, Batch 1000/1000: LR=1.66e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 10:07:32,704 | INFO | Epoch 1897 Train Time 38.605315923690796s

2025-10-15 10:08:09,335 | INFO | Training epoch 1898, Batch 1000/1000: LR=1.65e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:08:09,398 | INFO | Epoch 1898 Train Time 36.69317579269409s

2025-10-15 10:08:46,962 | INFO | Training epoch 1899, Batch 1000/1000: LR=1.63e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:08:47,028 | INFO | Epoch 1899 Train Time 37.62941265106201s

2025-10-15 10:09:23,852 | INFO | Training epoch 1900, Batch 1000/1000: LR=1.62e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:09:23,922 | INFO | Epoch 1900 Train Time 36.89189863204956s

2025-10-15 10:10:00,530 | INFO | Training epoch 1901, Batch 1000/1000: LR=1.61e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 10:10:00,604 | INFO | Epoch 1901 Train Time 36.67904710769653s

2025-10-15 10:10:37,334 | INFO | Training epoch 1902, Batch 1000/1000: LR=1.60e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:10:37,409 | INFO | Epoch 1902 Train Time 36.80440950393677s

2025-10-15 10:11:14,236 | INFO | Training epoch 1903, Batch 1000/1000: LR=1.59e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 10:11:14,317 | INFO | Epoch 1903 Train Time 36.90688228607178s

2025-10-15 10:11:50,975 | INFO | Training epoch 1904, Batch 1000/1000: LR=1.57e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 10:11:51,042 | INFO | Epoch 1904 Train Time 36.72382092475891s

2025-10-15 10:12:27,075 | INFO | Training epoch 1905, Batch 1000/1000: LR=1.56e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:12:27,158 | INFO | Epoch 1905 Train Time 36.11554670333862s

2025-10-15 10:13:04,019 | INFO | Training epoch 1906, Batch 1000/1000: LR=1.55e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:13:04,098 | INFO | Epoch 1906 Train Time 36.939141511917114s

2025-10-15 10:13:40,968 | INFO | Training epoch 1907, Batch 1000/1000: LR=1.54e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:13:41,050 | INFO | Epoch 1907 Train Time 36.9505078792572s

2025-10-15 10:14:18,240 | INFO | Training epoch 1908, Batch 1000/1000: LR=1.53e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:14:18,313 | INFO | Epoch 1908 Train Time 37.26217293739319s

2025-10-15 10:14:54,954 | INFO | Training epoch 1909, Batch 1000/1000: LR=1.52e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:14:55,018 | INFO | Epoch 1909 Train Time 36.70366930961609s

2025-10-15 10:15:31,533 | INFO | Training epoch 1910, Batch 1000/1000: LR=1.50e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:15:31,618 | INFO | Epoch 1910 Train Time 36.59825801849365s

2025-10-15 10:16:08,636 | INFO | Training epoch 1911, Batch 1000/1000: LR=1.49e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:16:08,714 | INFO | Epoch 1911 Train Time 37.095569372177124s

2025-10-15 10:16:45,226 | INFO | Training epoch 1912, Batch 1000/1000: LR=1.48e-06, Loss=2.73e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:16:45,305 | INFO | Epoch 1912 Train Time 36.589714765548706s

2025-10-15 10:17:21,933 | INFO | Training epoch 1913, Batch 1000/1000: LR=1.47e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:17:22,019 | INFO | Epoch 1913 Train Time 36.712987422943115s

2025-10-15 10:17:59,238 | INFO | Training epoch 1914, Batch 1000/1000: LR=1.46e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:17:59,314 | INFO | Epoch 1914 Train Time 37.2941632270813s

2025-10-15 10:18:35,941 | INFO | Training epoch 1915, Batch 1000/1000: LR=1.45e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:18:36,025 | INFO | Epoch 1915 Train Time 36.709646224975586s

2025-10-15 10:19:13,439 | INFO | Training epoch 1916, Batch 1000/1000: LR=1.44e-06, Loss=2.82e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 10:19:13,510 | INFO | Epoch 1916 Train Time 37.48427104949951s

2025-10-15 10:19:51,824 | INFO | Training epoch 1917, Batch 1000/1000: LR=1.43e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 10:19:51,888 | INFO | Epoch 1917 Train Time 38.37629151344299s

2025-10-15 10:20:28,641 | INFO | Training epoch 1918, Batch 1000/1000: LR=1.42e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:20:28,716 | INFO | Epoch 1918 Train Time 36.82726049423218s

2025-10-15 10:21:05,322 | INFO | Training epoch 1919, Batch 1000/1000: LR=1.41e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 10:21:05,384 | INFO | Epoch 1919 Train Time 36.66682052612305s

2025-10-15 10:21:42,442 | INFO | Training epoch 1920, Batch 1000/1000: LR=1.40e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 10:21:42,507 | INFO | Epoch 1920 Train Time 37.12110090255737s

2025-10-15 10:22:18,538 | INFO | Training epoch 1921, Batch 1000/1000: LR=1.39e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:22:18,619 | INFO | Epoch 1921 Train Time 36.11121726036072s

2025-10-15 10:22:55,249 | INFO | Training epoch 1922, Batch 1000/1000: LR=1.38e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:22:55,322 | INFO | Epoch 1922 Train Time 36.701704025268555s

2025-10-15 10:23:31,953 | INFO | Training epoch 1923, Batch 1000/1000: LR=1.37e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 10:23:32,037 | INFO | Epoch 1923 Train Time 36.713531255722046s

2025-10-15 10:24:09,140 | INFO | Training epoch 1924, Batch 1000/1000: LR=1.36e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 10:24:09,216 | INFO | Epoch 1924 Train Time 37.17857217788696s

2025-10-15 10:24:44,153 | INFO | Training epoch 1925, Batch 1000/1000: LR=1.35e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 10:24:44,219 | INFO | Epoch 1925 Train Time 35.00173592567444s

2025-10-15 10:25:20,731 | INFO | Training epoch 1926, Batch 1000/1000: LR=1.34e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:25:20,812 | INFO | Epoch 1926 Train Time 36.592204570770264s

2025-10-15 10:25:57,639 | INFO | Training epoch 1927, Batch 1000/1000: LR=1.33e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:25:57,723 | INFO | Epoch 1927 Train Time 36.91034722328186s

2025-10-15 10:26:34,335 | INFO | Training epoch 1928, Batch 1000/1000: LR=1.33e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:26:34,413 | INFO | Epoch 1928 Train Time 36.68878126144409s

2025-10-15 10:27:09,847 | INFO | Training epoch 1929, Batch 1000/1000: LR=1.32e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:27:09,921 | INFO | Epoch 1929 Train Time 35.50707292556763s

2025-10-15 10:27:47,132 | INFO | Training epoch 1930, Batch 1000/1000: LR=1.31e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:27:47,210 | INFO | Epoch 1930 Train Time 37.2865514755249s

2025-10-15 10:28:24,037 | INFO | Training epoch 1931, Batch 1000/1000: LR=1.30e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:28:24,107 | INFO | Epoch 1931 Train Time 36.896262645721436s

2025-10-15 10:29:01,052 | INFO | Training epoch 1932, Batch 1000/1000: LR=1.29e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:29:01,122 | INFO | Epoch 1932 Train Time 37.01412010192871s

2025-10-15 10:29:40,067 | INFO | Training epoch 1933, Batch 1000/1000: LR=1.28e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:29:40,134 | INFO | Epoch 1933 Train Time 39.0103063583374s

2025-10-15 10:30:16,975 | INFO | Training epoch 1934, Batch 1000/1000: LR=1.27e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:30:17,056 | INFO | Epoch 1934 Train Time 36.91989064216614s

2025-10-15 10:30:53,532 | INFO | Training epoch 1935, Batch 1000/1000: LR=1.27e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 10:30:53,610 | INFO | Epoch 1935 Train Time 36.55311560630798s

2025-10-15 10:31:29,976 | INFO | Training epoch 1936, Batch 1000/1000: LR=1.26e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:31:30,040 | INFO | Epoch 1936 Train Time 36.42959523200989s

2025-10-15 10:32:06,936 | INFO | Training epoch 1937, Batch 1000/1000: LR=1.25e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:32:07,041 | INFO | Epoch 1937 Train Time 36.999616861343384s

2025-10-15 10:32:43,544 | INFO | Training epoch 1938, Batch 1000/1000: LR=1.24e-06, Loss=2.82e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:32:43,619 | INFO | Epoch 1938 Train Time 36.575785636901855s

2025-10-15 10:33:20,638 | INFO | Training epoch 1939, Batch 1000/1000: LR=1.23e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:33:20,709 | INFO | Epoch 1939 Train Time 37.088897466659546s

2025-10-15 10:33:56,438 | INFO | Training epoch 1940, Batch 1000/1000: LR=1.23e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 10:33:56,520 | INFO | Epoch 1940 Train Time 35.808775901794434s

2025-10-15 10:34:33,438 | INFO | Training epoch 1941, Batch 1000/1000: LR=1.22e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:34:33,513 | INFO | Epoch 1941 Train Time 36.99151396751404s

2025-10-15 10:35:09,825 | INFO | Training epoch 1942, Batch 1000/1000: LR=1.21e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:35:09,891 | INFO | Epoch 1942 Train Time 36.377238035202026s

2025-10-15 10:35:46,630 | INFO | Training epoch 1943, Batch 1000/1000: LR=1.21e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:35:46,709 | INFO | Epoch 1943 Train Time 36.81571888923645s

2025-10-15 10:36:23,633 | INFO | Training epoch 1944, Batch 1000/1000: LR=1.20e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:36:23,721 | INFO | Epoch 1944 Train Time 37.011022329330444s

2025-10-15 10:37:00,564 | INFO | Training epoch 1945, Batch 1000/1000: LR=1.19e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 10:37:00,633 | INFO | Epoch 1945 Train Time 36.910895586013794s

2025-10-15 10:37:36,136 | INFO | Training epoch 1946, Batch 1000/1000: LR=1.18e-06, Loss=2.81e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:37:36,206 | INFO | Epoch 1946 Train Time 35.57227039337158s

2025-10-15 10:38:12,832 | INFO | Training epoch 1947, Batch 1000/1000: LR=1.18e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:38:12,898 | INFO | Epoch 1947 Train Time 36.69025635719299s

2025-10-15 10:38:49,546 | INFO | Training epoch 1948, Batch 1000/1000: LR=1.17e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 10:38:49,614 | INFO | Epoch 1948 Train Time 36.714871883392334s

2025-10-15 10:39:26,029 | INFO | Training epoch 1949, Batch 1000/1000: LR=1.17e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:39:26,104 | INFO | Epoch 1949 Train Time 36.48794865608215s

2025-10-15 10:40:02,735 | INFO | Training epoch 1950, Batch 1000/1000: LR=1.16e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 10:40:02,824 | INFO | Epoch 1950 Train Time 36.71861743927002s

2025-10-15 10:40:39,767 | INFO | Training epoch 1951, Batch 1000/1000: LR=1.15e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:40:39,843 | INFO | Epoch 1951 Train Time 37.017833948135376s

2025-10-15 10:41:16,640 | INFO | Training epoch 1952, Batch 1000/1000: LR=1.15e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.33e-01
2025-10-15 10:41:16,714 | INFO | Epoch 1952 Train Time 36.869643688201904s

2025-10-15 10:41:52,835 | INFO | Training epoch 1953, Batch 1000/1000: LR=1.14e-06, Loss=2.82e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 10:41:52,910 | INFO | Epoch 1953 Train Time 36.195170640945435s

2025-10-15 10:42:29,967 | INFO | Training epoch 1954, Batch 1000/1000: LR=1.13e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 10:42:30,037 | INFO | Epoch 1954 Train Time 37.125921964645386s

2025-10-15 10:43:06,841 | INFO | Training epoch 1955, Batch 1000/1000: LR=1.13e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:43:06,916 | INFO | Epoch 1955 Train Time 36.87831687927246s

2025-10-15 10:43:43,628 | INFO | Training epoch 1956, Batch 1000/1000: LR=1.12e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:43:43,709 | INFO | Epoch 1956 Train Time 36.791736364364624s

2025-10-15 10:44:20,581 | INFO | Training epoch 1957, Batch 1000/1000: LR=1.12e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:44:20,648 | INFO | Epoch 1957 Train Time 36.938684701919556s

2025-10-15 10:44:57,738 | INFO | Training epoch 1958, Batch 1000/1000: LR=1.11e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.34e-01
2025-10-15 10:44:57,825 | INFO | Epoch 1958 Train Time 37.17431855201721s

2025-10-15 10:45:34,441 | INFO | Training epoch 1959, Batch 1000/1000: LR=1.11e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:45:34,512 | INFO | Epoch 1959 Train Time 36.685975551605225s

2025-10-15 10:46:10,794 | INFO | Training epoch 1960, Batch 1000/1000: LR=1.10e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:46:10,869 | INFO | Epoch 1960 Train Time 36.356276512145996s

2025-10-15 10:46:47,136 | INFO | Training epoch 1961, Batch 1000/1000: LR=1.10e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:46:47,212 | INFO | Epoch 1961 Train Time 36.341182708740234s

2025-10-15 10:47:24,155 | INFO | Training epoch 1962, Batch 1000/1000: LR=1.09e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:47:24,232 | INFO | Epoch 1962 Train Time 37.01797795295715s

2025-10-15 10:48:01,027 | INFO | Training epoch 1963, Batch 1000/1000: LR=1.09e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:48:01,108 | INFO | Epoch 1963 Train Time 36.87581777572632s

2025-10-15 10:48:37,837 | INFO | Training epoch 1964, Batch 1000/1000: LR=1.08e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 10:48:37,904 | INFO | Epoch 1964 Train Time 36.79369854927063s

2025-10-15 10:49:14,234 | INFO | Training epoch 1965, Batch 1000/1000: LR=1.08e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:49:14,296 | INFO | Epoch 1965 Train Time 36.39164853096008s

2025-10-15 10:49:50,638 | INFO | Training epoch 1966, Batch 1000/1000: LR=1.07e-06, Loss=2.73e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 10:49:50,714 | INFO | Epoch 1966 Train Time 36.41668438911438s

2025-10-15 10:50:27,633 | INFO | Training epoch 1967, Batch 1000/1000: LR=1.07e-06, Loss=2.76e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 10:50:27,720 | INFO | Epoch 1967 Train Time 37.005484104156494s

2025-10-15 10:51:04,634 | INFO | Training epoch 1968, Batch 1000/1000: LR=1.07e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:51:04,723 | INFO | Epoch 1968 Train Time 37.00204873085022s

2025-10-15 10:51:41,442 | INFO | Training epoch 1969, Batch 1000/1000: LR=1.06e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:51:41,514 | INFO | Epoch 1969 Train Time 36.79021453857422s

2025-10-15 10:52:17,934 | INFO | Training epoch 1970, Batch 1000/1000: LR=1.06e-06, Loss=2.72e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 10:52:18,002 | INFO | Epoch 1970 Train Time 36.48610973358154s

2025-10-15 10:52:54,055 | INFO | Training epoch 1971, Batch 1000/1000: LR=1.05e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:52:54,117 | INFO | Epoch 1971 Train Time 36.11359524726868s

2025-10-15 10:53:30,434 | INFO | Training epoch 1972, Batch 1000/1000: LR=1.05e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:53:30,520 | INFO | Epoch 1972 Train Time 36.40088939666748s

2025-10-15 10:54:07,339 | INFO | Training epoch 1973, Batch 1000/1000: LR=1.05e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 10:54:07,427 | INFO | Epoch 1973 Train Time 36.906081199645996s

2025-10-15 10:54:44,273 | INFO | Training epoch 1974, Batch 1000/1000: LR=1.04e-06, Loss=2.80e-02 BER=1.06e-02 FER=1.36e-01
2025-10-15 10:54:44,353 | INFO | Epoch 1974 Train Time 36.92535948753357s

2025-10-15 10:55:21,140 | INFO | Training epoch 1975, Batch 1000/1000: LR=1.04e-06, Loss=2.78e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:55:21,222 | INFO | Epoch 1975 Train Time 36.86746168136597s

2025-10-15 10:55:58,032 | INFO | Training epoch 1976, Batch 1000/1000: LR=1.04e-06, Loss=2.80e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 10:55:58,114 | INFO | Epoch 1976 Train Time 36.88990378379822s

2025-10-15 10:56:34,924 | INFO | Training epoch 1977, Batch 1000/1000: LR=1.04e-06, Loss=2.80e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:56:34,991 | INFO | Epoch 1977 Train Time 36.875760316848755s

2025-10-15 10:57:11,830 | INFO | Training epoch 1978, Batch 1000/1000: LR=1.03e-06, Loss=2.79e-02 BER=1.05e-02 FER=1.35e-01
2025-10-15 10:57:11,920 | INFO | Epoch 1978 Train Time 36.92761254310608s

2025-10-15 10:57:48,738 | INFO | Training epoch 1979, Batch 1000/1000: LR=1.03e-06, Loss=2.74e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:57:48,819 | INFO | Epoch 1979 Train Time 36.898656606674194s

2025-10-15 10:58:25,633 | INFO | Training epoch 1980, Batch 1000/1000: LR=1.03e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 10:58:25,712 | INFO | Epoch 1980 Train Time 36.89241313934326s

2025-10-15 10:59:02,768 | INFO | Training epoch 1981, Batch 1000/1000: LR=1.02e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 10:59:02,844 | INFO | Epoch 1981 Train Time 37.130120038986206s

2025-10-15 10:59:39,238 | INFO | Training epoch 1982, Batch 1000/1000: LR=1.02e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 10:59:39,303 | INFO | Epoch 1982 Train Time 36.45905637741089s

2025-10-15 11:00:15,554 | INFO | Training epoch 1983, Batch 1000/1000: LR=1.02e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 11:00:15,621 | INFO | Epoch 1983 Train Time 36.3162317276001s

2025-10-15 11:00:52,236 | INFO | Training epoch 1984, Batch 1000/1000: LR=1.02e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 11:00:52,323 | INFO | Epoch 1984 Train Time 36.701274394989014s

2025-10-15 11:01:28,580 | INFO | Training epoch 1985, Batch 1000/1000: LR=1.02e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 11:01:28,672 | INFO | Epoch 1985 Train Time 36.34809398651123s

2025-10-15 11:02:04,861 | INFO | Training epoch 1986, Batch 1000/1000: LR=1.01e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 11:02:04,935 | INFO | Epoch 1986 Train Time 36.26129126548767s

2025-10-15 11:02:41,655 | INFO | Training epoch 1987, Batch 1000/1000: LR=1.01e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 11:02:41,721 | INFO | Epoch 1987 Train Time 36.78478670120239s

2025-10-15 11:03:18,331 | INFO | Training epoch 1988, Batch 1000/1000: LR=1.01e-06, Loss=2.74e-02 BER=1.02e-02 FER=1.32e-01
2025-10-15 11:03:18,405 | INFO | Epoch 1988 Train Time 36.68211603164673s

2025-10-15 11:03:54,655 | INFO | Training epoch 1989, Batch 1000/1000: LR=1.01e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 11:03:54,743 | INFO | Epoch 1989 Train Time 36.33723783493042s

2025-10-15 11:04:30,952 | INFO | Training epoch 1990, Batch 1000/1000: LR=1.01e-06, Loss=2.77e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 11:04:31,019 | INFO | Epoch 1990 Train Time 36.27512311935425s

2025-10-15 11:05:08,132 | INFO | Training epoch 1991, Batch 1000/1000: LR=1.01e-06, Loss=2.81e-02 BER=1.05e-02 FER=1.36e-01
2025-10-15 11:05:08,202 | INFO | Epoch 1991 Train Time 37.18216919898987s

2025-10-15 11:05:44,637 | INFO | Training epoch 1992, Batch 1000/1000: LR=1.00e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 11:05:44,699 | INFO | Epoch 1992 Train Time 36.49542689323425s

2025-10-15 11:06:21,224 | INFO | Training epoch 1993, Batch 1000/1000: LR=1.00e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 11:06:21,295 | INFO | Epoch 1993 Train Time 36.59552836418152s

2025-10-15 11:06:58,134 | INFO | Training epoch 1994, Batch 1000/1000: LR=1.00e-06, Loss=2.76e-02 BER=1.03e-02 FER=1.34e-01
2025-10-15 11:06:58,204 | INFO | Epoch 1994 Train Time 36.90756678581238s

2025-10-15 11:07:35,843 | INFO | Training epoch 1995, Batch 1000/1000: LR=1.00e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.34e-01
2025-10-15 11:07:35,929 | INFO | Epoch 1995 Train Time 37.72400975227356s

2025-10-15 11:08:13,339 | INFO | Training epoch 1996, Batch 1000/1000: LR=1.00e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.33e-01
2025-10-15 11:08:13,418 | INFO | Epoch 1996 Train Time 37.48804306983948s

2025-10-15 11:08:50,241 | INFO | Training epoch 1997, Batch 1000/1000: LR=1.00e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.35e-01
2025-10-15 11:08:50,334 | INFO | Epoch 1997 Train Time 36.912649631500244s

2025-10-15 11:09:26,945 | INFO | Training epoch 1998, Batch 1000/1000: LR=1.00e-06, Loss=2.77e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 11:09:27,019 | INFO | Epoch 1998 Train Time 36.68431329727173s

2025-10-15 11:10:03,136 | INFO | Training epoch 1999, Batch 1000/1000: LR=1.00e-06, Loss=2.75e-02 BER=1.03e-02 FER=1.32e-01
2025-10-15 11:10:03,204 | INFO | Epoch 1999 Train Time 36.183804750442505s

2025-10-15 11:10:40,027 | INFO | Training epoch 2000, Batch 1000/1000: LR=1.00e-06, Loss=2.79e-02 BER=1.04e-02 FER=1.33e-01
2025-10-15 11:10:40,113 | INFO | Epoch 2000 Train Time 36.90813589096069s

2025-10-15 11:10:40,122 | INFO | Checkpoint saved: runs/20251014_111436/stage2_qat__BCH_n31_k16__Ndec2_d32_h8.pth
2025-10-15 11:10:40,128 | INFO | Checkpoint saved: runs/20251014_111436/stage2_qat__BCH_n31_k16__Ndec2_d32_h8__e2000_loss0.027864.pth
2025-10-15 11:10:40,196 | INFO | Loaded checkpoint: runs/20251014_111436/stage2_qat__BCH_n31_k16__Ndec2_d32_h8.pth (strict=False)
2025-10-15 11:10:40,206 | INFO | Checkpoint saved: runs/20251014_111436/stage2_infer_frozen__BCH_n31_k16__Ndec2_d32_h8__e1875_loss0.027019.pth
2025-10-15 11:10:45,226 | INFO | FER count threshold reached for EbN0:4
2025-10-15 11:10:45,317 | INFO | Test EbN0=4, BER=1.54e-02
2025-10-15 11:10:50,107 | INFO | FER count threshold reached for EbN0:5
2025-10-15 11:10:50,203 | INFO | Test EbN0=5, BER=4.67e-03
2025-10-15 11:10:55,075 | INFO | FER count threshold reached for EbN0:6
2025-10-15 11:10:55,177 | INFO | Test EbN0=6, BER=9.57e-04
2025-10-15 11:10:55,177 | INFO | 
Test Loss 4: 4.0660e-02 5: 1.3434e-02 6: 3.2625e-03
2025-10-15 11:10:55,177 | INFO | Test FER 4: 2.0595e-01 5: 7.6391e-02 6: 1.8704e-02
2025-10-15 11:10:55,177 | INFO | Test BER 4: 1.5442e-02 5: 4.6671e-03 6: 9.5663e-04
2025-10-15 11:10:55,177 | INFO | Test -ln(BER) 4: 4.1707e+00 5: 5.3672e+00 6: 6.9521e+00
2025-10-15 11:10:55,177 | INFO | # of testing samples: [100352.0, 100352.0, 100352.0]
 Test Time 14.971242189407349 s

2025-10-15 11:10:55,180 | INFO | Done.
