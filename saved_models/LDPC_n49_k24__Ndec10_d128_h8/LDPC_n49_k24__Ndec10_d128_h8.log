2025-10-15 15:32:47,691 | INFO | Device: cuda
2025-10-15 15:32:48,919 | INFO | Loaded checkpoint: runs/LDPC_n49_k24__Ndec10_d128_h8/stage1_fp32__LDPC_n49_k24__Ndec10_d128_h8__e1000_loss0.004003.pth (strict=True)
2025-10-15 15:33:37,208 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=4.42e-03 BER=1.81e-03 FER=1.62e-02
2025-10-15 15:33:37,260 | INFO | Epoch 1 Train Time 48.3350887298584s

2025-10-15 15:33:37,261 | INFO | [P1] saving best_model with loss 0.004421 at epoch 1
2025-10-15 15:34:25,402 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=4.66e-03 BER=1.90e-03 FER=1.79e-02
2025-10-15 15:34:25,480 | INFO | Epoch 2 Train Time 48.13657522201538s

2025-10-15 15:35:11,350 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=4.46e-03 BER=1.84e-03 FER=1.67e-02
2025-10-15 15:35:11,411 | INFO | Epoch 3 Train Time 45.92792582511902s

2025-10-15 15:35:58,734 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=4.68e-03 BER=1.88e-03 FER=1.68e-02
2025-10-15 15:35:58,815 | INFO | Epoch 4 Train Time 47.40298008918762s

2025-10-15 15:36:46,778 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=5.59e-03 BER=2.18e-03 FER=2.20e-02
2025-10-15 15:36:46,862 | INFO | Epoch 5 Train Time 48.045350551605225s

2025-10-15 15:37:35,941 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=4.94e-03 BER=1.99e-03 FER=1.84e-02
2025-10-15 15:37:36,030 | INFO | Epoch 6 Train Time 49.16630840301514s

2025-10-15 15:38:22,378 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=4.75e-03 BER=1.92e-03 FER=1.74e-02
2025-10-15 15:38:22,442 | INFO | Epoch 7 Train Time 46.409491300582886s

2025-10-15 15:39:09,258 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=4.76e-03 BER=1.92e-03 FER=1.74e-02
2025-10-15 15:39:09,343 | INFO | Epoch 8 Train Time 46.89976143836975s

2025-10-15 15:39:57,165 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=4.81e-03 BER=1.94e-03 FER=1.75e-02
2025-10-15 15:39:57,228 | INFO | Epoch 9 Train Time 47.88334250450134s

2025-10-15 15:40:45,428 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=4.86e-03 BER=1.98e-03 FER=1.80e-02
2025-10-15 15:40:45,509 | INFO | Epoch 10 Train Time 48.27891516685486s

2025-10-15 15:41:32,332 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=4.86e-03 BER=1.98e-03 FER=1.76e-02
2025-10-15 15:41:32,430 | INFO | Epoch 11 Train Time 46.915523290634155s

2025-10-15 15:42:20,339 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=4.68e-03 BER=1.89e-03 FER=1.71e-02
2025-10-15 15:42:20,424 | INFO | Epoch 12 Train Time 47.99114418029785s

2025-10-15 15:43:08,524 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=4.78e-03 BER=1.93e-03 FER=1.75e-02
2025-10-15 15:43:08,595 | INFO | Epoch 13 Train Time 48.16926670074463s

2025-10-15 15:43:56,481 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=4.81e-03 BER=1.98e-03 FER=1.78e-02
2025-10-15 15:43:56,574 | INFO | Epoch 14 Train Time 47.973681688308716s

2025-10-15 15:44:44,736 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=4.68e-03 BER=1.91e-03 FER=1.71e-02
2025-10-15 15:44:44,823 | INFO | Epoch 15 Train Time 48.24773979187012s

2025-10-15 15:45:30,087 | INFO | Training epoch 16, Batch 1000/1000: LR=9.99e-05, Loss=4.69e-03 BER=1.93e-03 FER=1.74e-02
2025-10-15 15:45:30,148 | INFO | Epoch 16 Train Time 45.32265853881836s

2025-10-15 15:46:15,988 | INFO | Training epoch 17, Batch 1000/1000: LR=9.99e-05, Loss=4.90e-03 BER=2.00e-03 FER=1.81e-02
2025-10-15 15:46:16,049 | INFO | Epoch 17 Train Time 45.89973831176758s

2025-10-15 15:47:04,470 | INFO | Training epoch 18, Batch 1000/1000: LR=9.99e-05, Loss=4.90e-03 BER=2.00e-03 FER=1.80e-02
2025-10-15 15:47:04,536 | INFO | Epoch 18 Train Time 48.486002683639526s

2025-10-15 15:47:46,884 | INFO | Training epoch 19, Batch 1000/1000: LR=9.99e-05, Loss=4.61e-03 BER=1.93e-03 FER=1.72e-02
2025-10-15 15:47:46,943 | INFO | Epoch 19 Train Time 42.40387201309204s

2025-10-15 15:48:33,933 | INFO | Training epoch 20, Batch 1000/1000: LR=9.99e-05, Loss=4.73e-03 BER=1.94e-03 FER=1.72e-02
2025-10-15 15:48:33,991 | INFO | Epoch 20 Train Time 47.04654359817505s

2025-10-15 15:49:16,233 | INFO | Training epoch 21, Batch 1000/1000: LR=9.99e-05, Loss=4.74e-03 BER=1.95e-03 FER=1.72e-02
2025-10-15 15:49:16,299 | INFO | Epoch 21 Train Time 42.30657505989075s

2025-10-15 15:50:02,527 | INFO | Training epoch 22, Batch 1000/1000: LR=9.99e-05, Loss=4.61e-03 BER=1.88e-03 FER=1.70e-02
2025-10-15 15:50:02,601 | INFO | Epoch 22 Train Time 46.300403118133545s

2025-10-15 15:50:46,868 | INFO | Training epoch 23, Batch 1000/1000: LR=9.99e-05, Loss=4.66e-03 BER=1.92e-03 FER=1.69e-02
2025-10-15 15:50:46,940 | INFO | Epoch 23 Train Time 44.33745193481445s

2025-10-15 15:51:35,283 | INFO | Training epoch 24, Batch 1000/1000: LR=9.99e-05, Loss=5.04e-03 BER=2.05e-03 FER=1.83e-02
2025-10-15 15:51:35,345 | INFO | Epoch 24 Train Time 48.40341138839722s

2025-10-15 15:52:23,748 | INFO | Training epoch 25, Batch 1000/1000: LR=9.99e-05, Loss=4.79e-03 BER=1.98e-03 FER=1.77e-02
2025-10-15 15:52:23,807 | INFO | Epoch 25 Train Time 48.45992422103882s

2025-10-15 15:53:11,636 | INFO | Training epoch 26, Batch 1000/1000: LR=9.98e-05, Loss=4.72e-03 BER=1.92e-03 FER=1.72e-02
2025-10-15 15:53:11,732 | INFO | Epoch 26 Train Time 47.92317509651184s

2025-10-15 15:53:56,586 | INFO | Training epoch 27, Batch 1000/1000: LR=9.98e-05, Loss=4.76e-03 BER=1.97e-03 FER=1.75e-02
2025-10-15 15:53:56,643 | INFO | Epoch 27 Train Time 44.90267848968506s

2025-10-15 15:54:44,273 | INFO | Training epoch 28, Batch 1000/1000: LR=9.98e-05, Loss=4.77e-03 BER=1.95e-03 FER=1.75e-02
2025-10-15 15:54:44,330 | INFO | Epoch 28 Train Time 47.68610143661499s

2025-10-15 15:55:32,340 | INFO | Training epoch 29, Batch 1000/1000: LR=9.98e-05, Loss=4.87e-03 BER=2.00e-03 FER=1.80e-02
2025-10-15 15:55:32,420 | INFO | Epoch 29 Train Time 48.08801794052124s

2025-10-15 15:56:20,137 | INFO | Training epoch 30, Batch 1000/1000: LR=9.98e-05, Loss=5.09e-03 BER=2.09e-03 FER=1.84e-02
2025-10-15 15:56:20,200 | INFO | Epoch 30 Train Time 47.7781400680542s

2025-10-15 15:57:08,356 | INFO | Training epoch 31, Batch 1000/1000: LR=9.98e-05, Loss=4.86e-03 BER=1.99e-03 FER=1.78e-02
2025-10-15 15:57:08,425 | INFO | Epoch 31 Train Time 48.22308301925659s

2025-10-15 15:57:55,168 | INFO | Training epoch 32, Batch 1000/1000: LR=9.98e-05, Loss=4.74e-03 BER=1.93e-03 FER=1.75e-02
2025-10-15 15:57:55,252 | INFO | Epoch 32 Train Time 46.825623512268066s

2025-10-15 15:58:43,339 | INFO | Training epoch 33, Batch 1000/1000: LR=9.98e-05, Loss=4.83e-03 BER=1.97e-03 FER=1.78e-02
2025-10-15 15:58:43,416 | INFO | Epoch 33 Train Time 48.162205934524536s

2025-10-15 15:59:31,568 | INFO | Training epoch 34, Batch 1000/1000: LR=9.97e-05, Loss=4.80e-03 BER=1.95e-03 FER=1.85e-02
2025-10-15 15:59:31,662 | INFO | Epoch 34 Train Time 48.24474573135376s

2025-10-15 16:00:19,772 | INFO | Training epoch 35, Batch 1000/1000: LR=9.97e-05, Loss=4.74e-03 BER=1.91e-03 FER=1.73e-02
2025-10-15 16:00:19,843 | INFO | Epoch 35 Train Time 48.17860555648804s

2025-10-15 16:01:07,254 | INFO | Training epoch 36, Batch 1000/1000: LR=9.97e-05, Loss=4.69e-03 BER=1.91e-03 FER=1.71e-02
2025-10-15 16:01:07,319 | INFO | Epoch 36 Train Time 47.474703550338745s

2025-10-15 16:01:55,089 | INFO | Training epoch 37, Batch 1000/1000: LR=9.97e-05, Loss=4.86e-03 BER=1.97e-03 FER=1.77e-02
2025-10-15 16:01:55,143 | INFO | Epoch 37 Train Time 47.82296657562256s

2025-10-15 16:02:42,533 | INFO | Training epoch 38, Batch 1000/1000: LR=9.97e-05, Loss=4.98e-03 BER=2.05e-03 FER=1.83e-02
2025-10-15 16:02:42,604 | INFO | Epoch 38 Train Time 47.459351778030396s

2025-10-15 16:03:30,430 | INFO | Training epoch 39, Batch 1000/1000: LR=9.96e-05, Loss=4.69e-03 BER=1.90e-03 FER=1.70e-02
2025-10-15 16:03:30,494 | INFO | Epoch 39 Train Time 47.888662338256836s

2025-10-15 16:04:14,979 | INFO | Training epoch 40, Batch 1000/1000: LR=9.96e-05, Loss=4.81e-03 BER=1.94e-03 FER=1.73e-02
2025-10-15 16:04:15,042 | INFO | Epoch 40 Train Time 44.54622292518616s

2025-10-15 16:05:02,832 | INFO | Training epoch 41, Batch 1000/1000: LR=9.96e-05, Loss=4.72e-03 BER=1.93e-03 FER=1.74e-02
2025-10-15 16:05:02,911 | INFO | Epoch 41 Train Time 47.866623640060425s

2025-10-15 16:05:51,432 | INFO | Training epoch 42, Batch 1000/1000: LR=9.96e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.71e-02
2025-10-15 16:05:51,501 | INFO | Epoch 42 Train Time 48.58855414390564s

2025-10-15 16:06:39,894 | INFO | Training epoch 43, Batch 1000/1000: LR=9.96e-05, Loss=4.92e-03 BER=2.04e-03 FER=1.82e-02
2025-10-15 16:06:39,957 | INFO | Epoch 43 Train Time 48.454017639160156s

2025-10-15 16:07:25,887 | INFO | Training epoch 44, Batch 1000/1000: LR=9.95e-05, Loss=4.85e-03 BER=1.98e-03 FER=1.76e-02
2025-10-15 16:07:25,950 | INFO | Epoch 44 Train Time 45.99213910102844s

2025-10-15 16:08:08,872 | INFO | Training epoch 45, Batch 1000/1000: LR=9.95e-05, Loss=4.72e-03 BER=1.91e-03 FER=1.70e-02
2025-10-15 16:08:08,934 | INFO | Epoch 45 Train Time 42.98219585418701s

2025-10-15 16:08:59,417 | INFO | Training epoch 46, Batch 1000/1000: LR=9.95e-05, Loss=4.91e-03 BER=1.97e-03 FER=1.80e-02
2025-10-15 16:08:59,491 | INFO | Epoch 46 Train Time 50.55319356918335s

2025-10-15 16:09:48,675 | INFO | Training epoch 47, Batch 1000/1000: LR=9.95e-05, Loss=4.75e-03 BER=1.97e-03 FER=1.74e-02
2025-10-15 16:09:48,773 | INFO | Epoch 47 Train Time 49.28137445449829s

2025-10-15 16:10:36,005 | INFO | Training epoch 48, Batch 1000/1000: LR=9.95e-05, Loss=4.75e-03 BER=1.98e-03 FER=1.78e-02
2025-10-15 16:10:36,098 | INFO | Epoch 48 Train Time 47.31959652900696s

2025-10-15 16:11:28,390 | INFO | Training epoch 49, Batch 1000/1000: LR=9.94e-05, Loss=4.93e-03 BER=1.99e-03 FER=1.76e-02
2025-10-15 16:11:28,466 | INFO | Epoch 49 Train Time 52.365251302719116s

2025-10-15 16:12:15,835 | INFO | Training epoch 50, Batch 1000/1000: LR=9.94e-05, Loss=4.91e-03 BER=2.01e-03 FER=1.82e-02
2025-10-15 16:12:15,904 | INFO | Epoch 50 Train Time 47.4369421005249s

2025-10-15 16:13:06,611 | INFO | Training epoch 51, Batch 1000/1000: LR=9.94e-05, Loss=4.82e-03 BER=1.96e-03 FER=1.76e-02
2025-10-15 16:13:06,680 | INFO | Epoch 51 Train Time 50.774765968322754s

2025-10-15 16:13:54,208 | INFO | Training epoch 52, Batch 1000/1000: LR=9.94e-05, Loss=4.80e-03 BER=1.96e-03 FER=1.74e-02
2025-10-15 16:13:54,295 | INFO | Epoch 52 Train Time 47.61131525039673s

2025-10-15 16:14:46,795 | INFO | Training epoch 53, Batch 1000/1000: LR=9.93e-05, Loss=4.64e-03 BER=1.92e-03 FER=1.70e-02
2025-10-15 16:14:46,858 | INFO | Epoch 53 Train Time 52.560474157333374s

2025-10-15 16:15:28,482 | INFO | Training epoch 54, Batch 1000/1000: LR=9.93e-05, Loss=4.70e-03 BER=1.90e-03 FER=1.72e-02
2025-10-15 16:15:28,563 | INFO | Epoch 54 Train Time 41.70337772369385s

2025-10-15 16:16:12,136 | INFO | Training epoch 55, Batch 1000/1000: LR=9.93e-05, Loss=4.76e-03 BER=1.91e-03 FER=1.74e-02
2025-10-15 16:16:12,204 | INFO | Epoch 55 Train Time 43.63785481452942s

2025-10-15 16:16:55,348 | INFO | Training epoch 56, Batch 1000/1000: LR=9.93e-05, Loss=4.92e-03 BER=2.00e-03 FER=1.80e-02
2025-10-15 16:16:55,402 | INFO | Epoch 56 Train Time 43.196499824523926s

2025-10-15 16:17:41,021 | INFO | Training epoch 57, Batch 1000/1000: LR=9.92e-05, Loss=4.72e-03 BER=1.93e-03 FER=1.73e-02
2025-10-15 16:17:41,109 | INFO | Epoch 57 Train Time 45.70616030693054s

2025-10-15 16:18:29,553 | INFO | Training epoch 58, Batch 1000/1000: LR=9.92e-05, Loss=4.75e-03 BER=1.96e-03 FER=1.75e-02
2025-10-15 16:18:29,614 | INFO | Epoch 58 Train Time 48.50397515296936s

2025-10-15 16:19:15,505 | INFO | Training epoch 59, Batch 1000/1000: LR=9.92e-05, Loss=4.48e-03 BER=1.81e-03 FER=1.63e-02
2025-10-15 16:19:15,585 | INFO | Epoch 59 Train Time 45.96898794174194s

2025-10-15 16:20:01,760 | INFO | Training epoch 60, Batch 1000/1000: LR=9.92e-05, Loss=4.88e-03 BER=2.00e-03 FER=1.79e-02
2025-10-15 16:20:01,825 | INFO | Epoch 60 Train Time 46.23888921737671s

2025-10-15 16:20:44,208 | INFO | Training epoch 61, Batch 1000/1000: LR=9.91e-05, Loss=4.73e-03 BER=1.92e-03 FER=1.74e-02
2025-10-15 16:20:44,279 | INFO | Epoch 61 Train Time 42.45202851295471s

2025-10-15 16:21:30,750 | INFO | Training epoch 62, Batch 1000/1000: LR=9.91e-05, Loss=4.80e-03 BER=1.97e-03 FER=1.75e-02
2025-10-15 16:21:30,818 | INFO | Epoch 62 Train Time 46.53747057914734s

2025-10-15 16:22:14,532 | INFO | Training epoch 63, Batch 1000/1000: LR=9.91e-05, Loss=4.70e-03 BER=1.93e-03 FER=1.71e-02
2025-10-15 16:22:14,593 | INFO | Epoch 63 Train Time 43.77392077445984s

2025-10-15 16:22:59,707 | INFO | Training epoch 64, Batch 1000/1000: LR=9.90e-05, Loss=4.67e-03 BER=1.88e-03 FER=1.69e-02
2025-10-15 16:22:59,821 | INFO | Epoch 64 Train Time 45.22509002685547s

2025-10-15 16:23:45,063 | INFO | Training epoch 65, Batch 1000/1000: LR=9.90e-05, Loss=4.68e-03 BER=1.89e-03 FER=1.69e-02
2025-10-15 16:23:45,116 | INFO | Epoch 65 Train Time 45.290101289749146s

2025-10-15 16:24:29,433 | INFO | Training epoch 66, Batch 1000/1000: LR=9.90e-05, Loss=4.70e-03 BER=1.94e-03 FER=1.74e-02
2025-10-15 16:24:29,482 | INFO | Epoch 66 Train Time 44.36428093910217s

2025-10-15 16:25:11,698 | INFO | Training epoch 67, Batch 1000/1000: LR=9.89e-05, Loss=4.66e-03 BER=1.90e-03 FER=1.70e-02
2025-10-15 16:25:11,749 | INFO | Epoch 67 Train Time 42.26587128639221s

2025-10-15 16:25:54,398 | INFO | Training epoch 68, Batch 1000/1000: LR=9.89e-05, Loss=4.79e-03 BER=1.96e-03 FER=1.74e-02
2025-10-15 16:25:54,453 | INFO | Epoch 68 Train Time 42.70282244682312s

2025-10-15 16:26:38,524 | INFO | Training epoch 69, Batch 1000/1000: LR=9.89e-05, Loss=4.69e-03 BER=1.89e-03 FER=1.70e-02
2025-10-15 16:26:38,593 | INFO | Epoch 69 Train Time 44.13620924949646s

2025-10-15 16:27:22,968 | INFO | Training epoch 70, Batch 1000/1000: LR=9.88e-05, Loss=4.77e-03 BER=1.95e-03 FER=1.73e-02
2025-10-15 16:27:23,028 | INFO | Epoch 70 Train Time 44.432974338531494s

2025-10-15 16:28:06,037 | INFO | Training epoch 71, Batch 1000/1000: LR=9.88e-05, Loss=4.68e-03 BER=1.91e-03 FER=1.71e-02
2025-10-15 16:28:06,088 | INFO | Epoch 71 Train Time 43.05815625190735s

2025-10-15 16:28:50,029 | INFO | Training epoch 72, Batch 1000/1000: LR=9.88e-05, Loss=4.67e-03 BER=1.90e-03 FER=1.72e-02
2025-10-15 16:28:50,079 | INFO | Epoch 72 Train Time 43.98964500427246s

2025-10-15 16:29:33,203 | INFO | Training epoch 73, Batch 1000/1000: LR=9.87e-05, Loss=4.75e-03 BER=1.93e-03 FER=1.75e-02
2025-10-15 16:29:33,268 | INFO | Epoch 73 Train Time 43.18714642524719s

2025-10-15 16:30:16,705 | INFO | Training epoch 74, Batch 1000/1000: LR=9.87e-05, Loss=4.67e-03 BER=1.93e-03 FER=1.74e-02
2025-10-15 16:30:16,763 | INFO | Epoch 74 Train Time 43.49363350868225s

2025-10-15 16:31:00,020 | INFO | Training epoch 75, Batch 1000/1000: LR=9.87e-05, Loss=4.77e-03 BER=1.95e-03 FER=1.75e-02
2025-10-15 16:31:00,073 | INFO | Epoch 75 Train Time 43.309478521347046s

2025-10-15 16:31:43,335 | INFO | Training epoch 76, Batch 1000/1000: LR=9.86e-05, Loss=4.76e-03 BER=1.94e-03 FER=1.75e-02
2025-10-15 16:31:43,380 | INFO | Epoch 76 Train Time 43.305710315704346s

2025-10-15 16:32:25,903 | INFO | Training epoch 77, Batch 1000/1000: LR=9.86e-05, Loss=4.59e-03 BER=1.89e-03 FER=1.69e-02
2025-10-15 16:32:25,961 | INFO | Epoch 77 Train Time 42.579113483428955s

2025-10-15 16:33:08,619 | INFO | Training epoch 78, Batch 1000/1000: LR=9.86e-05, Loss=4.79e-03 BER=1.96e-03 FER=1.78e-02
2025-10-15 16:33:08,666 | INFO | Epoch 78 Train Time 42.70364451408386s

2025-10-15 16:33:53,219 | INFO | Training epoch 79, Batch 1000/1000: LR=9.85e-05, Loss=4.74e-03 BER=1.92e-03 FER=1.72e-02
2025-10-15 16:33:53,272 | INFO | Epoch 79 Train Time 44.604403257369995s

2025-10-15 16:34:36,216 | INFO | Training epoch 80, Batch 1000/1000: LR=9.85e-05, Loss=4.69e-03 BER=1.92e-03 FER=1.72e-02
2025-10-15 16:34:36,268 | INFO | Epoch 80 Train Time 42.99446725845337s

2025-10-15 16:35:20,505 | INFO | Training epoch 81, Batch 1000/1000: LR=9.84e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.68e-02
2025-10-15 16:35:20,564 | INFO | Epoch 81 Train Time 44.29341173171997s

2025-10-15 16:36:03,605 | INFO | Training epoch 82, Batch 1000/1000: LR=9.84e-05, Loss=4.71e-03 BER=1.92e-03 FER=1.72e-02
2025-10-15 16:36:03,655 | INFO | Epoch 82 Train Time 43.090046882629395s

2025-10-15 16:36:47,108 | INFO | Training epoch 83, Batch 1000/1000: LR=9.84e-05, Loss=4.61e-03 BER=1.86e-03 FER=1.69e-02
2025-10-15 16:36:47,163 | INFO | Epoch 83 Train Time 43.50568747520447s

2025-10-15 16:37:30,106 | INFO | Training epoch 84, Batch 1000/1000: LR=9.83e-05, Loss=4.55e-03 BER=1.86e-03 FER=1.69e-02
2025-10-15 16:37:30,169 | INFO | Epoch 84 Train Time 43.00504183769226s

2025-10-15 16:38:13,798 | INFO | Training epoch 85, Batch 1000/1000: LR=9.83e-05, Loss=4.71e-03 BER=1.93e-03 FER=1.72e-02
2025-10-15 16:38:13,852 | INFO | Epoch 85 Train Time 43.681336402893066s

2025-10-15 16:38:57,396 | INFO | Training epoch 86, Batch 1000/1000: LR=9.82e-05, Loss=4.57e-03 BER=1.85e-03 FER=1.65e-02
2025-10-15 16:38:57,459 | INFO | Epoch 86 Train Time 43.60564875602722s

2025-10-15 16:39:41,969 | INFO | Training epoch 87, Batch 1000/1000: LR=9.82e-05, Loss=4.89e-03 BER=2.00e-03 FER=1.78e-02
2025-10-15 16:39:42,038 | INFO | Epoch 87 Train Time 44.57733941078186s

2025-10-15 16:40:26,599 | INFO | Training epoch 88, Batch 1000/1000: LR=9.82e-05, Loss=4.78e-03 BER=1.95e-03 FER=1.74e-02
2025-10-15 16:40:26,649 | INFO | Epoch 88 Train Time 44.610199213027954s

2025-10-15 16:41:10,839 | INFO | Training epoch 89, Batch 1000/1000: LR=9.81e-05, Loss=4.67e-03 BER=1.92e-03 FER=1.69e-02
2025-10-15 16:41:10,894 | INFO | Epoch 89 Train Time 44.24239492416382s

2025-10-15 16:41:54,519 | INFO | Training epoch 90, Batch 1000/1000: LR=9.81e-05, Loss=4.73e-03 BER=1.90e-03 FER=1.75e-02
2025-10-15 16:41:54,570 | INFO | Epoch 90 Train Time 43.67408776283264s

2025-10-15 16:42:39,633 | INFO | Training epoch 91, Batch 1000/1000: LR=9.80e-05, Loss=4.60e-03 BER=1.89e-03 FER=1.67e-02
2025-10-15 16:42:39,698 | INFO | Epoch 91 Train Time 45.12723708152771s

2025-10-15 16:43:23,502 | INFO | Training epoch 92, Batch 1000/1000: LR=9.80e-05, Loss=4.48e-03 BER=1.87e-03 FER=1.68e-02
2025-10-15 16:43:23,573 | INFO | Epoch 92 Train Time 43.87283682823181s

2025-10-15 16:44:07,108 | INFO | Training epoch 93, Batch 1000/1000: LR=9.79e-05, Loss=4.44e-03 BER=1.84e-03 FER=1.65e-02
2025-10-15 16:44:07,175 | INFO | Epoch 93 Train Time 43.600830078125s

2025-10-15 16:44:51,603 | INFO | Training epoch 94, Batch 1000/1000: LR=9.79e-05, Loss=4.76e-03 BER=1.92e-03 FER=1.73e-02
2025-10-15 16:44:51,666 | INFO | Epoch 94 Train Time 44.48726415634155s

2025-10-15 16:45:35,601 | INFO | Training epoch 95, Batch 1000/1000: LR=9.79e-05, Loss=4.70e-03 BER=1.94e-03 FER=1.73e-02
2025-10-15 16:45:35,668 | INFO | Epoch 95 Train Time 44.000977754592896s

2025-10-15 16:46:18,419 | INFO | Training epoch 96, Batch 1000/1000: LR=9.78e-05, Loss=4.47e-03 BER=1.85e-03 FER=1.67e-02
2025-10-15 16:46:18,468 | INFO | Epoch 96 Train Time 42.798120975494385s

2025-10-15 16:47:02,638 | INFO | Training epoch 97, Batch 1000/1000: LR=9.78e-05, Loss=4.70e-03 BER=1.91e-03 FER=1.73e-02
2025-10-15 16:47:02,694 | INFO | Epoch 97 Train Time 44.225032329559326s

2025-10-15 16:47:45,933 | INFO | Training epoch 98, Batch 1000/1000: LR=9.77e-05, Loss=4.65e-03 BER=1.90e-03 FER=1.71e-02
2025-10-15 16:47:45,977 | INFO | Epoch 98 Train Time 43.28142285346985s

2025-10-15 16:48:29,222 | INFO | Training epoch 99, Batch 1000/1000: LR=9.77e-05, Loss=4.44e-03 BER=1.84e-03 FER=1.64e-02
2025-10-15 16:48:29,276 | INFO | Epoch 99 Train Time 43.29818940162659s

2025-10-15 16:49:12,930 | INFO | Training epoch 100, Batch 1000/1000: LR=9.76e-05, Loss=4.79e-03 BER=1.98e-03 FER=1.76e-02
2025-10-15 16:49:12,999 | INFO | Epoch 100 Train Time 43.72106337547302s

2025-10-15 16:49:56,496 | INFO | Training epoch 101, Batch 1000/1000: LR=9.76e-05, Loss=4.83e-03 BER=1.97e-03 FER=1.79e-02
2025-10-15 16:49:56,547 | INFO | Epoch 101 Train Time 43.546332120895386s

2025-10-15 16:50:39,707 | INFO | Training epoch 102, Batch 1000/1000: LR=9.75e-05, Loss=4.69e-03 BER=1.93e-03 FER=1.74e-02
2025-10-15 16:50:39,759 | INFO | Epoch 102 Train Time 43.21080803871155s

2025-10-15 16:51:22,898 | INFO | Training epoch 103, Batch 1000/1000: LR=9.75e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.70e-02
2025-10-15 16:51:22,956 | INFO | Epoch 103 Train Time 43.19549322128296s

2025-10-15 16:52:06,527 | INFO | Training epoch 104, Batch 1000/1000: LR=9.74e-05, Loss=4.83e-03 BER=1.98e-03 FER=1.77e-02
2025-10-15 16:52:06,588 | INFO | Epoch 104 Train Time 43.630476236343384s

2025-10-15 16:52:49,759 | INFO | Training epoch 105, Batch 1000/1000: LR=9.74e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.65e-02
2025-10-15 16:52:49,822 | INFO | Epoch 105 Train Time 43.2307870388031s

2025-10-15 16:53:32,809 | INFO | Training epoch 106, Batch 1000/1000: LR=9.73e-05, Loss=4.63e-03 BER=1.90e-03 FER=1.74e-02
2025-10-15 16:53:32,861 | INFO | Epoch 106 Train Time 43.03683662414551s

2025-10-15 16:54:17,519 | INFO | Training epoch 107, Batch 1000/1000: LR=9.73e-05, Loss=4.66e-03 BER=1.89e-03 FER=1.69e-02
2025-10-15 16:54:17,569 | INFO | Epoch 107 Train Time 44.70665121078491s

2025-10-15 16:55:00,829 | INFO | Training epoch 108, Batch 1000/1000: LR=9.72e-05, Loss=4.72e-03 BER=1.92e-03 FER=1.68e-02
2025-10-15 16:55:00,890 | INFO | Epoch 108 Train Time 43.31964898109436s

2025-10-15 16:55:43,499 | INFO | Training epoch 109, Batch 1000/1000: LR=9.72e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.69e-02
2025-10-15 16:55:43,581 | INFO | Epoch 109 Train Time 42.6900749206543s

2025-10-15 16:56:27,933 | INFO | Training epoch 110, Batch 1000/1000: LR=9.71e-05, Loss=4.58e-03 BER=1.86e-03 FER=1.67e-02
2025-10-15 16:56:27,990 | INFO | Epoch 110 Train Time 44.40674614906311s

2025-10-15 16:57:11,306 | INFO | Training epoch 111, Batch 1000/1000: LR=9.71e-05, Loss=4.69e-03 BER=1.91e-03 FER=1.70e-02
2025-10-15 16:57:11,364 | INFO | Epoch 111 Train Time 43.37274432182312s

2025-10-15 16:57:55,830 | INFO | Training epoch 112, Batch 1000/1000: LR=9.70e-05, Loss=4.78e-03 BER=1.96e-03 FER=1.75e-02
2025-10-15 16:57:55,900 | INFO | Epoch 112 Train Time 44.53400731086731s

2025-10-15 16:58:39,401 | INFO | Training epoch 113, Batch 1000/1000: LR=9.70e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.67e-02
2025-10-15 16:58:39,451 | INFO | Epoch 113 Train Time 43.54983139038086s

2025-10-15 16:59:22,940 | INFO | Training epoch 114, Batch 1000/1000: LR=9.69e-05, Loss=4.65e-03 BER=1.91e-03 FER=1.70e-02
2025-10-15 16:59:22,991 | INFO | Epoch 114 Train Time 43.53816747665405s

2025-10-15 17:00:06,114 | INFO | Training epoch 115, Batch 1000/1000: LR=9.69e-05, Loss=4.60e-03 BER=1.88e-03 FER=1.70e-02
2025-10-15 17:00:06,171 | INFO | Epoch 115 Train Time 43.1792733669281s

2025-10-15 17:00:49,697 | INFO | Training epoch 116, Batch 1000/1000: LR=9.68e-05, Loss=4.78e-03 BER=1.93e-03 FER=1.72e-02
2025-10-15 17:00:49,748 | INFO | Epoch 116 Train Time 43.57563042640686s

2025-10-15 17:01:32,586 | INFO | Training epoch 117, Batch 1000/1000: LR=9.67e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.66e-02
2025-10-15 17:01:32,647 | INFO | Epoch 117 Train Time 42.896926164627075s

2025-10-15 17:02:16,205 | INFO | Training epoch 118, Batch 1000/1000: LR=9.67e-05, Loss=4.63e-03 BER=1.89e-03 FER=1.68e-02
2025-10-15 17:02:16,251 | INFO | Epoch 118 Train Time 43.60252499580383s

2025-10-15 17:02:59,542 | INFO | Training epoch 119, Batch 1000/1000: LR=9.66e-05, Loss=4.53e-03 BER=1.85e-03 FER=1.69e-02
2025-10-15 17:02:59,624 | INFO | Epoch 119 Train Time 43.37099862098694s

2025-10-15 17:03:42,923 | INFO | Training epoch 120, Batch 1000/1000: LR=9.66e-05, Loss=4.76e-03 BER=1.96e-03 FER=1.70e-02
2025-10-15 17:03:42,969 | INFO | Epoch 120 Train Time 43.34271454811096s

2025-10-15 17:04:26,227 | INFO | Training epoch 121, Batch 1000/1000: LR=9.65e-05, Loss=4.78e-03 BER=1.95e-03 FER=1.76e-02
2025-10-15 17:04:26,290 | INFO | Epoch 121 Train Time 43.31873536109924s

2025-10-15 17:05:10,506 | INFO | Training epoch 122, Batch 1000/1000: LR=9.65e-05, Loss=4.89e-03 BER=1.99e-03 FER=1.75e-02
2025-10-15 17:05:10,554 | INFO | Epoch 122 Train Time 44.262171506881714s

2025-10-15 17:05:54,207 | INFO | Training epoch 123, Batch 1000/1000: LR=9.64e-05, Loss=4.71e-03 BER=1.92e-03 FER=1.69e-02
2025-10-15 17:05:54,250 | INFO | Epoch 123 Train Time 43.69315481185913s

2025-10-15 17:06:37,917 | INFO | Training epoch 124, Batch 1000/1000: LR=9.64e-05, Loss=4.79e-03 BER=1.98e-03 FER=1.76e-02
2025-10-15 17:06:37,971 | INFO | Epoch 124 Train Time 43.720237493515015s

2025-10-15 17:07:21,805 | INFO | Training epoch 125, Batch 1000/1000: LR=9.63e-05, Loss=4.48e-03 BER=1.83e-03 FER=1.65e-02
2025-10-15 17:07:21,864 | INFO | Epoch 125 Train Time 43.89066171646118s

2025-10-15 17:08:05,502 | INFO | Training epoch 126, Batch 1000/1000: LR=9.62e-05, Loss=4.62e-03 BER=1.88e-03 FER=1.66e-02
2025-10-15 17:08:05,559 | INFO | Epoch 126 Train Time 43.69319415092468s

2025-10-15 17:08:48,603 | INFO | Training epoch 127, Batch 1000/1000: LR=9.62e-05, Loss=4.57e-03 BER=1.89e-03 FER=1.68e-02
2025-10-15 17:08:48,654 | INFO | Epoch 127 Train Time 43.09424042701721s

2025-10-15 17:09:32,890 | INFO | Training epoch 128, Batch 1000/1000: LR=9.61e-05, Loss=4.67e-03 BER=1.92e-03 FER=1.69e-02
2025-10-15 17:09:32,948 | INFO | Epoch 128 Train Time 44.290138959884644s

2025-10-15 17:10:16,199 | INFO | Training epoch 129, Batch 1000/1000: LR=9.61e-05, Loss=4.54e-03 BER=1.90e-03 FER=1.70e-02
2025-10-15 17:10:16,262 | INFO | Epoch 129 Train Time 43.31173825263977s

2025-10-15 17:11:00,221 | INFO | Training epoch 130, Batch 1000/1000: LR=9.60e-05, Loss=4.87e-03 BER=1.98e-03 FER=1.75e-02
2025-10-15 17:11:00,273 | INFO | Epoch 130 Train Time 44.00998377799988s

2025-10-15 17:11:43,605 | INFO | Training epoch 131, Batch 1000/1000: LR=9.59e-05, Loss=4.37e-03 BER=1.76e-03 FER=1.58e-02
2025-10-15 17:11:43,650 | INFO | Epoch 131 Train Time 43.37595343589783s

2025-10-15 17:11:43,652 | INFO | [P1] saving best_model with loss 0.004368 at epoch 131
2025-10-15 17:12:28,188 | INFO | Training epoch 132, Batch 1000/1000: LR=9.59e-05, Loss=4.43e-03 BER=1.85e-03 FER=1.65e-02
2025-10-15 17:12:28,232 | INFO | Epoch 132 Train Time 44.48090434074402s

2025-10-15 17:13:12,156 | INFO | Training epoch 133, Batch 1000/1000: LR=9.58e-05, Loss=4.86e-03 BER=1.99e-03 FER=1.80e-02
2025-10-15 17:13:12,225 | INFO | Epoch 133 Train Time 43.992192029953s

2025-10-15 17:13:55,926 | INFO | Training epoch 134, Batch 1000/1000: LR=9.57e-05, Loss=4.82e-03 BER=1.99e-03 FER=1.76e-02
2025-10-15 17:13:55,983 | INFO | Epoch 134 Train Time 43.7561571598053s

2025-10-15 17:14:39,017 | INFO | Training epoch 135, Batch 1000/1000: LR=9.57e-05, Loss=4.58e-03 BER=1.86e-03 FER=1.66e-02
2025-10-15 17:14:39,077 | INFO | Epoch 135 Train Time 43.090909004211426s

2025-10-15 17:15:22,933 | INFO | Training epoch 136, Batch 1000/1000: LR=9.56e-05, Loss=4.55e-03 BER=1.88e-03 FER=1.66e-02
2025-10-15 17:15:22,991 | INFO | Epoch 136 Train Time 43.91149687767029s

2025-10-15 17:16:07,048 | INFO | Training epoch 137, Batch 1000/1000: LR=9.56e-05, Loss=4.46e-03 BER=1.83e-03 FER=1.64e-02
2025-10-15 17:16:07,092 | INFO | Epoch 137 Train Time 44.09880089759827s

2025-10-15 17:16:50,791 | INFO | Training epoch 138, Batch 1000/1000: LR=9.55e-05, Loss=4.79e-03 BER=1.95e-03 FER=1.75e-02
2025-10-15 17:16:50,834 | INFO | Epoch 138 Train Time 43.73997139930725s

2025-10-15 17:17:32,714 | INFO | Training epoch 139, Batch 1000/1000: LR=9.54e-05, Loss=4.64e-03 BER=1.93e-03 FER=1.69e-02
2025-10-15 17:17:32,761 | INFO | Epoch 139 Train Time 41.92593193054199s

2025-10-15 17:18:15,517 | INFO | Training epoch 140, Batch 1000/1000: LR=9.54e-05, Loss=4.50e-03 BER=1.86e-03 FER=1.66e-02
2025-10-15 17:18:15,569 | INFO | Epoch 140 Train Time 42.80631947517395s

2025-10-15 17:19:00,300 | INFO | Training epoch 141, Batch 1000/1000: LR=9.53e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.68e-02
2025-10-15 17:19:00,389 | INFO | Epoch 141 Train Time 44.81860017776489s

2025-10-15 17:19:45,600 | INFO | Training epoch 142, Batch 1000/1000: LR=9.52e-05, Loss=4.70e-03 BER=1.90e-03 FER=1.69e-02
2025-10-15 17:19:45,658 | INFO | Epoch 142 Train Time 45.26848530769348s

2025-10-15 17:20:28,906 | INFO | Training epoch 143, Batch 1000/1000: LR=9.52e-05, Loss=4.71e-03 BER=1.92e-03 FER=1.70e-02
2025-10-15 17:20:28,948 | INFO | Epoch 143 Train Time 43.28909134864807s

2025-10-15 17:21:08,503 | INFO | Training epoch 144, Batch 1000/1000: LR=9.51e-05, Loss=4.52e-03 BER=1.85e-03 FER=1.65e-02
2025-10-15 17:21:08,555 | INFO | Epoch 144 Train Time 39.604297399520874s

2025-10-15 17:21:52,308 | INFO | Training epoch 145, Batch 1000/1000: LR=9.50e-05, Loss=4.81e-03 BER=1.97e-03 FER=1.77e-02
2025-10-15 17:21:52,362 | INFO | Epoch 145 Train Time 43.8046190738678s

2025-10-15 17:22:36,322 | INFO | Training epoch 146, Batch 1000/1000: LR=9.50e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.64e-02
2025-10-15 17:22:36,381 | INFO | Epoch 146 Train Time 44.01777768135071s

2025-10-15 17:23:21,920 | INFO | Training epoch 147, Batch 1000/1000: LR=9.49e-05, Loss=4.63e-03 BER=1.87e-03 FER=1.68e-02
2025-10-15 17:23:21,982 | INFO | Epoch 147 Train Time 45.598570823669434s

2025-10-15 17:24:04,624 | INFO | Training epoch 148, Batch 1000/1000: LR=9.48e-05, Loss=4.65e-03 BER=1.90e-03 FER=1.67e-02
2025-10-15 17:24:04,673 | INFO | Epoch 148 Train Time 42.689451456069946s

2025-10-15 17:24:47,506 | INFO | Training epoch 149, Batch 1000/1000: LR=9.47e-05, Loss=4.38e-03 BER=1.77e-03 FER=1.60e-02
2025-10-15 17:24:47,549 | INFO | Epoch 149 Train Time 42.87458324432373s

2025-10-15 17:25:30,916 | INFO | Training epoch 150, Batch 1000/1000: LR=9.47e-05, Loss=4.49e-03 BER=1.84e-03 FER=1.66e-02
2025-10-15 17:25:30,978 | INFO | Epoch 150 Train Time 43.42718029022217s

2025-10-15 17:26:14,217 | INFO | Training epoch 151, Batch 1000/1000: LR=9.46e-05, Loss=4.40e-03 BER=1.79e-03 FER=1.61e-02
2025-10-15 17:26:14,285 | INFO | Epoch 151 Train Time 43.30565309524536s

2025-10-15 17:26:58,003 | INFO | Training epoch 152, Batch 1000/1000: LR=9.45e-05, Loss=4.80e-03 BER=1.96e-03 FER=1.77e-02
2025-10-15 17:26:58,046 | INFO | Epoch 152 Train Time 43.759963274002075s

2025-10-15 17:27:41,168 | INFO | Training epoch 153, Batch 1000/1000: LR=9.45e-05, Loss=4.68e-03 BER=1.92e-03 FER=1.72e-02
2025-10-15 17:27:41,218 | INFO | Epoch 153 Train Time 43.16998839378357s

2025-10-15 17:28:24,497 | INFO | Training epoch 154, Batch 1000/1000: LR=9.44e-05, Loss=4.80e-03 BER=1.97e-03 FER=1.74e-02
2025-10-15 17:28:24,543 | INFO | Epoch 154 Train Time 43.32279014587402s

2025-10-15 17:29:07,536 | INFO | Training epoch 155, Batch 1000/1000: LR=9.43e-05, Loss=4.44e-03 BER=1.81e-03 FER=1.60e-02
2025-10-15 17:29:07,584 | INFO | Epoch 155 Train Time 43.03914999961853s

2025-10-15 17:29:51,943 | INFO | Training epoch 156, Batch 1000/1000: LR=9.42e-05, Loss=4.49e-03 BER=1.84e-03 FER=1.66e-02
2025-10-15 17:29:51,996 | INFO | Epoch 156 Train Time 44.41120481491089s

2025-10-15 17:30:35,604 | INFO | Training epoch 157, Batch 1000/1000: LR=9.42e-05, Loss=4.69e-03 BER=1.96e-03 FER=1.70e-02
2025-10-15 17:30:35,675 | INFO | Epoch 157 Train Time 43.6777822971344s

2025-10-15 17:31:19,098 | INFO | Training epoch 158, Batch 1000/1000: LR=9.41e-05, Loss=4.70e-03 BER=1.93e-03 FER=1.71e-02
2025-10-15 17:31:19,142 | INFO | Epoch 158 Train Time 43.46536326408386s

2025-10-15 17:32:02,807 | INFO | Training epoch 159, Batch 1000/1000: LR=9.40e-05, Loss=4.66e-03 BER=1.88e-03 FER=1.69e-02
2025-10-15 17:32:02,866 | INFO | Epoch 159 Train Time 43.72153091430664s

2025-10-15 17:32:46,103 | INFO | Training epoch 160, Batch 1000/1000: LR=9.40e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.67e-02
2025-10-15 17:32:46,159 | INFO | Epoch 160 Train Time 43.292261600494385s

2025-10-15 17:33:28,806 | INFO | Training epoch 161, Batch 1000/1000: LR=9.39e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.68e-02
2025-10-15 17:33:28,855 | INFO | Epoch 161 Train Time 42.693199157714844s

2025-10-15 17:34:12,914 | INFO | Training epoch 162, Batch 1000/1000: LR=9.38e-05, Loss=4.67e-03 BER=1.93e-03 FER=1.71e-02
2025-10-15 17:34:12,969 | INFO | Epoch 162 Train Time 44.111740827560425s

2025-10-15 17:34:56,849 | INFO | Training epoch 163, Batch 1000/1000: LR=9.37e-05, Loss=4.62e-03 BER=1.88e-03 FER=1.69e-02
2025-10-15 17:34:56,895 | INFO | Epoch 163 Train Time 43.92482376098633s

2025-10-15 17:35:40,507 | INFO | Training epoch 164, Batch 1000/1000: LR=9.37e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.67e-02
2025-10-15 17:35:40,559 | INFO | Epoch 164 Train Time 43.66284108161926s

2025-10-15 17:36:24,549 | INFO | Training epoch 165, Batch 1000/1000: LR=9.36e-05, Loss=4.58e-03 BER=1.86e-03 FER=1.65e-02
2025-10-15 17:36:24,607 | INFO | Epoch 165 Train Time 44.046841859817505s

2025-10-15 17:37:08,626 | INFO | Training epoch 166, Batch 1000/1000: LR=9.35e-05, Loss=4.65e-03 BER=1.89e-03 FER=1.69e-02
2025-10-15 17:37:08,668 | INFO | Epoch 166 Train Time 44.05824327468872s

2025-10-15 17:37:51,846 | INFO | Training epoch 167, Batch 1000/1000: LR=9.34e-05, Loss=4.39e-03 BER=1.82e-03 FER=1.64e-02
2025-10-15 17:37:51,901 | INFO | Epoch 167 Train Time 43.23095774650574s

2025-10-15 17:38:35,336 | INFO | Training epoch 168, Batch 1000/1000: LR=9.33e-05, Loss=4.60e-03 BER=1.86e-03 FER=1.69e-02
2025-10-15 17:38:35,390 | INFO | Epoch 168 Train Time 43.48789668083191s

2025-10-15 17:39:18,708 | INFO | Training epoch 169, Batch 1000/1000: LR=9.33e-05, Loss=4.45e-03 BER=1.83e-03 FER=1.63e-02
2025-10-15 17:39:18,753 | INFO | Epoch 169 Train Time 43.361407995224s

2025-10-15 17:40:02,034 | INFO | Training epoch 170, Batch 1000/1000: LR=9.32e-05, Loss=4.52e-03 BER=1.88e-03 FER=1.68e-02
2025-10-15 17:40:02,133 | INFO | Epoch 170 Train Time 43.37800359725952s

2025-10-15 17:40:45,108 | INFO | Training epoch 171, Batch 1000/1000: LR=9.31e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.64e-02
2025-10-15 17:40:45,153 | INFO | Epoch 171 Train Time 43.018940925598145s

2025-10-15 17:41:28,408 | INFO | Training epoch 172, Batch 1000/1000: LR=9.30e-05, Loss=4.60e-03 BER=1.88e-03 FER=1.66e-02
2025-10-15 17:41:28,452 | INFO | Epoch 172 Train Time 43.297086238861084s

2025-10-15 17:42:12,804 | INFO | Training epoch 173, Batch 1000/1000: LR=9.29e-05, Loss=4.82e-03 BER=1.97e-03 FER=1.74e-02
2025-10-15 17:42:12,851 | INFO | Epoch 173 Train Time 44.39822196960449s

2025-10-15 17:42:56,594 | INFO | Training epoch 174, Batch 1000/1000: LR=9.29e-05, Loss=4.58e-03 BER=1.89e-03 FER=1.70e-02
2025-10-15 17:42:56,637 | INFO | Epoch 174 Train Time 43.785071849823s

2025-10-15 17:43:39,723 | INFO | Training epoch 175, Batch 1000/1000: LR=9.28e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.68e-02
2025-10-15 17:43:39,797 | INFO | Epoch 175 Train Time 43.15887212753296s

2025-10-15 17:44:23,535 | INFO | Training epoch 176, Batch 1000/1000: LR=9.27e-05, Loss=4.51e-03 BER=1.86e-03 FER=1.67e-02
2025-10-15 17:44:23,599 | INFO | Epoch 176 Train Time 43.799333810806274s

2025-10-15 17:45:07,955 | INFO | Training epoch 177, Batch 1000/1000: LR=9.26e-05, Loss=4.54e-03 BER=1.86e-03 FER=1.68e-02
2025-10-15 17:45:08,039 | INFO | Epoch 177 Train Time 44.43867564201355s

2025-10-15 17:45:49,170 | INFO | Training epoch 178, Batch 1000/1000: LR=9.25e-05, Loss=4.47e-03 BER=1.81e-03 FER=1.61e-02
2025-10-15 17:45:49,215 | INFO | Epoch 178 Train Time 41.17499017715454s

2025-10-15 17:46:29,987 | INFO | Training epoch 179, Batch 1000/1000: LR=9.25e-05, Loss=4.51e-03 BER=1.87e-03 FER=1.64e-02
2025-10-15 17:46:30,048 | INFO | Epoch 179 Train Time 40.83116698265076s

2025-10-15 17:47:12,504 | INFO | Training epoch 180, Batch 1000/1000: LR=9.24e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.65e-02
2025-10-15 17:47:12,562 | INFO | Epoch 180 Train Time 42.51330041885376s

2025-10-15 17:47:55,103 | INFO | Training epoch 181, Batch 1000/1000: LR=9.23e-05, Loss=4.54e-03 BER=1.86e-03 FER=1.66e-02
2025-10-15 17:47:55,155 | INFO | Epoch 181 Train Time 42.59213304519653s

2025-10-15 17:48:39,526 | INFO | Training epoch 182, Batch 1000/1000: LR=9.22e-05, Loss=4.53e-03 BER=1.83e-03 FER=1.64e-02
2025-10-15 17:48:39,575 | INFO | Epoch 182 Train Time 44.41842341423035s

2025-10-15 17:49:23,804 | INFO | Training epoch 183, Batch 1000/1000: LR=9.21e-05, Loss=4.71e-03 BER=1.93e-03 FER=1.73e-02
2025-10-15 17:49:23,847 | INFO | Epoch 183 Train Time 44.270634174346924s

2025-10-15 17:50:05,950 | INFO | Training epoch 184, Batch 1000/1000: LR=9.20e-05, Loss=4.40e-03 BER=1.81e-03 FER=1.59e-02
2025-10-15 17:50:06,026 | INFO | Epoch 184 Train Time 42.177732944488525s

2025-10-15 17:50:50,375 | INFO | Training epoch 185, Batch 1000/1000: LR=9.20e-05, Loss=4.60e-03 BER=1.89e-03 FER=1.67e-02
2025-10-15 17:50:50,432 | INFO | Epoch 185 Train Time 44.40439057350159s

2025-10-15 17:51:34,297 | INFO | Training epoch 186, Batch 1000/1000: LR=9.19e-05, Loss=4.60e-03 BER=1.88e-03 FER=1.68e-02
2025-10-15 17:51:34,338 | INFO | Epoch 186 Train Time 43.90359044075012s

2025-10-15 17:52:18,728 | INFO | Training epoch 187, Batch 1000/1000: LR=9.18e-05, Loss=4.55e-03 BER=1.86e-03 FER=1.67e-02
2025-10-15 17:52:18,780 | INFO | Epoch 187 Train Time 44.44076991081238s

2025-10-15 17:53:01,495 | INFO | Training epoch 188, Batch 1000/1000: LR=9.17e-05, Loss=4.80e-03 BER=1.96e-03 FER=1.74e-02
2025-10-15 17:53:01,546 | INFO | Epoch 188 Train Time 42.76567602157593s

2025-10-15 17:53:44,398 | INFO | Training epoch 189, Batch 1000/1000: LR=9.16e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.66e-02
2025-10-15 17:53:44,455 | INFO | Epoch 189 Train Time 42.90767765045166s

2025-10-15 17:54:27,796 | INFO | Training epoch 190, Batch 1000/1000: LR=9.15e-05, Loss=4.56e-03 BER=1.88e-03 FER=1.70e-02
2025-10-15 17:54:27,841 | INFO | Epoch 190 Train Time 43.38398599624634s

2025-10-15 17:55:11,135 | INFO | Training epoch 191, Batch 1000/1000: LR=9.14e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.65e-02
2025-10-15 17:55:11,199 | INFO | Epoch 191 Train Time 43.356932640075684s

2025-10-15 17:55:54,199 | INFO | Training epoch 192, Batch 1000/1000: LR=9.14e-05, Loss=4.59e-03 BER=1.89e-03 FER=1.69e-02
2025-10-15 17:55:54,243 | INFO | Epoch 192 Train Time 43.04008960723877s

2025-10-15 17:56:36,499 | INFO | Training epoch 193, Batch 1000/1000: LR=9.13e-05, Loss=4.60e-03 BER=1.89e-03 FER=1.67e-02
2025-10-15 17:56:36,548 | INFO | Epoch 193 Train Time 42.302974462509155s

2025-10-15 17:57:20,098 | INFO | Training epoch 194, Batch 1000/1000: LR=9.12e-05, Loss=4.49e-03 BER=1.85e-03 FER=1.66e-02
2025-10-15 17:57:20,135 | INFO | Epoch 194 Train Time 43.586079359054565s

2025-10-15 17:58:04,260 | INFO | Training epoch 195, Batch 1000/1000: LR=9.11e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.66e-02
2025-10-15 17:58:04,301 | INFO | Epoch 195 Train Time 44.16347694396973s

2025-10-15 17:58:48,405 | INFO | Training epoch 196, Batch 1000/1000: LR=9.10e-05, Loss=4.83e-03 BER=1.96e-03 FER=1.76e-02
2025-10-15 17:58:48,477 | INFO | Epoch 196 Train Time 44.17411470413208s

2025-10-15 17:59:31,193 | INFO | Training epoch 197, Batch 1000/1000: LR=9.09e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.68e-02
2025-10-15 17:59:31,241 | INFO | Epoch 197 Train Time 42.762234926223755s

2025-10-15 18:00:15,460 | INFO | Training epoch 198, Batch 1000/1000: LR=9.08e-05, Loss=4.57e-03 BER=1.89e-03 FER=1.70e-02
2025-10-15 18:00:15,524 | INFO | Epoch 198 Train Time 44.2812705039978s

2025-10-15 18:00:59,106 | INFO | Training epoch 199, Batch 1000/1000: LR=9.07e-05, Loss=4.46e-03 BER=1.86e-03 FER=1.66e-02
2025-10-15 18:00:59,167 | INFO | Epoch 199 Train Time 43.64116358757019s

2025-10-15 18:01:42,770 | INFO | Training epoch 200, Batch 1000/1000: LR=9.06e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.68e-02
2025-10-15 18:01:42,807 | INFO | Epoch 200 Train Time 43.637595415115356s

2025-10-15 18:02:26,000 | INFO | Training epoch 201, Batch 1000/1000: LR=9.05e-05, Loss=4.65e-03 BER=1.90e-03 FER=1.69e-02
2025-10-15 18:02:26,053 | INFO | Epoch 201 Train Time 43.24558472633362s

2025-10-15 18:03:09,149 | INFO | Training epoch 202, Batch 1000/1000: LR=9.05e-05, Loss=4.63e-03 BER=1.89e-03 FER=1.67e-02
2025-10-15 18:03:09,193 | INFO | Epoch 202 Train Time 43.13804244995117s

2025-10-15 18:03:52,303 | INFO | Training epoch 203, Batch 1000/1000: LR=9.04e-05, Loss=4.32e-03 BER=1.78e-03 FER=1.61e-02
2025-10-15 18:03:52,364 | INFO | Epoch 203 Train Time 43.16995096206665s

2025-10-15 18:03:52,365 | INFO | [P1] saving best_model with loss 0.004325 at epoch 203
2025-10-15 18:04:36,792 | INFO | Training epoch 204, Batch 1000/1000: LR=9.03e-05, Loss=4.52e-03 BER=1.85e-03 FER=1.66e-02
2025-10-15 18:04:36,837 | INFO | Epoch 204 Train Time 44.38349533081055s

2025-10-15 18:05:19,308 | INFO | Training epoch 205, Batch 1000/1000: LR=9.02e-05, Loss=4.47e-03 BER=1.84e-03 FER=1.63e-02
2025-10-15 18:05:19,361 | INFO | Epoch 205 Train Time 42.522228717803955s

2025-10-15 18:06:02,921 | INFO | Training epoch 206, Batch 1000/1000: LR=9.01e-05, Loss=4.59e-03 BER=1.85e-03 FER=1.66e-02
2025-10-15 18:06:02,974 | INFO | Epoch 206 Train Time 43.612080097198486s

2025-10-15 18:06:46,603 | INFO | Training epoch 207, Batch 1000/1000: LR=9.00e-05, Loss=4.46e-03 BER=1.81e-03 FER=1.65e-02
2025-10-15 18:06:46,650 | INFO | Epoch 207 Train Time 43.67515420913696s

2025-10-15 18:07:29,509 | INFO | Training epoch 208, Batch 1000/1000: LR=8.99e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.60e-02
2025-10-15 18:07:29,558 | INFO | Epoch 208 Train Time 42.90668058395386s

2025-10-15 18:08:12,877 | INFO | Training epoch 209, Batch 1000/1000: LR=8.98e-05, Loss=4.75e-03 BER=1.93e-03 FER=1.74e-02
2025-10-15 18:08:12,934 | INFO | Epoch 209 Train Time 43.37406063079834s

2025-10-15 18:08:56,923 | INFO | Training epoch 210, Batch 1000/1000: LR=8.97e-05, Loss=4.64e-03 BER=1.92e-03 FER=1.68e-02
2025-10-15 18:08:56,967 | INFO | Epoch 210 Train Time 44.03173565864563s

2025-10-15 18:09:40,832 | INFO | Training epoch 211, Batch 1000/1000: LR=8.96e-05, Loss=4.62e-03 BER=1.91e-03 FER=1.67e-02
2025-10-15 18:09:40,911 | INFO | Epoch 211 Train Time 43.94255590438843s

2025-10-15 18:10:25,235 | INFO | Training epoch 212, Batch 1000/1000: LR=8.95e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.60e-02
2025-10-15 18:10:25,287 | INFO | Epoch 212 Train Time 44.37434935569763s

2025-10-15 18:11:08,118 | INFO | Training epoch 213, Batch 1000/1000: LR=8.94e-05, Loss=4.43e-03 BER=1.82e-03 FER=1.64e-02
2025-10-15 18:11:08,156 | INFO | Epoch 213 Train Time 42.8673357963562s

2025-10-15 18:11:51,942 | INFO | Training epoch 214, Batch 1000/1000: LR=8.93e-05, Loss=4.48e-03 BER=1.85e-03 FER=1.64e-02
2025-10-15 18:11:51,998 | INFO | Epoch 214 Train Time 43.83986186981201s

2025-10-15 18:12:35,913 | INFO | Training epoch 215, Batch 1000/1000: LR=8.92e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.66e-02
2025-10-15 18:12:35,976 | INFO | Epoch 215 Train Time 43.97745203971863s

2025-10-15 18:13:19,942 | INFO | Training epoch 216, Batch 1000/1000: LR=8.91e-05, Loss=4.58e-03 BER=1.89e-03 FER=1.66e-02
2025-10-15 18:13:20,014 | INFO | Epoch 216 Train Time 44.03639054298401s

2025-10-15 18:14:03,310 | INFO | Training epoch 217, Batch 1000/1000: LR=8.90e-05, Loss=4.51e-03 BER=1.85e-03 FER=1.64e-02
2025-10-15 18:14:03,352 | INFO | Epoch 217 Train Time 43.3363037109375s

2025-10-15 18:14:44,320 | INFO | Training epoch 218, Batch 1000/1000: LR=8.89e-05, Loss=4.42e-03 BER=1.81e-03 FER=1.63e-02
2025-10-15 18:14:44,362 | INFO | Epoch 218 Train Time 41.00799298286438s

2025-10-15 18:15:28,802 | INFO | Training epoch 219, Batch 1000/1000: LR=8.88e-05, Loss=4.79e-03 BER=1.97e-03 FER=1.81e-02
2025-10-15 18:15:28,846 | INFO | Epoch 219 Train Time 44.48303484916687s

2025-10-15 18:16:11,047 | INFO | Training epoch 220, Batch 1000/1000: LR=8.87e-05, Loss=4.46e-03 BER=1.80e-03 FER=1.61e-02
2025-10-15 18:16:11,112 | INFO | Epoch 220 Train Time 42.26343035697937s

2025-10-15 18:16:55,314 | INFO | Training epoch 221, Batch 1000/1000: LR=8.86e-05, Loss=4.48e-03 BER=1.81e-03 FER=1.61e-02
2025-10-15 18:16:55,358 | INFO | Epoch 221 Train Time 44.24408793449402s

2025-10-15 18:17:38,131 | INFO | Training epoch 222, Batch 1000/1000: LR=8.85e-05, Loss=4.63e-03 BER=1.90e-03 FER=1.68e-02
2025-10-15 18:17:38,186 | INFO | Epoch 222 Train Time 42.826234340667725s

2025-10-15 18:18:22,400 | INFO | Training epoch 223, Batch 1000/1000: LR=8.84e-05, Loss=4.30e-03 BER=1.76e-03 FER=1.56e-02
2025-10-15 18:18:22,450 | INFO | Epoch 223 Train Time 44.26246690750122s

2025-10-15 18:18:22,452 | INFO | [P1] saving best_model with loss 0.004302 at epoch 223
2025-10-15 18:19:06,099 | INFO | Training epoch 224, Batch 1000/1000: LR=8.83e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.62e-02
2025-10-15 18:19:06,145 | INFO | Epoch 224 Train Time 43.60227060317993s

2025-10-15 18:19:50,005 | INFO | Training epoch 225, Batch 1000/1000: LR=8.82e-05, Loss=4.59e-03 BER=1.86e-03 FER=1.65e-02
2025-10-15 18:19:50,052 | INFO | Epoch 225 Train Time 43.90583682060242s

2025-10-15 18:20:33,372 | INFO | Training epoch 226, Batch 1000/1000: LR=8.81e-05, Loss=4.60e-03 BER=1.85e-03 FER=1.71e-02
2025-10-15 18:20:33,436 | INFO | Epoch 226 Train Time 43.3825306892395s

2025-10-15 18:21:16,832 | INFO | Training epoch 227, Batch 1000/1000: LR=8.80e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.60e-02
2025-10-15 18:21:16,893 | INFO | Epoch 227 Train Time 43.45557165145874s

2025-10-15 18:22:00,245 | INFO | Training epoch 228, Batch 1000/1000: LR=8.79e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.62e-02
2025-10-15 18:22:00,313 | INFO | Epoch 228 Train Time 43.418835163116455s

2025-10-15 18:22:45,000 | INFO | Training epoch 229, Batch 1000/1000: LR=8.78e-05, Loss=4.48e-03 BER=1.85e-03 FER=1.64e-02
2025-10-15 18:22:45,047 | INFO | Epoch 229 Train Time 44.732346534729004s

2025-10-15 18:23:27,503 | INFO | Training epoch 230, Batch 1000/1000: LR=8.77e-05, Loss=4.56e-03 BER=1.87e-03 FER=1.64e-02
2025-10-15 18:23:27,569 | INFO | Epoch 230 Train Time 42.52010440826416s

2025-10-15 18:24:10,027 | INFO | Training epoch 231, Batch 1000/1000: LR=8.76e-05, Loss=4.59e-03 BER=1.89e-03 FER=1.66e-02
2025-10-15 18:24:10,076 | INFO | Epoch 231 Train Time 42.50554609298706s

2025-10-15 18:24:51,088 | INFO | Training epoch 232, Batch 1000/1000: LR=8.75e-05, Loss=4.37e-03 BER=1.78e-03 FER=1.61e-02
2025-10-15 18:24:51,136 | INFO | Epoch 232 Train Time 41.058677196502686s

2025-10-15 18:25:34,611 | INFO | Training epoch 233, Batch 1000/1000: LR=8.74e-05, Loss=4.24e-03 BER=1.73e-03 FER=1.54e-02
2025-10-15 18:25:34,663 | INFO | Epoch 233 Train Time 43.524888038635254s

2025-10-15 18:25:34,663 | INFO | [P1] saving best_model with loss 0.004237 at epoch 233
2025-10-15 18:26:18,225 | INFO | Training epoch 234, Batch 1000/1000: LR=8.73e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.61e-02
2025-10-15 18:26:18,273 | INFO | Epoch 234 Train Time 43.51951622962952s

2025-10-15 18:27:01,496 | INFO | Training epoch 235, Batch 1000/1000: LR=8.72e-05, Loss=4.41e-03 BER=1.82e-03 FER=1.64e-02
2025-10-15 18:27:01,541 | INFO | Epoch 235 Train Time 43.26666498184204s

2025-10-15 18:27:45,322 | INFO | Training epoch 236, Batch 1000/1000: LR=8.71e-05, Loss=4.44e-03 BER=1.81e-03 FER=1.60e-02
2025-10-15 18:27:45,376 | INFO | Epoch 236 Train Time 43.83354926109314s

2025-10-15 18:28:28,601 | INFO | Training epoch 237, Batch 1000/1000: LR=8.70e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.61e-02
2025-10-15 18:28:28,671 | INFO | Epoch 237 Train Time 43.29140901565552s

2025-10-15 18:29:12,417 | INFO | Training epoch 238, Batch 1000/1000: LR=8.69e-05, Loss=4.49e-03 BER=1.83e-03 FER=1.62e-02
2025-10-15 18:29:12,462 | INFO | Epoch 238 Train Time 43.78892135620117s

2025-10-15 18:29:56,837 | INFO | Training epoch 239, Batch 1000/1000: LR=8.68e-05, Loss=4.63e-03 BER=1.88e-03 FER=1.66e-02
2025-10-15 18:29:56,879 | INFO | Epoch 239 Train Time 44.41355872154236s

2025-10-15 18:30:39,522 | INFO | Training epoch 240, Batch 1000/1000: LR=8.67e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.58e-02
2025-10-15 18:30:39,567 | INFO | Epoch 240 Train Time 42.68698024749756s

2025-10-15 18:31:22,325 | INFO | Training epoch 241, Batch 1000/1000: LR=8.66e-05, Loss=4.43e-03 BER=1.82e-03 FER=1.60e-02
2025-10-15 18:31:22,387 | INFO | Epoch 241 Train Time 42.817413568496704s

2025-10-15 18:32:05,833 | INFO | Training epoch 242, Batch 1000/1000: LR=8.65e-05, Loss=4.58e-03 BER=1.86e-03 FER=1.65e-02
2025-10-15 18:32:05,917 | INFO | Epoch 242 Train Time 43.527947425842285s

2025-10-15 18:32:49,442 | INFO | Training epoch 243, Batch 1000/1000: LR=8.64e-05, Loss=4.37e-03 BER=1.81e-03 FER=1.62e-02
2025-10-15 18:32:49,514 | INFO | Epoch 243 Train Time 43.594826221466064s

2025-10-15 18:33:33,964 | INFO | Training epoch 244, Batch 1000/1000: LR=8.63e-05, Loss=4.45e-03 BER=1.85e-03 FER=1.63e-02
2025-10-15 18:33:34,025 | INFO | Epoch 244 Train Time 44.50893545150757s

2025-10-15 18:34:18,705 | INFO | Training epoch 245, Batch 1000/1000: LR=8.62e-05, Loss=4.45e-03 BER=1.86e-03 FER=1.64e-02
2025-10-15 18:34:18,753 | INFO | Epoch 245 Train Time 44.72577404975891s

2025-10-15 18:35:02,886 | INFO | Training epoch 246, Batch 1000/1000: LR=8.60e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.60e-02
2025-10-15 18:35:02,933 | INFO | Epoch 246 Train Time 44.177512645721436s

2025-10-15 18:35:45,902 | INFO | Training epoch 247, Batch 1000/1000: LR=8.59e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.62e-02
2025-10-15 18:35:45,941 | INFO | Epoch 247 Train Time 43.00674819946289s

2025-10-15 18:36:29,095 | INFO | Training epoch 248, Batch 1000/1000: LR=8.58e-05, Loss=4.38e-03 BER=1.80e-03 FER=1.61e-02
2025-10-15 18:36:29,137 | INFO | Epoch 248 Train Time 43.194300174713135s

2025-10-15 18:37:12,165 | INFO | Training epoch 249, Batch 1000/1000: LR=8.57e-05, Loss=4.44e-03 BER=1.79e-03 FER=1.59e-02
2025-10-15 18:37:12,208 | INFO | Epoch 249 Train Time 43.0690336227417s

2025-10-15 18:37:54,110 | INFO | Training epoch 250, Batch 1000/1000: LR=8.56e-05, Loss=4.50e-03 BER=1.85e-03 FER=1.63e-02
2025-10-15 18:37:54,151 | INFO | Epoch 250 Train Time 41.942288637161255s

2025-10-15 18:38:38,297 | INFO | Training epoch 251, Batch 1000/1000: LR=8.55e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.57e-02
2025-10-15 18:38:38,343 | INFO | Epoch 251 Train Time 44.18951344490051s

2025-10-15 18:39:21,202 | INFO | Training epoch 252, Batch 1000/1000: LR=8.54e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.67e-02
2025-10-15 18:39:21,254 | INFO | Epoch 252 Train Time 42.90992259979248s

2025-10-15 18:40:03,618 | INFO | Training epoch 253, Batch 1000/1000: LR=8.53e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.62e-02
2025-10-15 18:40:03,673 | INFO | Epoch 253 Train Time 42.417784690856934s

2025-10-15 18:40:47,607 | INFO | Training epoch 254, Batch 1000/1000: LR=8.52e-05, Loss=4.37e-03 BER=1.81e-03 FER=1.60e-02
2025-10-15 18:40:47,649 | INFO | Epoch 254 Train Time 43.97135519981384s

2025-10-15 18:41:31,414 | INFO | Training epoch 255, Batch 1000/1000: LR=8.51e-05, Loss=4.48e-03 BER=1.85e-03 FER=1.62e-02
2025-10-15 18:41:31,450 | INFO | Epoch 255 Train Time 43.79913663864136s

2025-10-15 18:42:14,514 | INFO | Training epoch 256, Batch 1000/1000: LR=8.49e-05, Loss=4.42e-03 BER=1.78e-03 FER=1.59e-02
2025-10-15 18:42:14,557 | INFO | Epoch 256 Train Time 43.105281829833984s

2025-10-15 18:42:57,218 | INFO | Training epoch 257, Batch 1000/1000: LR=8.48e-05, Loss=4.68e-03 BER=1.93e-03 FER=1.68e-02
2025-10-15 18:42:57,255 | INFO | Epoch 257 Train Time 42.69673442840576s

2025-10-15 18:43:41,918 | INFO | Training epoch 258, Batch 1000/1000: LR=8.47e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.67e-02
2025-10-15 18:43:41,962 | INFO | Epoch 258 Train Time 44.70602107048035s

2025-10-15 18:44:24,163 | INFO | Training epoch 259, Batch 1000/1000: LR=8.46e-05, Loss=4.42e-03 BER=1.81e-03 FER=1.58e-02
2025-10-15 18:44:24,217 | INFO | Epoch 259 Train Time 42.25420522689819s

2025-10-15 18:45:08,743 | INFO | Training epoch 260, Batch 1000/1000: LR=8.45e-05, Loss=4.34e-03 BER=1.77e-03 FER=1.56e-02
2025-10-15 18:45:08,809 | INFO | Epoch 260 Train Time 44.589998722076416s

2025-10-15 18:45:52,922 | INFO | Training epoch 261, Batch 1000/1000: LR=8.44e-05, Loss=4.41e-03 BER=1.80e-03 FER=1.58e-02
2025-10-15 18:45:52,962 | INFO | Epoch 261 Train Time 44.151047229766846s

2025-10-15 18:46:36,223 | INFO | Training epoch 262, Batch 1000/1000: LR=8.43e-05, Loss=4.23e-03 BER=1.75e-03 FER=1.57e-02
2025-10-15 18:46:36,260 | INFO | Epoch 262 Train Time 43.29672193527222s

2025-10-15 18:46:36,260 | INFO | [P1] saving best_model with loss 0.004229 at epoch 262
2025-10-15 18:47:19,706 | INFO | Training epoch 263, Batch 1000/1000: LR=8.42e-05, Loss=4.38e-03 BER=1.80e-03 FER=1.61e-02
2025-10-15 18:47:19,758 | INFO | Epoch 263 Train Time 43.398934841156006s

2025-10-15 18:48:03,185 | INFO | Training epoch 264, Batch 1000/1000: LR=8.40e-05, Loss=4.34e-03 BER=1.75e-03 FER=1.58e-02
2025-10-15 18:48:03,222 | INFO | Epoch 264 Train Time 43.46154165267944s

2025-10-15 18:48:46,922 | INFO | Training epoch 265, Batch 1000/1000: LR=8.39e-05, Loss=4.33e-03 BER=1.77e-03 FER=1.55e-02
2025-10-15 18:48:46,975 | INFO | Epoch 265 Train Time 43.75168418884277s

2025-10-15 18:49:31,502 | INFO | Training epoch 266, Batch 1000/1000: LR=8.38e-05, Loss=4.43e-03 BER=1.80e-03 FER=1.61e-02
2025-10-15 18:49:31,554 | INFO | Epoch 266 Train Time 44.57800531387329s

2025-10-15 18:50:13,305 | INFO | Training epoch 267, Batch 1000/1000: LR=8.37e-05, Loss=4.24e-03 BER=1.71e-03 FER=1.54e-02
2025-10-15 18:50:13,349 | INFO | Epoch 267 Train Time 41.79421377182007s

2025-10-15 18:50:56,304 | INFO | Training epoch 268, Batch 1000/1000: LR=8.36e-05, Loss=4.40e-03 BER=1.78e-03 FER=1.59e-02
2025-10-15 18:50:56,349 | INFO | Epoch 268 Train Time 42.998164892196655s

2025-10-15 18:51:39,700 | INFO | Training epoch 269, Batch 1000/1000: LR=8.35e-05, Loss=4.55e-03 BER=1.84e-03 FER=1.62e-02
2025-10-15 18:51:39,744 | INFO | Epoch 269 Train Time 43.3942346572876s

2025-10-15 18:52:23,923 | INFO | Training epoch 270, Batch 1000/1000: LR=8.34e-05, Loss=4.52e-03 BER=1.83e-03 FER=1.63e-02
2025-10-15 18:52:23,969 | INFO | Epoch 270 Train Time 44.223588705062866s

2025-10-15 18:53:08,310 | INFO | Training epoch 271, Batch 1000/1000: LR=8.32e-05, Loss=4.55e-03 BER=1.85e-03 FER=1.66e-02
2025-10-15 18:53:08,381 | INFO | Epoch 271 Train Time 44.41024088859558s

2025-10-15 18:53:51,506 | INFO | Training epoch 272, Batch 1000/1000: LR=8.31e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.66e-02
2025-10-15 18:53:51,549 | INFO | Epoch 272 Train Time 43.166027307510376s

2025-10-15 18:54:35,934 | INFO | Training epoch 273, Batch 1000/1000: LR=8.30e-05, Loss=4.31e-03 BER=1.79e-03 FER=1.58e-02
2025-10-15 18:54:35,977 | INFO | Epoch 273 Train Time 44.42691683769226s

2025-10-15 18:55:19,493 | INFO | Training epoch 274, Batch 1000/1000: LR=8.29e-05, Loss=4.43e-03 BER=1.83e-03 FER=1.59e-02
2025-10-15 18:55:19,545 | INFO | Epoch 274 Train Time 43.56553506851196s

2025-10-15 18:56:02,813 | INFO | Training epoch 275, Batch 1000/1000: LR=8.28e-05, Loss=4.38e-03 BER=1.79e-03 FER=1.59e-02
2025-10-15 18:56:02,855 | INFO | Epoch 275 Train Time 43.30868935585022s

2025-10-15 18:56:46,825 | INFO | Training epoch 276, Batch 1000/1000: LR=8.26e-05, Loss=4.38e-03 BER=1.81e-03 FER=1.60e-02
2025-10-15 18:56:46,874 | INFO | Epoch 276 Train Time 44.01847219467163s

2025-10-15 18:57:29,909 | INFO | Training epoch 277, Batch 1000/1000: LR=8.25e-05, Loss=4.45e-03 BER=1.84e-03 FER=1.61e-02
2025-10-15 18:57:29,945 | INFO | Epoch 277 Train Time 43.06869721412659s

2025-10-15 18:58:13,699 | INFO | Training epoch 278, Batch 1000/1000: LR=8.24e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.62e-02
2025-10-15 18:58:13,745 | INFO | Epoch 278 Train Time 43.79922580718994s

2025-10-15 18:58:56,967 | INFO | Training epoch 279, Batch 1000/1000: LR=8.23e-05, Loss=4.28e-03 BER=1.77e-03 FER=1.57e-02
2025-10-15 18:58:57,003 | INFO | Epoch 279 Train Time 43.25562930107117s

2025-10-15 18:59:39,798 | INFO | Training epoch 280, Batch 1000/1000: LR=8.22e-05, Loss=4.30e-03 BER=1.76e-03 FER=1.57e-02
2025-10-15 18:59:39,867 | INFO | Epoch 280 Train Time 42.86303377151489s

2025-10-15 19:00:23,230 | INFO | Training epoch 281, Batch 1000/1000: LR=8.21e-05, Loss=4.24e-03 BER=1.75e-03 FER=1.55e-02
2025-10-15 19:00:23,277 | INFO | Epoch 281 Train Time 43.407267808914185s

2025-10-15 19:01:05,804 | INFO | Training epoch 282, Batch 1000/1000: LR=8.19e-05, Loss=4.55e-03 BER=1.87e-03 FER=1.63e-02
2025-10-15 19:01:05,859 | INFO | Epoch 282 Train Time 42.58028054237366s

2025-10-15 19:01:49,317 | INFO | Training epoch 283, Batch 1000/1000: LR=8.18e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.60e-02
2025-10-15 19:01:49,359 | INFO | Epoch 283 Train Time 43.49905180931091s

2025-10-15 19:02:33,433 | INFO | Training epoch 284, Batch 1000/1000: LR=8.17e-05, Loss=4.12e-03 BER=1.68e-03 FER=1.51e-02
2025-10-15 19:02:33,501 | INFO | Epoch 284 Train Time 44.140273094177246s

2025-10-15 19:02:33,502 | INFO | [P1] saving best_model with loss 0.004120 at epoch 284
2025-10-15 19:03:17,598 | INFO | Training epoch 285, Batch 1000/1000: LR=8.16e-05, Loss=4.45e-03 BER=1.80e-03 FER=1.60e-02
2025-10-15 19:03:17,670 | INFO | Epoch 285 Train Time 44.06226587295532s

2025-10-15 19:04:01,812 | INFO | Training epoch 286, Batch 1000/1000: LR=8.14e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.67e-02
2025-10-15 19:04:01,861 | INFO | Epoch 286 Train Time 44.1898136138916s

2025-10-15 19:04:43,697 | INFO | Training epoch 287, Batch 1000/1000: LR=8.13e-05, Loss=4.32e-03 BER=1.77e-03 FER=1.55e-02
2025-10-15 19:04:43,739 | INFO | Epoch 287 Train Time 41.87703537940979s

2025-10-15 19:05:27,155 | INFO | Training epoch 288, Batch 1000/1000: LR=8.12e-05, Loss=4.36e-03 BER=1.77e-03 FER=1.57e-02
2025-10-15 19:05:27,216 | INFO | Epoch 288 Train Time 43.47541260719299s

2025-10-15 19:06:10,751 | INFO | Training epoch 289, Batch 1000/1000: LR=8.11e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.57e-02
2025-10-15 19:06:10,792 | INFO | Epoch 289 Train Time 43.5744788646698s

2025-10-15 19:06:54,995 | INFO | Training epoch 290, Batch 1000/1000: LR=8.10e-05, Loss=4.43e-03 BER=1.82e-03 FER=1.57e-02
2025-10-15 19:06:55,052 | INFO | Epoch 290 Train Time 44.258556842803955s

2025-10-15 19:07:38,716 | INFO | Training epoch 291, Batch 1000/1000: LR=8.08e-05, Loss=4.40e-03 BER=1.78e-03 FER=1.59e-02
2025-10-15 19:07:38,766 | INFO | Epoch 291 Train Time 43.71281337738037s

2025-10-15 19:08:21,504 | INFO | Training epoch 292, Batch 1000/1000: LR=8.07e-05, Loss=4.20e-03 BER=1.74e-03 FER=1.55e-02
2025-10-15 19:08:21,547 | INFO | Epoch 292 Train Time 42.77901005744934s

2025-10-15 19:09:05,533 | INFO | Training epoch 293, Batch 1000/1000: LR=8.06e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.57e-02
2025-10-15 19:09:05,607 | INFO | Epoch 293 Train Time 44.057533264160156s

2025-10-15 19:09:48,352 | INFO | Training epoch 294, Batch 1000/1000: LR=8.05e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.59e-02
2025-10-15 19:09:48,438 | INFO | Epoch 294 Train Time 42.828410387039185s

2025-10-15 19:10:34,013 | INFO | Training epoch 295, Batch 1000/1000: LR=8.03e-05, Loss=4.36e-03 BER=1.76e-03 FER=1.57e-02
2025-10-15 19:10:34,058 | INFO | Epoch 295 Train Time 45.61824083328247s

2025-10-15 19:11:17,731 | INFO | Training epoch 296, Batch 1000/1000: LR=8.02e-05, Loss=4.29e-03 BER=1.75e-03 FER=1.56e-02
2025-10-15 19:11:17,783 | INFO | Epoch 296 Train Time 43.72329592704773s

2025-10-15 19:12:00,831 | INFO | Training epoch 297, Batch 1000/1000: LR=8.01e-05, Loss=4.43e-03 BER=1.81e-03 FER=1.58e-02
2025-10-15 19:12:00,873 | INFO | Epoch 297 Train Time 43.08828067779541s

2025-10-15 19:12:43,699 | INFO | Training epoch 298, Batch 1000/1000: LR=8.00e-05, Loss=4.44e-03 BER=1.81e-03 FER=1.60e-02
2025-10-15 19:12:43,743 | INFO | Epoch 298 Train Time 42.86843538284302s

2025-10-15 19:13:26,297 | INFO | Training epoch 299, Batch 1000/1000: LR=7.98e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.59e-02
2025-10-15 19:13:26,340 | INFO | Epoch 299 Train Time 42.5955548286438s

2025-10-15 19:14:09,911 | INFO | Training epoch 300, Batch 1000/1000: LR=7.97e-05, Loss=4.44e-03 BER=1.83e-03 FER=1.61e-02
2025-10-15 19:14:09,964 | INFO | Epoch 300 Train Time 43.622353076934814s

2025-10-15 19:14:53,499 | INFO | Training epoch 301, Batch 1000/1000: LR=7.96e-05, Loss=4.34e-03 BER=1.76e-03 FER=1.55e-02
2025-10-15 19:14:53,563 | INFO | Epoch 301 Train Time 43.59726858139038s

2025-10-15 19:15:37,416 | INFO | Training epoch 302, Batch 1000/1000: LR=7.95e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.60e-02
2025-10-15 19:15:37,462 | INFO | Epoch 302 Train Time 43.897109270095825s

2025-10-15 19:16:20,806 | INFO | Training epoch 303, Batch 1000/1000: LR=7.93e-05, Loss=4.40e-03 BER=1.78e-03 FER=1.60e-02
2025-10-15 19:16:20,841 | INFO | Epoch 303 Train Time 43.378153562545776s

2025-10-15 19:17:05,730 | INFO | Training epoch 304, Batch 1000/1000: LR=7.92e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.59e-02
2025-10-15 19:17:05,792 | INFO | Epoch 304 Train Time 44.948240995407104s

2025-10-15 19:17:47,608 | INFO | Training epoch 305, Batch 1000/1000: LR=7.91e-05, Loss=4.37e-03 BER=1.81e-03 FER=1.60e-02
2025-10-15 19:17:47,658 | INFO | Epoch 305 Train Time 41.86347532272339s

2025-10-15 19:18:30,827 | INFO | Training epoch 306, Batch 1000/1000: LR=7.90e-05, Loss=4.43e-03 BER=1.81e-03 FER=1.62e-02
2025-10-15 19:18:30,871 | INFO | Epoch 306 Train Time 43.21156167984009s

2025-10-15 19:19:14,323 | INFO | Training epoch 307, Batch 1000/1000: LR=7.88e-05, Loss=4.48e-03 BER=1.82e-03 FER=1.59e-02
2025-10-15 19:19:14,366 | INFO | Epoch 307 Train Time 43.493722438812256s

2025-10-15 19:19:57,809 | INFO | Training epoch 308, Batch 1000/1000: LR=7.87e-05, Loss=4.26e-03 BER=1.74e-03 FER=1.53e-02
2025-10-15 19:19:57,850 | INFO | Epoch 308 Train Time 43.48177695274353s

2025-10-15 19:20:41,807 | INFO | Training epoch 309, Batch 1000/1000: LR=7.86e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.56e-02
2025-10-15 19:20:41,851 | INFO | Epoch 309 Train Time 44.00052237510681s

2025-10-15 19:21:25,156 | INFO | Training epoch 310, Batch 1000/1000: LR=7.85e-05, Loss=4.58e-03 BER=1.86e-03 FER=1.65e-02
2025-10-15 19:21:25,214 | INFO | Epoch 310 Train Time 43.36186099052429s

2025-10-15 19:22:08,242 | INFO | Training epoch 311, Batch 1000/1000: LR=7.83e-05, Loss=4.47e-03 BER=1.84e-03 FER=1.64e-02
2025-10-15 19:22:08,329 | INFO | Epoch 311 Train Time 43.11256432533264s

2025-10-15 19:22:52,548 | INFO | Training epoch 312, Batch 1000/1000: LR=7.82e-05, Loss=4.19e-03 BER=1.74e-03 FER=1.54e-02
2025-10-15 19:22:52,612 | INFO | Epoch 312 Train Time 44.28085517883301s

2025-10-15 19:23:37,344 | INFO | Training epoch 313, Batch 1000/1000: LR=7.81e-05, Loss=4.41e-03 BER=1.81e-03 FER=1.61e-02
2025-10-15 19:23:37,400 | INFO | Epoch 313 Train Time 44.78559160232544s

2025-10-15 19:24:20,337 | INFO | Training epoch 314, Batch 1000/1000: LR=7.79e-05, Loss=4.40e-03 BER=1.80e-03 FER=1.59e-02
2025-10-15 19:24:20,388 | INFO | Epoch 314 Train Time 42.98627519607544s

2025-10-15 19:25:04,514 | INFO | Training epoch 315, Batch 1000/1000: LR=7.78e-05, Loss=4.35e-03 BER=1.76e-03 FER=1.57e-02
2025-10-15 19:25:04,555 | INFO | Epoch 315 Train Time 44.16528081893921s

2025-10-15 19:25:48,864 | INFO | Training epoch 316, Batch 1000/1000: LR=7.77e-05, Loss=4.17e-03 BER=1.72e-03 FER=1.52e-02
2025-10-15 19:25:48,937 | INFO | Epoch 316 Train Time 44.380560874938965s

2025-10-15 19:26:32,110 | INFO | Training epoch 317, Batch 1000/1000: LR=7.75e-05, Loss=4.32e-03 BER=1.79e-03 FER=1.57e-02
2025-10-15 19:26:32,154 | INFO | Epoch 317 Train Time 43.21492266654968s

2025-10-15 19:27:15,013 | INFO | Training epoch 318, Batch 1000/1000: LR=7.74e-05, Loss=4.38e-03 BER=1.78e-03 FER=1.56e-02
2025-10-15 19:27:15,057 | INFO | Epoch 318 Train Time 42.90223288536072s

2025-10-15 19:27:58,317 | INFO | Training epoch 319, Batch 1000/1000: LR=7.73e-05, Loss=4.31e-03 BER=1.76e-03 FER=1.57e-02
2025-10-15 19:27:58,374 | INFO | Epoch 319 Train Time 43.3154821395874s

2025-10-15 19:28:40,228 | INFO | Training epoch 320, Batch 1000/1000: LR=7.72e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.61e-02
2025-10-15 19:28:40,302 | INFO | Epoch 320 Train Time 41.92569088935852s

2025-10-15 19:29:24,503 | INFO | Training epoch 321, Batch 1000/1000: LR=7.70e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.57e-02
2025-10-15 19:29:24,553 | INFO | Epoch 321 Train Time 44.24866843223572s

2025-10-15 19:30:08,072 | INFO | Training epoch 322, Batch 1000/1000: LR=7.69e-05, Loss=4.50e-03 BER=1.85e-03 FER=1.63e-02
2025-10-15 19:30:08,142 | INFO | Epoch 322 Train Time 43.58775615692139s

2025-10-15 19:30:51,800 | INFO | Training epoch 323, Batch 1000/1000: LR=7.68e-05, Loss=4.31e-03 BER=1.76e-03 FER=1.57e-02
2025-10-15 19:30:51,847 | INFO | Epoch 323 Train Time 43.70234513282776s

2025-10-15 19:31:34,525 | INFO | Training epoch 324, Batch 1000/1000: LR=7.66e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.52e-02
2025-10-15 19:31:34,582 | INFO | Epoch 324 Train Time 42.734238147735596s

2025-10-15 19:32:17,904 | INFO | Training epoch 325, Batch 1000/1000: LR=7.65e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.60e-02
2025-10-15 19:32:17,955 | INFO | Epoch 325 Train Time 43.37112998962402s

2025-10-15 19:33:02,140 | INFO | Training epoch 326, Batch 1000/1000: LR=7.64e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.64e-02
2025-10-15 19:33:02,200 | INFO | Epoch 326 Train Time 44.24387836456299s

2025-10-15 19:33:46,364 | INFO | Training epoch 327, Batch 1000/1000: LR=7.62e-05, Loss=4.26e-03 BER=1.73e-03 FER=1.54e-02
2025-10-15 19:33:46,439 | INFO | Epoch 327 Train Time 44.23728919029236s

2025-10-15 19:34:29,799 | INFO | Training epoch 328, Batch 1000/1000: LR=7.61e-05, Loss=4.29e-03 BER=1.75e-03 FER=1.55e-02
2025-10-15 19:34:29,861 | INFO | Epoch 328 Train Time 43.42060923576355s

2025-10-15 19:35:13,331 | INFO | Training epoch 329, Batch 1000/1000: LR=7.60e-05, Loss=4.25e-03 BER=1.73e-03 FER=1.54e-02
2025-10-15 19:35:13,375 | INFO | Epoch 329 Train Time 43.512847900390625s

2025-10-15 19:35:56,318 | INFO | Training epoch 330, Batch 1000/1000: LR=7.58e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.60e-02
2025-10-15 19:35:56,359 | INFO | Epoch 330 Train Time 42.982258319854736s

2025-10-15 19:36:40,930 | INFO | Training epoch 331, Batch 1000/1000: LR=7.57e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.57e-02
2025-10-15 19:36:40,980 | INFO | Epoch 331 Train Time 44.619141578674316s

2025-10-15 19:37:24,306 | INFO | Training epoch 332, Batch 1000/1000: LR=7.56e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.58e-02
2025-10-15 19:37:24,356 | INFO | Epoch 332 Train Time 43.374197483062744s

2025-10-15 19:38:07,698 | INFO | Training epoch 333, Batch 1000/1000: LR=7.54e-05, Loss=4.28e-03 BER=1.75e-03 FER=1.53e-02
2025-10-15 19:38:07,740 | INFO | Epoch 333 Train Time 43.38204908370972s

2025-10-15 19:38:50,343 | INFO | Training epoch 334, Batch 1000/1000: LR=7.53e-05, Loss=4.49e-03 BER=1.84e-03 FER=1.64e-02
2025-10-15 19:38:50,400 | INFO | Epoch 334 Train Time 42.65845727920532s

2025-10-15 19:39:34,398 | INFO | Training epoch 335, Batch 1000/1000: LR=7.52e-05, Loss=4.20e-03 BER=1.74e-03 FER=1.53e-02
2025-10-15 19:39:34,448 | INFO | Epoch 335 Train Time 44.04723334312439s

2025-10-15 19:40:17,506 | INFO | Training epoch 336, Batch 1000/1000: LR=7.50e-05, Loss=4.34e-03 BER=1.79e-03 FER=1.57e-02
2025-10-15 19:40:17,557 | INFO | Epoch 336 Train Time 43.10810589790344s

2025-10-15 19:41:01,299 | INFO | Training epoch 337, Batch 1000/1000: LR=7.49e-05, Loss=4.38e-03 BER=1.79e-03 FER=1.53e-02
2025-10-15 19:41:01,363 | INFO | Epoch 337 Train Time 43.80333495140076s

2025-10-15 19:41:44,466 | INFO | Training epoch 338, Batch 1000/1000: LR=7.48e-05, Loss=4.29e-03 BER=1.77e-03 FER=1.56e-02
2025-10-15 19:41:44,552 | INFO | Epoch 338 Train Time 43.1878125667572s

2025-10-15 19:42:27,698 | INFO | Training epoch 339, Batch 1000/1000: LR=7.46e-05, Loss=4.27e-03 BER=1.78e-03 FER=1.55e-02
2025-10-15 19:42:27,751 | INFO | Epoch 339 Train Time 43.196428298950195s

2025-10-15 19:43:11,646 | INFO | Training epoch 340, Batch 1000/1000: LR=7.45e-05, Loss=4.16e-03 BER=1.70e-03 FER=1.51e-02
2025-10-15 19:43:11,707 | INFO | Epoch 340 Train Time 43.95486378669739s

2025-10-15 19:43:55,210 | INFO | Training epoch 341, Batch 1000/1000: LR=7.43e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.56e-02
2025-10-15 19:43:55,250 | INFO | Epoch 341 Train Time 43.54172897338867s

2025-10-15 19:44:38,625 | INFO | Training epoch 342, Batch 1000/1000: LR=7.42e-05, Loss=4.24e-03 BER=1.74e-03 FER=1.55e-02
2025-10-15 19:44:38,687 | INFO | Epoch 342 Train Time 43.43545436859131s

2025-10-15 19:45:21,959 | INFO | Training epoch 343, Batch 1000/1000: LR=7.41e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.52e-02
2025-10-15 19:45:22,036 | INFO | Epoch 343 Train Time 43.34718728065491s

2025-10-15 19:46:05,999 | INFO | Training epoch 344, Batch 1000/1000: LR=7.39e-05, Loss=4.44e-03 BER=1.79e-03 FER=1.60e-02
2025-10-15 19:46:06,035 | INFO | Epoch 344 Train Time 43.99797582626343s

2025-10-15 19:46:49,836 | INFO | Training epoch 345, Batch 1000/1000: LR=7.38e-05, Loss=4.35e-03 BER=1.78e-03 FER=1.59e-02
2025-10-15 19:46:49,889 | INFO | Epoch 345 Train Time 43.8521363735199s

2025-10-15 19:47:33,606 | INFO | Training epoch 346, Batch 1000/1000: LR=7.37e-05, Loss=4.26e-03 BER=1.72e-03 FER=1.50e-02
2025-10-15 19:47:33,656 | INFO | Epoch 346 Train Time 43.76559829711914s

2025-10-15 19:48:16,384 | INFO | Training epoch 347, Batch 1000/1000: LR=7.35e-05, Loss=4.24e-03 BER=1.73e-03 FER=1.56e-02
2025-10-15 19:48:16,444 | INFO | Epoch 347 Train Time 42.78662586212158s

2025-10-15 19:49:01,125 | INFO | Training epoch 348, Batch 1000/1000: LR=7.34e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.52e-02
2025-10-15 19:49:01,186 | INFO | Epoch 348 Train Time 44.74031400680542s

2025-10-15 19:49:44,505 | INFO | Training epoch 349, Batch 1000/1000: LR=7.32e-05, Loss=4.11e-03 BER=1.69e-03 FER=1.52e-02
2025-10-15 19:49:44,570 | INFO | Epoch 349 Train Time 43.3823721408844s

2025-10-15 19:49:44,571 | INFO | [P1] saving best_model with loss 0.004113 at epoch 349
2025-10-15 19:50:29,324 | INFO | Training epoch 350, Batch 1000/1000: LR=7.31e-05, Loss=4.38e-03 BER=1.81e-03 FER=1.56e-02
2025-10-15 19:50:29,364 | INFO | Epoch 350 Train Time 44.67597317695618s

2025-10-15 19:51:12,099 | INFO | Training epoch 351, Batch 1000/1000: LR=7.30e-05, Loss=4.35e-03 BER=1.77e-03 FER=1.57e-02
2025-10-15 19:51:12,151 | INFO | Epoch 351 Train Time 42.78566789627075s

2025-10-15 19:51:55,954 | INFO | Training epoch 352, Batch 1000/1000: LR=7.28e-05, Loss=4.30e-03 BER=1.79e-03 FER=1.57e-02
2025-10-15 19:51:56,032 | INFO | Epoch 352 Train Time 43.87962365150452s

2025-10-15 19:52:39,625 | INFO | Training epoch 353, Batch 1000/1000: LR=7.27e-05, Loss=4.33e-03 BER=1.76e-03 FER=1.58e-02
2025-10-15 19:52:39,676 | INFO | Epoch 353 Train Time 43.642436265945435s

2025-10-15 19:53:22,497 | INFO | Training epoch 354, Batch 1000/1000: LR=7.26e-05, Loss=4.29e-03 BER=1.77e-03 FER=1.53e-02
2025-10-15 19:53:22,547 | INFO | Epoch 354 Train Time 42.86888909339905s

2025-10-15 19:54:08,227 | INFO | Training epoch 355, Batch 1000/1000: LR=7.24e-05, Loss=4.05e-03 BER=1.64e-03 FER=1.47e-02
2025-10-15 19:54:08,274 | INFO | Epoch 355 Train Time 45.72522759437561s

2025-10-15 19:54:08,275 | INFO | [P1] saving best_model with loss 0.004045 at epoch 355
2025-10-15 19:54:52,331 | INFO | Training epoch 356, Batch 1000/1000: LR=7.23e-05, Loss=4.33e-03 BER=1.80e-03 FER=1.59e-02
2025-10-15 19:54:52,375 | INFO | Epoch 356 Train Time 44.00992155075073s

2025-10-15 19:55:35,824 | INFO | Training epoch 357, Batch 1000/1000: LR=7.21e-05, Loss=4.34e-03 BER=1.77e-03 FER=1.58e-02
2025-10-15 19:55:35,871 | INFO | Epoch 357 Train Time 43.49427628517151s

2025-10-15 19:56:19,018 | INFO | Training epoch 358, Batch 1000/1000: LR=7.20e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.58e-02
2025-10-15 19:56:19,064 | INFO | Epoch 358 Train Time 43.19111156463623s

2025-10-15 19:57:01,714 | INFO | Training epoch 359, Batch 1000/1000: LR=7.19e-05, Loss=4.30e-03 BER=1.73e-03 FER=1.50e-02
2025-10-15 19:57:01,762 | INFO | Epoch 359 Train Time 42.69694757461548s

2025-10-15 19:57:44,085 | INFO | Training epoch 360, Batch 1000/1000: LR=7.17e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.57e-02
2025-10-15 19:57:44,140 | INFO | Epoch 360 Train Time 42.37631440162659s

2025-10-15 19:58:26,954 | INFO | Training epoch 361, Batch 1000/1000: LR=7.16e-05, Loss=4.46e-03 BER=1.81e-03 FER=1.58e-02
2025-10-15 19:58:26,995 | INFO | Epoch 361 Train Time 42.85350775718689s

2025-10-15 19:59:10,615 | INFO | Training epoch 362, Batch 1000/1000: LR=7.14e-05, Loss=4.28e-03 BER=1.77e-03 FER=1.60e-02
2025-10-15 19:59:10,665 | INFO | Epoch 362 Train Time 43.668163537979126s

2025-10-15 19:59:54,192 | INFO | Training epoch 363, Batch 1000/1000: LR=7.13e-05, Loss=4.19e-03 BER=1.74e-03 FER=1.54e-02
2025-10-15 19:59:54,246 | INFO | Epoch 363 Train Time 43.580350160598755s

2025-10-15 20:00:37,019 | INFO | Training epoch 364, Batch 1000/1000: LR=7.12e-05, Loss=4.25e-03 BER=1.75e-03 FER=1.56e-02
2025-10-15 20:00:37,067 | INFO | Epoch 364 Train Time 42.819578409194946s

2025-10-15 20:01:19,932 | INFO | Training epoch 365, Batch 1000/1000: LR=7.10e-05, Loss=4.33e-03 BER=1.75e-03 FER=1.51e-02
2025-10-15 20:01:19,985 | INFO | Epoch 365 Train Time 42.91613554954529s

2025-10-15 20:02:03,900 | INFO | Training epoch 366, Batch 1000/1000: LR=7.09e-05, Loss=4.33e-03 BER=1.76e-03 FER=1.53e-02
2025-10-15 20:02:03,964 | INFO | Epoch 366 Train Time 43.97800326347351s

2025-10-15 20:02:48,136 | INFO | Training epoch 367, Batch 1000/1000: LR=7.07e-05, Loss=4.36e-03 BER=1.80e-03 FER=1.57e-02
2025-10-15 20:02:48,188 | INFO | Epoch 367 Train Time 44.222479820251465s

2025-10-15 20:03:31,298 | INFO | Training epoch 368, Batch 1000/1000: LR=7.06e-05, Loss=4.28e-03 BER=1.76e-03 FER=1.54e-02
2025-10-15 20:03:31,351 | INFO | Epoch 368 Train Time 43.16154861450195s

2025-10-15 20:04:15,772 | INFO | Training epoch 369, Batch 1000/1000: LR=7.04e-05, Loss=4.18e-03 BER=1.73e-03 FER=1.50e-02
2025-10-15 20:04:15,834 | INFO | Epoch 369 Train Time 44.481120347976685s

2025-10-15 20:04:59,739 | INFO | Training epoch 370, Batch 1000/1000: LR=7.03e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.59e-02
2025-10-15 20:04:59,782 | INFO | Epoch 370 Train Time 43.94717288017273s

2025-10-15 20:05:42,910 | INFO | Training epoch 371, Batch 1000/1000: LR=7.02e-05, Loss=4.38e-03 BER=1.80e-03 FER=1.58e-02
2025-10-15 20:05:42,949 | INFO | Epoch 371 Train Time 43.16436457633972s

2025-10-15 20:06:26,003 | INFO | Training epoch 372, Batch 1000/1000: LR=7.00e-05, Loss=4.36e-03 BER=1.83e-03 FER=1.58e-02
2025-10-15 20:06:26,052 | INFO | Epoch 372 Train Time 43.1016788482666s

2025-10-15 20:07:09,214 | INFO | Training epoch 373, Batch 1000/1000: LR=6.99e-05, Loss=4.09e-03 BER=1.70e-03 FER=1.51e-02
2025-10-15 20:07:09,269 | INFO | Epoch 373 Train Time 43.21674108505249s

2025-10-15 20:07:53,199 | INFO | Training epoch 374, Batch 1000/1000: LR=6.97e-05, Loss=4.19e-03 BER=1.69e-03 FER=1.50e-02
2025-10-15 20:07:53,247 | INFO | Epoch 374 Train Time 43.976053953170776s

2025-10-15 20:08:36,867 | INFO | Training epoch 375, Batch 1000/1000: LR=6.96e-05, Loss=4.28e-03 BER=1.75e-03 FER=1.54e-02
2025-10-15 20:08:36,927 | INFO | Epoch 375 Train Time 43.67865014076233s

2025-10-15 20:09:20,498 | INFO | Training epoch 376, Batch 1000/1000: LR=6.94e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.47e-02
2025-10-15 20:09:20,540 | INFO | Epoch 376 Train Time 43.611814737319946s

2025-10-15 20:09:20,540 | INFO | [P1] saving best_model with loss 0.004025 at epoch 376
2025-10-15 20:10:03,490 | INFO | Training epoch 377, Batch 1000/1000: LR=6.93e-05, Loss=4.17e-03 BER=1.70e-03 FER=1.47e-02
2025-10-15 20:10:03,566 | INFO | Epoch 377 Train Time 42.934239864349365s

2025-10-15 20:10:47,597 | INFO | Training epoch 378, Batch 1000/1000: LR=6.92e-05, Loss=4.14e-03 BER=1.67e-03 FER=1.47e-02
2025-10-15 20:10:47,644 | INFO | Epoch 378 Train Time 44.07558298110962s

2025-10-15 20:11:31,100 | INFO | Training epoch 379, Batch 1000/1000: LR=6.90e-05, Loss=4.34e-03 BER=1.77e-03 FER=1.57e-02
2025-10-15 20:11:31,158 | INFO | Epoch 379 Train Time 43.511200189590454s

2025-10-15 20:12:13,406 | INFO | Training epoch 380, Batch 1000/1000: LR=6.89e-05, Loss=4.21e-03 BER=1.74e-03 FER=1.52e-02
2025-10-15 20:12:13,473 | INFO | Epoch 380 Train Time 42.313358783721924s

2025-10-15 20:12:55,802 | INFO | Training epoch 381, Batch 1000/1000: LR=6.87e-05, Loss=4.30e-03 BER=1.76e-03 FER=1.53e-02
2025-10-15 20:12:55,851 | INFO | Epoch 381 Train Time 42.37707042694092s

2025-10-15 20:13:38,021 | INFO | Training epoch 382, Batch 1000/1000: LR=6.86e-05, Loss=4.11e-03 BER=1.67e-03 FER=1.51e-02
2025-10-15 20:13:38,069 | INFO | Epoch 382 Train Time 42.2155487537384s

2025-10-15 20:14:22,210 | INFO | Training epoch 383, Batch 1000/1000: LR=6.84e-05, Loss=4.11e-03 BER=1.69e-03 FER=1.45e-02
2025-10-15 20:14:22,259 | INFO | Epoch 383 Train Time 44.188862562179565s

2025-10-15 20:15:06,006 | INFO | Training epoch 384, Batch 1000/1000: LR=6.83e-05, Loss=4.32e-03 BER=1.73e-03 FER=1.55e-02
2025-10-15 20:15:06,046 | INFO | Epoch 384 Train Time 43.784326791763306s

2025-10-15 20:15:49,311 | INFO | Training epoch 385, Batch 1000/1000: LR=6.81e-05, Loss=4.31e-03 BER=1.78e-03 FER=1.57e-02
2025-10-15 20:15:49,358 | INFO | Epoch 385 Train Time 43.31140995025635s

2025-10-15 20:16:32,399 | INFO | Training epoch 386, Batch 1000/1000: LR=6.80e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.58e-02
2025-10-15 20:16:32,468 | INFO | Epoch 386 Train Time 43.108346462249756s

2025-10-15 20:17:16,805 | INFO | Training epoch 387, Batch 1000/1000: LR=6.79e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.52e-02
2025-10-15 20:17:16,849 | INFO | Epoch 387 Train Time 44.378050804138184s

2025-10-15 20:17:59,999 | INFO | Training epoch 388, Batch 1000/1000: LR=6.77e-05, Loss=4.41e-03 BER=1.79e-03 FER=1.57e-02
2025-10-15 20:18:00,047 | INFO | Epoch 388 Train Time 43.19638442993164s

2025-10-15 20:18:43,105 | INFO | Training epoch 389, Batch 1000/1000: LR=6.76e-05, Loss=4.09e-03 BER=1.69e-03 FER=1.48e-02
2025-10-15 20:18:43,164 | INFO | Epoch 389 Train Time 43.11640930175781s

2025-10-15 20:19:25,828 | INFO | Training epoch 390, Batch 1000/1000: LR=6.74e-05, Loss=4.47e-03 BER=1.82e-03 FER=1.59e-02
2025-10-15 20:19:25,891 | INFO | Epoch 390 Train Time 42.724409341812134s

2025-10-15 20:20:09,502 | INFO | Training epoch 391, Batch 1000/1000: LR=6.73e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.49e-02
2025-10-15 20:20:09,544 | INFO | Epoch 391 Train Time 43.65153479576111s

2025-10-15 20:20:09,545 | INFO | [P1] saving best_model with loss 0.003998 at epoch 391
2025-10-15 20:20:52,627 | INFO | Training epoch 392, Batch 1000/1000: LR=6.71e-05, Loss=4.32e-03 BER=1.80e-03 FER=1.55e-02
2025-10-15 20:20:52,695 | INFO | Epoch 392 Train Time 43.05428862571716s

2025-10-15 20:21:36,298 | INFO | Training epoch 393, Batch 1000/1000: LR=6.70e-05, Loss=4.33e-03 BER=1.76e-03 FER=1.55e-02
2025-10-15 20:21:36,356 | INFO | Epoch 393 Train Time 43.65962362289429s

2025-10-15 20:22:19,407 | INFO | Training epoch 394, Batch 1000/1000: LR=6.68e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.53e-02
2025-10-15 20:22:19,461 | INFO | Epoch 394 Train Time 43.10315179824829s

2025-10-15 20:23:01,904 | INFO | Training epoch 395, Batch 1000/1000: LR=6.67e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.48e-02
2025-10-15 20:23:01,948 | INFO | Epoch 395 Train Time 42.485801696777344s

2025-10-15 20:23:44,814 | INFO | Training epoch 396, Batch 1000/1000: LR=6.65e-05, Loss=4.18e-03 BER=1.74e-03 FER=1.52e-02
2025-10-15 20:23:44,862 | INFO | Epoch 396 Train Time 42.912155628204346s

2025-10-15 20:24:27,904 | INFO | Training epoch 397, Batch 1000/1000: LR=6.64e-05, Loss=4.49e-03 BER=1.84e-03 FER=1.60e-02
2025-10-15 20:24:27,949 | INFO | Epoch 397 Train Time 43.08554720878601s

2025-10-15 20:25:10,906 | INFO | Training epoch 398, Batch 1000/1000: LR=6.62e-05, Loss=4.12e-03 BER=1.66e-03 FER=1.50e-02
2025-10-15 20:25:10,957 | INFO | Epoch 398 Train Time 43.00620913505554s

2025-10-15 20:25:54,443 | INFO | Training epoch 399, Batch 1000/1000: LR=6.61e-05, Loss=4.12e-03 BER=1.68e-03 FER=1.46e-02
2025-10-15 20:25:54,495 | INFO | Epoch 399 Train Time 43.536212682724s

2025-10-15 20:26:37,713 | INFO | Training epoch 400, Batch 1000/1000: LR=6.59e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 20:26:37,769 | INFO | Epoch 400 Train Time 43.27358675003052s

2025-10-15 20:27:21,195 | INFO | Training epoch 401, Batch 1000/1000: LR=6.58e-05, Loss=4.31e-03 BER=1.78e-03 FER=1.56e-02
2025-10-15 20:27:21,237 | INFO | Epoch 401 Train Time 43.46607065200806s

2025-10-15 20:28:05,209 | INFO | Training epoch 402, Batch 1000/1000: LR=6.56e-05, Loss=4.35e-03 BER=1.76e-03 FER=1.55e-02
2025-10-15 20:28:05,259 | INFO | Epoch 402 Train Time 44.02005982398987s

2025-10-15 20:28:48,314 | INFO | Training epoch 403, Batch 1000/1000: LR=6.55e-05, Loss=4.19e-03 BER=1.73e-03 FER=1.53e-02
2025-10-15 20:28:48,353 | INFO | Epoch 403 Train Time 43.09221291542053s

2025-10-15 20:29:31,398 | INFO | Training epoch 404, Batch 1000/1000: LR=6.54e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.50e-02
2025-10-15 20:29:31,436 | INFO | Epoch 404 Train Time 43.08068871498108s

2025-10-15 20:30:15,028 | INFO | Training epoch 405, Batch 1000/1000: LR=6.52e-05, Loss=4.18e-03 BER=1.70e-03 FER=1.48e-02
2025-10-15 20:30:15,071 | INFO | Epoch 405 Train Time 43.63244652748108s

2025-10-15 20:30:59,499 | INFO | Training epoch 406, Batch 1000/1000: LR=6.51e-05, Loss=4.52e-03 BER=1.83e-03 FER=1.61e-02
2025-10-15 20:30:59,546 | INFO | Epoch 406 Train Time 44.473331451416016s

2025-10-15 20:31:43,417 | INFO | Training epoch 407, Batch 1000/1000: LR=6.49e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.49e-02
2025-10-15 20:31:43,466 | INFO | Epoch 407 Train Time 43.919628381729126s

2025-10-15 20:32:26,701 | INFO | Training epoch 408, Batch 1000/1000: LR=6.48e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.49e-02
2025-10-15 20:32:26,770 | INFO | Epoch 408 Train Time 43.30217671394348s

2025-10-15 20:33:09,606 | INFO | Training epoch 409, Batch 1000/1000: LR=6.46e-05, Loss=4.02e-03 BER=1.64e-03 FER=1.47e-02
2025-10-15 20:33:09,657 | INFO | Epoch 409 Train Time 42.88612985610962s

2025-10-15 20:33:53,900 | INFO | Training epoch 410, Batch 1000/1000: LR=6.45e-05, Loss=4.23e-03 BER=1.72e-03 FER=1.53e-02
2025-10-15 20:33:53,945 | INFO | Epoch 410 Train Time 44.28545260429382s

2025-10-15 20:34:38,822 | INFO | Training epoch 411, Batch 1000/1000: LR=6.43e-05, Loss=4.22e-03 BER=1.74e-03 FER=1.52e-02
2025-10-15 20:34:38,867 | INFO | Epoch 411 Train Time 44.92064166069031s

2025-10-15 20:35:21,244 | INFO | Training epoch 412, Batch 1000/1000: LR=6.42e-05, Loss=4.36e-03 BER=1.77e-03 FER=1.58e-02
2025-10-15 20:35:21,297 | INFO | Epoch 412 Train Time 42.428861141204834s

2025-10-15 20:36:04,499 | INFO | Training epoch 413, Batch 1000/1000: LR=6.40e-05, Loss=4.19e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 20:36:04,545 | INFO | Epoch 413 Train Time 43.24578261375427s

2025-10-15 20:36:48,247 | INFO | Training epoch 414, Batch 1000/1000: LR=6.39e-05, Loss=4.30e-03 BER=1.74e-03 FER=1.52e-02
2025-10-15 20:36:48,307 | INFO | Epoch 414 Train Time 43.76083993911743s

2025-10-15 20:37:31,532 | INFO | Training epoch 415, Batch 1000/1000: LR=6.37e-05, Loss=4.28e-03 BER=1.76e-03 FER=1.53e-02
2025-10-15 20:37:31,583 | INFO | Epoch 415 Train Time 43.27290177345276s

2025-10-15 20:38:14,513 | INFO | Training epoch 416, Batch 1000/1000: LR=6.36e-05, Loss=4.21e-03 BER=1.73e-03 FER=1.48e-02
2025-10-15 20:38:14,550 | INFO | Epoch 416 Train Time 42.96650242805481s

2025-10-15 20:38:58,105 | INFO | Training epoch 417, Batch 1000/1000: LR=6.34e-05, Loss=4.26e-03 BER=1.73e-03 FER=1.53e-02
2025-10-15 20:38:58,151 | INFO | Epoch 417 Train Time 43.599963426589966s

2025-10-15 20:39:40,713 | INFO | Training epoch 418, Batch 1000/1000: LR=6.33e-05, Loss=4.24e-03 BER=1.78e-03 FER=1.55e-02
2025-10-15 20:39:40,766 | INFO | Epoch 418 Train Time 42.613213777542114s

2025-10-15 20:40:24,528 | INFO | Training epoch 419, Batch 1000/1000: LR=6.31e-05, Loss=4.21e-03 BER=1.73e-03 FER=1.52e-02
2025-10-15 20:40:24,571 | INFO | Epoch 419 Train Time 43.803330421447754s

2025-10-15 20:41:07,743 | INFO | Training epoch 420, Batch 1000/1000: LR=6.30e-05, Loss=4.03e-03 BER=1.64e-03 FER=1.45e-02
2025-10-15 20:41:07,804 | INFO | Epoch 420 Train Time 43.230950593948364s

2025-10-15 20:41:51,458 | INFO | Training epoch 421, Batch 1000/1000: LR=6.28e-05, Loss=4.23e-03 BER=1.75e-03 FER=1.53e-02
2025-10-15 20:41:51,518 | INFO | Epoch 421 Train Time 43.71259903907776s

2025-10-15 20:42:35,814 | INFO | Training epoch 422, Batch 1000/1000: LR=6.27e-05, Loss=4.22e-03 BER=1.72e-03 FER=1.51e-02
2025-10-15 20:42:35,871 | INFO | Epoch 422 Train Time 44.350239515304565s

2025-10-15 20:43:18,808 | INFO | Training epoch 423, Batch 1000/1000: LR=6.25e-05, Loss=4.45e-03 BER=1.78e-03 FER=1.54e-02
2025-10-15 20:43:18,845 | INFO | Epoch 423 Train Time 42.97301244735718s

2025-10-15 20:44:02,738 | INFO | Training epoch 424, Batch 1000/1000: LR=6.24e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 20:44:02,785 | INFO | Epoch 424 Train Time 43.93834114074707s

2025-10-15 20:44:45,312 | INFO | Training epoch 425, Batch 1000/1000: LR=6.22e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.52e-02
2025-10-15 20:44:45,356 | INFO | Epoch 425 Train Time 42.56860828399658s

2025-10-15 20:45:28,402 | INFO | Training epoch 426, Batch 1000/1000: LR=6.21e-05, Loss=4.12e-03 BER=1.70e-03 FER=1.50e-02
2025-10-15 20:45:28,452 | INFO | Epoch 426 Train Time 43.09548807144165s

2025-10-15 20:46:12,336 | INFO | Training epoch 427, Batch 1000/1000: LR=6.19e-05, Loss=4.15e-03 BER=1.68e-03 FER=1.47e-02
2025-10-15 20:46:12,393 | INFO | Epoch 427 Train Time 43.939244508743286s

2025-10-15 20:46:56,142 | INFO | Training epoch 428, Batch 1000/1000: LR=6.18e-05, Loss=4.01e-03 BER=1.65e-03 FER=1.43e-02
2025-10-15 20:46:56,182 | INFO | Epoch 428 Train Time 43.78657364845276s

2025-10-15 20:47:38,728 | INFO | Training epoch 429, Batch 1000/1000: LR=6.16e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.50e-02
2025-10-15 20:47:38,784 | INFO | Epoch 429 Train Time 42.60022234916687s

2025-10-15 20:48:22,095 | INFO | Training epoch 430, Batch 1000/1000: LR=6.14e-05, Loss=4.19e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 20:48:22,142 | INFO | Epoch 430 Train Time 43.355475187301636s

2025-10-15 20:49:05,473 | INFO | Training epoch 431, Batch 1000/1000: LR=6.13e-05, Loss=4.17e-03 BER=1.72e-03 FER=1.48e-02
2025-10-15 20:49:05,519 | INFO | Epoch 431 Train Time 43.37498950958252s

2025-10-15 20:49:49,813 | INFO | Training epoch 432, Batch 1000/1000: LR=6.11e-05, Loss=3.99e-03 BER=1.67e-03 FER=1.44e-02
2025-10-15 20:49:49,865 | INFO | Epoch 432 Train Time 44.34503650665283s

2025-10-15 20:49:49,866 | INFO | [P1] saving best_model with loss 0.003994 at epoch 432
2025-10-15 20:50:34,609 | INFO | Training epoch 433, Batch 1000/1000: LR=6.10e-05, Loss=4.23e-03 BER=1.73e-03 FER=1.51e-02
2025-10-15 20:50:34,666 | INFO | Epoch 433 Train Time 44.71156191825867s

2025-10-15 20:51:18,629 | INFO | Training epoch 434, Batch 1000/1000: LR=6.08e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.49e-02
2025-10-15 20:51:18,682 | INFO | Epoch 434 Train Time 44.015031814575195s

2025-10-15 20:52:01,612 | INFO | Training epoch 435, Batch 1000/1000: LR=6.07e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.45e-02
2025-10-15 20:52:01,660 | INFO | Epoch 435 Train Time 42.976052045822144s

2025-10-15 20:52:44,728 | INFO | Training epoch 436, Batch 1000/1000: LR=6.05e-05, Loss=4.16e-03 BER=1.73e-03 FER=1.52e-02
2025-10-15 20:52:44,762 | INFO | Epoch 436 Train Time 43.09998393058777s

2025-10-15 20:53:28,335 | INFO | Training epoch 437, Batch 1000/1000: LR=6.04e-05, Loss=4.09e-03 BER=1.70e-03 FER=1.50e-02
2025-10-15 20:53:28,409 | INFO | Epoch 437 Train Time 43.644978046417236s

2025-10-15 20:54:11,902 | INFO | Training epoch 438, Batch 1000/1000: LR=6.02e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.51e-02
2025-10-15 20:54:11,951 | INFO | Epoch 438 Train Time 43.540430784225464s

2025-10-15 20:54:55,807 | INFO | Training epoch 439, Batch 1000/1000: LR=6.01e-05, Loss=4.31e-03 BER=1.78e-03 FER=1.56e-02
2025-10-15 20:54:55,862 | INFO | Epoch 439 Train Time 43.907461643218994s

2025-10-15 20:55:40,104 | INFO | Training epoch 440, Batch 1000/1000: LR=5.99e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.44e-02
2025-10-15 20:55:40,170 | INFO | Epoch 440 Train Time 44.30680799484253s

2025-10-15 20:55:40,171 | INFO | [P1] saving best_model with loss 0.003950 at epoch 440
2025-10-15 20:56:22,106 | INFO | Training epoch 441, Batch 1000/1000: LR=5.98e-05, Loss=4.24e-03 BER=1.74e-03 FER=1.53e-02
2025-10-15 20:56:22,149 | INFO | Epoch 441 Train Time 41.887001037597656s

2025-10-15 20:57:05,537 | INFO | Training epoch 442, Batch 1000/1000: LR=5.96e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.44e-02
2025-10-15 20:57:05,595 | INFO | Epoch 442 Train Time 43.445175647735596s

2025-10-15 20:57:48,916 | INFO | Training epoch 443, Batch 1000/1000: LR=5.95e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.48e-02
2025-10-15 20:57:48,960 | INFO | Epoch 443 Train Time 43.363346338272095s

2025-10-15 20:58:32,103 | INFO | Training epoch 444, Batch 1000/1000: LR=5.93e-05, Loss=4.20e-03 BER=1.74e-03 FER=1.55e-02
2025-10-15 20:58:32,151 | INFO | Epoch 444 Train Time 43.1895112991333s

2025-10-15 20:59:16,205 | INFO | Training epoch 445, Batch 1000/1000: LR=5.92e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.47e-02
2025-10-15 20:59:16,248 | INFO | Epoch 445 Train Time 44.09545588493347s

2025-10-15 20:59:59,602 | INFO | Training epoch 446, Batch 1000/1000: LR=5.90e-05, Loss=4.04e-03 BER=1.68e-03 FER=1.47e-02
2025-10-15 20:59:59,658 | INFO | Epoch 446 Train Time 43.408282995224s

2025-10-15 21:00:42,020 | INFO | Training epoch 447, Batch 1000/1000: LR=5.89e-05, Loss=4.22e-03 BER=1.71e-03 FER=1.50e-02
2025-10-15 21:00:42,061 | INFO | Epoch 447 Train Time 42.40204215049744s

2025-10-15 21:01:25,936 | INFO | Training epoch 448, Batch 1000/1000: LR=5.87e-05, Loss=4.23e-03 BER=1.73e-03 FER=1.51e-02
2025-10-15 21:01:25,989 | INFO | Epoch 448 Train Time 43.926661014556885s

2025-10-15 21:02:10,109 | INFO | Training epoch 449, Batch 1000/1000: LR=5.86e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.43e-02
2025-10-15 21:02:10,153 | INFO | Epoch 449 Train Time 44.1622200012207s

2025-10-15 21:02:52,806 | INFO | Training epoch 450, Batch 1000/1000: LR=5.84e-05, Loss=4.20e-03 BER=1.73e-03 FER=1.50e-02
2025-10-15 21:02:52,851 | INFO | Epoch 450 Train Time 42.69746017456055s

2025-10-15 21:03:36,304 | INFO | Training epoch 451, Batch 1000/1000: LR=5.82e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.43e-02
2025-10-15 21:03:36,356 | INFO | Epoch 451 Train Time 43.50314784049988s

2025-10-15 21:03:36,357 | INFO | [P1] saving best_model with loss 0.003889 at epoch 451
2025-10-15 21:04:19,700 | INFO | Training epoch 452, Batch 1000/1000: LR=5.81e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.55e-02
2025-10-15 21:04:19,749 | INFO | Epoch 452 Train Time 43.29466152191162s

2025-10-15 21:05:02,897 | INFO | Training epoch 453, Batch 1000/1000: LR=5.79e-05, Loss=4.06e-03 BER=1.64e-03 FER=1.45e-02
2025-10-15 21:05:02,938 | INFO | Epoch 453 Train Time 43.18744945526123s

2025-10-15 21:05:45,443 | INFO | Training epoch 454, Batch 1000/1000: LR=5.78e-05, Loss=4.25e-03 BER=1.78e-03 FER=1.53e-02
2025-10-15 21:05:45,506 | INFO | Epoch 454 Train Time 42.567115783691406s

2025-10-15 21:06:29,795 | INFO | Training epoch 455, Batch 1000/1000: LR=5.76e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.45e-02
2025-10-15 21:06:29,848 | INFO | Epoch 455 Train Time 44.33995199203491s

2025-10-15 21:07:12,633 | INFO | Training epoch 456, Batch 1000/1000: LR=5.75e-05, Loss=4.17e-03 BER=1.68e-03 FER=1.48e-02
2025-10-15 21:07:12,699 | INFO | Epoch 456 Train Time 42.848851442337036s

2025-10-15 21:07:58,028 | INFO | Training epoch 457, Batch 1000/1000: LR=5.73e-05, Loss=3.98e-03 BER=1.61e-03 FER=1.42e-02
2025-10-15 21:07:58,103 | INFO | Epoch 457 Train Time 45.40140104293823s

2025-10-15 21:08:42,103 | INFO | Training epoch 458, Batch 1000/1000: LR=5.72e-05, Loss=4.18e-03 BER=1.71e-03 FER=1.47e-02
2025-10-15 21:08:42,148 | INFO | Epoch 458 Train Time 44.04342794418335s

2025-10-15 21:09:25,699 | INFO | Training epoch 459, Batch 1000/1000: LR=5.70e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.46e-02
2025-10-15 21:09:25,737 | INFO | Epoch 459 Train Time 43.588059425354004s

2025-10-15 21:10:08,647 | INFO | Training epoch 460, Batch 1000/1000: LR=5.69e-05, Loss=4.13e-03 BER=1.67e-03 FER=1.47e-02
2025-10-15 21:10:08,710 | INFO | Epoch 460 Train Time 42.97164535522461s

2025-10-15 21:10:53,006 | INFO | Training epoch 461, Batch 1000/1000: LR=5.67e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.49e-02
2025-10-15 21:10:53,055 | INFO | Epoch 461 Train Time 44.34136939048767s

2025-10-15 21:11:36,241 | INFO | Training epoch 462, Batch 1000/1000: LR=5.65e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.51e-02
2025-10-15 21:11:36,324 | INFO | Epoch 462 Train Time 43.26840162277222s

2025-10-15 21:12:19,905 | INFO | Training epoch 463, Batch 1000/1000: LR=5.64e-05, Loss=4.08e-03 BER=1.66e-03 FER=1.48e-02
2025-10-15 21:12:19,952 | INFO | Epoch 463 Train Time 43.62640118598938s

2025-10-15 21:13:04,609 | INFO | Training epoch 464, Batch 1000/1000: LR=5.62e-05, Loss=3.99e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 21:13:04,650 | INFO | Epoch 464 Train Time 44.69620323181152s

2025-10-15 21:13:47,542 | INFO | Training epoch 465, Batch 1000/1000: LR=5.61e-05, Loss=3.89e-03 BER=1.61e-03 FER=1.42e-02
2025-10-15 21:13:47,596 | INFO | Epoch 465 Train Time 42.94505071640015s

2025-10-15 21:14:31,639 | INFO | Training epoch 466, Batch 1000/1000: LR=5.59e-05, Loss=4.09e-03 BER=1.67e-03 FER=1.46e-02
2025-10-15 21:14:31,691 | INFO | Epoch 466 Train Time 44.0928418636322s

2025-10-15 21:15:15,632 | INFO | Training epoch 467, Batch 1000/1000: LR=5.58e-05, Loss=4.20e-03 BER=1.74e-03 FER=1.51e-02
2025-10-15 21:15:15,693 | INFO | Epoch 467 Train Time 44.00102210044861s

2025-10-15 21:15:59,028 | INFO | Training epoch 468, Batch 1000/1000: LR=5.56e-05, Loss=4.21e-03 BER=1.76e-03 FER=1.54e-02
2025-10-15 21:15:59,094 | INFO | Epoch 468 Train Time 43.3992555141449s

2025-10-15 21:16:41,803 | INFO | Training epoch 469, Batch 1000/1000: LR=5.55e-05, Loss=3.94e-03 BER=1.59e-03 FER=1.42e-02
2025-10-15 21:16:41,847 | INFO | Epoch 469 Train Time 42.75150156021118s

2025-10-15 21:17:24,582 | INFO | Training epoch 470, Batch 1000/1000: LR=5.53e-05, Loss=3.96e-03 BER=1.65e-03 FER=1.40e-02
2025-10-15 21:17:24,627 | INFO | Epoch 470 Train Time 42.77898645401001s

2025-10-15 21:18:08,105 | INFO | Training epoch 471, Batch 1000/1000: LR=5.52e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.48e-02
2025-10-15 21:18:08,166 | INFO | Epoch 471 Train Time 43.53653001785278s

2025-10-15 21:18:50,756 | INFO | Training epoch 472, Batch 1000/1000: LR=5.50e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.51e-02
2025-10-15 21:18:50,834 | INFO | Epoch 472 Train Time 42.66659903526306s

2025-10-15 21:19:34,906 | INFO | Training epoch 473, Batch 1000/1000: LR=5.48e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.45e-02
2025-10-15 21:19:34,966 | INFO | Epoch 473 Train Time 44.13057041168213s

2025-10-15 21:20:17,716 | INFO | Training epoch 474, Batch 1000/1000: LR=5.47e-05, Loss=3.98e-03 BER=1.65e-03 FER=1.47e-02
2025-10-15 21:20:17,765 | INFO | Epoch 474 Train Time 42.79751920700073s

2025-10-15 21:21:01,198 | INFO | Training epoch 475, Batch 1000/1000: LR=5.45e-05, Loss=4.08e-03 BER=1.67e-03 FER=1.48e-02
2025-10-15 21:21:01,246 | INFO | Epoch 475 Train Time 43.478792667388916s

2025-10-15 21:21:43,667 | INFO | Training epoch 476, Batch 1000/1000: LR=5.44e-05, Loss=4.03e-03 BER=1.66e-03 FER=1.45e-02
2025-10-15 21:21:43,728 | INFO | Epoch 476 Train Time 42.478631258010864s

2025-10-15 21:22:26,687 | INFO | Training epoch 477, Batch 1000/1000: LR=5.42e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.51e-02
2025-10-15 21:22:26,738 | INFO | Epoch 477 Train Time 43.00827622413635s

2025-10-15 21:23:10,021 | INFO | Training epoch 478, Batch 1000/1000: LR=5.41e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.45e-02
2025-10-15 21:23:10,076 | INFO | Epoch 478 Train Time 43.337191104888916s

2025-10-15 21:23:53,044 | INFO | Training epoch 479, Batch 1000/1000: LR=5.39e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.44e-02
2025-10-15 21:23:53,104 | INFO | Epoch 479 Train Time 43.0259165763855s

2025-10-15 21:24:37,827 | INFO | Training epoch 480, Batch 1000/1000: LR=5.38e-05, Loss=3.90e-03 BER=1.62e-03 FER=1.40e-02
2025-10-15 21:24:37,875 | INFO | Epoch 480 Train Time 44.768943071365356s

2025-10-15 21:25:22,612 | INFO | Training epoch 481, Batch 1000/1000: LR=5.36e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.45e-02
2025-10-15 21:25:22,679 | INFO | Epoch 481 Train Time 44.80293297767639s

2025-10-15 21:26:06,054 | INFO | Training epoch 482, Batch 1000/1000: LR=5.35e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.44e-02
2025-10-15 21:26:06,108 | INFO | Epoch 482 Train Time 43.42700815200806s

2025-10-15 21:26:48,434 | INFO | Training epoch 483, Batch 1000/1000: LR=5.33e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.47e-02
2025-10-15 21:26:48,496 | INFO | Epoch 483 Train Time 42.3869903087616s

2025-10-15 21:27:32,716 | INFO | Training epoch 484, Batch 1000/1000: LR=5.31e-05, Loss=3.98e-03 BER=1.62e-03 FER=1.41e-02
2025-10-15 21:27:32,763 | INFO | Epoch 484 Train Time 44.264073610305786s

2025-10-15 21:28:17,168 | INFO | Training epoch 485, Batch 1000/1000: LR=5.30e-05, Loss=3.98e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 21:28:17,228 | INFO | Epoch 485 Train Time 44.46298408508301s

2025-10-15 21:29:02,402 | INFO | Training epoch 486, Batch 1000/1000: LR=5.28e-05, Loss=3.94e-03 BER=1.66e-03 FER=1.44e-02
2025-10-15 21:29:02,446 | INFO | Epoch 486 Train Time 45.21639347076416s

2025-10-15 21:29:45,801 | INFO | Training epoch 487, Batch 1000/1000: LR=5.27e-05, Loss=4.19e-03 BER=1.73e-03 FER=1.50e-02
2025-10-15 21:29:45,861 | INFO | Epoch 487 Train Time 43.413005113601685s

2025-10-15 21:30:30,105 | INFO | Training epoch 488, Batch 1000/1000: LR=5.25e-05, Loss=3.94e-03 BER=1.60e-03 FER=1.42e-02
2025-10-15 21:30:30,156 | INFO | Epoch 488 Train Time 44.293033838272095s

2025-10-15 21:31:14,034 | INFO | Training epoch 489, Batch 1000/1000: LR=5.24e-05, Loss=4.08e-03 BER=1.69e-03 FER=1.44e-02
2025-10-15 21:31:14,079 | INFO | Epoch 489 Train Time 43.921825647354126s

2025-10-15 21:31:57,819 | INFO | Training epoch 490, Batch 1000/1000: LR=5.22e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.46e-02
2025-10-15 21:31:57,868 | INFO | Epoch 490 Train Time 43.787413597106934s

2025-10-15 21:32:41,400 | INFO | Training epoch 491, Batch 1000/1000: LR=5.21e-05, Loss=4.13e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 21:32:41,446 | INFO | Epoch 491 Train Time 43.57544660568237s

2025-10-15 21:33:25,303 | INFO | Training epoch 492, Batch 1000/1000: LR=5.19e-05, Loss=4.10e-03 BER=1.68e-03 FER=1.46e-02
2025-10-15 21:33:25,348 | INFO | Epoch 492 Train Time 43.90087580680847s

2025-10-15 21:34:09,005 | INFO | Training epoch 493, Batch 1000/1000: LR=5.17e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.48e-02
2025-10-15 21:34:09,041 | INFO | Epoch 493 Train Time 43.691659450531006s

2025-10-15 21:34:52,901 | INFO | Training epoch 494, Batch 1000/1000: LR=5.16e-05, Loss=3.79e-03 BER=1.56e-03 FER=1.36e-02
2025-10-15 21:34:52,949 | INFO | Epoch 494 Train Time 43.906672954559326s

2025-10-15 21:34:52,950 | INFO | [P1] saving best_model with loss 0.003792 at epoch 494
2025-10-15 21:35:36,238 | INFO | Training epoch 495, Batch 1000/1000: LR=5.14e-05, Loss=4.10e-03 BER=1.68e-03 FER=1.49e-02
2025-10-15 21:35:36,294 | INFO | Epoch 495 Train Time 43.25018334388733s

2025-10-15 21:36:20,161 | INFO | Training epoch 496, Batch 1000/1000: LR=5.13e-05, Loss=4.00e-03 BER=1.63e-03 FER=1.44e-02
2025-10-15 21:36:20,219 | INFO | Epoch 496 Train Time 43.92168068885803s

2025-10-15 21:37:03,641 | INFO | Training epoch 497, Batch 1000/1000: LR=5.11e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.46e-02
2025-10-15 21:37:03,701 | INFO | Epoch 497 Train Time 43.47753381729126s

2025-10-15 21:37:47,225 | INFO | Training epoch 498, Batch 1000/1000: LR=5.10e-05, Loss=4.24e-03 BER=1.76e-03 FER=1.54e-02
2025-10-15 21:37:47,272 | INFO | Epoch 498 Train Time 43.56921601295471s

2025-10-15 21:38:31,804 | INFO | Training epoch 499, Batch 1000/1000: LR=5.08e-05, Loss=4.06e-03 BER=1.67e-03 FER=1.45e-02
2025-10-15 21:38:31,849 | INFO | Epoch 499 Train Time 44.5757372379303s

2025-10-15 21:39:15,806 | INFO | Training epoch 500, Batch 1000/1000: LR=5.07e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.47e-02
2025-10-15 21:39:15,863 | INFO | Epoch 500 Train Time 44.01305389404297s

2025-10-15 21:40:00,000 | INFO | Training epoch 501, Batch 1000/1000: LR=5.05e-05, Loss=3.94e-03 BER=1.61e-03 FER=1.41e-02
2025-10-15 21:40:00,065 | INFO | Epoch 501 Train Time 44.199252128601074s

2025-10-15 21:40:44,403 | INFO | Training epoch 502, Batch 1000/1000: LR=5.03e-05, Loss=4.18e-03 BER=1.72e-03 FER=1.50e-02
2025-10-15 21:40:44,455 | INFO | Epoch 502 Train Time 44.38801050186157s

2025-10-15 21:41:28,141 | INFO | Training epoch 503, Batch 1000/1000: LR=5.02e-05, Loss=3.92e-03 BER=1.62e-03 FER=1.42e-02
2025-10-15 21:41:28,198 | INFO | Epoch 503 Train Time 43.74090909957886s

2025-10-15 21:42:11,303 | INFO | Training epoch 504, Batch 1000/1000: LR=5.00e-05, Loss=4.13e-03 BER=1.69e-03 FER=1.47e-02
2025-10-15 21:42:11,367 | INFO | Epoch 504 Train Time 43.168123960494995s

2025-10-15 21:42:54,921 | INFO | Training epoch 505, Batch 1000/1000: LR=4.99e-05, Loss=3.99e-03 BER=1.65e-03 FER=1.45e-02
2025-10-15 21:42:54,975 | INFO | Epoch 505 Train Time 43.60534930229187s

2025-10-15 21:43:38,706 | INFO | Training epoch 506, Batch 1000/1000: LR=4.97e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.44e-02
2025-10-15 21:43:38,743 | INFO | Epoch 506 Train Time 43.767053842544556s

2025-10-15 21:44:22,612 | INFO | Training epoch 507, Batch 1000/1000: LR=4.96e-05, Loss=4.20e-03 BER=1.73e-03 FER=1.51e-02
2025-10-15 21:44:22,665 | INFO | Epoch 507 Train Time 43.920191049575806s

2025-10-15 21:45:05,543 | INFO | Training epoch 508, Batch 1000/1000: LR=4.94e-05, Loss=4.23e-03 BER=1.74e-03 FER=1.50e-02
2025-10-15 21:45:05,608 | INFO | Epoch 508 Train Time 42.94165778160095s

2025-10-15 21:45:49,355 | INFO | Training epoch 509, Batch 1000/1000: LR=4.93e-05, Loss=3.92e-03 BER=1.64e-03 FER=1.42e-02
2025-10-15 21:45:49,438 | INFO | Epoch 509 Train Time 43.828428506851196s

2025-10-15 21:46:33,437 | INFO | Training epoch 510, Batch 1000/1000: LR=4.91e-05, Loss=4.17e-03 BER=1.70e-03 FER=1.49e-02
2025-10-15 21:46:33,498 | INFO | Epoch 510 Train Time 44.05820107460022s

2025-10-15 21:47:15,979 | INFO | Training epoch 511, Batch 1000/1000: LR=4.89e-05, Loss=3.93e-03 BER=1.59e-03 FER=1.39e-02
2025-10-15 21:47:16,042 | INFO | Epoch 511 Train Time 42.543171405792236s

2025-10-15 21:47:58,926 | INFO | Training epoch 512, Batch 1000/1000: LR=4.88e-05, Loss=4.04e-03 BER=1.64e-03 FER=1.44e-02
2025-10-15 21:47:58,970 | INFO | Epoch 512 Train Time 42.92651319503784s

2025-10-15 21:48:42,611 | INFO | Training epoch 513, Batch 1000/1000: LR=4.86e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.42e-02
2025-10-15 21:48:42,662 | INFO | Epoch 513 Train Time 43.690523862838745s

2025-10-15 21:49:26,244 | INFO | Training epoch 514, Batch 1000/1000: LR=4.85e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.41e-02
2025-10-15 21:49:26,296 | INFO | Epoch 514 Train Time 43.63148307800293s

2025-10-15 21:50:09,794 | INFO | Training epoch 515, Batch 1000/1000: LR=4.83e-05, Loss=3.99e-03 BER=1.64e-03 FER=1.40e-02
2025-10-15 21:50:09,853 | INFO | Epoch 515 Train Time 43.55530762672424s

2025-10-15 21:50:53,503 | INFO | Training epoch 516, Batch 1000/1000: LR=4.82e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.47e-02
2025-10-15 21:50:53,564 | INFO | Epoch 516 Train Time 43.70837068557739s

2025-10-15 21:51:37,669 | INFO | Training epoch 517, Batch 1000/1000: LR=4.80e-05, Loss=4.03e-03 BER=1.66e-03 FER=1.45e-02
2025-10-15 21:51:37,706 | INFO | Epoch 517 Train Time 44.14094281196594s

2025-10-15 21:52:20,518 | INFO | Training epoch 518, Batch 1000/1000: LR=4.79e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 21:52:20,563 | INFO | Epoch 518 Train Time 42.85562229156494s

2025-10-15 21:53:04,110 | INFO | Training epoch 519, Batch 1000/1000: LR=4.77e-05, Loss=4.00e-03 BER=1.67e-03 FER=1.44e-02
2025-10-15 21:53:04,161 | INFO | Epoch 519 Train Time 43.596184492111206s

2025-10-15 21:53:47,410 | INFO | Training epoch 520, Batch 1000/1000: LR=4.75e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 21:53:47,456 | INFO | Epoch 520 Train Time 43.294037103652954s

2025-10-15 21:54:31,536 | INFO | Training epoch 521, Batch 1000/1000: LR=4.74e-05, Loss=3.85e-03 BER=1.61e-03 FER=1.37e-02
2025-10-15 21:54:31,591 | INFO | Epoch 521 Train Time 44.133073806762695s

2025-10-15 21:55:14,407 | INFO | Training epoch 522, Batch 1000/1000: LR=4.72e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.46e-02
2025-10-15 21:55:14,459 | INFO | Epoch 522 Train Time 42.867045879364014s

2025-10-15 21:55:57,622 | INFO | Training epoch 523, Batch 1000/1000: LR=4.71e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.41e-02
2025-10-15 21:55:57,673 | INFO | Epoch 523 Train Time 43.211726903915405s

2025-10-15 21:56:40,525 | INFO | Training epoch 524, Batch 1000/1000: LR=4.69e-05, Loss=4.07e-03 BER=1.69e-03 FER=1.47e-02
2025-10-15 21:56:40,587 | INFO | Epoch 524 Train Time 42.91281270980835s

2025-10-15 21:57:24,802 | INFO | Training epoch 525, Batch 1000/1000: LR=4.68e-05, Loss=4.15e-03 BER=1.72e-03 FER=1.48e-02
2025-10-15 21:57:24,854 | INFO | Epoch 525 Train Time 44.265724658966064s

2025-10-15 21:58:08,613 | INFO | Training epoch 526, Batch 1000/1000: LR=4.66e-05, Loss=3.99e-03 BER=1.65e-03 FER=1.43e-02
2025-10-15 21:58:08,671 | INFO | Epoch 526 Train Time 43.815678119659424s

2025-10-15 21:58:51,498 | INFO | Training epoch 527, Batch 1000/1000: LR=4.65e-05, Loss=3.99e-03 BER=1.63e-03 FER=1.42e-02
2025-10-15 21:58:51,544 | INFO | Epoch 527 Train Time 42.87195944786072s

2025-10-15 21:59:35,306 | INFO | Training epoch 528, Batch 1000/1000: LR=4.63e-05, Loss=4.05e-03 BER=1.65e-03 FER=1.45e-02
2025-10-15 21:59:35,357 | INFO | Epoch 528 Train Time 43.811375856399536s

2025-10-15 22:00:19,821 | INFO | Training epoch 529, Batch 1000/1000: LR=4.62e-05, Loss=4.03e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 22:00:19,858 | INFO | Epoch 529 Train Time 44.49887275695801s

2025-10-15 22:01:03,117 | INFO | Training epoch 530, Batch 1000/1000: LR=4.60e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 22:01:03,160 | INFO | Epoch 530 Train Time 43.30114817619324s

2025-10-15 22:01:45,946 | INFO | Training epoch 531, Batch 1000/1000: LR=4.58e-05, Loss=4.12e-03 BER=1.70e-03 FER=1.48e-02
2025-10-15 22:01:46,009 | INFO | Epoch 531 Train Time 42.847065448760986s

2025-10-15 22:02:28,263 | INFO | Training epoch 532, Batch 1000/1000: LR=4.57e-05, Loss=3.84e-03 BER=1.59e-03 FER=1.39e-02
2025-10-15 22:02:28,308 | INFO | Epoch 532 Train Time 42.297508239746094s

2025-10-15 22:03:11,023 | INFO | Training epoch 533, Batch 1000/1000: LR=4.55e-05, Loss=4.03e-03 BER=1.66e-03 FER=1.42e-02
2025-10-15 22:03:11,084 | INFO | Epoch 533 Train Time 42.774874448776245s

2025-10-15 22:03:55,110 | INFO | Training epoch 534, Batch 1000/1000: LR=4.54e-05, Loss=4.12e-03 BER=1.71e-03 FER=1.49e-02
2025-10-15 22:03:55,153 | INFO | Epoch 534 Train Time 44.06723380088806s

2025-10-15 22:04:35,290 | INFO | Training epoch 535, Batch 1000/1000: LR=4.52e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.40e-02
2025-10-15 22:04:35,346 | INFO | Epoch 535 Train Time 40.19163513183594s

2025-10-15 22:05:18,831 | INFO | Training epoch 536, Batch 1000/1000: LR=4.51e-05, Loss=3.81e-03 BER=1.54e-03 FER=1.36e-02
2025-10-15 22:05:18,890 | INFO | Epoch 536 Train Time 43.542683601379395s

2025-10-15 22:06:02,013 | INFO | Training epoch 537, Batch 1000/1000: LR=4.49e-05, Loss=4.06e-03 BER=1.65e-03 FER=1.44e-02
2025-10-15 22:06:02,051 | INFO | Epoch 537 Train Time 43.15938067436218s

2025-10-15 22:06:45,645 | INFO | Training epoch 538, Batch 1000/1000: LR=4.48e-05, Loss=3.96e-03 BER=1.61e-03 FER=1.40e-02
2025-10-15 22:06:45,727 | INFO | Epoch 538 Train Time 43.67484164237976s

2025-10-15 22:07:30,007 | INFO | Training epoch 539, Batch 1000/1000: LR=4.46e-05, Loss=3.84e-03 BER=1.59e-03 FER=1.36e-02
2025-10-15 22:07:30,065 | INFO | Epoch 539 Train Time 44.337111473083496s

2025-10-15 22:08:13,316 | INFO | Training epoch 540, Batch 1000/1000: LR=4.45e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.42e-02
2025-10-15 22:08:13,368 | INFO | Epoch 540 Train Time 43.30156469345093s

2025-10-15 22:08:57,174 | INFO | Training epoch 541, Batch 1000/1000: LR=4.43e-05, Loss=3.87e-03 BER=1.58e-03 FER=1.40e-02
2025-10-15 22:08:57,240 | INFO | Epoch 541 Train Time 43.87060737609863s

2025-10-15 22:09:39,507 | INFO | Training epoch 542, Batch 1000/1000: LR=4.41e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.47e-02
2025-10-15 22:09:39,577 | INFO | Epoch 542 Train Time 42.33584022521973s

2025-10-15 22:10:23,221 | INFO | Training epoch 543, Batch 1000/1000: LR=4.40e-05, Loss=3.86e-03 BER=1.60e-03 FER=1.40e-02
2025-10-15 22:10:23,274 | INFO | Epoch 543 Train Time 43.69405722618103s

2025-10-15 22:11:07,803 | INFO | Training epoch 544, Batch 1000/1000: LR=4.38e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.44e-02
2025-10-15 22:11:07,853 | INFO | Epoch 544 Train Time 44.57753872871399s

2025-10-15 22:11:51,502 | INFO | Training epoch 545, Batch 1000/1000: LR=4.37e-05, Loss=4.00e-03 BER=1.62e-03 FER=1.40e-02
2025-10-15 22:11:51,548 | INFO | Epoch 545 Train Time 43.69364285469055s

2025-10-15 22:12:35,201 | INFO | Training epoch 546, Batch 1000/1000: LR=4.35e-05, Loss=4.06e-03 BER=1.65e-03 FER=1.44e-02
2025-10-15 22:12:35,246 | INFO | Epoch 546 Train Time 43.69705891609192s

2025-10-15 22:13:18,596 | INFO | Training epoch 547, Batch 1000/1000: LR=4.34e-05, Loss=3.98e-03 BER=1.64e-03 FER=1.43e-02
2025-10-15 22:13:18,641 | INFO | Epoch 547 Train Time 43.3927903175354s

2025-10-15 22:14:02,695 | INFO | Training epoch 548, Batch 1000/1000: LR=4.32e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.42e-02
2025-10-15 22:14:02,747 | INFO | Epoch 548 Train Time 44.10524225234985s

2025-10-15 22:14:47,544 | INFO | Training epoch 549, Batch 1000/1000: LR=4.31e-05, Loss=4.16e-03 BER=1.72e-03 FER=1.48e-02
2025-10-15 22:14:47,601 | INFO | Epoch 549 Train Time 44.85076355934143s

2025-10-15 22:15:31,194 | INFO | Training epoch 550, Batch 1000/1000: LR=4.29e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.45e-02
2025-10-15 22:15:31,238 | INFO | Epoch 550 Train Time 43.63443899154663s

2025-10-15 22:16:14,237 | INFO | Training epoch 551, Batch 1000/1000: LR=4.28e-05, Loss=3.99e-03 BER=1.63e-03 FER=1.42e-02
2025-10-15 22:16:14,305 | INFO | Epoch 551 Train Time 43.0660445690155s

2025-10-15 22:16:58,223 | INFO | Training epoch 552, Batch 1000/1000: LR=4.26e-05, Loss=3.92e-03 BER=1.65e-03 FER=1.43e-02
2025-10-15 22:16:58,269 | INFO | Epoch 552 Train Time 43.96243143081665s

2025-10-15 22:17:41,012 | INFO | Training epoch 553, Batch 1000/1000: LR=4.24e-05, Loss=3.93e-03 BER=1.64e-03 FER=1.43e-02
2025-10-15 22:17:41,057 | INFO | Epoch 553 Train Time 42.78672814369202s

2025-10-15 22:18:23,499 | INFO | Training epoch 554, Batch 1000/1000: LR=4.23e-05, Loss=3.89e-03 BER=1.59e-03 FER=1.39e-02
2025-10-15 22:18:23,562 | INFO | Epoch 554 Train Time 42.5043830871582s

2025-10-15 22:19:06,939 | INFO | Training epoch 555, Batch 1000/1000: LR=4.21e-05, Loss=3.96e-03 BER=1.60e-03 FER=1.39e-02
2025-10-15 22:19:06,999 | INFO | Epoch 555 Train Time 43.435473918914795s

2025-10-15 22:19:51,226 | INFO | Training epoch 556, Batch 1000/1000: LR=4.20e-05, Loss=3.87e-03 BER=1.59e-03 FER=1.40e-02
2025-10-15 22:19:51,268 | INFO | Epoch 556 Train Time 44.26737856864929s

2025-10-15 22:20:34,123 | INFO | Training epoch 557, Batch 1000/1000: LR=4.18e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 22:20:34,175 | INFO | Epoch 557 Train Time 42.90574550628662s

2025-10-15 22:21:16,288 | INFO | Training epoch 558, Batch 1000/1000: LR=4.17e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.47e-02
2025-10-15 22:21:16,331 | INFO | Epoch 558 Train Time 42.1532518863678s

2025-10-15 22:21:59,208 | INFO | Training epoch 559, Batch 1000/1000: LR=4.15e-05, Loss=3.89e-03 BER=1.58e-03 FER=1.38e-02
2025-10-15 22:21:59,264 | INFO | Epoch 559 Train Time 42.931174993515015s

2025-10-15 22:22:41,996 | INFO | Training epoch 560, Batch 1000/1000: LR=4.14e-05, Loss=3.74e-03 BER=1.53e-03 FER=1.36e-02
2025-10-15 22:22:42,041 | INFO | Epoch 560 Train Time 42.776660442352295s

2025-10-15 22:22:42,042 | INFO | [P1] saving best_model with loss 0.003741 at epoch 560
2025-10-15 22:23:26,620 | INFO | Training epoch 561, Batch 1000/1000: LR=4.12e-05, Loss=3.95e-03 BER=1.62e-03 FER=1.43e-02
2025-10-15 22:23:26,679 | INFO | Epoch 561 Train Time 44.54943084716797s

2025-10-15 22:24:09,787 | INFO | Training epoch 562, Batch 1000/1000: LR=4.11e-05, Loss=3.81e-03 BER=1.55e-03 FER=1.34e-02
2025-10-15 22:24:09,853 | INFO | Epoch 562 Train Time 43.173110485076904s

2025-10-15 22:24:54,311 | INFO | Training epoch 563, Batch 1000/1000: LR=4.09e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.42e-02
2025-10-15 22:24:54,351 | INFO | Epoch 563 Train Time 44.49474835395813s

2025-10-15 22:25:38,037 | INFO | Training epoch 564, Batch 1000/1000: LR=4.08e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.42e-02
2025-10-15 22:25:38,112 | INFO | Epoch 564 Train Time 43.758991718292236s

2025-10-15 22:26:21,402 | INFO | Training epoch 565, Batch 1000/1000: LR=4.06e-05, Loss=3.93e-03 BER=1.59e-03 FER=1.40e-02
2025-10-15 22:26:21,451 | INFO | Epoch 565 Train Time 43.33624720573425s

2025-10-15 22:27:05,178 | INFO | Training epoch 566, Batch 1000/1000: LR=4.05e-05, Loss=3.95e-03 BER=1.60e-03 FER=1.39e-02
2025-10-15 22:27:05,243 | INFO | Epoch 566 Train Time 43.79127907752991s

2025-10-15 22:27:48,404 | INFO | Training epoch 567, Batch 1000/1000: LR=4.03e-05, Loss=3.95e-03 BER=1.62e-03 FER=1.37e-02
2025-10-15 22:27:48,455 | INFO | Epoch 567 Train Time 43.20976424217224s

2025-10-15 22:28:32,421 | INFO | Training epoch 568, Batch 1000/1000: LR=4.02e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.46e-02
2025-10-15 22:28:32,468 | INFO | Epoch 568 Train Time 44.011789321899414s

2025-10-15 22:29:15,709 | INFO | Training epoch 569, Batch 1000/1000: LR=4.00e-05, Loss=3.90e-03 BER=1.62e-03 FER=1.39e-02
2025-10-15 22:29:15,755 | INFO | Epoch 569 Train Time 43.28510141372681s

2025-10-15 22:29:59,461 | INFO | Training epoch 570, Batch 1000/1000: LR=3.99e-05, Loss=3.97e-03 BER=1.61e-03 FER=1.40e-02
2025-10-15 22:29:59,517 | INFO | Epoch 570 Train Time 43.75890398025513s

2025-10-15 22:30:42,487 | INFO | Training epoch 571, Batch 1000/1000: LR=3.97e-05, Loss=4.01e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 22:30:42,528 | INFO | Epoch 571 Train Time 43.0088255405426s

2025-10-15 22:31:25,632 | INFO | Training epoch 572, Batch 1000/1000: LR=3.96e-05, Loss=3.83e-03 BER=1.56e-03 FER=1.37e-02
2025-10-15 22:31:25,708 | INFO | Epoch 572 Train Time 43.17911505699158s

2025-10-15 22:32:10,814 | INFO | Training epoch 573, Batch 1000/1000: LR=3.94e-05, Loss=3.96e-03 BER=1.64e-03 FER=1.43e-02
2025-10-15 22:32:10,854 | INFO | Epoch 573 Train Time 45.14489984512329s

2025-10-15 22:32:54,625 | INFO | Training epoch 574, Batch 1000/1000: LR=3.92e-05, Loss=4.09e-03 BER=1.67e-03 FER=1.44e-02
2025-10-15 22:32:54,672 | INFO | Epoch 574 Train Time 43.81648802757263s

2025-10-15 22:33:38,205 | INFO | Training epoch 575, Batch 1000/1000: LR=3.91e-05, Loss=3.97e-03 BER=1.61e-03 FER=1.40e-02
2025-10-15 22:33:38,269 | INFO | Epoch 575 Train Time 43.594199895858765s

2025-10-15 22:34:20,696 | INFO | Training epoch 576, Batch 1000/1000: LR=3.89e-05, Loss=3.85e-03 BER=1.57e-03 FER=1.37e-02
2025-10-15 22:34:20,751 | INFO | Epoch 576 Train Time 42.48119831085205s

2025-10-15 22:35:04,310 | INFO | Training epoch 577, Batch 1000/1000: LR=3.88e-05, Loss=4.06e-03 BER=1.67e-03 FER=1.43e-02
2025-10-15 22:35:04,365 | INFO | Epoch 577 Train Time 43.61241579055786s

2025-10-15 22:35:47,642 | INFO | Training epoch 578, Batch 1000/1000: LR=3.86e-05, Loss=4.04e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 22:35:47,712 | INFO | Epoch 578 Train Time 43.345402240753174s

2025-10-15 22:36:31,752 | INFO | Training epoch 579, Batch 1000/1000: LR=3.85e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.42e-02
2025-10-15 22:36:31,808 | INFO | Epoch 579 Train Time 44.09410810470581s

2025-10-15 22:37:15,213 | INFO | Training epoch 580, Batch 1000/1000: LR=3.83e-05, Loss=3.91e-03 BER=1.61e-03 FER=1.41e-02
2025-10-15 22:37:15,255 | INFO | Epoch 580 Train Time 43.4444465637207s

2025-10-15 22:37:58,776 | INFO | Training epoch 581, Batch 1000/1000: LR=3.82e-05, Loss=4.08e-03 BER=1.69e-03 FER=1.46e-02
2025-10-15 22:37:58,845 | INFO | Epoch 581 Train Time 43.58850121498108s

2025-10-15 22:38:43,018 | INFO | Training epoch 582, Batch 1000/1000: LR=3.80e-05, Loss=3.87e-03 BER=1.56e-03 FER=1.37e-02
2025-10-15 22:38:43,075 | INFO | Epoch 582 Train Time 44.228978872299194s

2025-10-15 22:39:26,199 | INFO | Training epoch 583, Batch 1000/1000: LR=3.79e-05, Loss=3.91e-03 BER=1.60e-03 FER=1.41e-02
2025-10-15 22:39:26,245 | INFO | Epoch 583 Train Time 43.16898560523987s

2025-10-15 22:40:09,686 | INFO | Training epoch 584, Batch 1000/1000: LR=3.77e-05, Loss=3.91e-03 BER=1.62e-03 FER=1.39e-02
2025-10-15 22:40:09,736 | INFO | Epoch 584 Train Time 43.48995304107666s

2025-10-15 22:40:52,305 | INFO | Training epoch 585, Batch 1000/1000: LR=3.76e-05, Loss=3.73e-03 BER=1.55e-03 FER=1.34e-02
2025-10-15 22:40:52,355 | INFO | Epoch 585 Train Time 42.61748719215393s

2025-10-15 22:40:52,355 | INFO | [P1] saving best_model with loss 0.003729 at epoch 585
2025-10-15 22:41:36,619 | INFO | Training epoch 586, Batch 1000/1000: LR=3.74e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.43e-02
2025-10-15 22:41:36,663 | INFO | Epoch 586 Train Time 44.217264890670776s

2025-10-15 22:42:20,516 | INFO | Training epoch 587, Batch 1000/1000: LR=3.73e-05, Loss=3.81e-03 BER=1.58e-03 FER=1.36e-02
2025-10-15 22:42:20,570 | INFO | Epoch 587 Train Time 43.90603280067444s

2025-10-15 22:43:03,236 | INFO | Training epoch 588, Batch 1000/1000: LR=3.71e-05, Loss=3.97e-03 BER=1.62e-03 FER=1.41e-02
2025-10-15 22:43:03,291 | INFO | Epoch 588 Train Time 42.71960115432739s

2025-10-15 22:43:46,102 | INFO | Training epoch 589, Batch 1000/1000: LR=3.70e-05, Loss=3.95e-03 BER=1.60e-03 FER=1.42e-02
2025-10-15 22:43:46,154 | INFO | Epoch 589 Train Time 42.86126708984375s

2025-10-15 22:44:30,460 | INFO | Training epoch 590, Batch 1000/1000: LR=3.68e-05, Loss=3.90e-03 BER=1.56e-03 FER=1.35e-02
2025-10-15 22:44:30,512 | INFO | Epoch 590 Train Time 44.35649132728577s

2025-10-15 22:45:12,520 | INFO | Training epoch 591, Batch 1000/1000: LR=3.67e-05, Loss=3.71e-03 BER=1.53e-03 FER=1.34e-02
2025-10-15 22:45:12,577 | INFO | Epoch 591 Train Time 42.06427502632141s

2025-10-15 22:45:12,578 | INFO | [P1] saving best_model with loss 0.003706 at epoch 591
2025-10-15 22:45:56,301 | INFO | Training epoch 592, Batch 1000/1000: LR=3.65e-05, Loss=4.00e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 22:45:56,361 | INFO | Epoch 592 Train Time 43.689592361450195s

2025-10-15 22:46:39,208 | INFO | Training epoch 593, Batch 1000/1000: LR=3.64e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.42e-02
2025-10-15 22:46:39,259 | INFO | Epoch 593 Train Time 42.894888401031494s

2025-10-15 22:47:22,599 | INFO | Training epoch 594, Batch 1000/1000: LR=3.62e-05, Loss=3.99e-03 BER=1.61e-03 FER=1.41e-02
2025-10-15 22:47:22,662 | INFO | Epoch 594 Train Time 43.40115427970886s

2025-10-15 22:48:06,100 | INFO | Training epoch 595, Batch 1000/1000: LR=3.61e-05, Loss=3.84e-03 BER=1.55e-03 FER=1.35e-02
2025-10-15 22:48:06,159 | INFO | Epoch 595 Train Time 43.49587273597717s

2025-10-15 22:48:50,396 | INFO | Training epoch 596, Batch 1000/1000: LR=3.59e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.35e-02
2025-10-15 22:48:50,443 | INFO | Epoch 596 Train Time 44.28225660324097s

2025-10-15 22:49:34,848 | INFO | Training epoch 597, Batch 1000/1000: LR=3.58e-05, Loss=3.67e-03 BER=1.52e-03 FER=1.30e-02
2025-10-15 22:49:34,916 | INFO | Epoch 597 Train Time 44.47152614593506s

2025-10-15 22:49:34,916 | INFO | [P1] saving best_model with loss 0.003672 at epoch 597
2025-10-15 22:50:19,839 | INFO | Training epoch 598, Batch 1000/1000: LR=3.56e-05, Loss=4.03e-03 BER=1.64e-03 FER=1.44e-02
2025-10-15 22:50:19,897 | INFO | Epoch 598 Train Time 44.88228249549866s

2025-10-15 22:51:04,106 | INFO | Training epoch 599, Batch 1000/1000: LR=3.55e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 22:51:04,151 | INFO | Epoch 599 Train Time 44.25181460380554s

2025-10-15 22:51:47,900 | INFO | Training epoch 600, Batch 1000/1000: LR=3.54e-05, Loss=3.96e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 22:51:47,964 | INFO | Epoch 600 Train Time 43.81177067756653s

2025-10-15 22:52:31,506 | INFO | Training epoch 601, Batch 1000/1000: LR=3.52e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.36e-02
2025-10-15 22:52:31,575 | INFO | Epoch 601 Train Time 43.60911202430725s

2025-10-15 22:53:15,143 | INFO | Training epoch 602, Batch 1000/1000: LR=3.51e-05, Loss=3.92e-03 BER=1.63e-03 FER=1.42e-02
2025-10-15 22:53:15,219 | INFO | Epoch 602 Train Time 43.64242744445801s

2025-10-15 22:53:58,243 | INFO | Training epoch 603, Batch 1000/1000: LR=3.49e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 22:53:58,310 | INFO | Epoch 603 Train Time 43.08809232711792s

2025-10-15 22:54:42,326 | INFO | Training epoch 604, Batch 1000/1000: LR=3.48e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.44e-02
2025-10-15 22:54:42,374 | INFO | Epoch 604 Train Time 44.06069231033325s

2025-10-15 22:55:26,807 | INFO | Training epoch 605, Batch 1000/1000: LR=3.46e-05, Loss=3.81e-03 BER=1.59e-03 FER=1.39e-02
2025-10-15 22:55:26,866 | INFO | Epoch 605 Train Time 44.48946928977966s

2025-10-15 22:56:10,099 | INFO | Training epoch 606, Batch 1000/1000: LR=3.45e-05, Loss=3.91e-03 BER=1.60e-03 FER=1.39e-02
2025-10-15 22:56:10,161 | INFO | Epoch 606 Train Time 43.29295325279236s

2025-10-15 22:56:53,906 | INFO | Training epoch 607, Batch 1000/1000: LR=3.43e-05, Loss=3.91e-03 BER=1.59e-03 FER=1.41e-02
2025-10-15 22:56:53,967 | INFO | Epoch 607 Train Time 43.80254006385803s

2025-10-15 22:57:37,132 | INFO | Training epoch 608, Batch 1000/1000: LR=3.42e-05, Loss=3.74e-03 BER=1.54e-03 FER=1.33e-02
2025-10-15 22:57:37,182 | INFO | Epoch 608 Train Time 43.214359760284424s

2025-10-15 22:58:19,815 | INFO | Training epoch 609, Batch 1000/1000: LR=3.40e-05, Loss=3.97e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 22:58:19,865 | INFO | Epoch 609 Train Time 42.68135356903076s

2025-10-15 22:59:03,120 | INFO | Training epoch 610, Batch 1000/1000: LR=3.39e-05, Loss=4.01e-03 BER=1.67e-03 FER=1.45e-02
2025-10-15 22:59:03,171 | INFO | Epoch 610 Train Time 43.304526805877686s

2025-10-15 22:59:48,026 | INFO | Training epoch 611, Batch 1000/1000: LR=3.37e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.36e-02
2025-10-15 22:59:48,075 | INFO | Epoch 611 Train Time 44.901719093322754s

2025-10-15 23:00:31,507 | INFO | Training epoch 612, Batch 1000/1000: LR=3.36e-05, Loss=3.87e-03 BER=1.61e-03 FER=1.39e-02
2025-10-15 23:00:31,550 | INFO | Epoch 612 Train Time 43.47100234031677s

2025-10-15 23:01:14,601 | INFO | Training epoch 613, Batch 1000/1000: LR=3.34e-05, Loss=4.03e-03 BER=1.67e-03 FER=1.44e-02
2025-10-15 23:01:14,653 | INFO | Epoch 613 Train Time 43.10108041763306s

2025-10-15 23:01:59,101 | INFO | Training epoch 614, Batch 1000/1000: LR=3.33e-05, Loss=3.84e-03 BER=1.56e-03 FER=1.36e-02
2025-10-15 23:01:59,151 | INFO | Epoch 614 Train Time 44.49696493148804s

2025-10-15 23:02:44,018 | INFO | Training epoch 615, Batch 1000/1000: LR=3.31e-05, Loss=3.97e-03 BER=1.61e-03 FER=1.38e-02
2025-10-15 23:02:44,076 | INFO | Epoch 615 Train Time 44.92379713058472s

2025-10-15 23:03:27,601 | INFO | Training epoch 616, Batch 1000/1000: LR=3.30e-05, Loss=3.88e-03 BER=1.61e-03 FER=1.38e-02
2025-10-15 23:03:27,655 | INFO | Epoch 616 Train Time 43.57652401924133s

2025-10-15 23:04:10,423 | INFO | Training epoch 617, Batch 1000/1000: LR=3.29e-05, Loss=3.89e-03 BER=1.58e-03 FER=1.39e-02
2025-10-15 23:04:10,501 | INFO | Epoch 617 Train Time 42.84485864639282s

2025-10-15 23:04:54,699 | INFO | Training epoch 618, Batch 1000/1000: LR=3.27e-05, Loss=3.89e-03 BER=1.61e-03 FER=1.39e-02
2025-10-15 23:04:54,754 | INFO | Epoch 618 Train Time 44.25107526779175s

2025-10-15 23:05:38,510 | INFO | Training epoch 619, Batch 1000/1000: LR=3.26e-05, Loss=3.84e-03 BER=1.58e-03 FER=1.38e-02
2025-10-15 23:05:38,561 | INFO | Epoch 619 Train Time 43.806411027908325s

2025-10-15 23:06:21,263 | INFO | Training epoch 620, Batch 1000/1000: LR=3.24e-05, Loss=3.75e-03 BER=1.56e-03 FER=1.37e-02
2025-10-15 23:06:21,313 | INFO | Epoch 620 Train Time 42.75041389465332s

2025-10-15 23:07:04,876 | INFO | Training epoch 621, Batch 1000/1000: LR=3.23e-05, Loss=3.89e-03 BER=1.59e-03 FER=1.37e-02
2025-10-15 23:07:04,968 | INFO | Epoch 621 Train Time 43.65382623672485s

2025-10-15 23:07:47,833 | INFO | Training epoch 622, Batch 1000/1000: LR=3.21e-05, Loss=3.92e-03 BER=1.61e-03 FER=1.40e-02
2025-10-15 23:07:47,905 | INFO | Epoch 622 Train Time 42.93446493148804s

2025-10-15 23:08:32,022 | INFO | Training epoch 623, Batch 1000/1000: LR=3.20e-05, Loss=3.88e-03 BER=1.61e-03 FER=1.39e-02
2025-10-15 23:08:32,076 | INFO | Epoch 623 Train Time 44.169689893722534s

2025-10-15 23:09:15,338 | INFO | Training epoch 624, Batch 1000/1000: LR=3.18e-05, Loss=3.75e-03 BER=1.55e-03 FER=1.35e-02
2025-10-15 23:09:15,388 | INFO | Epoch 624 Train Time 43.309860944747925s

2025-10-15 23:09:59,422 | INFO | Training epoch 625, Batch 1000/1000: LR=3.17e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.39e-02
2025-10-15 23:09:59,462 | INFO | Epoch 625 Train Time 44.073343992233276s

2025-10-15 23:10:42,201 | INFO | Training epoch 626, Batch 1000/1000: LR=3.16e-05, Loss=3.71e-03 BER=1.52e-03 FER=1.32e-02
2025-10-15 23:10:42,255 | INFO | Epoch 626 Train Time 42.791157960891724s

2025-10-15 23:11:25,613 | INFO | Training epoch 627, Batch 1000/1000: LR=3.14e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.41e-02
2025-10-15 23:11:25,673 | INFO | Epoch 627 Train Time 43.416714906692505s

2025-10-15 23:12:08,412 | INFO | Training epoch 628, Batch 1000/1000: LR=3.13e-05, Loss=3.86e-03 BER=1.57e-03 FER=1.36e-02
2025-10-15 23:12:08,480 | INFO | Epoch 628 Train Time 42.80562686920166s

2025-10-15 23:12:51,799 | INFO | Training epoch 629, Batch 1000/1000: LR=3.11e-05, Loss=3.74e-03 BER=1.55e-03 FER=1.31e-02
2025-10-15 23:12:51,858 | INFO | Epoch 629 Train Time 43.37607264518738s

2025-10-15 23:13:36,501 | INFO | Training epoch 630, Batch 1000/1000: LR=3.10e-05, Loss=3.80e-03 BER=1.54e-03 FER=1.34e-02
2025-10-15 23:13:36,548 | INFO | Epoch 630 Train Time 44.68825602531433s

2025-10-15 23:14:20,442 | INFO | Training epoch 631, Batch 1000/1000: LR=3.08e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.34e-02
2025-10-15 23:14:20,509 | INFO | Epoch 631 Train Time 43.95993661880493s

2025-10-15 23:15:04,243 | INFO | Training epoch 632, Batch 1000/1000: LR=3.07e-05, Loss=3.95e-03 BER=1.62e-03 FER=1.38e-02
2025-10-15 23:15:04,305 | INFO | Epoch 632 Train Time 43.795201778411865s

2025-10-15 23:15:48,304 | INFO | Training epoch 633, Batch 1000/1000: LR=3.06e-05, Loss=3.90e-03 BER=1.58e-03 FER=1.36e-02
2025-10-15 23:15:48,354 | INFO | Epoch 633 Train Time 44.04671335220337s

2025-10-15 23:16:31,819 | INFO | Training epoch 634, Batch 1000/1000: LR=3.04e-05, Loss=4.04e-03 BER=1.66e-03 FER=1.43e-02
2025-10-15 23:16:31,866 | INFO | Epoch 634 Train Time 43.51169180870056s

2025-10-15 23:17:14,609 | INFO | Training epoch 635, Batch 1000/1000: LR=3.03e-05, Loss=3.88e-03 BER=1.59e-03 FER=1.40e-02
2025-10-15 23:17:14,664 | INFO | Epoch 635 Train Time 42.79590106010437s

2025-10-15 23:17:58,273 | INFO | Training epoch 636, Batch 1000/1000: LR=3.01e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.39e-02
2025-10-15 23:17:58,343 | INFO | Epoch 636 Train Time 43.677711963653564s

2025-10-15 23:18:43,017 | INFO | Training epoch 637, Batch 1000/1000: LR=3.00e-05, Loss=3.68e-03 BER=1.50e-03 FER=1.29e-02
2025-10-15 23:18:43,066 | INFO | Epoch 637 Train Time 44.72237825393677s

2025-10-15 23:19:26,742 | INFO | Training epoch 638, Batch 1000/1000: LR=2.98e-05, Loss=3.71e-03 BER=1.52e-03 FER=1.30e-02
2025-10-15 23:19:26,788 | INFO | Epoch 638 Train Time 43.720306396484375s

2025-10-15 23:20:09,838 | INFO | Training epoch 639, Batch 1000/1000: LR=2.97e-05, Loss=3.93e-03 BER=1.63e-03 FER=1.40e-02
2025-10-15 23:20:09,901 | INFO | Epoch 639 Train Time 43.11044526100159s

2025-10-15 23:20:53,361 | INFO | Training epoch 640, Batch 1000/1000: LR=2.96e-05, Loss=3.71e-03 BER=1.51e-03 FER=1.30e-02
2025-10-15 23:20:53,425 | INFO | Epoch 640 Train Time 43.521416425704956s

2025-10-15 23:21:37,615 | INFO | Training epoch 641, Batch 1000/1000: LR=2.94e-05, Loss=3.85e-03 BER=1.58e-03 FER=1.37e-02
2025-10-15 23:21:37,686 | INFO | Epoch 641 Train Time 44.25891709327698s

2025-10-15 23:22:20,898 | INFO | Training epoch 642, Batch 1000/1000: LR=2.93e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.41e-02
2025-10-15 23:22:20,956 | INFO | Epoch 642 Train Time 43.26803946495056s

2025-10-15 23:23:02,800 | INFO | Training epoch 643, Batch 1000/1000: LR=2.91e-05, Loss=3.95e-03 BER=1.62e-03 FER=1.40e-02
2025-10-15 23:23:02,850 | INFO | Epoch 643 Train Time 41.89212727546692s

2025-10-15 23:23:46,011 | INFO | Training epoch 644, Batch 1000/1000: LR=2.90e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.37e-02
2025-10-15 23:23:46,061 | INFO | Epoch 644 Train Time 43.209619760513306s

2025-10-15 23:24:29,321 | INFO | Training epoch 645, Batch 1000/1000: LR=2.89e-05, Loss=3.98e-03 BER=1.64e-03 FER=1.38e-02
2025-10-15 23:24:29,372 | INFO | Epoch 645 Train Time 43.30902624130249s

2025-10-15 23:25:12,704 | INFO | Training epoch 646, Batch 1000/1000: LR=2.87e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.38e-02
2025-10-15 23:25:12,754 | INFO | Epoch 646 Train Time 43.379658937454224s

2025-10-15 23:25:55,733 | INFO | Training epoch 647, Batch 1000/1000: LR=2.86e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.42e-02
2025-10-15 23:25:55,795 | INFO | Epoch 647 Train Time 43.03905940055847s

2025-10-15 23:26:38,334 | INFO | Training epoch 648, Batch 1000/1000: LR=2.84e-05, Loss=3.79e-03 BER=1.56e-03 FER=1.33e-02
2025-10-15 23:26:38,381 | INFO | Epoch 648 Train Time 42.583598613739014s

2025-10-15 23:27:21,697 | INFO | Training epoch 649, Batch 1000/1000: LR=2.83e-05, Loss=3.88e-03 BER=1.59e-03 FER=1.37e-02
2025-10-15 23:27:21,757 | INFO | Epoch 649 Train Time 43.37493872642517s

2025-10-15 23:28:04,118 | INFO | Training epoch 650, Batch 1000/1000: LR=2.82e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.32e-02
2025-10-15 23:28:04,163 | INFO | Epoch 650 Train Time 42.40418291091919s

2025-10-15 23:28:47,001 | INFO | Training epoch 651, Batch 1000/1000: LR=2.80e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.42e-02
2025-10-15 23:28:47,062 | INFO | Epoch 651 Train Time 42.89787173271179s

2025-10-15 23:29:30,921 | INFO | Training epoch 652, Batch 1000/1000: LR=2.79e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.40e-02
2025-10-15 23:29:30,966 | INFO | Epoch 652 Train Time 43.901559829711914s

2025-10-15 23:30:13,701 | INFO | Training epoch 653, Batch 1000/1000: LR=2.78e-05, Loss=4.01e-03 BER=1.63e-03 FER=1.44e-02
2025-10-15 23:30:13,778 | INFO | Epoch 653 Train Time 42.811012744903564s

2025-10-15 23:30:56,804 | INFO | Training epoch 654, Batch 1000/1000: LR=2.76e-05, Loss=3.72e-03 BER=1.55e-03 FER=1.33e-02
2025-10-15 23:30:56,855 | INFO | Epoch 654 Train Time 43.075562477111816s

2025-10-15 23:31:39,607 | INFO | Training epoch 655, Batch 1000/1000: LR=2.75e-05, Loss=3.99e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 23:31:39,661 | INFO | Epoch 655 Train Time 42.80475330352783s

2025-10-15 23:32:23,017 | INFO | Training epoch 656, Batch 1000/1000: LR=2.73e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.41e-02
2025-10-15 23:32:23,084 | INFO | Epoch 656 Train Time 43.421727657318115s

2025-10-15 23:33:06,198 | INFO | Training epoch 657, Batch 1000/1000: LR=2.72e-05, Loss=3.59e-03 BER=1.50e-03 FER=1.26e-02
2025-10-15 23:33:06,265 | INFO | Epoch 657 Train Time 43.17859768867493s

2025-10-15 23:33:06,266 | INFO | [P1] saving best_model with loss 0.003590 at epoch 657
2025-10-15 23:33:50,608 | INFO | Training epoch 658, Batch 1000/1000: LR=2.71e-05, Loss=3.69e-03 BER=1.51e-03 FER=1.30e-02
2025-10-15 23:33:50,664 | INFO | Epoch 658 Train Time 44.2969696521759s

2025-10-15 23:34:34,424 | INFO | Training epoch 659, Batch 1000/1000: LR=2.69e-05, Loss=3.89e-03 BER=1.59e-03 FER=1.38e-02
2025-10-15 23:34:34,470 | INFO | Epoch 659 Train Time 43.80421590805054s

2025-10-15 23:35:17,509 | INFO | Training epoch 660, Batch 1000/1000: LR=2.68e-05, Loss=3.83e-03 BER=1.60e-03 FER=1.41e-02
2025-10-15 23:35:17,566 | INFO | Epoch 660 Train Time 43.09546089172363s

2025-10-15 23:36:01,804 | INFO | Training epoch 661, Batch 1000/1000: LR=2.67e-05, Loss=3.79e-03 BER=1.55e-03 FER=1.36e-02
2025-10-15 23:36:01,853 | INFO | Epoch 661 Train Time 44.28497934341431s

2025-10-15 23:36:44,506 | INFO | Training epoch 662, Batch 1000/1000: LR=2.65e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.35e-02
2025-10-15 23:36:44,572 | INFO | Epoch 662 Train Time 42.718284130096436s

2025-10-15 23:37:28,105 | INFO | Training epoch 663, Batch 1000/1000: LR=2.64e-05, Loss=3.71e-03 BER=1.54e-03 FER=1.32e-02
2025-10-15 23:37:28,174 | INFO | Epoch 663 Train Time 43.60086631774902s

2025-10-15 23:38:11,238 | INFO | Training epoch 664, Batch 1000/1000: LR=2.62e-05, Loss=3.71e-03 BER=1.53e-03 FER=1.30e-02
2025-10-15 23:38:11,297 | INFO | Epoch 664 Train Time 43.1210560798645s

2025-10-15 23:38:55,536 | INFO | Training epoch 665, Batch 1000/1000: LR=2.61e-05, Loss=3.90e-03 BER=1.59e-03 FER=1.37e-02
2025-10-15 23:38:55,597 | INFO | Epoch 665 Train Time 44.298301458358765s

2025-10-15 23:39:39,200 | INFO | Training epoch 666, Batch 1000/1000: LR=2.60e-05, Loss=3.90e-03 BER=1.61e-03 FER=1.38e-02
2025-10-15 23:39:39,253 | INFO | Epoch 666 Train Time 43.653626918792725s

2025-10-15 23:40:23,919 | INFO | Training epoch 667, Batch 1000/1000: LR=2.58e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.36e-02
2025-10-15 23:40:23,967 | INFO | Epoch 667 Train Time 44.71177363395691s

2025-10-15 23:41:07,506 | INFO | Training epoch 668, Batch 1000/1000: LR=2.57e-05, Loss=3.77e-03 BER=1.54e-03 FER=1.33e-02
2025-10-15 23:41:07,561 | INFO | Epoch 668 Train Time 43.593196630477905s

2025-10-15 23:41:51,433 | INFO | Training epoch 669, Batch 1000/1000: LR=2.56e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.35e-02
2025-10-15 23:41:51,493 | INFO | Epoch 669 Train Time 43.93052816390991s

2025-10-15 23:42:36,214 | INFO | Training epoch 670, Batch 1000/1000: LR=2.54e-05, Loss=3.62e-03 BER=1.49e-03 FER=1.31e-02
2025-10-15 23:42:36,269 | INFO | Epoch 670 Train Time 44.77425503730774s

2025-10-15 23:43:19,608 | INFO | Training epoch 671, Batch 1000/1000: LR=2.53e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.38e-02
2025-10-15 23:43:19,668 | INFO | Epoch 671 Train Time 43.397369623184204s

2025-10-15 23:44:03,021 | INFO | Training epoch 672, Batch 1000/1000: LR=2.52e-05, Loss=3.71e-03 BER=1.54e-03 FER=1.34e-02
2025-10-15 23:44:03,072 | INFO | Epoch 672 Train Time 43.402475357055664s

2025-10-15 23:44:46,698 | INFO | Training epoch 673, Batch 1000/1000: LR=2.50e-05, Loss=3.80e-03 BER=1.57e-03 FER=1.35e-02
2025-10-15 23:44:46,748 | INFO | Epoch 673 Train Time 43.67421746253967s

2025-10-15 23:45:29,897 | INFO | Training epoch 674, Batch 1000/1000: LR=2.49e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.38e-02
2025-10-15 23:45:29,947 | INFO | Epoch 674 Train Time 43.1978178024292s

2025-10-15 23:46:12,902 | INFO | Training epoch 675, Batch 1000/1000: LR=2.48e-05, Loss=3.94e-03 BER=1.63e-03 FER=1.38e-02
2025-10-15 23:46:12,974 | INFO | Epoch 675 Train Time 43.02495527267456s

2025-10-15 23:46:55,801 | INFO | Training epoch 676, Batch 1000/1000: LR=2.46e-05, Loss=3.80e-03 BER=1.57e-03 FER=1.35e-02
2025-10-15 23:46:55,865 | INFO | Epoch 676 Train Time 42.89019417762756s

2025-10-15 23:47:38,925 | INFO | Training epoch 677, Batch 1000/1000: LR=2.45e-05, Loss=3.68e-03 BER=1.51e-03 FER=1.32e-02
2025-10-15 23:47:38,982 | INFO | Epoch 677 Train Time 43.1137900352478s

2025-10-15 23:48:23,835 | INFO | Training epoch 678, Batch 1000/1000: LR=2.44e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.35e-02
2025-10-15 23:48:23,916 | INFO | Epoch 678 Train Time 44.93154501914978s

2025-10-15 23:49:07,838 | INFO | Training epoch 679, Batch 1000/1000: LR=2.42e-05, Loss=3.84e-03 BER=1.59e-03 FER=1.36e-02
2025-10-15 23:49:07,904 | INFO | Epoch 679 Train Time 43.98716878890991s

2025-10-15 23:49:52,020 | INFO | Training epoch 680, Batch 1000/1000: LR=2.41e-05, Loss=3.69e-03 BER=1.52e-03 FER=1.32e-02
2025-10-15 23:49:52,080 | INFO | Epoch 680 Train Time 44.173614740371704s

2025-10-15 23:50:35,804 | INFO | Training epoch 681, Batch 1000/1000: LR=2.40e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.34e-02
2025-10-15 23:50:35,876 | INFO | Epoch 681 Train Time 43.79467272758484s

2025-10-15 23:51:19,702 | INFO | Training epoch 682, Batch 1000/1000: LR=2.38e-05, Loss=3.90e-03 BER=1.58e-03 FER=1.36e-02
2025-10-15 23:51:19,761 | INFO | Epoch 682 Train Time 43.883530139923096s

2025-10-15 23:52:01,911 | INFO | Training epoch 683, Batch 1000/1000: LR=2.37e-05, Loss=3.87e-03 BER=1.58e-03 FER=1.36e-02
2025-10-15 23:52:01,963 | INFO | Epoch 683 Train Time 42.20075964927673s

2025-10-15 23:52:44,688 | INFO | Training epoch 684, Batch 1000/1000: LR=2.36e-05, Loss=3.75e-03 BER=1.54e-03 FER=1.32e-02
2025-10-15 23:52:44,734 | INFO | Epoch 684 Train Time 42.769285917282104s

2025-10-15 23:53:26,562 | INFO | Training epoch 685, Batch 1000/1000: LR=2.35e-05, Loss=3.73e-03 BER=1.55e-03 FER=1.33e-02
2025-10-15 23:53:26,641 | INFO | Epoch 685 Train Time 41.9061381816864s

2025-10-15 23:54:11,632 | INFO | Training epoch 686, Batch 1000/1000: LR=2.33e-05, Loss=3.73e-03 BER=1.54e-03 FER=1.34e-02
2025-10-15 23:54:11,688 | INFO | Epoch 686 Train Time 45.04422450065613s

2025-10-15 23:54:55,213 | INFO | Training epoch 687, Batch 1000/1000: LR=2.32e-05, Loss=4.04e-03 BER=1.68e-03 FER=1.44e-02
2025-10-15 23:54:55,265 | INFO | Epoch 687 Train Time 43.57438802719116s

2025-10-15 23:55:38,404 | INFO | Training epoch 688, Batch 1000/1000: LR=2.31e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.40e-02
2025-10-15 23:55:38,452 | INFO | Epoch 688 Train Time 43.18614411354065s

2025-10-15 23:56:22,602 | INFO | Training epoch 689, Batch 1000/1000: LR=2.29e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.35e-02
2025-10-15 23:56:22,652 | INFO | Epoch 689 Train Time 44.19923448562622s

2025-10-15 23:57:06,202 | INFO | Training epoch 690, Batch 1000/1000: LR=2.28e-05, Loss=3.62e-03 BER=1.49e-03 FER=1.29e-02
2025-10-15 23:57:06,253 | INFO | Epoch 690 Train Time 43.59977459907532s

2025-10-15 23:57:50,205 | INFO | Training epoch 691, Batch 1000/1000: LR=2.27e-05, Loss=3.86e-03 BER=1.59e-03 FER=1.37e-02
2025-10-15 23:57:50,249 | INFO | Epoch 691 Train Time 43.99328398704529s

2025-10-15 23:58:33,997 | INFO | Training epoch 692, Batch 1000/1000: LR=2.25e-05, Loss=3.82e-03 BER=1.57e-03 FER=1.35e-02
2025-10-15 23:58:34,053 | INFO | Epoch 692 Train Time 43.80246376991272s

2025-10-15 23:59:16,728 | INFO | Training epoch 693, Batch 1000/1000: LR=2.24e-05, Loss=3.72e-03 BER=1.52e-03 FER=1.34e-02
2025-10-15 23:59:16,786 | INFO | Epoch 693 Train Time 42.731831073760986s

2025-10-16 00:00:00,307 | INFO | Training epoch 694, Batch 1000/1000: LR=2.23e-05, Loss=3.85e-03 BER=1.57e-03 FER=1.36e-02
2025-10-16 00:00:00,367 | INFO | Epoch 694 Train Time 43.57776117324829s

2025-10-16 00:00:44,633 | INFO | Training epoch 695, Batch 1000/1000: LR=2.22e-05, Loss=3.67e-03 BER=1.50e-03 FER=1.32e-02
2025-10-16 00:00:44,701 | INFO | Epoch 695 Train Time 44.33220982551575s

2025-10-16 00:01:27,766 | INFO | Training epoch 696, Batch 1000/1000: LR=2.20e-05, Loss=3.74e-03 BER=1.54e-03 FER=1.35e-02
2025-10-16 00:01:27,827 | INFO | Epoch 696 Train Time 43.124600648880005s

2025-10-16 00:02:13,440 | INFO | Training epoch 697, Batch 1000/1000: LR=2.19e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.33e-02
2025-10-16 00:02:13,508 | INFO | Epoch 697 Train Time 45.68000411987305s

2025-10-16 00:02:56,984 | INFO | Training epoch 698, Batch 1000/1000: LR=2.18e-05, Loss=3.82e-03 BER=1.60e-03 FER=1.35e-02
2025-10-16 00:02:57,036 | INFO | Epoch 698 Train Time 43.52581977844238s

2025-10-16 00:03:40,497 | INFO | Training epoch 699, Batch 1000/1000: LR=2.17e-05, Loss=3.78e-03 BER=1.57e-03 FER=1.35e-02
2025-10-16 00:03:40,554 | INFO | Epoch 699 Train Time 43.51706027984619s

2025-10-16 00:04:24,299 | INFO | Training epoch 700, Batch 1000/1000: LR=2.15e-05, Loss=3.70e-03 BER=1.53e-03 FER=1.33e-02
2025-10-16 00:04:24,349 | INFO | Epoch 700 Train Time 43.79360032081604s

2025-10-16 00:05:07,938 | INFO | Training epoch 701, Batch 1000/1000: LR=2.14e-05, Loss=3.85e-03 BER=1.60e-03 FER=1.37e-02
2025-10-16 00:05:07,984 | INFO | Epoch 701 Train Time 43.6327383518219s

2025-10-16 00:05:51,999 | INFO | Training epoch 702, Batch 1000/1000: LR=2.13e-05, Loss=3.83e-03 BER=1.55e-03 FER=1.35e-02
2025-10-16 00:05:52,058 | INFO | Epoch 702 Train Time 44.072266817092896s

2025-10-16 00:06:35,624 | INFO | Training epoch 703, Batch 1000/1000: LR=2.12e-05, Loss=3.86e-03 BER=1.56e-03 FER=1.34e-02
2025-10-16 00:06:35,696 | INFO | Epoch 703 Train Time 43.636343240737915s

2025-10-16 00:07:20,464 | INFO | Training epoch 704, Batch 1000/1000: LR=2.10e-05, Loss=3.50e-03 BER=1.43e-03 FER=1.23e-02
2025-10-16 00:07:20,527 | INFO | Epoch 704 Train Time 44.82895827293396s

2025-10-16 00:07:20,528 | INFO | [P1] saving best_model with loss 0.003500 at epoch 704
2025-10-16 00:08:03,708 | INFO | Training epoch 705, Batch 1000/1000: LR=2.09e-05, Loss=3.90e-03 BER=1.61e-03 FER=1.38e-02
2025-10-16 00:08:03,769 | INFO | Epoch 705 Train Time 43.148390769958496s

2025-10-16 00:08:47,852 | INFO | Training epoch 706, Batch 1000/1000: LR=2.08e-05, Loss=3.92e-03 BER=1.60e-03 FER=1.36e-02
2025-10-16 00:08:47,917 | INFO | Epoch 706 Train Time 44.146336793899536s

2025-10-16 00:09:30,947 | INFO | Training epoch 707, Batch 1000/1000: LR=2.07e-05, Loss=3.93e-03 BER=1.64e-03 FER=1.39e-02
2025-10-16 00:09:31,001 | INFO | Epoch 707 Train Time 43.08160090446472s

2025-10-16 00:10:15,105 | INFO | Training epoch 708, Batch 1000/1000: LR=2.05e-05, Loss=3.82e-03 BER=1.55e-03 FER=1.34e-02
2025-10-16 00:10:15,156 | INFO | Epoch 708 Train Time 44.152753591537476s

2025-10-16 00:10:57,952 | INFO | Training epoch 709, Batch 1000/1000: LR=2.04e-05, Loss=3.84e-03 BER=1.60e-03 FER=1.36e-02
2025-10-16 00:10:58,020 | INFO | Epoch 709 Train Time 42.863105058670044s

2025-10-16 00:11:42,842 | INFO | Training epoch 710, Batch 1000/1000: LR=2.03e-05, Loss=3.82e-03 BER=1.60e-03 FER=1.35e-02
2025-10-16 00:11:42,904 | INFO | Epoch 710 Train Time 44.881914377212524s

2025-10-16 00:12:26,303 | INFO | Training epoch 711, Batch 1000/1000: LR=2.02e-05, Loss=3.85e-03 BER=1.59e-03 FER=1.35e-02
2025-10-16 00:12:26,349 | INFO | Epoch 711 Train Time 43.441651344299316s

2025-10-16 00:13:10,038 | INFO | Training epoch 712, Batch 1000/1000: LR=2.00e-05, Loss=3.72e-03 BER=1.53e-03 FER=1.30e-02
2025-10-16 00:13:10,102 | INFO | Epoch 712 Train Time 43.75166153907776s

2025-10-16 00:13:53,983 | INFO | Training epoch 713, Batch 1000/1000: LR=1.99e-05, Loss=3.69e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 00:13:54,034 | INFO | Epoch 713 Train Time 43.93068838119507s

2025-10-16 00:14:37,698 | INFO | Training epoch 714, Batch 1000/1000: LR=1.98e-05, Loss=3.53e-03 BER=1.43e-03 FER=1.25e-02
2025-10-16 00:14:37,760 | INFO | Epoch 714 Train Time 43.72436237335205s

2025-10-16 00:15:21,421 | INFO | Training epoch 715, Batch 1000/1000: LR=1.97e-05, Loss=3.81e-03 BER=1.58e-03 FER=1.33e-02
2025-10-16 00:15:21,471 | INFO | Epoch 715 Train Time 43.70932698249817s

2025-10-16 00:16:05,698 | INFO | Training epoch 716, Batch 1000/1000: LR=1.96e-05, Loss=3.75e-03 BER=1.53e-03 FER=1.33e-02
2025-10-16 00:16:05,762 | INFO | Epoch 716 Train Time 44.289427518844604s

2025-10-16 00:16:48,745 | INFO | Training epoch 717, Batch 1000/1000: LR=1.94e-05, Loss=3.76e-03 BER=1.56e-03 FER=1.33e-02
2025-10-16 00:16:48,813 | INFO | Epoch 717 Train Time 43.04996991157532s

2025-10-16 00:17:31,701 | INFO | Training epoch 718, Batch 1000/1000: LR=1.93e-05, Loss=3.66e-03 BER=1.49e-03 FER=1.28e-02
2025-10-16 00:17:31,753 | INFO | Epoch 718 Train Time 42.938759088516235s

2025-10-16 00:18:14,644 | INFO | Training epoch 719, Batch 1000/1000: LR=1.92e-05, Loss=3.69e-03 BER=1.53e-03 FER=1.31e-02
2025-10-16 00:18:14,712 | INFO | Epoch 719 Train Time 42.95803356170654s

2025-10-16 00:18:58,926 | INFO | Training epoch 720, Batch 1000/1000: LR=1.91e-05, Loss=3.88e-03 BER=1.59e-03 FER=1.37e-02
2025-10-16 00:18:58,985 | INFO | Epoch 720 Train Time 44.269474029541016s

2025-10-16 00:19:42,912 | INFO | Training epoch 721, Batch 1000/1000: LR=1.89e-05, Loss=3.65e-03 BER=1.48e-03 FER=1.29e-02
2025-10-16 00:19:42,968 | INFO | Epoch 721 Train Time 43.98216533660889s

2025-10-16 00:20:26,022 | INFO | Training epoch 722, Batch 1000/1000: LR=1.88e-05, Loss=3.85e-03 BER=1.58e-03 FER=1.34e-02
2025-10-16 00:20:26,075 | INFO | Epoch 722 Train Time 43.104737281799316s

2025-10-16 00:21:09,125 | INFO | Training epoch 723, Batch 1000/1000: LR=1.87e-05, Loss=3.76e-03 BER=1.53e-03 FER=1.32e-02
2025-10-16 00:21:09,181 | INFO | Epoch 723 Train Time 43.10518002510071s

2025-10-16 00:21:52,412 | INFO | Training epoch 724, Batch 1000/1000: LR=1.86e-05, Loss=3.62e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 00:21:52,461 | INFO | Epoch 724 Train Time 43.27844977378845s

2025-10-16 00:22:36,795 | INFO | Training epoch 725, Batch 1000/1000: LR=1.85e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.31e-02
2025-10-16 00:22:36,847 | INFO | Epoch 725 Train Time 44.38519811630249s

2025-10-16 00:23:20,104 | INFO | Training epoch 726, Batch 1000/1000: LR=1.84e-05, Loss=3.76e-03 BER=1.56e-03 FER=1.35e-02
2025-10-16 00:23:20,154 | INFO | Epoch 726 Train Time 43.303701639175415s

2025-10-16 00:24:03,506 | INFO | Training epoch 727, Batch 1000/1000: LR=1.82e-05, Loss=3.63e-03 BER=1.52e-03 FER=1.33e-02
2025-10-16 00:24:03,552 | INFO | Epoch 727 Train Time 43.39631199836731s

2025-10-16 00:24:47,503 | INFO | Training epoch 728, Batch 1000/1000: LR=1.81e-05, Loss=3.77e-03 BER=1.53e-03 FER=1.32e-02
2025-10-16 00:24:47,565 | INFO | Epoch 728 Train Time 44.01208257675171s

2025-10-16 00:25:30,650 | INFO | Training epoch 729, Batch 1000/1000: LR=1.80e-05, Loss=3.71e-03 BER=1.56e-03 FER=1.32e-02
2025-10-16 00:25:30,718 | INFO | Epoch 729 Train Time 43.15164923667908s

2025-10-16 00:26:14,996 | INFO | Training epoch 730, Batch 1000/1000: LR=1.79e-05, Loss=3.74e-03 BER=1.53e-03 FER=1.33e-02
2025-10-16 00:26:15,047 | INFO | Epoch 730 Train Time 44.32749271392822s

2025-10-16 00:26:58,334 | INFO | Training epoch 731, Batch 1000/1000: LR=1.78e-05, Loss=3.67e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 00:26:58,386 | INFO | Epoch 731 Train Time 43.33720660209656s

2025-10-16 00:27:42,701 | INFO | Training epoch 732, Batch 1000/1000: LR=1.76e-05, Loss=3.64e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 00:27:42,745 | INFO | Epoch 732 Train Time 44.35777831077576s

2025-10-16 00:28:25,800 | INFO | Training epoch 733, Batch 1000/1000: LR=1.75e-05, Loss=3.73e-03 BER=1.54e-03 FER=1.32e-02
2025-10-16 00:28:25,861 | INFO | Epoch 733 Train Time 43.11453032493591s

2025-10-16 00:29:09,410 | INFO | Training epoch 734, Batch 1000/1000: LR=1.74e-05, Loss=3.63e-03 BER=1.47e-03 FER=1.29e-02
2025-10-16 00:29:09,485 | INFO | Epoch 734 Train Time 43.62268829345703s

2025-10-16 00:29:54,324 | INFO | Training epoch 735, Batch 1000/1000: LR=1.73e-05, Loss=3.74e-03 BER=1.52e-03 FER=1.31e-02
2025-10-16 00:29:54,400 | INFO | Epoch 735 Train Time 44.913121700286865s

2025-10-16 00:30:38,304 | INFO | Training epoch 736, Batch 1000/1000: LR=1.72e-05, Loss=3.67e-03 BER=1.52e-03 FER=1.33e-02
2025-10-16 00:30:38,369 | INFO | Epoch 736 Train Time 43.96780014038086s

2025-10-16 00:31:21,908 | INFO | Training epoch 737, Batch 1000/1000: LR=1.71e-05, Loss=3.69e-03 BER=1.55e-03 FER=1.29e-02
2025-10-16 00:31:21,963 | INFO | Epoch 737 Train Time 43.592631816864014s

2025-10-16 00:32:05,501 | INFO | Training epoch 738, Batch 1000/1000: LR=1.70e-05, Loss=3.75e-03 BER=1.55e-03 FER=1.33e-02
2025-10-16 00:32:05,569 | INFO | Epoch 738 Train Time 43.605000019073486s

2025-10-16 00:32:48,818 | INFO | Training epoch 739, Batch 1000/1000: LR=1.68e-05, Loss=3.70e-03 BER=1.55e-03 FER=1.33e-02
2025-10-16 00:32:48,864 | INFO | Epoch 739 Train Time 43.29362654685974s

2025-10-16 00:33:33,033 | INFO | Training epoch 740, Batch 1000/1000: LR=1.67e-05, Loss=3.64e-03 BER=1.49e-03 FER=1.30e-02
2025-10-16 00:33:33,093 | INFO | Epoch 740 Train Time 44.22730326652527s

2025-10-16 00:34:15,119 | INFO | Training epoch 741, Batch 1000/1000: LR=1.66e-05, Loss=3.87e-03 BER=1.56e-03 FER=1.36e-02
2025-10-16 00:34:15,159 | INFO | Epoch 741 Train Time 42.06483221054077s

2025-10-16 00:34:58,498 | INFO | Training epoch 742, Batch 1000/1000: LR=1.65e-05, Loss=3.63e-03 BER=1.48e-03 FER=1.29e-02
2025-10-16 00:34:58,546 | INFO | Epoch 742 Train Time 43.38606667518616s

2025-10-16 00:35:42,305 | INFO | Training epoch 743, Batch 1000/1000: LR=1.64e-05, Loss=3.77e-03 BER=1.54e-03 FER=1.33e-02
2025-10-16 00:35:42,353 | INFO | Epoch 743 Train Time 43.805464029312134s

2025-10-16 00:36:25,943 | INFO | Training epoch 744, Batch 1000/1000: LR=1.63e-05, Loss=3.83e-03 BER=1.57e-03 FER=1.35e-02
2025-10-16 00:36:26,017 | INFO | Epoch 744 Train Time 43.66189646720886s

2025-10-16 00:37:09,405 | INFO | Training epoch 745, Batch 1000/1000: LR=1.62e-05, Loss=3.76e-03 BER=1.56e-03 FER=1.33e-02
2025-10-16 00:37:09,460 | INFO | Epoch 745 Train Time 43.441044092178345s

2025-10-16 00:37:52,142 | INFO | Training epoch 746, Batch 1000/1000: LR=1.61e-05, Loss=3.91e-03 BER=1.61e-03 FER=1.37e-02
2025-10-16 00:37:52,214 | INFO | Epoch 746 Train Time 42.750425815582275s

2025-10-16 00:38:35,555 | INFO | Training epoch 747, Batch 1000/1000: LR=1.59e-05, Loss=3.77e-03 BER=1.55e-03 FER=1.33e-02
2025-10-16 00:38:35,619 | INFO | Epoch 747 Train Time 43.40380644798279s

2025-10-16 00:39:18,573 | INFO | Training epoch 748, Batch 1000/1000: LR=1.58e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.36e-02
2025-10-16 00:39:18,621 | INFO | Epoch 748 Train Time 42.99904203414917s

2025-10-16 00:40:03,098 | INFO | Training epoch 749, Batch 1000/1000: LR=1.57e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.32e-02
2025-10-16 00:40:03,155 | INFO | Epoch 749 Train Time 44.53237199783325s

2025-10-16 00:40:47,075 | INFO | Training epoch 750, Batch 1000/1000: LR=1.56e-05, Loss=3.65e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 00:40:47,144 | INFO | Epoch 750 Train Time 43.98658037185669s

2025-10-16 00:41:30,838 | INFO | Training epoch 751, Batch 1000/1000: LR=1.55e-05, Loss=3.91e-03 BER=1.61e-03 FER=1.36e-02
2025-10-16 00:41:30,890 | INFO | Epoch 751 Train Time 43.74299740791321s

2025-10-16 00:42:12,006 | INFO | Training epoch 752, Batch 1000/1000: LR=1.54e-05, Loss=3.85e-03 BER=1.57e-03 FER=1.35e-02
2025-10-16 00:42:12,056 | INFO | Epoch 752 Train Time 41.16456174850464s

2025-10-16 00:42:55,476 | INFO | Training epoch 753, Batch 1000/1000: LR=1.53e-05, Loss=3.73e-03 BER=1.53e-03 FER=1.29e-02
2025-10-16 00:42:55,547 | INFO | Epoch 753 Train Time 43.488595724105835s

2025-10-16 00:43:39,369 | INFO | Training epoch 754, Batch 1000/1000: LR=1.52e-05, Loss=3.70e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 00:43:39,427 | INFO | Epoch 754 Train Time 43.87764263153076s

2025-10-16 00:44:22,730 | INFO | Training epoch 755, Batch 1000/1000: LR=1.51e-05, Loss=3.78e-03 BER=1.57e-03 FER=1.34e-02
2025-10-16 00:44:22,779 | INFO | Epoch 755 Train Time 43.349979639053345s

2025-10-16 00:45:05,520 | INFO | Training epoch 756, Batch 1000/1000: LR=1.50e-05, Loss=3.78e-03 BER=1.58e-03 FER=1.34e-02
2025-10-16 00:45:05,574 | INFO | Epoch 756 Train Time 42.793750286102295s

2025-10-16 00:45:48,859 | INFO | Training epoch 757, Batch 1000/1000: LR=1.48e-05, Loss=3.72e-03 BER=1.52e-03 FER=1.31e-02
2025-10-16 00:45:48,918 | INFO | Epoch 757 Train Time 43.3433096408844s

2025-10-16 00:46:31,665 | INFO | Training epoch 758, Batch 1000/1000: LR=1.47e-05, Loss=3.55e-03 BER=1.45e-03 FER=1.27e-02
2025-10-16 00:46:31,723 | INFO | Epoch 758 Train Time 42.8013436794281s

2025-10-16 00:47:15,837 | INFO | Training epoch 759, Batch 1000/1000: LR=1.46e-05, Loss=3.68e-03 BER=1.53e-03 FER=1.34e-02
2025-10-16 00:47:15,898 | INFO | Epoch 759 Train Time 44.173283100128174s

2025-10-16 00:47:57,874 | INFO | Training epoch 760, Batch 1000/1000: LR=1.45e-05, Loss=3.66e-03 BER=1.51e-03 FER=1.32e-02
2025-10-16 00:47:57,941 | INFO | Epoch 760 Train Time 42.04139471054077s

2025-10-16 00:48:40,604 | INFO | Training epoch 761, Batch 1000/1000: LR=1.44e-05, Loss=3.75e-03 BER=1.54e-03 FER=1.32e-02
2025-10-16 00:48:40,665 | INFO | Epoch 761 Train Time 42.72336268424988s

2025-10-16 00:49:23,555 | INFO | Training epoch 762, Batch 1000/1000: LR=1.43e-05, Loss=3.79e-03 BER=1.55e-03 FER=1.31e-02
2025-10-16 00:49:23,623 | INFO | Epoch 762 Train Time 42.956547498703s

2025-10-16 00:50:08,208 | INFO | Training epoch 763, Batch 1000/1000: LR=1.42e-05, Loss=3.55e-03 BER=1.45e-03 FER=1.25e-02
2025-10-16 00:50:08,288 | INFO | Epoch 763 Train Time 44.662166357040405s

2025-10-16 00:50:51,199 | INFO | Training epoch 764, Batch 1000/1000: LR=1.41e-05, Loss=3.74e-03 BER=1.56e-03 FER=1.32e-02
2025-10-16 00:50:51,267 | INFO | Epoch 764 Train Time 42.97695755958557s

2025-10-16 00:51:34,342 | INFO | Training epoch 765, Batch 1000/1000: LR=1.40e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.31e-02
2025-10-16 00:51:34,432 | INFO | Epoch 765 Train Time 43.16294980049133s

2025-10-16 00:52:18,713 | INFO | Training epoch 766, Batch 1000/1000: LR=1.39e-05, Loss=3.61e-03 BER=1.50e-03 FER=1.27e-02
2025-10-16 00:52:18,764 | INFO | Epoch 766 Train Time 44.32932662963867s

2025-10-16 00:53:02,407 | INFO | Training epoch 767, Batch 1000/1000: LR=1.38e-05, Loss=3.79e-03 BER=1.56e-03 FER=1.29e-02
2025-10-16 00:53:02,457 | INFO | Epoch 767 Train Time 43.69221806526184s

2025-10-16 00:53:46,120 | INFO | Training epoch 768, Batch 1000/1000: LR=1.37e-05, Loss=3.76e-03 BER=1.53e-03 FER=1.33e-02
2025-10-16 00:53:46,172 | INFO | Epoch 768 Train Time 43.713740825653076s

2025-10-16 00:54:29,527 | INFO | Training epoch 769, Batch 1000/1000: LR=1.36e-05, Loss=3.49e-03 BER=1.45e-03 FER=1.27e-02
2025-10-16 00:54:29,583 | INFO | Epoch 769 Train Time 43.409259557724s

2025-10-16 00:54:29,584 | INFO | [P1] saving best_model with loss 0.003492 at epoch 769
2025-10-16 00:55:13,206 | INFO | Training epoch 770, Batch 1000/1000: LR=1.35e-05, Loss=3.84e-03 BER=1.58e-03 FER=1.34e-02
2025-10-16 00:55:13,258 | INFO | Epoch 770 Train Time 43.56132435798645s

2025-10-16 00:55:57,818 | INFO | Training epoch 771, Batch 1000/1000: LR=1.34e-05, Loss=3.57e-03 BER=1.47e-03 FER=1.25e-02
2025-10-16 00:55:57,867 | INFO | Epoch 771 Train Time 44.60696721076965s

2025-10-16 00:56:41,719 | INFO | Training epoch 772, Batch 1000/1000: LR=1.33e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.31e-02
2025-10-16 00:56:41,763 | INFO | Epoch 772 Train Time 43.89457178115845s

2025-10-16 00:57:24,642 | INFO | Training epoch 773, Batch 1000/1000: LR=1.32e-05, Loss=3.77e-03 BER=1.54e-03 FER=1.33e-02
2025-10-16 00:57:24,700 | INFO | Epoch 773 Train Time 42.93566823005676s

2025-10-16 00:58:08,108 | INFO | Training epoch 774, Batch 1000/1000: LR=1.31e-05, Loss=3.70e-03 BER=1.51e-03 FER=1.29e-02
2025-10-16 00:58:08,158 | INFO | Epoch 774 Train Time 43.45623016357422s

2025-10-16 00:58:50,733 | INFO | Training epoch 775, Batch 1000/1000: LR=1.30e-05, Loss=3.78e-03 BER=1.57e-03 FER=1.34e-02
2025-10-16 00:58:50,784 | INFO | Epoch 775 Train Time 42.62487077713013s

2025-10-16 00:59:33,641 | INFO | Training epoch 776, Batch 1000/1000: LR=1.29e-05, Loss=3.68e-03 BER=1.50e-03 FER=1.27e-02
2025-10-16 00:59:33,699 | INFO | Epoch 776 Train Time 42.91301703453064s

2025-10-16 01:00:16,801 | INFO | Training epoch 777, Batch 1000/1000: LR=1.28e-05, Loss=3.85e-03 BER=1.57e-03 FER=1.32e-02
2025-10-16 01:00:16,856 | INFO | Epoch 777 Train Time 43.154128313064575s

2025-10-16 01:00:59,408 | INFO | Training epoch 778, Batch 1000/1000: LR=1.27e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.34e-02
2025-10-16 01:00:59,457 | INFO | Epoch 778 Train Time 42.59945726394653s

2025-10-16 01:01:40,742 | INFO | Training epoch 779, Batch 1000/1000: LR=1.26e-05, Loss=3.66e-03 BER=1.53e-03 FER=1.29e-02
2025-10-16 01:01:40,797 | INFO | Epoch 779 Train Time 41.33854150772095s

2025-10-16 01:02:24,100 | INFO | Training epoch 780, Batch 1000/1000: LR=1.25e-05, Loss=3.70e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 01:02:24,169 | INFO | Epoch 780 Train Time 43.37118053436279s

2025-10-16 01:03:07,603 | INFO | Training epoch 781, Batch 1000/1000: LR=1.24e-05, Loss=3.71e-03 BER=1.54e-03 FER=1.30e-02
2025-10-16 01:03:07,653 | INFO | Epoch 781 Train Time 43.48229646682739s

2025-10-16 01:03:50,607 | INFO | Training epoch 782, Batch 1000/1000: LR=1.23e-05, Loss=3.66e-03 BER=1.51e-03 FER=1.30e-02
2025-10-16 01:03:50,662 | INFO | Epoch 782 Train Time 43.00817012786865s

2025-10-16 01:04:34,104 | INFO | Training epoch 783, Batch 1000/1000: LR=1.22e-05, Loss=3.68e-03 BER=1.53e-03 FER=1.31e-02
2025-10-16 01:04:34,162 | INFO | Epoch 783 Train Time 43.498291015625s

2025-10-16 01:05:15,246 | INFO | Training epoch 784, Batch 1000/1000: LR=1.21e-05, Loss=3.50e-03 BER=1.44e-03 FER=1.24e-02
2025-10-16 01:05:15,310 | INFO | Epoch 784 Train Time 41.14633870124817s

2025-10-16 01:05:59,040 | INFO | Training epoch 785, Batch 1000/1000: LR=1.20e-05, Loss=3.95e-03 BER=1.62e-03 FER=1.37e-02
2025-10-16 01:05:59,086 | INFO | Epoch 785 Train Time 43.77398204803467s

2025-10-16 01:06:39,956 | INFO | Training epoch 786, Batch 1000/1000: LR=1.19e-05, Loss=3.70e-03 BER=1.53e-03 FER=1.30e-02
2025-10-16 01:06:40,026 | INFO | Epoch 786 Train Time 40.939133167266846s

2025-10-16 01:07:22,118 | INFO | Training epoch 787, Batch 1000/1000: LR=1.18e-05, Loss=3.65e-03 BER=1.48e-03 FER=1.29e-02
2025-10-16 01:07:22,163 | INFO | Epoch 787 Train Time 42.13382387161255s

2025-10-16 01:08:03,703 | INFO | Training epoch 788, Batch 1000/1000: LR=1.17e-05, Loss=3.75e-03 BER=1.56e-03 FER=1.32e-02
2025-10-16 01:08:03,779 | INFO | Epoch 788 Train Time 41.61506104469299s

2025-10-16 01:08:45,362 | INFO | Training epoch 789, Batch 1000/1000: LR=1.16e-05, Loss=3.57e-03 BER=1.49e-03 FER=1.28e-02
2025-10-16 01:08:45,437 | INFO | Epoch 789 Train Time 41.65563464164734s

2025-10-16 01:09:28,603 | INFO | Training epoch 790, Batch 1000/1000: LR=1.15e-05, Loss=3.59e-03 BER=1.45e-03 FER=1.26e-02
2025-10-16 01:09:28,657 | INFO | Epoch 790 Train Time 43.21883821487427s

2025-10-16 01:10:12,307 | INFO | Training epoch 791, Batch 1000/1000: LR=1.14e-05, Loss=3.56e-03 BER=1.44e-03 FER=1.22e-02
2025-10-16 01:10:12,366 | INFO | Epoch 791 Train Time 43.70764946937561s

2025-10-16 01:10:55,699 | INFO | Training epoch 792, Batch 1000/1000: LR=1.13e-05, Loss=3.69e-03 BER=1.52e-03 FER=1.32e-02
2025-10-16 01:10:55,761 | INFO | Epoch 792 Train Time 43.39394164085388s

2025-10-16 01:11:38,613 | INFO | Training epoch 793, Batch 1000/1000: LR=1.12e-05, Loss=3.67e-03 BER=1.49e-03 FER=1.28e-02
2025-10-16 01:11:38,658 | INFO | Epoch 793 Train Time 42.89588260650635s

2025-10-16 01:12:22,414 | INFO | Training epoch 794, Batch 1000/1000: LR=1.11e-05, Loss=3.79e-03 BER=1.57e-03 FER=1.32e-02
2025-10-16 01:12:22,473 | INFO | Epoch 794 Train Time 43.813639879226685s

2025-10-16 01:13:06,097 | INFO | Training epoch 795, Batch 1000/1000: LR=1.10e-05, Loss=3.63e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 01:13:06,156 | INFO | Epoch 795 Train Time 43.681538343429565s

2025-10-16 01:13:48,925 | INFO | Training epoch 796, Batch 1000/1000: LR=1.09e-05, Loss=3.77e-03 BER=1.54e-03 FER=1.33e-02
2025-10-16 01:13:48,993 | INFO | Epoch 796 Train Time 42.83509039878845s

2025-10-16 01:14:32,230 | INFO | Training epoch 797, Batch 1000/1000: LR=1.08e-05, Loss=3.58e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 01:14:32,273 | INFO | Epoch 797 Train Time 43.27890229225159s

2025-10-16 01:15:15,626 | INFO | Training epoch 798, Batch 1000/1000: LR=1.07e-05, Loss=3.68e-03 BER=1.53e-03 FER=1.31e-02
2025-10-16 01:15:15,669 | INFO | Epoch 798 Train Time 43.393367290496826s

2025-10-16 01:15:59,016 | INFO | Training epoch 799, Batch 1000/1000: LR=1.06e-05, Loss=3.70e-03 BER=1.54e-03 FER=1.32e-02
2025-10-16 01:15:59,080 | INFO | Epoch 799 Train Time 43.4102087020874s

2025-10-16 01:16:42,233 | INFO | Training epoch 800, Batch 1000/1000: LR=1.05e-05, Loss=3.71e-03 BER=1.53e-03 FER=1.29e-02
2025-10-16 01:16:42,292 | INFO | Epoch 800 Train Time 43.20970630645752s

2025-10-16 01:17:25,808 | INFO | Training epoch 801, Batch 1000/1000: LR=1.05e-05, Loss=3.59e-03 BER=1.47e-03 FER=1.27e-02
2025-10-16 01:17:25,865 | INFO | Epoch 801 Train Time 43.57032799720764s

2025-10-16 01:18:10,519 | INFO | Training epoch 802, Batch 1000/1000: LR=1.04e-05, Loss=3.70e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 01:18:10,570 | INFO | Epoch 802 Train Time 44.704132080078125s

2025-10-16 01:18:55,121 | INFO | Training epoch 803, Batch 1000/1000: LR=1.03e-05, Loss=3.70e-03 BER=1.53e-03 FER=1.31e-02
2025-10-16 01:18:55,161 | INFO | Epoch 803 Train Time 44.58958172798157s

2025-10-16 01:19:39,680 | INFO | Training epoch 804, Batch 1000/1000: LR=1.02e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.32e-02
2025-10-16 01:19:39,753 | INFO | Epoch 804 Train Time 44.59027171134949s

2025-10-16 01:20:24,401 | INFO | Training epoch 805, Batch 1000/1000: LR=1.01e-05, Loss=3.64e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 01:20:24,450 | INFO | Epoch 805 Train Time 44.696457624435425s

2025-10-16 01:21:04,823 | INFO | Training epoch 806, Batch 1000/1000: LR=1.00e-05, Loss=3.65e-03 BER=1.51e-03 FER=1.27e-02
2025-10-16 01:21:04,892 | INFO | Epoch 806 Train Time 40.44027900695801s

2025-10-16 01:21:46,597 | INFO | Training epoch 807, Batch 1000/1000: LR=9.91e-06, Loss=3.74e-03 BER=1.55e-03 FER=1.33e-02
2025-10-16 01:21:46,641 | INFO | Epoch 807 Train Time 41.746495485305786s

2025-10-16 01:22:30,328 | INFO | Training epoch 808, Batch 1000/1000: LR=9.82e-06, Loss=3.58e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 01:22:30,377 | INFO | Epoch 808 Train Time 43.73493456840515s

2025-10-16 01:23:14,739 | INFO | Training epoch 809, Batch 1000/1000: LR=9.74e-06, Loss=3.66e-03 BER=1.50e-03 FER=1.31e-02
2025-10-16 01:23:14,783 | INFO | Epoch 809 Train Time 44.40509796142578s

2025-10-16 01:23:58,438 | INFO | Training epoch 810, Batch 1000/1000: LR=9.65e-06, Loss=3.76e-03 BER=1.54e-03 FER=1.32e-02
2025-10-16 01:23:58,496 | INFO | Epoch 810 Train Time 43.7117977142334s

2025-10-16 01:24:42,695 | INFO | Training epoch 811, Batch 1000/1000: LR=9.56e-06, Loss=3.62e-03 BER=1.50e-03 FER=1.30e-02
2025-10-16 01:24:42,741 | INFO | Epoch 811 Train Time 44.24271368980408s

2025-10-16 01:25:26,315 | INFO | Training epoch 812, Batch 1000/1000: LR=9.47e-06, Loss=3.64e-03 BER=1.47e-03 FER=1.28e-02
2025-10-16 01:25:26,365 | INFO | Epoch 812 Train Time 43.62289881706238s

2025-10-16 01:26:09,169 | INFO | Training epoch 813, Batch 1000/1000: LR=9.39e-06, Loss=3.55e-03 BER=1.48e-03 FER=1.26e-02
2025-10-16 01:26:09,211 | INFO | Epoch 813 Train Time 42.84427094459534s

2025-10-16 01:26:53,216 | INFO | Training epoch 814, Batch 1000/1000: LR=9.30e-06, Loss=3.79e-03 BER=1.55e-03 FER=1.32e-02
2025-10-16 01:26:53,265 | INFO | Epoch 814 Train Time 44.05289053916931s

2025-10-16 01:27:36,706 | INFO | Training epoch 815, Batch 1000/1000: LR=9.21e-06, Loss=3.45e-03 BER=1.42e-03 FER=1.21e-02
2025-10-16 01:27:36,751 | INFO | Epoch 815 Train Time 43.4844069480896s

2025-10-16 01:27:36,751 | INFO | [P1] saving best_model with loss 0.003455 at epoch 815
2025-10-16 01:28:20,404 | INFO | Training epoch 816, Batch 1000/1000: LR=9.13e-06, Loss=3.54e-03 BER=1.44e-03 FER=1.25e-02
2025-10-16 01:28:20,472 | INFO | Epoch 816 Train Time 43.628817558288574s

2025-10-16 01:29:03,899 | INFO | Training epoch 817, Batch 1000/1000: LR=9.04e-06, Loss=3.72e-03 BER=1.55e-03 FER=1.30e-02
2025-10-16 01:29:03,944 | INFO | Epoch 817 Train Time 43.47008204460144s

2025-10-16 01:29:47,416 | INFO | Training epoch 818, Batch 1000/1000: LR=8.96e-06, Loss=3.82e-03 BER=1.57e-03 FER=1.33e-02
2025-10-16 01:29:47,459 | INFO | Epoch 818 Train Time 43.51250386238098s

2025-10-16 01:30:29,989 | INFO | Training epoch 819, Batch 1000/1000: LR=8.87e-06, Loss=3.84e-03 BER=1.56e-03 FER=1.34e-02
2025-10-16 01:30:30,032 | INFO | Epoch 819 Train Time 42.57172226905823s

2025-10-16 01:31:13,201 | INFO | Training epoch 820, Batch 1000/1000: LR=8.79e-06, Loss=3.89e-03 BER=1.62e-03 FER=1.40e-02
2025-10-16 01:31:13,260 | INFO | Epoch 820 Train Time 43.22582221031189s

2025-10-16 01:31:56,027 | INFO | Training epoch 821, Batch 1000/1000: LR=8.71e-06, Loss=3.53e-03 BER=1.46e-03 FER=1.26e-02
2025-10-16 01:31:56,091 | INFO | Epoch 821 Train Time 42.82876515388489s

2025-10-16 01:32:39,130 | INFO | Training epoch 822, Batch 1000/1000: LR=8.62e-06, Loss=3.66e-03 BER=1.53e-03 FER=1.30e-02
2025-10-16 01:32:39,184 | INFO | Epoch 822 Train Time 43.09108757972717s

2025-10-16 01:33:23,104 | INFO | Training epoch 823, Batch 1000/1000: LR=8.54e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 01:33:23,146 | INFO | Epoch 823 Train Time 43.96083402633667s

2025-10-16 01:34:06,511 | INFO | Training epoch 824, Batch 1000/1000: LR=8.46e-06, Loss=3.73e-03 BER=1.54e-03 FER=1.28e-02
2025-10-16 01:34:06,556 | INFO | Epoch 824 Train Time 43.40862512588501s

2025-10-16 01:34:49,917 | INFO | Training epoch 825, Batch 1000/1000: LR=8.38e-06, Loss=3.73e-03 BER=1.53e-03 FER=1.30e-02
2025-10-16 01:34:49,984 | INFO | Epoch 825 Train Time 43.4261794090271s

2025-10-16 01:35:35,050 | INFO | Training epoch 826, Batch 1000/1000: LR=8.29e-06, Loss=3.74e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 01:35:35,113 | INFO | Epoch 826 Train Time 45.126930713653564s

2025-10-16 01:36:18,633 | INFO | Training epoch 827, Batch 1000/1000: LR=8.21e-06, Loss=3.71e-03 BER=1.55e-03 FER=1.31e-02
2025-10-16 01:36:18,694 | INFO | Epoch 827 Train Time 43.5786075592041s

2025-10-16 01:37:02,881 | INFO | Training epoch 828, Batch 1000/1000: LR=8.13e-06, Loss=3.69e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 01:37:02,955 | INFO | Epoch 828 Train Time 44.259196758270264s

2025-10-16 01:37:46,600 | INFO | Training epoch 829, Batch 1000/1000: LR=8.05e-06, Loss=3.69e-03 BER=1.52e-03 FER=1.28e-02
2025-10-16 01:37:46,652 | INFO | Epoch 829 Train Time 43.694913148880005s

2025-10-16 01:38:30,424 | INFO | Training epoch 830, Batch 1000/1000: LR=7.97e-06, Loss=3.57e-03 BER=1.48e-03 FER=1.26e-02
2025-10-16 01:38:30,484 | INFO | Epoch 830 Train Time 43.83057117462158s

2025-10-16 01:39:13,831 | INFO | Training epoch 831, Batch 1000/1000: LR=7.89e-06, Loss=3.52e-03 BER=1.43e-03 FER=1.23e-02
2025-10-16 01:39:13,896 | INFO | Epoch 831 Train Time 43.410245418548584s

2025-10-16 01:39:57,961 | INFO | Training epoch 832, Batch 1000/1000: LR=7.81e-06, Loss=3.48e-03 BER=1.42e-03 FER=1.22e-02
2025-10-16 01:39:58,007 | INFO | Epoch 832 Train Time 44.109899282455444s

2025-10-16 01:40:40,924 | INFO | Training epoch 833, Batch 1000/1000: LR=7.74e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 01:40:40,967 | INFO | Epoch 833 Train Time 42.958329916000366s

2025-10-16 01:41:24,900 | INFO | Training epoch 834, Batch 1000/1000: LR=7.66e-06, Loss=3.69e-03 BER=1.50e-03 FER=1.26e-02
2025-10-16 01:41:24,936 | INFO | Epoch 834 Train Time 43.967591285705566s

2025-10-16 01:42:08,315 | INFO | Training epoch 835, Batch 1000/1000: LR=7.58e-06, Loss=3.68e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 01:42:08,369 | INFO | Epoch 835 Train Time 43.4308602809906s

2025-10-16 01:42:53,415 | INFO | Training epoch 836, Batch 1000/1000: LR=7.50e-06, Loss=3.56e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 01:42:53,466 | INFO | Epoch 836 Train Time 45.095569133758545s

2025-10-16 01:43:36,407 | INFO | Training epoch 837, Batch 1000/1000: LR=7.43e-06, Loss=3.67e-03 BER=1.50e-03 FER=1.30e-02
2025-10-16 01:43:36,461 | INFO | Epoch 837 Train Time 42.993252992630005s

2025-10-16 01:44:19,611 | INFO | Training epoch 838, Batch 1000/1000: LR=7.35e-06, Loss=3.70e-03 BER=1.49e-03 FER=1.29e-02
2025-10-16 01:44:19,671 | INFO | Epoch 838 Train Time 43.20760464668274s

2025-10-16 01:45:03,757 | INFO | Training epoch 839, Batch 1000/1000: LR=7.27e-06, Loss=3.75e-03 BER=1.56e-03 FER=1.32e-02
2025-10-16 01:45:03,822 | INFO | Epoch 839 Train Time 44.14962458610535s

2025-10-16 01:45:48,328 | INFO | Training epoch 840, Batch 1000/1000: LR=7.20e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.28e-02
2025-10-16 01:45:48,367 | INFO | Epoch 840 Train Time 44.541804790496826s

2025-10-16 01:46:34,427 | INFO | Training epoch 841, Batch 1000/1000: LR=7.12e-06, Loss=3.76e-03 BER=1.54e-03 FER=1.32e-02
2025-10-16 01:46:34,461 | INFO | Epoch 841 Train Time 46.09156346321106s

2025-10-16 01:47:19,057 | INFO | Training epoch 842, Batch 1000/1000: LR=7.05e-06, Loss=3.61e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 01:47:19,122 | INFO | Epoch 842 Train Time 44.65963292121887s

2025-10-16 01:48:02,297 | INFO | Training epoch 843, Batch 1000/1000: LR=6.97e-06, Loss=3.58e-03 BER=1.50e-03 FER=1.25e-02
2025-10-16 01:48:02,341 | INFO | Epoch 843 Train Time 43.21726679801941s

2025-10-16 01:48:45,205 | INFO | Training epoch 844, Batch 1000/1000: LR=6.90e-06, Loss=3.67e-03 BER=1.49e-03 FER=1.29e-02
2025-10-16 01:48:45,259 | INFO | Epoch 844 Train Time 42.91580128669739s

2025-10-16 01:49:27,940 | INFO | Training epoch 845, Batch 1000/1000: LR=6.83e-06, Loss=3.70e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 01:49:28,015 | INFO | Epoch 845 Train Time 42.75468039512634s

2025-10-16 01:50:11,805 | INFO | Training epoch 846, Batch 1000/1000: LR=6.75e-06, Loss=3.58e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 01:50:11,856 | INFO | Epoch 846 Train Time 43.83914852142334s

2025-10-16 01:50:55,282 | INFO | Training epoch 847, Batch 1000/1000: LR=6.68e-06, Loss=3.58e-03 BER=1.49e-03 FER=1.26e-02
2025-10-16 01:50:55,328 | INFO | Epoch 847 Train Time 43.47133779525757s

2025-10-16 01:51:38,694 | INFO | Training epoch 848, Batch 1000/1000: LR=6.61e-06, Loss=3.53e-03 BER=1.44e-03 FER=1.24e-02
2025-10-16 01:51:38,750 | INFO | Epoch 848 Train Time 43.42025828361511s

2025-10-16 01:52:21,806 | INFO | Training epoch 849, Batch 1000/1000: LR=6.54e-06, Loss=3.65e-03 BER=1.50e-03 FER=1.28e-02
2025-10-16 01:52:21,865 | INFO | Epoch 849 Train Time 43.11403942108154s

2025-10-16 01:53:04,804 | INFO | Training epoch 850, Batch 1000/1000: LR=6.47e-06, Loss=3.59e-03 BER=1.46e-03 FER=1.26e-02
2025-10-16 01:53:04,861 | INFO | Epoch 850 Train Time 42.99471092224121s

2025-10-16 01:53:47,023 | INFO | Training epoch 851, Batch 1000/1000: LR=6.40e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.26e-02
2025-10-16 01:53:47,073 | INFO | Epoch 851 Train Time 42.20630860328674s

2025-10-16 01:54:29,217 | INFO | Training epoch 852, Batch 1000/1000: LR=6.32e-06, Loss=3.68e-03 BER=1.50e-03 FER=1.30e-02
2025-10-16 01:54:29,282 | INFO | Epoch 852 Train Time 42.20779514312744s

2025-10-16 01:55:12,220 | INFO | Training epoch 853, Batch 1000/1000: LR=6.25e-06, Loss=3.64e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 01:55:12,284 | INFO | Epoch 853 Train Time 43.00066041946411s

2025-10-16 01:55:55,308 | INFO | Training epoch 854, Batch 1000/1000: LR=6.19e-06, Loss=3.52e-03 BER=1.49e-03 FER=1.24e-02
2025-10-16 01:55:55,349 | INFO | Epoch 854 Train Time 43.06272077560425s

2025-10-16 01:56:37,905 | INFO | Training epoch 855, Batch 1000/1000: LR=6.12e-06, Loss=3.75e-03 BER=1.55e-03 FER=1.32e-02
2025-10-16 01:56:37,956 | INFO | Epoch 855 Train Time 42.604751110076904s

2025-10-16 01:57:20,699 | INFO | Training epoch 856, Batch 1000/1000: LR=6.05e-06, Loss=3.58e-03 BER=1.45e-03 FER=1.24e-02
2025-10-16 01:57:20,745 | INFO | Epoch 856 Train Time 42.78725719451904s

2025-10-16 01:58:03,706 | INFO | Training epoch 857, Batch 1000/1000: LR=5.98e-06, Loss=3.56e-03 BER=1.45e-03 FER=1.25e-02
2025-10-16 01:58:03,759 | INFO | Epoch 857 Train Time 43.01346826553345s

2025-10-16 01:58:46,702 | INFO | Training epoch 858, Batch 1000/1000: LR=5.91e-06, Loss=3.59e-03 BER=1.48e-03 FER=1.29e-02
2025-10-16 01:58:46,753 | INFO | Epoch 858 Train Time 42.99233269691467s

2025-10-16 01:59:29,309 | INFO | Training epoch 859, Batch 1000/1000: LR=5.84e-06, Loss=3.66e-03 BER=1.51e-03 FER=1.27e-02
2025-10-16 01:59:29,358 | INFO | Epoch 859 Train Time 42.60233998298645s

2025-10-16 02:00:13,213 | INFO | Training epoch 860, Batch 1000/1000: LR=5.78e-06, Loss=3.83e-03 BER=1.58e-03 FER=1.33e-02
2025-10-16 02:00:13,261 | INFO | Epoch 860 Train Time 43.90225148200989s

2025-10-16 02:00:55,503 | INFO | Training epoch 861, Batch 1000/1000: LR=5.71e-06, Loss=3.76e-03 BER=1.54e-03 FER=1.33e-02
2025-10-16 02:00:55,552 | INFO | Epoch 861 Train Time 42.290061712265015s

2025-10-16 02:01:38,400 | INFO | Training epoch 862, Batch 1000/1000: LR=5.65e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.27e-02
2025-10-16 02:01:38,444 | INFO | Epoch 862 Train Time 42.889118671417236s

2025-10-16 02:02:21,307 | INFO | Training epoch 863, Batch 1000/1000: LR=5.58e-06, Loss=3.67e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 02:02:21,365 | INFO | Epoch 863 Train Time 42.91976571083069s

2025-10-16 02:03:04,798 | INFO | Training epoch 864, Batch 1000/1000: LR=5.51e-06, Loss=3.59e-03 BER=1.49e-03 FER=1.26e-02
2025-10-16 02:03:04,849 | INFO | Epoch 864 Train Time 43.482699155807495s

2025-10-16 02:03:47,703 | INFO | Training epoch 865, Batch 1000/1000: LR=5.45e-06, Loss=3.56e-03 BER=1.47e-03 FER=1.27e-02
2025-10-16 02:03:47,754 | INFO | Epoch 865 Train Time 42.903629541397095s

2025-10-16 02:04:31,602 | INFO | Training epoch 866, Batch 1000/1000: LR=5.39e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 02:04:31,645 | INFO | Epoch 866 Train Time 43.88980412483215s

2025-10-16 02:05:13,199 | INFO | Training epoch 867, Batch 1000/1000: LR=5.32e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.24e-02
2025-10-16 02:05:13,245 | INFO | Epoch 867 Train Time 41.597824573516846s

2025-10-16 02:05:55,912 | INFO | Training epoch 868, Batch 1000/1000: LR=5.26e-06, Loss=3.61e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 02:05:55,953 | INFO | Epoch 868 Train Time 42.70642018318176s

2025-10-16 02:06:38,398 | INFO | Training epoch 869, Batch 1000/1000: LR=5.20e-06, Loss=3.63e-03 BER=1.50e-03 FER=1.27e-02
2025-10-16 02:06:38,447 | INFO | Epoch 869 Train Time 42.49325156211853s

2025-10-16 02:07:22,346 | INFO | Training epoch 870, Batch 1000/1000: LR=5.13e-06, Loss=3.72e-03 BER=1.50e-03 FER=1.28e-02
2025-10-16 02:07:22,419 | INFO | Epoch 870 Train Time 43.971190452575684s

2025-10-16 02:08:05,605 | INFO | Training epoch 871, Batch 1000/1000: LR=5.07e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.27e-02
2025-10-16 02:08:05,658 | INFO | Epoch 871 Train Time 43.2369065284729s

2025-10-16 02:08:48,703 | INFO | Training epoch 872, Batch 1000/1000: LR=5.01e-06, Loss=3.33e-03 BER=1.36e-03 FER=1.17e-02
2025-10-16 02:08:48,750 | INFO | Epoch 872 Train Time 43.090988874435425s

2025-10-16 02:08:48,751 | INFO | [P1] saving best_model with loss 0.003330 at epoch 872
2025-10-16 02:09:32,101 | INFO | Training epoch 873, Batch 1000/1000: LR=4.95e-06, Loss=3.59e-03 BER=1.47e-03 FER=1.25e-02
2025-10-16 02:09:32,143 | INFO | Epoch 873 Train Time 43.29240703582764s

2025-10-16 02:10:14,708 | INFO | Training epoch 874, Batch 1000/1000: LR=4.89e-06, Loss=3.61e-03 BER=1.47e-03 FER=1.27e-02
2025-10-16 02:10:14,757 | INFO | Epoch 874 Train Time 42.613280057907104s

2025-10-16 02:10:58,409 | INFO | Training epoch 875, Batch 1000/1000: LR=4.83e-06, Loss=3.62e-03 BER=1.50e-03 FER=1.28e-02
2025-10-16 02:10:58,470 | INFO | Epoch 875 Train Time 43.71134543418884s

2025-10-16 02:11:40,917 | INFO | Training epoch 876, Batch 1000/1000: LR=4.77e-06, Loss=3.52e-03 BER=1.46e-03 FER=1.24e-02
2025-10-16 02:11:40,984 | INFO | Epoch 876 Train Time 42.51276755332947s

2025-10-16 02:12:25,768 | INFO | Training epoch 877, Batch 1000/1000: LR=4.71e-06, Loss=3.69e-03 BER=1.50e-03 FER=1.28e-02
2025-10-16 02:12:25,838 | INFO | Epoch 877 Train Time 44.852498292922974s

2025-10-16 02:13:08,335 | INFO | Training epoch 878, Batch 1000/1000: LR=4.65e-06, Loss=3.68e-03 BER=1.51e-03 FER=1.27e-02
2025-10-16 02:13:08,385 | INFO | Epoch 878 Train Time 42.5410418510437s

2025-10-16 02:13:52,098 | INFO | Training epoch 879, Batch 1000/1000: LR=4.59e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.24e-02
2025-10-16 02:13:52,164 | INFO | Epoch 879 Train Time 43.77709770202637s

2025-10-16 02:14:35,904 | INFO | Training epoch 880, Batch 1000/1000: LR=4.53e-06, Loss=3.61e-03 BER=1.47e-03 FER=1.24e-02
2025-10-16 02:14:35,957 | INFO | Epoch 880 Train Time 43.79228115081787s

2025-10-16 02:15:20,000 | INFO | Training epoch 881, Batch 1000/1000: LR=4.48e-06, Loss=3.56e-03 BER=1.44e-03 FER=1.24e-02
2025-10-16 02:15:20,041 | INFO | Epoch 881 Train Time 44.081645250320435s

2025-10-16 02:16:04,331 | INFO | Training epoch 882, Batch 1000/1000: LR=4.42e-06, Loss=3.91e-03 BER=1.60e-03 FER=1.35e-02
2025-10-16 02:16:04,376 | INFO | Epoch 882 Train Time 44.33367466926575s

2025-10-16 02:16:46,905 | INFO | Training epoch 883, Batch 1000/1000: LR=4.36e-06, Loss=3.70e-03 BER=1.53e-03 FER=1.30e-02
2025-10-16 02:16:46,956 | INFO | Epoch 883 Train Time 42.57856202125549s

2025-10-16 02:17:29,407 | INFO | Training epoch 884, Batch 1000/1000: LR=4.31e-06, Loss=3.73e-03 BER=1.55e-03 FER=1.31e-02
2025-10-16 02:17:29,458 | INFO | Epoch 884 Train Time 42.49959468841553s

2025-10-16 02:18:12,498 | INFO | Training epoch 885, Batch 1000/1000: LR=4.25e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.29e-02
2025-10-16 02:18:12,542 | INFO | Epoch 885 Train Time 43.08222484588623s

2025-10-16 02:18:55,153 | INFO | Training epoch 886, Batch 1000/1000: LR=4.20e-06, Loss=3.53e-03 BER=1.47e-03 FER=1.27e-02
2025-10-16 02:18:55,197 | INFO | Epoch 886 Train Time 42.654541015625s

2025-10-16 02:19:38,941 | INFO | Training epoch 887, Batch 1000/1000: LR=4.14e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.27e-02
2025-10-16 02:19:39,013 | INFO | Epoch 887 Train Time 43.81396150588989s

2025-10-16 02:20:22,303 | INFO | Training epoch 888, Batch 1000/1000: LR=4.09e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 02:20:22,354 | INFO | Epoch 888 Train Time 43.339255571365356s

2025-10-16 02:21:06,243 | INFO | Training epoch 889, Batch 1000/1000: LR=4.03e-06, Loss=3.49e-03 BER=1.44e-03 FER=1.23e-02
2025-10-16 02:21:06,301 | INFO | Epoch 889 Train Time 43.94550108909607s

2025-10-16 02:21:49,705 | INFO | Training epoch 890, Batch 1000/1000: LR=3.98e-06, Loss=3.75e-03 BER=1.53e-03 FER=1.30e-02
2025-10-16 02:21:49,758 | INFO | Epoch 890 Train Time 43.455069065093994s

2025-10-16 02:22:33,208 | INFO | Training epoch 891, Batch 1000/1000: LR=3.93e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 02:22:33,251 | INFO | Epoch 891 Train Time 43.492504596710205s

2025-10-16 02:23:17,323 | INFO | Training epoch 892, Batch 1000/1000: LR=3.87e-06, Loss=3.54e-03 BER=1.48e-03 FER=1.25e-02
2025-10-16 02:23:17,386 | INFO | Epoch 892 Train Time 44.13293528556824s

2025-10-16 02:24:02,563 | INFO | Training epoch 893, Batch 1000/1000: LR=3.82e-06, Loss=3.65e-03 BER=1.52e-03 FER=1.28e-02
2025-10-16 02:24:02,654 | INFO | Epoch 893 Train Time 45.26637649536133s

2025-10-16 02:24:46,507 | INFO | Training epoch 894, Batch 1000/1000: LR=3.77e-06, Loss=3.44e-03 BER=1.41e-03 FER=1.22e-02
2025-10-16 02:24:46,555 | INFO | Epoch 894 Train Time 43.89816069602966s

2025-10-16 02:25:30,119 | INFO | Training epoch 895, Batch 1000/1000: LR=3.72e-06, Loss=3.65e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 02:25:30,164 | INFO | Epoch 895 Train Time 43.60805130004883s

2025-10-16 02:26:13,443 | INFO | Training epoch 896, Batch 1000/1000: LR=3.67e-06, Loss=3.61e-03 BER=1.46e-03 FER=1.26e-02
2025-10-16 02:26:13,500 | INFO | Epoch 896 Train Time 43.334726095199585s

2025-10-16 02:26:56,506 | INFO | Training epoch 897, Batch 1000/1000: LR=3.62e-06, Loss=3.71e-03 BER=1.53e-03 FER=1.29e-02
2025-10-16 02:26:56,548 | INFO | Epoch 897 Train Time 43.046225786209106s

2025-10-16 02:27:41,116 | INFO | Training epoch 898, Batch 1000/1000: LR=3.57e-06, Loss=3.70e-03 BER=1.53e-03 FER=1.31e-02
2025-10-16 02:27:41,172 | INFO | Epoch 898 Train Time 44.62254762649536s

2025-10-16 02:28:24,447 | INFO | Training epoch 899, Batch 1000/1000: LR=3.52e-06, Loss=3.52e-03 BER=1.42e-03 FER=1.22e-02
2025-10-16 02:28:24,493 | INFO | Epoch 899 Train Time 43.3181688785553s

2025-10-16 02:29:08,110 | INFO | Training epoch 900, Batch 1000/1000: LR=3.47e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 02:29:08,156 | INFO | Epoch 900 Train Time 43.66166639328003s

2025-10-16 02:29:50,497 | INFO | Training epoch 901, Batch 1000/1000: LR=3.42e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 02:29:50,550 | INFO | Epoch 901 Train Time 42.39291548728943s

2025-10-16 02:30:34,360 | INFO | Training epoch 902, Batch 1000/1000: LR=3.37e-06, Loss=3.51e-03 BER=1.44e-03 FER=1.23e-02
2025-10-16 02:30:34,421 | INFO | Epoch 902 Train Time 43.8686957359314s

2025-10-16 02:31:18,568 | INFO | Training epoch 903, Batch 1000/1000: LR=3.33e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.25e-02
2025-10-16 02:31:18,621 | INFO | Epoch 903 Train Time 44.199039459228516s

2025-10-16 02:32:03,638 | INFO | Training epoch 904, Batch 1000/1000: LR=3.28e-06, Loss=3.42e-03 BER=1.39e-03 FER=1.19e-02
2025-10-16 02:32:03,694 | INFO | Epoch 904 Train Time 45.07129955291748s

2025-10-16 02:32:46,639 | INFO | Training epoch 905, Batch 1000/1000: LR=3.23e-06, Loss=3.58e-03 BER=1.45e-03 FER=1.28e-02
2025-10-16 02:32:46,680 | INFO | Epoch 905 Train Time 42.984142780303955s

2025-10-16 02:33:30,130 | INFO | Training epoch 906, Batch 1000/1000: LR=3.19e-06, Loss=3.62e-03 BER=1.47e-03 FER=1.24e-02
2025-10-16 02:33:30,177 | INFO | Epoch 906 Train Time 43.495574712753296s

2025-10-16 02:34:13,406 | INFO | Training epoch 907, Batch 1000/1000: LR=3.14e-06, Loss=3.65e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 02:34:13,451 | INFO | Epoch 907 Train Time 43.272977113723755s

2025-10-16 02:34:57,377 | INFO | Training epoch 908, Batch 1000/1000: LR=3.10e-06, Loss=3.54e-03 BER=1.45e-03 FER=1.26e-02
2025-10-16 02:34:57,455 | INFO | Epoch 908 Train Time 44.00201225280762s

2025-10-16 02:35:41,201 | INFO | Training epoch 909, Batch 1000/1000: LR=3.05e-06, Loss=3.54e-03 BER=1.45e-03 FER=1.24e-02
2025-10-16 02:35:41,251 | INFO | Epoch 909 Train Time 43.79391622543335s

2025-10-16 02:36:25,008 | INFO | Training epoch 910, Batch 1000/1000: LR=3.01e-06, Loss=3.53e-03 BER=1.45e-03 FER=1.24e-02
2025-10-16 02:36:25,054 | INFO | Epoch 910 Train Time 43.80158853530884s

2025-10-16 02:37:06,079 | INFO | Training epoch 911, Batch 1000/1000: LR=2.97e-06, Loss=3.54e-03 BER=1.44e-03 FER=1.24e-02
2025-10-16 02:37:06,134 | INFO | Epoch 911 Train Time 41.078022956848145s

2025-10-16 02:37:47,628 | INFO | Training epoch 912, Batch 1000/1000: LR=2.92e-06, Loss=3.72e-03 BER=1.49e-03 FER=1.29e-02
2025-10-16 02:37:47,677 | INFO | Epoch 912 Train Time 41.54092812538147s

2025-10-16 02:38:31,206 | INFO | Training epoch 913, Batch 1000/1000: LR=2.88e-06, Loss=3.65e-03 BER=1.49e-03 FER=1.28e-02
2025-10-16 02:38:31,263 | INFO | Epoch 913 Train Time 43.584911584854126s

2025-10-16 02:39:12,600 | INFO | Training epoch 914, Batch 1000/1000: LR=2.84e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.27e-02
2025-10-16 02:39:12,645 | INFO | Epoch 914 Train Time 41.3807418346405s

2025-10-16 02:39:56,099 | INFO | Training epoch 915, Batch 1000/1000: LR=2.80e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 02:39:56,145 | INFO | Epoch 915 Train Time 43.49834394454956s

2025-10-16 02:40:41,026 | INFO | Training epoch 916, Batch 1000/1000: LR=2.75e-06, Loss=3.66e-03 BER=1.51e-03 FER=1.29e-02
2025-10-16 02:40:41,081 | INFO | Epoch 916 Train Time 44.93483376502991s

2025-10-16 02:41:25,405 | INFO | Training epoch 917, Batch 1000/1000: LR=2.71e-06, Loss=3.67e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 02:41:25,465 | INFO | Epoch 917 Train Time 44.383039474487305s

2025-10-16 02:42:08,821 | INFO | Training epoch 918, Batch 1000/1000: LR=2.67e-06, Loss=3.51e-03 BER=1.45e-03 FER=1.25e-02
2025-10-16 02:42:08,868 | INFO | Epoch 918 Train Time 43.40079689025879s

2025-10-16 02:42:51,701 | INFO | Training epoch 919, Batch 1000/1000: LR=2.63e-06, Loss=3.68e-03 BER=1.51e-03 FER=1.27e-02
2025-10-16 02:42:51,750 | INFO | Epoch 919 Train Time 42.8809494972229s

2025-10-16 02:43:34,965 | INFO | Training epoch 920, Batch 1000/1000: LR=2.59e-06, Loss=3.70e-03 BER=1.52e-03 FER=1.30e-02
2025-10-16 02:43:35,029 | INFO | Epoch 920 Train Time 43.277286529541016s

2025-10-16 02:44:18,507 | INFO | Training epoch 921, Batch 1000/1000: LR=2.56e-06, Loss=3.68e-03 BER=1.51e-03 FER=1.29e-02
2025-10-16 02:44:18,552 | INFO | Epoch 921 Train Time 43.521727323532104s

2025-10-16 02:45:01,323 | INFO | Training epoch 922, Batch 1000/1000: LR=2.52e-06, Loss=3.62e-03 BER=1.48e-03 FER=1.25e-02
2025-10-16 02:45:01,397 | INFO | Epoch 922 Train Time 42.842350482940674s

2025-10-16 02:45:44,962 | INFO | Training epoch 923, Batch 1000/1000: LR=2.48e-06, Loss=3.64e-03 BER=1.48e-03 FER=1.27e-02
2025-10-16 02:45:45,025 | INFO | Epoch 923 Train Time 43.62747240066528s

2025-10-16 02:46:28,429 | INFO | Training epoch 924, Batch 1000/1000: LR=2.44e-06, Loss=3.68e-03 BER=1.49e-03 FER=1.28e-02
2025-10-16 02:46:28,477 | INFO | Epoch 924 Train Time 43.44964122772217s

2025-10-16 02:47:12,317 | INFO | Training epoch 925, Batch 1000/1000: LR=2.40e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.29e-02
2025-10-16 02:47:12,367 | INFO | Epoch 925 Train Time 43.888073444366455s

2025-10-16 02:47:55,109 | INFO | Training epoch 926, Batch 1000/1000: LR=2.37e-06, Loss=3.48e-03 BER=1.43e-03 FER=1.23e-02
2025-10-16 02:47:55,157 | INFO | Epoch 926 Train Time 42.7889940738678s

2025-10-16 02:48:37,683 | INFO | Training epoch 927, Batch 1000/1000: LR=2.33e-06, Loss=3.50e-03 BER=1.47e-03 FER=1.23e-02
2025-10-16 02:48:37,732 | INFO | Epoch 927 Train Time 42.57304525375366s

2025-10-16 02:49:22,209 | INFO | Training epoch 928, Batch 1000/1000: LR=2.30e-06, Loss=3.66e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 02:49:22,246 | INFO | Epoch 928 Train Time 44.51205253601074s

2025-10-16 02:50:05,299 | INFO | Training epoch 929, Batch 1000/1000: LR=2.26e-06, Loss=3.45e-03 BER=1.44e-03 FER=1.23e-02
2025-10-16 02:50:05,355 | INFO | Epoch 929 Train Time 43.10700702667236s

2025-10-16 02:50:48,301 | INFO | Training epoch 930, Batch 1000/1000: LR=2.23e-06, Loss=3.67e-03 BER=1.53e-03 FER=1.28e-02
2025-10-16 02:50:48,359 | INFO | Epoch 930 Train Time 43.002692222595215s

2025-10-16 02:51:32,002 | INFO | Training epoch 931, Batch 1000/1000: LR=2.19e-06, Loss=3.56e-03 BER=1.42e-03 FER=1.21e-02
2025-10-16 02:51:32,044 | INFO | Epoch 931 Train Time 43.6830689907074s

2025-10-16 02:52:16,200 | INFO | Training epoch 932, Batch 1000/1000: LR=2.16e-06, Loss=3.67e-03 BER=1.53e-03 FER=1.28e-02
2025-10-16 02:52:16,252 | INFO | Epoch 932 Train Time 44.207149505615234s

2025-10-16 02:52:59,127 | INFO | Training epoch 933, Batch 1000/1000: LR=2.13e-06, Loss=3.48e-03 BER=1.42e-03 FER=1.23e-02
2025-10-16 02:52:59,182 | INFO | Epoch 933 Train Time 42.92746949195862s

2025-10-16 02:53:42,806 | INFO | Training epoch 934, Batch 1000/1000: LR=2.09e-06, Loss=3.43e-03 BER=1.40e-03 FER=1.20e-02
2025-10-16 02:53:42,858 | INFO | Epoch 934 Train Time 43.6750226020813s

2025-10-16 02:54:26,802 | INFO | Training epoch 935, Batch 1000/1000: LR=2.06e-06, Loss=3.52e-03 BER=1.47e-03 FER=1.24e-02
2025-10-16 02:54:26,881 | INFO | Epoch 935 Train Time 44.021745681762695s

2025-10-16 02:55:11,919 | INFO | Training epoch 936, Batch 1000/1000: LR=2.03e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.25e-02
2025-10-16 02:55:11,971 | INFO | Epoch 936 Train Time 45.08822178840637s

2025-10-16 02:55:55,434 | INFO | Training epoch 937, Batch 1000/1000: LR=2.00e-06, Loss=3.46e-03 BER=1.41e-03 FER=1.24e-02
2025-10-16 02:55:55,478 | INFO | Epoch 937 Train Time 43.506009101867676s

2025-10-16 02:56:38,233 | INFO | Training epoch 938, Batch 1000/1000: LR=1.97e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.26e-02
2025-10-16 02:56:38,290 | INFO | Epoch 938 Train Time 42.81086325645447s

2025-10-16 02:57:21,539 | INFO | Training epoch 939, Batch 1000/1000: LR=1.94e-06, Loss=3.55e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 02:57:21,604 | INFO | Epoch 939 Train Time 43.31281113624573s

2025-10-16 02:58:04,399 | INFO | Training epoch 940, Batch 1000/1000: LR=1.91e-06, Loss=3.53e-03 BER=1.44e-03 FER=1.24e-02
2025-10-16 02:58:04,442 | INFO | Epoch 940 Train Time 42.835779666900635s

2025-10-16 02:58:48,731 | INFO | Training epoch 941, Batch 1000/1000: LR=1.88e-06, Loss=3.61e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 02:58:48,793 | INFO | Epoch 941 Train Time 44.3503475189209s

2025-10-16 02:59:32,207 | INFO | Training epoch 942, Batch 1000/1000: LR=1.85e-06, Loss=3.64e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 02:59:32,252 | INFO | Epoch 942 Train Time 43.45556426048279s

2025-10-16 03:00:15,722 | INFO | Training epoch 943, Batch 1000/1000: LR=1.82e-06, Loss=3.69e-03 BER=1.53e-03 FER=1.31e-02
2025-10-16 03:00:15,765 | INFO | Epoch 943 Train Time 43.512168884277344s

2025-10-16 03:00:59,813 | INFO | Training epoch 944, Batch 1000/1000: LR=1.79e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.25e-02
2025-10-16 03:00:59,861 | INFO | Epoch 944 Train Time 44.09468913078308s

2025-10-16 03:01:44,605 | INFO | Training epoch 945, Batch 1000/1000: LR=1.76e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 03:01:44,652 | INFO | Epoch 945 Train Time 44.78947710990906s

2025-10-16 03:02:27,170 | INFO | Training epoch 946, Batch 1000/1000: LR=1.74e-06, Loss=3.64e-03 BER=1.52e-03 FER=1.28e-02
2025-10-16 03:02:27,230 | INFO | Epoch 946 Train Time 42.574926137924194s

2025-10-16 03:03:10,545 | INFO | Training epoch 947, Batch 1000/1000: LR=1.71e-06, Loss=3.45e-03 BER=1.43e-03 FER=1.21e-02
2025-10-16 03:03:10,610 | INFO | Epoch 947 Train Time 43.37875556945801s

2025-10-16 03:03:54,200 | INFO | Training epoch 948, Batch 1000/1000: LR=1.68e-06, Loss=3.55e-03 BER=1.47e-03 FER=1.23e-02
2025-10-16 03:03:54,260 | INFO | Epoch 948 Train Time 43.64854836463928s

2025-10-16 03:04:39,364 | INFO | Training epoch 949, Batch 1000/1000: LR=1.66e-06, Loss=3.48e-03 BER=1.43e-03 FER=1.23e-02
2025-10-16 03:04:39,422 | INFO | Epoch 949 Train Time 45.159454107284546s

2025-10-16 03:05:23,695 | INFO | Training epoch 950, Batch 1000/1000: LR=1.63e-06, Loss=3.73e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 03:05:23,750 | INFO | Epoch 950 Train Time 44.32587218284607s

2025-10-16 03:06:08,205 | INFO | Training epoch 951, Batch 1000/1000: LR=1.61e-06, Loss=3.71e-03 BER=1.52e-03 FER=1.29e-02
2025-10-16 03:06:08,267 | INFO | Epoch 951 Train Time 44.515188455581665s

2025-10-16 03:06:51,870 | INFO | Training epoch 952, Batch 1000/1000: LR=1.59e-06, Loss=3.65e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 03:06:51,929 | INFO | Epoch 952 Train Time 43.65912914276123s

2025-10-16 03:07:36,210 | INFO | Training epoch 953, Batch 1000/1000: LR=1.56e-06, Loss=3.60e-03 BER=1.46e-03 FER=1.26e-02
2025-10-16 03:07:36,252 | INFO | Epoch 953 Train Time 44.32183265686035s

2025-10-16 03:08:19,405 | INFO | Training epoch 954, Batch 1000/1000: LR=1.54e-06, Loss=3.50e-03 BER=1.46e-03 FER=1.25e-02
2025-10-16 03:08:19,446 | INFO | Epoch 954 Train Time 43.19184899330139s

2025-10-16 03:09:01,602 | INFO | Training epoch 955, Batch 1000/1000: LR=1.52e-06, Loss=3.45e-03 BER=1.44e-03 FER=1.22e-02
2025-10-16 03:09:01,657 | INFO | Epoch 955 Train Time 42.21018028259277s

2025-10-16 03:09:44,937 | INFO | Training epoch 956, Batch 1000/1000: LR=1.49e-06, Loss=3.73e-03 BER=1.57e-03 FER=1.30e-02
2025-10-16 03:09:44,984 | INFO | Epoch 956 Train Time 43.3243682384491s

2025-10-16 03:10:29,512 | INFO | Training epoch 957, Batch 1000/1000: LR=1.47e-06, Loss=3.64e-03 BER=1.47e-03 FER=1.25e-02
2025-10-16 03:10:29,566 | INFO | Epoch 957 Train Time 44.58066439628601s

2025-10-16 03:11:13,020 | INFO | Training epoch 958, Batch 1000/1000: LR=1.45e-06, Loss=3.56e-03 BER=1.48e-03 FER=1.26e-02
2025-10-16 03:11:13,058 | INFO | Epoch 958 Train Time 43.49048113822937s

2025-10-16 03:11:56,877 | INFO | Training epoch 959, Batch 1000/1000: LR=1.43e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.27e-02
2025-10-16 03:11:56,927 | INFO | Epoch 959 Train Time 43.86761164665222s

2025-10-16 03:12:40,323 | INFO | Training epoch 960, Batch 1000/1000: LR=1.41e-06, Loss=3.53e-03 BER=1.45e-03 FER=1.22e-02
2025-10-16 03:12:40,379 | INFO | Epoch 960 Train Time 43.450156688690186s

2025-10-16 03:13:24,400 | INFO | Training epoch 961, Batch 1000/1000: LR=1.39e-06, Loss=3.66e-03 BER=1.50e-03 FER=1.29e-02
2025-10-16 03:13:24,459 | INFO | Epoch 961 Train Time 44.077471017837524s

2025-10-16 03:14:09,739 | INFO | Training epoch 962, Batch 1000/1000: LR=1.37e-06, Loss=3.60e-03 BER=1.46e-03 FER=1.27e-02
2025-10-16 03:14:09,811 | INFO | Epoch 962 Train Time 45.351834535598755s

2025-10-16 03:14:53,211 | INFO | Training epoch 963, Batch 1000/1000: LR=1.35e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 03:14:53,252 | INFO | Epoch 963 Train Time 43.43948245048523s

2025-10-16 03:15:37,698 | INFO | Training epoch 964, Batch 1000/1000: LR=1.33e-06, Loss=3.71e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 03:15:37,739 | INFO | Epoch 964 Train Time 44.485602140426636s

2025-10-16 03:16:21,908 | INFO | Training epoch 965, Batch 1000/1000: LR=1.32e-06, Loss=3.68e-03 BER=1.55e-03 FER=1.31e-02
2025-10-16 03:16:21,953 | INFO | Epoch 965 Train Time 44.21248173713684s

2025-10-16 03:17:05,101 | INFO | Training epoch 966, Batch 1000/1000: LR=1.30e-06, Loss=3.60e-03 BER=1.48e-03 FER=1.26e-02
2025-10-16 03:17:05,156 | INFO | Epoch 966 Train Time 43.20230221748352s

2025-10-16 03:17:47,485 | INFO | Training epoch 967, Batch 1000/1000: LR=1.28e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.24e-02
2025-10-16 03:17:47,557 | INFO | Epoch 967 Train Time 42.398884773254395s

2025-10-16 03:18:31,134 | INFO | Training epoch 968, Batch 1000/1000: LR=1.27e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.28e-02
2025-10-16 03:18:31,182 | INFO | Epoch 968 Train Time 43.62228536605835s

2025-10-16 03:19:12,506 | INFO | Training epoch 969, Batch 1000/1000: LR=1.25e-06, Loss=3.76e-03 BER=1.52e-03 FER=1.28e-02
2025-10-16 03:19:12,555 | INFO | Epoch 969 Train Time 41.371633529663086s

2025-10-16 03:19:56,114 | INFO | Training epoch 970, Batch 1000/1000: LR=1.23e-06, Loss=3.63e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 03:19:56,174 | INFO | Epoch 970 Train Time 43.61748552322388s

2025-10-16 03:20:40,144 | INFO | Training epoch 971, Batch 1000/1000: LR=1.22e-06, Loss=3.49e-03 BER=1.44e-03 FER=1.23e-02
2025-10-16 03:20:40,223 | INFO | Epoch 971 Train Time 44.0474898815155s

2025-10-16 03:21:23,613 | INFO | Training epoch 972, Batch 1000/1000: LR=1.21e-06, Loss=3.39e-03 BER=1.39e-03 FER=1.20e-02
2025-10-16 03:21:23,665 | INFO | Epoch 972 Train Time 43.43999099731445s

2025-10-16 03:22:06,809 | INFO | Training epoch 973, Batch 1000/1000: LR=1.19e-06, Loss=3.52e-03 BER=1.44e-03 FER=1.23e-02
2025-10-16 03:22:06,851 | INFO | Epoch 973 Train Time 43.18447732925415s

2025-10-16 03:22:50,818 | INFO | Training epoch 974, Batch 1000/1000: LR=1.18e-06, Loss=3.72e-03 BER=1.55e-03 FER=1.33e-02
2025-10-16 03:22:50,861 | INFO | Epoch 974 Train Time 44.00699758529663s

2025-10-16 03:23:34,295 | INFO | Training epoch 975, Batch 1000/1000: LR=1.17e-06, Loss=3.51e-03 BER=1.42e-03 FER=1.24e-02
2025-10-16 03:23:34,342 | INFO | Epoch 975 Train Time 43.479777336120605s

2025-10-16 03:24:19,054 | INFO | Training epoch 976, Batch 1000/1000: LR=1.15e-06, Loss=3.51e-03 BER=1.42e-03 FER=1.22e-02
2025-10-16 03:24:19,111 | INFO | Epoch 976 Train Time 44.767624855041504s

2025-10-16 03:25:03,225 | INFO | Training epoch 977, Batch 1000/1000: LR=1.14e-06, Loss=3.63e-03 BER=1.50e-03 FER=1.28e-02
2025-10-16 03:25:03,282 | INFO | Epoch 977 Train Time 44.16809678077698s

2025-10-16 03:25:47,538 | INFO | Training epoch 978, Batch 1000/1000: LR=1.13e-06, Loss=3.44e-03 BER=1.42e-03 FER=1.22e-02
2025-10-16 03:25:47,610 | INFO | Epoch 978 Train Time 44.32635569572449s

2025-10-16 03:26:32,004 | INFO | Training epoch 979, Batch 1000/1000: LR=1.12e-06, Loss=3.54e-03 BER=1.45e-03 FER=1.22e-02
2025-10-16 03:26:32,050 | INFO | Epoch 979 Train Time 44.43859934806824s

2025-10-16 03:27:14,758 | INFO | Training epoch 980, Batch 1000/1000: LR=1.11e-06, Loss=3.67e-03 BER=1.49e-03 FER=1.29e-02
2025-10-16 03:27:14,813 | INFO | Epoch 980 Train Time 42.761414527893066s

2025-10-16 03:27:56,801 | INFO | Training epoch 981, Batch 1000/1000: LR=1.10e-06, Loss=3.44e-03 BER=1.39e-03 FER=1.22e-02
2025-10-16 03:27:56,844 | INFO | Epoch 981 Train Time 42.02925229072571s

2025-10-16 03:28:40,041 | INFO | Training epoch 982, Batch 1000/1000: LR=1.09e-06, Loss=3.55e-03 BER=1.45e-03 FER=1.24e-02
2025-10-16 03:28:40,123 | INFO | Epoch 982 Train Time 43.277833223342896s

2025-10-16 03:29:23,920 | INFO | Training epoch 983, Batch 1000/1000: LR=1.08e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 03:29:23,965 | INFO | Epoch 983 Train Time 43.84051465988159s

2025-10-16 03:30:06,501 | INFO | Training epoch 984, Batch 1000/1000: LR=1.07e-06, Loss=3.55e-03 BER=1.47e-03 FER=1.26e-02
2025-10-16 03:30:06,554 | INFO | Epoch 984 Train Time 42.58760666847229s

2025-10-16 03:30:49,598 | INFO | Training epoch 985, Batch 1000/1000: LR=1.06e-06, Loss=3.52e-03 BER=1.46e-03 FER=1.24e-02
2025-10-16 03:30:49,648 | INFO | Epoch 985 Train Time 43.0930757522583s

2025-10-16 03:31:33,051 | INFO | Training epoch 986, Batch 1000/1000: LR=1.05e-06, Loss=3.51e-03 BER=1.45e-03 FER=1.23e-02
2025-10-16 03:31:33,108 | INFO | Epoch 986 Train Time 43.45773506164551s

2025-10-16 03:32:16,606 | INFO | Training epoch 987, Batch 1000/1000: LR=1.05e-06, Loss=3.57e-03 BER=1.50e-03 FER=1.26e-02
2025-10-16 03:32:16,675 | INFO | Epoch 987 Train Time 43.565043210983276s

2025-10-16 03:32:59,733 | INFO | Training epoch 988, Batch 1000/1000: LR=1.04e-06, Loss=3.57e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 03:32:59,815 | INFO | Epoch 988 Train Time 43.138675689697266s

2025-10-16 03:33:43,702 | INFO | Training epoch 989, Batch 1000/1000: LR=1.04e-06, Loss=3.67e-03 BER=1.53e-03 FER=1.29e-02
2025-10-16 03:33:43,753 | INFO | Epoch 989 Train Time 43.93770360946655s

2025-10-16 03:34:26,521 | INFO | Training epoch 990, Batch 1000/1000: LR=1.03e-06, Loss=3.51e-03 BER=1.43e-03 FER=1.23e-02
2025-10-16 03:34:26,582 | INFO | Epoch 990 Train Time 42.82735466957092s

2025-10-16 03:35:11,329 | INFO | Training epoch 991, Batch 1000/1000: LR=1.02e-06, Loss=3.38e-03 BER=1.38e-03 FER=1.21e-02
2025-10-16 03:35:11,375 | INFO | Epoch 991 Train Time 44.79123067855835s

2025-10-16 03:35:53,203 | INFO | Training epoch 992, Batch 1000/1000: LR=1.02e-06, Loss=3.63e-03 BER=1.50e-03 FER=1.26e-02
2025-10-16 03:35:53,247 | INFO | Epoch 992 Train Time 41.87051701545715s

2025-10-16 03:36:37,112 | INFO | Training epoch 993, Batch 1000/1000: LR=1.02e-06, Loss=3.64e-03 BER=1.48e-03 FER=1.30e-02
2025-10-16 03:36:37,176 | INFO | Epoch 993 Train Time 43.92741632461548s

2025-10-16 03:37:21,000 | INFO | Training epoch 994, Batch 1000/1000: LR=1.01e-06, Loss=3.54e-03 BER=1.50e-03 FER=1.27e-02
2025-10-16 03:37:21,037 | INFO | Epoch 994 Train Time 43.856942653656006s

2025-10-16 03:38:03,955 | INFO | Training epoch 995, Batch 1000/1000: LR=1.01e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.26e-02
2025-10-16 03:38:04,014 | INFO | Epoch 995 Train Time 42.976112604141235s

2025-10-16 03:38:47,323 | INFO | Training epoch 996, Batch 1000/1000: LR=1.01e-06, Loss=3.54e-03 BER=1.44e-03 FER=1.23e-02
2025-10-16 03:38:47,362 | INFO | Epoch 996 Train Time 43.346524715423584s

2025-10-16 03:39:30,949 | INFO | Training epoch 997, Batch 1000/1000: LR=1.00e-06, Loss=3.58e-03 BER=1.48e-03 FER=1.27e-02
2025-10-16 03:39:30,994 | INFO | Epoch 997 Train Time 43.63067650794983s

2025-10-16 03:40:14,198 | INFO | Training epoch 998, Batch 1000/1000: LR=1.00e-06, Loss=3.68e-03 BER=1.49e-03 FER=1.27e-02
2025-10-16 03:40:14,242 | INFO | Epoch 998 Train Time 43.24447727203369s

2025-10-16 03:40:58,032 | INFO | Training epoch 999, Batch 1000/1000: LR=1.00e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.24e-02
2025-10-16 03:40:58,076 | INFO | Epoch 999 Train Time 43.83364176750183s

2025-10-16 03:41:41,242 | INFO | Training epoch 1000, Batch 1000/1000: LR=1.00e-06, Loss=3.67e-03 BER=1.51e-03 FER=1.28e-02
2025-10-16 03:41:41,284 | INFO | Epoch 1000 Train Time 43.206214904785156s

2025-10-16 03:41:41,309 | INFO | Checkpoint saved: runs/20251015_153247/stage1_fp32__LDPC_n49_k24__Ndec10_d128_h8.pth
2025-10-16 03:41:41,332 | INFO | Checkpoint saved: runs/20251015_153247/stage1_fp32__LDPC_n49_k24__Ndec10_d128_h8__e1000_loss0.003673.pth
2025-10-16 03:41:47,424 | INFO | FER count threshold reached for EbN0:4
2025-10-16 03:41:47,479 | INFO | Test EbN0=4, BER=1.19e-03
2025-10-16 03:41:55,586 | INFO | FER count threshold reached for EbN0:5
2025-10-16 03:41:55,638 | INFO | Test EbN0=5, BER=6.88e-05
2025-10-16 03:48:14,255 | INFO | FER count threshold reached for EbN0:6
2025-10-16 03:48:14,325 | INFO | Test EbN0=6, BER=1.57e-06
2025-10-16 03:48:14,325 | INFO | 
Test Loss 4: 2.9114e-03 5: 1.8892e-04 6: 4.7117e-06
2025-10-16 03:48:14,325 | INFO | Test FER 4: 1.0224e-02 5: 7.5871e-04 6: 1.5880e-05
2025-10-16 03:48:14,325 | INFO | Test BER 4: 1.1862e-03 5: 6.8835e-05 6: 1.5659e-06
2025-10-16 03:48:14,325 | INFO | Test -ln(BER) 4: 6.7370e+00 5: 9.5838e+00 6: 1.3367e+01
2025-10-16 03:48:14,325 | INFO | # of testing samples: [100352.0, 133120.0, 6360064.0]
 Test Time 392.9927821159363 s

2025-10-16 03:48:14,847 | INFO | Loaded checkpoint: runs/20251015_153247/stage1_fp32__LDPC_n49_k24__Ndec10_d128_h8.pth (strict=False)
2025-10-16 03:49:49,423 | INFO | Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=1.88e-02 BER=6.60e-03 FER=9.97e-02
2025-10-16 03:49:49,469 | INFO | Epoch 1 Train Time 94.61769199371338s

2025-10-16 03:49:49,470 | INFO | [P2] saving best_model (QAT) with loss 0.018788 at epoch 1
2025-10-16 03:51:26,657 | INFO | Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=8.91e-03 BER=3.46e-03 FER=3.69e-02
2025-10-16 03:51:26,736 | INFO | Epoch 2 Train Time 97.1748480796814s

2025-10-16 03:51:26,737 | INFO | [P2] saving best_model (QAT) with loss 0.008915 at epoch 2
2025-10-16 03:52:57,933 | INFO | Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=7.40e-03 BER=2.94e-03 FER=2.96e-02
2025-10-16 03:52:57,995 | INFO | Epoch 3 Train Time 91.16081142425537s

2025-10-16 03:52:57,995 | INFO | [P2] saving best_model (QAT) with loss 0.007404 at epoch 3
2025-10-16 03:54:30,359 | INFO | Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=6.78e-03 BER=2.71e-03 FER=2.70e-02
2025-10-16 03:54:30,411 | INFO | Epoch 4 Train Time 92.3280258178711s

2025-10-16 03:54:30,413 | INFO | [P2] saving best_model (QAT) with loss 0.006779 at epoch 4
2025-10-16 03:56:06,255 | INFO | Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=6.56e-03 BER=2.64e-03 FER=2.55e-02
2025-10-16 03:56:06,298 | INFO | Epoch 5 Train Time 95.80351543426514s

2025-10-16 03:56:06,299 | INFO | [P2] saving best_model (QAT) with loss 0.006559 at epoch 5
2025-10-16 03:57:42,830 | INFO | Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=6.05e-03 BER=2.45e-03 FER=2.34e-02
2025-10-16 03:57:42,879 | INFO | Epoch 6 Train Time 96.49296975135803s

2025-10-16 03:57:42,879 | INFO | [P2] saving best_model (QAT) with loss 0.006051 at epoch 6
2025-10-16 03:59:18,337 | INFO | Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=5.82e-03 BER=2.37e-03 FER=2.21e-02
2025-10-16 03:59:18,396 | INFO | Epoch 7 Train Time 95.43755388259888s

2025-10-16 03:59:18,398 | INFO | [P2] saving best_model (QAT) with loss 0.005821 at epoch 7
2025-10-16 04:00:52,693 | INFO | Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=5.57e-03 BER=2.28e-03 FER=2.11e-02
2025-10-16 04:00:52,752 | INFO | Epoch 8 Train Time 94.27757406234741s

2025-10-16 04:00:52,753 | INFO | [P2] saving best_model (QAT) with loss 0.005569 at epoch 8
2025-10-16 04:02:28,169 | INFO | Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=5.55e-03 BER=2.25e-03 FER=2.10e-02
2025-10-16 04:02:28,211 | INFO | Epoch 9 Train Time 95.3562388420105s

2025-10-16 04:02:28,212 | INFO | [P2] saving best_model (QAT) with loss 0.005545 at epoch 9
2025-10-16 04:04:04,028 | INFO | Training epoch 10, Batch 1000/1000: LR=1.00e-04, Loss=5.37e-03 BER=2.19e-03 FER=2.06e-02
2025-10-16 04:04:04,083 | INFO | Epoch 10 Train Time 95.79624247550964s

2025-10-16 04:04:04,083 | INFO | [P2] saving best_model (QAT) with loss 0.005375 at epoch 10
2025-10-16 04:05:38,262 | INFO | Training epoch 11, Batch 1000/1000: LR=1.00e-04, Loss=5.33e-03 BER=2.19e-03 FER=2.00e-02
2025-10-16 04:05:38,315 | INFO | Epoch 11 Train Time 94.1267352104187s

2025-10-16 04:05:38,316 | INFO | [P2] saving best_model (QAT) with loss 0.005328 at epoch 11
2025-10-16 04:07:13,835 | INFO | Training epoch 12, Batch 1000/1000: LR=1.00e-04, Loss=5.13e-03 BER=2.08e-03 FER=1.91e-02
2025-10-16 04:07:13,897 | INFO | Epoch 12 Train Time 95.50602269172668s

2025-10-16 04:07:13,898 | INFO | [P2] saving best_model (QAT) with loss 0.005127 at epoch 12
2025-10-16 04:08:46,818 | INFO | Training epoch 13, Batch 1000/1000: LR=1.00e-04, Loss=5.15e-03 BER=2.09e-03 FER=1.95e-02
2025-10-16 04:08:46,881 | INFO | Epoch 13 Train Time 92.88201212882996s

2025-10-16 04:10:21,290 | INFO | Training epoch 14, Batch 1000/1000: LR=1.00e-04, Loss=5.14e-03 BER=2.08e-03 FER=1.91e-02
2025-10-16 04:10:21,334 | INFO | Epoch 14 Train Time 94.45078229904175s

2025-10-16 04:11:53,408 | INFO | Training epoch 15, Batch 1000/1000: LR=1.00e-04, Loss=5.05e-03 BER=2.05e-03 FER=1.90e-02
2025-10-16 04:11:53,465 | INFO | Epoch 15 Train Time 92.12846350669861s

2025-10-16 04:11:53,467 | INFO | [P2] saving best_model (QAT) with loss 0.005045 at epoch 15
2025-10-16 04:13:28,758 | INFO | Training epoch 16, Batch 1000/1000: LR=1.00e-04, Loss=5.21e-03 BER=2.12e-03 FER=1.92e-02
2025-10-16 04:13:28,834 | INFO | Epoch 16 Train Time 95.29121780395508s

2025-10-16 04:15:05,872 | INFO | Training epoch 17, Batch 1000/1000: LR=1.00e-04, Loss=4.87e-03 BER=1.97e-03 FER=1.83e-02
2025-10-16 04:15:05,919 | INFO | Epoch 17 Train Time 97.08073449134827s

2025-10-16 04:15:05,919 | INFO | [P2] saving best_model (QAT) with loss 0.004873 at epoch 17
2025-10-16 04:16:34,029 | INFO | Training epoch 18, Batch 1000/1000: LR=1.00e-04, Loss=4.91e-03 BER=2.00e-03 FER=1.83e-02
2025-10-16 04:16:34,084 | INFO | Epoch 18 Train Time 88.07255625724792s

2025-10-16 04:18:06,332 | INFO | Training epoch 19, Batch 1000/1000: LR=1.00e-04, Loss=4.83e-03 BER=1.97e-03 FER=1.82e-02
2025-10-16 04:18:06,396 | INFO | Epoch 19 Train Time 92.31067419052124s

2025-10-16 04:18:06,397 | INFO | [P2] saving best_model (QAT) with loss 0.004828 at epoch 19
2025-10-16 04:19:36,915 | INFO | Training epoch 20, Batch 1000/1000: LR=1.00e-04, Loss=5.18e-03 BER=2.10e-03 FER=1.92e-02
2025-10-16 04:19:36,977 | INFO | Epoch 20 Train Time 90.4780068397522s

2025-10-16 04:21:18,454 | INFO | Training epoch 21, Batch 1000/1000: LR=1.00e-04, Loss=5.00e-03 BER=2.03e-03 FER=1.84e-02
2025-10-16 04:21:18,497 | INFO | Epoch 21 Train Time 101.51889514923096s

2025-10-16 04:22:51,865 | INFO | Training epoch 22, Batch 1000/1000: LR=1.00e-04, Loss=5.06e-03 BER=2.07e-03 FER=1.86e-02
2025-10-16 04:22:51,918 | INFO | Epoch 22 Train Time 93.41982936859131s

2025-10-16 04:24:29,984 | INFO | Training epoch 23, Batch 1000/1000: LR=1.00e-04, Loss=5.09e-03 BER=2.07e-03 FER=1.84e-02
2025-10-16 04:24:30,032 | INFO | Epoch 23 Train Time 98.11307191848755s

2025-10-16 04:26:09,216 | INFO | Training epoch 24, Batch 1000/1000: LR=1.00e-04, Loss=4.99e-03 BER=2.06e-03 FER=1.86e-02
2025-10-16 04:26:09,278 | INFO | Epoch 24 Train Time 99.2448787689209s

2025-10-16 04:27:49,560 | INFO | Training epoch 25, Batch 1000/1000: LR=1.00e-04, Loss=4.91e-03 BER=2.00e-03 FER=1.80e-02
2025-10-16 04:27:49,603 | INFO | Epoch 25 Train Time 100.32316851615906s

2025-10-16 04:29:23,769 | INFO | Training epoch 26, Batch 1000/1000: LR=1.00e-04, Loss=4.80e-03 BER=1.94e-03 FER=1.77e-02
2025-10-16 04:29:23,809 | INFO | Epoch 26 Train Time 94.20451855659485s

2025-10-16 04:29:23,810 | INFO | [P2] saving best_model (QAT) with loss 0.004800 at epoch 26
2025-10-16 04:30:56,714 | INFO | Training epoch 27, Batch 1000/1000: LR=1.00e-04, Loss=4.80e-03 BER=1.97e-03 FER=1.78e-02
2025-10-16 04:30:56,780 | INFO | Epoch 27 Train Time 92.89980363845825s

2025-10-16 04:30:56,780 | INFO | [P2] saving best_model (QAT) with loss 0.004795 at epoch 27
2025-10-16 04:32:33,859 | INFO | Training epoch 28, Batch 1000/1000: LR=1.00e-04, Loss=4.91e-03 BER=2.00e-03 FER=1.81e-02
2025-10-16 04:32:33,904 | INFO | Epoch 28 Train Time 96.9995105266571s

2025-10-16 04:34:02,894 | INFO | Training epoch 29, Batch 1000/1000: LR=1.00e-04, Loss=4.83e-03 BER=1.97e-03 FER=1.78e-02
2025-10-16 04:34:02,946 | INFO | Epoch 29 Train Time 89.04080820083618s

2025-10-16 04:35:38,766 | INFO | Training epoch 30, Batch 1000/1000: LR=9.99e-05, Loss=4.84e-03 BER=1.99e-03 FER=1.80e-02
2025-10-16 04:35:38,805 | INFO | Epoch 30 Train Time 95.85750770568848s

2025-10-16 04:37:26,019 | INFO | Training epoch 31, Batch 1000/1000: LR=9.99e-05, Loss=4.84e-03 BER=2.00e-03 FER=1.80e-02
2025-10-16 04:37:26,085 | INFO | Epoch 31 Train Time 107.2782347202301s

2025-10-16 04:39:05,353 | INFO | Training epoch 32, Batch 1000/1000: LR=9.99e-05, Loss=5.07e-03 BER=2.02e-03 FER=1.83e-02
2025-10-16 04:39:05,407 | INFO | Epoch 32 Train Time 99.32074451446533s

2025-10-16 04:40:51,896 | INFO | Training epoch 33, Batch 1000/1000: LR=9.99e-05, Loss=4.82e-03 BER=1.97e-03 FER=1.76e-02
2025-10-16 04:40:51,948 | INFO | Epoch 33 Train Time 106.538738489151s

2025-10-16 04:42:27,153 | INFO | Training epoch 34, Batch 1000/1000: LR=9.99e-05, Loss=4.96e-03 BER=2.02e-03 FER=1.83e-02
2025-10-16 04:42:27,203 | INFO | Epoch 34 Train Time 95.25422739982605s

2025-10-16 04:44:07,599 | INFO | Training epoch 35, Batch 1000/1000: LR=9.99e-05, Loss=4.89e-03 BER=1.99e-03 FER=1.79e-02
2025-10-16 04:44:07,673 | INFO | Epoch 35 Train Time 100.46755480766296s

2025-10-16 04:45:41,007 | INFO | Training epoch 36, Batch 1000/1000: LR=9.99e-05, Loss=4.66e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 04:45:41,050 | INFO | Epoch 36 Train Time 93.37385654449463s

2025-10-16 04:45:41,051 | INFO | [P2] saving best_model (QAT) with loss 0.004659 at epoch 36
2025-10-16 04:47:15,032 | INFO | Training epoch 37, Batch 1000/1000: LR=9.99e-05, Loss=4.87e-03 BER=1.98e-03 FER=1.77e-02
2025-10-16 04:47:15,077 | INFO | Epoch 37 Train Time 93.92182040214539s

2025-10-16 04:48:57,961 | INFO | Training epoch 38, Batch 1000/1000: LR=9.99e-05, Loss=4.77e-03 BER=1.93e-03 FER=1.73e-02
2025-10-16 04:48:58,009 | INFO | Epoch 38 Train Time 102.9306538105011s

2025-10-16 04:50:36,574 | INFO | Training epoch 39, Batch 1000/1000: LR=9.99e-05, Loss=4.71e-03 BER=1.95e-03 FER=1.73e-02
2025-10-16 04:50:36,637 | INFO | Epoch 39 Train Time 98.6263165473938s

2025-10-16 04:52:19,937 | INFO | Training epoch 40, Batch 1000/1000: LR=9.99e-05, Loss=4.71e-03 BER=1.89e-03 FER=1.73e-02
2025-10-16 04:52:19,980 | INFO | Epoch 40 Train Time 103.34023332595825s

2025-10-16 04:53:56,835 | INFO | Training epoch 41, Batch 1000/1000: LR=9.99e-05, Loss=4.79e-03 BER=1.94e-03 FER=1.75e-02
2025-10-16 04:53:56,896 | INFO | Epoch 41 Train Time 96.91548705101013s

2025-10-16 04:55:28,979 | INFO | Training epoch 42, Batch 1000/1000: LR=9.99e-05, Loss=5.03e-03 BER=2.07e-03 FER=1.84e-02
2025-10-16 04:55:29,025 | INFO | Epoch 42 Train Time 92.1275463104248s

2025-10-16 04:57:02,337 | INFO | Training epoch 43, Batch 1000/1000: LR=9.99e-05, Loss=4.96e-03 BER=2.04e-03 FER=1.83e-02
2025-10-16 04:57:02,396 | INFO | Epoch 43 Train Time 93.36905598640442s

2025-10-16 04:58:38,404 | INFO | Training epoch 44, Batch 1000/1000: LR=9.99e-05, Loss=4.84e-03 BER=1.97e-03 FER=1.78e-02
2025-10-16 04:58:38,474 | INFO | Epoch 44 Train Time 96.07658052444458s

2025-10-16 05:00:12,423 | INFO | Training epoch 45, Batch 1000/1000: LR=9.99e-05, Loss=4.76e-03 BER=1.94e-03 FER=1.75e-02
2025-10-16 05:00:12,470 | INFO | Epoch 45 Train Time 93.99248027801514s

2025-10-16 05:01:47,953 | INFO | Training epoch 46, Batch 1000/1000: LR=9.99e-05, Loss=4.69e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 05:01:48,007 | INFO | Epoch 46 Train Time 95.53477740287781s

2025-10-16 05:03:25,365 | INFO | Training epoch 47, Batch 1000/1000: LR=9.99e-05, Loss=4.81e-03 BER=1.99e-03 FER=1.77e-02
2025-10-16 05:03:25,424 | INFO | Epoch 47 Train Time 97.41443634033203s

2025-10-16 05:05:07,159 | INFO | Training epoch 48, Batch 1000/1000: LR=9.99e-05, Loss=4.58e-03 BER=1.87e-03 FER=1.69e-02
2025-10-16 05:05:07,214 | INFO | Epoch 48 Train Time 101.78980302810669s

2025-10-16 05:05:07,215 | INFO | [P2] saving best_model (QAT) with loss 0.004578 at epoch 48
2025-10-16 05:06:40,285 | INFO | Training epoch 49, Batch 1000/1000: LR=9.99e-05, Loss=4.77e-03 BER=1.97e-03 FER=1.78e-02
2025-10-16 05:06:40,354 | INFO | Epoch 49 Train Time 93.03711581230164s

2025-10-16 05:08:16,607 | INFO | Training epoch 50, Batch 1000/1000: LR=9.99e-05, Loss=4.76e-03 BER=1.92e-03 FER=1.75e-02
2025-10-16 05:08:16,659 | INFO | Epoch 50 Train Time 96.30384373664856s

2025-10-16 05:09:44,145 | INFO | Training epoch 51, Batch 1000/1000: LR=9.98e-05, Loss=4.73e-03 BER=1.97e-03 FER=1.77e-02
2025-10-16 05:09:44,196 | INFO | Epoch 51 Train Time 87.53495407104492s

2025-10-16 05:11:14,329 | INFO | Training epoch 52, Batch 1000/1000: LR=9.98e-05, Loss=4.72e-03 BER=1.94e-03 FER=1.75e-02
2025-10-16 05:11:14,383 | INFO | Epoch 52 Train Time 90.18534827232361s

2025-10-16 05:12:50,939 | INFO | Training epoch 53, Batch 1000/1000: LR=9.98e-05, Loss=4.69e-03 BER=1.91e-03 FER=1.73e-02
2025-10-16 05:12:51,000 | INFO | Epoch 53 Train Time 96.61565256118774s

2025-10-16 05:14:23,859 | INFO | Training epoch 54, Batch 1000/1000: LR=9.98e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.73e-02
2025-10-16 05:14:23,905 | INFO | Epoch 54 Train Time 92.90330719947815s

2025-10-16 05:15:57,810 | INFO | Training epoch 55, Batch 1000/1000: LR=9.98e-05, Loss=5.07e-03 BER=2.06e-03 FER=1.98e-02
2025-10-16 05:15:57,866 | INFO | Epoch 55 Train Time 93.95972871780396s

2025-10-16 05:17:30,719 | INFO | Training epoch 56, Batch 1000/1000: LR=9.98e-05, Loss=4.88e-03 BER=1.98e-03 FER=1.78e-02
2025-10-16 05:17:30,763 | INFO | Epoch 56 Train Time 92.8944103717804s

2025-10-16 05:19:02,061 | INFO | Training epoch 57, Batch 1000/1000: LR=9.98e-05, Loss=4.65e-03 BER=1.93e-03 FER=1.74e-02
2025-10-16 05:19:02,112 | INFO | Epoch 57 Train Time 91.34642958641052s

2025-10-16 05:20:34,115 | INFO | Training epoch 58, Batch 1000/1000: LR=9.98e-05, Loss=4.62e-03 BER=1.86e-03 FER=1.71e-02
2025-10-16 05:20:34,168 | INFO | Epoch 58 Train Time 92.05599784851074s

2025-10-16 05:22:09,679 | INFO | Training epoch 59, Batch 1000/1000: LR=9.98e-05, Loss=4.53e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 05:22:09,735 | INFO | Epoch 59 Train Time 95.5647919178009s

2025-10-16 05:22:09,736 | INFO | [P2] saving best_model (QAT) with loss 0.004534 at epoch 59
2025-10-16 05:23:42,931 | INFO | Training epoch 60, Batch 1000/1000: LR=9.98e-05, Loss=4.83e-03 BER=1.96e-03 FER=1.76e-02
2025-10-16 05:23:43,010 | INFO | Epoch 60 Train Time 93.19911527633667s

2025-10-16 05:25:19,745 | INFO | Training epoch 61, Batch 1000/1000: LR=9.98e-05, Loss=4.76e-03 BER=1.92e-03 FER=1.74e-02
2025-10-16 05:25:19,801 | INFO | Epoch 61 Train Time 96.78911280632019s

2025-10-16 05:26:57,766 | INFO | Training epoch 62, Batch 1000/1000: LR=9.98e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.69e-02
2025-10-16 05:26:57,811 | INFO | Epoch 62 Train Time 98.00904631614685s

2025-10-16 05:26:57,812 | INFO | [P2] saving best_model (QAT) with loss 0.004524 at epoch 62
2025-10-16 05:28:28,446 | INFO | Training epoch 63, Batch 1000/1000: LR=9.98e-05, Loss=4.79e-03 BER=1.98e-03 FER=1.77e-02
2025-10-16 05:28:28,526 | INFO | Epoch 63 Train Time 90.63280534744263s

2025-10-16 05:30:12,904 | INFO | Training epoch 64, Batch 1000/1000: LR=9.98e-05, Loss=4.80e-03 BER=1.97e-03 FER=1.76e-02
2025-10-16 05:30:12,966 | INFO | Epoch 64 Train Time 104.43900203704834s

2025-10-16 05:31:39,400 | INFO | Training epoch 65, Batch 1000/1000: LR=9.98e-05, Loss=4.78e-03 BER=1.96e-03 FER=1.77e-02
2025-10-16 05:31:39,448 | INFO | Epoch 65 Train Time 86.47991633415222s

2025-10-16 05:33:09,343 | INFO | Training epoch 66, Batch 1000/1000: LR=9.97e-05, Loss=4.80e-03 BER=1.99e-03 FER=1.75e-02
2025-10-16 05:33:09,393 | INFO | Epoch 66 Train Time 89.94431757926941s

2025-10-16 05:34:36,638 | INFO | Training epoch 67, Batch 1000/1000: LR=9.97e-05, Loss=4.69e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 05:34:36,705 | INFO | Epoch 67 Train Time 87.31043410301208s

2025-10-16 05:36:11,338 | INFO | Training epoch 68, Batch 1000/1000: LR=9.97e-05, Loss=4.92e-03 BER=2.01e-03 FER=1.82e-02
2025-10-16 05:36:11,387 | INFO | Epoch 68 Train Time 94.6814546585083s

2025-10-16 05:37:44,837 | INFO | Training epoch 69, Batch 1000/1000: LR=9.97e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.69e-02
2025-10-16 05:37:44,886 | INFO | Epoch 69 Train Time 93.49729299545288s

2025-10-16 05:39:17,264 | INFO | Training epoch 70, Batch 1000/1000: LR=9.97e-05, Loss=4.68e-03 BER=1.90e-03 FER=1.72e-02
2025-10-16 05:39:17,321 | INFO | Epoch 70 Train Time 92.43382811546326s

2025-10-16 05:40:51,555 | INFO | Training epoch 71, Batch 1000/1000: LR=9.97e-05, Loss=4.82e-03 BER=1.99e-03 FER=1.76e-02
2025-10-16 05:40:51,633 | INFO | Epoch 71 Train Time 94.30537104606628s

2025-10-16 05:42:31,853 | INFO | Training epoch 72, Batch 1000/1000: LR=9.97e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.70e-02
2025-10-16 05:42:31,924 | INFO | Epoch 72 Train Time 100.28934669494629s

2025-10-16 05:44:10,195 | INFO | Training epoch 73, Batch 1000/1000: LR=9.97e-05, Loss=4.84e-03 BER=1.97e-03 FER=1.76e-02
2025-10-16 05:44:10,235 | INFO | Epoch 73 Train Time 98.3091561794281s

2025-10-16 05:45:47,963 | INFO | Training epoch 74, Batch 1000/1000: LR=9.97e-05, Loss=4.72e-03 BER=1.93e-03 FER=1.73e-02
2025-10-16 05:45:48,008 | INFO | Epoch 74 Train Time 97.77188301086426s

2025-10-16 05:47:22,216 | INFO | Training epoch 75, Batch 1000/1000: LR=9.97e-05, Loss=4.81e-03 BER=1.94e-03 FER=1.75e-02
2025-10-16 05:47:22,259 | INFO | Epoch 75 Train Time 94.24871039390564s

2025-10-16 05:48:52,654 | INFO | Training epoch 76, Batch 1000/1000: LR=9.97e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 05:48:52,699 | INFO | Epoch 76 Train Time 90.43876028060913s

2025-10-16 05:50:28,656 | INFO | Training epoch 77, Batch 1000/1000: LR=9.96e-05, Loss=4.73e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 05:50:28,726 | INFO | Epoch 77 Train Time 96.02508568763733s

2025-10-16 05:52:10,482 | INFO | Training epoch 78, Batch 1000/1000: LR=9.96e-05, Loss=4.83e-03 BER=2.00e-03 FER=1.78e-02
2025-10-16 05:52:10,521 | INFO | Epoch 78 Train Time 101.79379224777222s

2025-10-16 05:53:43,916 | INFO | Training epoch 79, Batch 1000/1000: LR=9.96e-05, Loss=4.85e-03 BER=1.95e-03 FER=1.75e-02
2025-10-16 05:53:44,006 | INFO | Epoch 79 Train Time 93.48382377624512s

2025-10-16 05:55:24,011 | INFO | Training epoch 80, Batch 1000/1000: LR=9.96e-05, Loss=4.72e-03 BER=1.95e-03 FER=1.74e-02
2025-10-16 05:55:24,056 | INFO | Epoch 80 Train Time 100.04861521720886s

2025-10-16 05:57:01,528 | INFO | Training epoch 81, Batch 1000/1000: LR=9.96e-05, Loss=4.68e-03 BER=1.94e-03 FER=1.72e-02
2025-10-16 05:57:01,573 | INFO | Epoch 81 Train Time 97.51586484909058s

2025-10-16 05:58:32,871 | INFO | Training epoch 82, Batch 1000/1000: LR=9.96e-05, Loss=4.89e-03 BER=2.01e-03 FER=1.79e-02
2025-10-16 05:58:32,925 | INFO | Epoch 82 Train Time 91.35111665725708s

2025-10-16 06:00:07,132 | INFO | Training epoch 83, Batch 1000/1000: LR=9.96e-05, Loss=4.48e-03 BER=1.82e-03 FER=1.64e-02
2025-10-16 06:00:07,185 | INFO | Epoch 83 Train Time 94.25913572311401s

2025-10-16 06:00:07,186 | INFO | [P2] saving best_model (QAT) with loss 0.004480 at epoch 83
2025-10-16 06:01:36,859 | INFO | Training epoch 84, Batch 1000/1000: LR=9.96e-05, Loss=4.89e-03 BER=2.00e-03 FER=1.79e-02
2025-10-16 06:01:36,922 | INFO | Epoch 84 Train Time 89.63683700561523s

2025-10-16 06:03:12,043 | INFO | Training epoch 85, Batch 1000/1000: LR=9.96e-05, Loss=4.94e-03 BER=2.01e-03 FER=1.81e-02
2025-10-16 06:03:12,087 | INFO | Epoch 85 Train Time 95.16336965560913s

2025-10-16 06:04:43,075 | INFO | Training epoch 86, Batch 1000/1000: LR=9.96e-05, Loss=4.83e-03 BER=1.96e-03 FER=1.75e-02
2025-10-16 06:04:43,125 | INFO | Epoch 86 Train Time 91.03691577911377s

2025-10-16 06:06:15,824 | INFO | Training epoch 87, Batch 1000/1000: LR=9.95e-05, Loss=4.75e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 06:06:15,886 | INFO | Epoch 87 Train Time 92.76019883155823s

2025-10-16 06:07:46,226 | INFO | Training epoch 88, Batch 1000/1000: LR=9.95e-05, Loss=4.73e-03 BER=1.93e-03 FER=1.72e-02
2025-10-16 06:07:46,277 | INFO | Epoch 88 Train Time 90.38984322547913s

2025-10-16 06:09:23,958 | INFO | Training epoch 89, Batch 1000/1000: LR=9.95e-05, Loss=4.63e-03 BER=1.90e-03 FER=1.70e-02
2025-10-16 06:09:24,005 | INFO | Epoch 89 Train Time 97.72559905052185s

2025-10-16 06:11:02,338 | INFO | Training epoch 90, Batch 1000/1000: LR=9.95e-05, Loss=4.73e-03 BER=1.94e-03 FER=1.71e-02
2025-10-16 06:11:02,393 | INFO | Epoch 90 Train Time 98.38775777816772s

2025-10-16 06:12:34,705 | INFO | Training epoch 91, Batch 1000/1000: LR=9.95e-05, Loss=4.59e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 06:12:34,771 | INFO | Epoch 91 Train Time 92.37613010406494s

2025-10-16 06:14:12,640 | INFO | Training epoch 92, Batch 1000/1000: LR=9.95e-05, Loss=4.76e-03 BER=1.93e-03 FER=1.77e-02
2025-10-16 06:14:12,694 | INFO | Epoch 92 Train Time 97.9212281703949s

2025-10-16 06:15:40,837 | INFO | Training epoch 93, Batch 1000/1000: LR=9.95e-05, Loss=4.90e-03 BER=2.00e-03 FER=1.80e-02
2025-10-16 06:15:40,890 | INFO | Epoch 93 Train Time 88.19351077079773s

2025-10-16 06:17:17,110 | INFO | Training epoch 94, Batch 1000/1000: LR=9.95e-05, Loss=4.75e-03 BER=1.95e-03 FER=1.78e-02
2025-10-16 06:17:17,164 | INFO | Epoch 94 Train Time 96.2726833820343s

2025-10-16 06:18:51,729 | INFO | Training epoch 95, Batch 1000/1000: LR=9.95e-05, Loss=4.71e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 06:18:51,789 | INFO | Epoch 95 Train Time 94.62344646453857s

2025-10-16 06:20:24,519 | INFO | Training epoch 96, Batch 1000/1000: LR=9.94e-05, Loss=4.73e-03 BER=1.92e-03 FER=1.73e-02
2025-10-16 06:20:24,581 | INFO | Epoch 96 Train Time 92.79077053070068s

2025-10-16 06:21:56,491 | INFO | Training epoch 97, Batch 1000/1000: LR=9.94e-05, Loss=4.76e-03 BER=1.95e-03 FER=1.75e-02
2025-10-16 06:21:56,543 | INFO | Epoch 97 Train Time 91.96096706390381s

2025-10-16 06:23:26,697 | INFO | Training epoch 98, Batch 1000/1000: LR=9.94e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.67e-02
2025-10-16 06:23:26,749 | INFO | Epoch 98 Train Time 90.20474696159363s

2025-10-16 06:25:08,725 | INFO | Training epoch 99, Batch 1000/1000: LR=9.94e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.72e-02
2025-10-16 06:25:08,780 | INFO | Epoch 99 Train Time 102.0295102596283s

2025-10-16 06:26:39,598 | INFO | Training epoch 100, Batch 1000/1000: LR=9.94e-05, Loss=4.97e-03 BER=2.04e-03 FER=1.86e-02
2025-10-16 06:26:39,655 | INFO | Epoch 100 Train Time 90.8734085559845s

2025-10-16 06:28:13,435 | INFO | Training epoch 101, Batch 1000/1000: LR=9.94e-05, Loss=4.56e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 06:28:13,501 | INFO | Epoch 101 Train Time 93.84504127502441s

2025-10-16 06:29:47,936 | INFO | Training epoch 102, Batch 1000/1000: LR=9.94e-05, Loss=4.67e-03 BER=1.89e-03 FER=1.73e-02
2025-10-16 06:29:47,983 | INFO | Epoch 102 Train Time 94.48097944259644s

2025-10-16 06:31:23,837 | INFO | Training epoch 103, Batch 1000/1000: LR=9.94e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.73e-02
2025-10-16 06:31:23,893 | INFO | Epoch 103 Train Time 95.90905284881592s

2025-10-16 06:32:56,051 | INFO | Training epoch 104, Batch 1000/1000: LR=9.94e-05, Loss=4.58e-03 BER=1.87e-03 FER=1.69e-02
2025-10-16 06:32:56,104 | INFO | Epoch 104 Train Time 92.20807456970215s

2025-10-16 06:34:25,755 | INFO | Training epoch 105, Batch 1000/1000: LR=9.93e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.69e-02
2025-10-16 06:34:25,805 | INFO | Epoch 105 Train Time 89.69989252090454s

2025-10-16 06:36:08,394 | INFO | Training epoch 106, Batch 1000/1000: LR=9.93e-05, Loss=4.63e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 06:36:08,458 | INFO | Epoch 106 Train Time 102.65172624588013s

2025-10-16 06:37:43,039 | INFO | Training epoch 107, Batch 1000/1000: LR=9.93e-05, Loss=4.81e-03 BER=1.97e-03 FER=1.77e-02
2025-10-16 06:37:43,079 | INFO | Epoch 107 Train Time 94.61962819099426s

2025-10-16 06:39:14,863 | INFO | Training epoch 108, Batch 1000/1000: LR=9.93e-05, Loss=4.77e-03 BER=1.94e-03 FER=1.75e-02
2025-10-16 06:39:14,921 | INFO | Epoch 108 Train Time 91.84155368804932s

2025-10-16 06:40:48,306 | INFO | Training epoch 109, Batch 1000/1000: LR=9.93e-05, Loss=4.77e-03 BER=1.97e-03 FER=1.77e-02
2025-10-16 06:40:48,359 | INFO | Epoch 109 Train Time 93.43698740005493s

2025-10-16 06:42:24,424 | INFO | Training epoch 110, Batch 1000/1000: LR=9.93e-05, Loss=4.62e-03 BER=1.88e-03 FER=1.71e-02
2025-10-16 06:42:24,482 | INFO | Epoch 110 Train Time 96.12066602706909s

2025-10-16 06:44:03,646 | INFO | Training epoch 111, Batch 1000/1000: LR=9.93e-05, Loss=4.72e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 06:44:03,696 | INFO | Epoch 111 Train Time 99.21210622787476s

2025-10-16 06:45:47,019 | INFO | Training epoch 112, Batch 1000/1000: LR=9.92e-05, Loss=4.67e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 06:45:47,079 | INFO | Epoch 112 Train Time 103.38145875930786s

2025-10-16 06:47:20,089 | INFO | Training epoch 113, Batch 1000/1000: LR=9.92e-05, Loss=4.73e-03 BER=1.95e-03 FER=1.74e-02
2025-10-16 06:47:20,133 | INFO | Epoch 113 Train Time 93.05180549621582s

2025-10-16 06:48:49,480 | INFO | Training epoch 114, Batch 1000/1000: LR=9.92e-05, Loss=4.65e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 06:48:49,553 | INFO | Epoch 114 Train Time 89.41901111602783s

2025-10-16 06:50:24,931 | INFO | Training epoch 115, Batch 1000/1000: LR=9.92e-05, Loss=4.55e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 06:50:24,975 | INFO | Epoch 115 Train Time 95.42114090919495s

2025-10-16 06:51:59,855 | INFO | Training epoch 116, Batch 1000/1000: LR=9.92e-05, Loss=4.53e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 06:51:59,896 | INFO | Epoch 116 Train Time 94.91926217079163s

2025-10-16 06:53:28,232 | INFO | Training epoch 117, Batch 1000/1000: LR=9.92e-05, Loss=4.71e-03 BER=1.94e-03 FER=1.69e-02
2025-10-16 06:53:28,298 | INFO | Epoch 117 Train Time 88.39934372901917s

2025-10-16 06:55:09,218 | INFO | Training epoch 118, Batch 1000/1000: LR=9.92e-05, Loss=4.66e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 06:55:09,276 | INFO | Epoch 118 Train Time 100.97698068618774s

2025-10-16 06:56:44,424 | INFO | Training epoch 119, Batch 1000/1000: LR=9.92e-05, Loss=4.86e-03 BER=1.99e-03 FER=1.78e-02
2025-10-16 06:56:44,488 | INFO | Epoch 119 Train Time 95.21100521087646s

2025-10-16 06:58:21,923 | INFO | Training epoch 120, Batch 1000/1000: LR=9.91e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.69e-02
2025-10-16 06:58:21,971 | INFO | Epoch 120 Train Time 97.48144626617432s

2025-10-16 06:59:54,704 | INFO | Training epoch 121, Batch 1000/1000: LR=9.91e-05, Loss=4.66e-03 BER=1.92e-03 FER=1.69e-02
2025-10-16 06:59:54,747 | INFO | Epoch 121 Train Time 92.77458119392395s

2025-10-16 07:01:20,557 | INFO | Training epoch 122, Batch 1000/1000: LR=9.91e-05, Loss=4.63e-03 BER=1.93e-03 FER=1.74e-02
2025-10-16 07:01:20,604 | INFO | Epoch 122 Train Time 85.85621643066406s

2025-10-16 07:02:55,660 | INFO | Training epoch 123, Batch 1000/1000: LR=9.91e-05, Loss=4.38e-03 BER=1.79e-03 FER=1.64e-02
2025-10-16 07:02:55,706 | INFO | Epoch 123 Train Time 95.10002017021179s

2025-10-16 07:02:55,707 | INFO | [P2] saving best_model (QAT) with loss 0.004383 at epoch 123
2025-10-16 07:04:35,734 | INFO | Training epoch 124, Batch 1000/1000: LR=9.91e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.72e-02
2025-10-16 07:04:35,778 | INFO | Epoch 124 Train Time 99.9979841709137s

2025-10-16 07:06:15,566 | INFO | Training epoch 125, Batch 1000/1000: LR=9.91e-05, Loss=4.60e-03 BER=1.90e-03 FER=1.67e-02
2025-10-16 07:06:15,615 | INFO | Epoch 125 Train Time 99.83513188362122s

2025-10-16 07:07:48,508 | INFO | Training epoch 126, Batch 1000/1000: LR=9.90e-05, Loss=4.71e-03 BER=1.93e-03 FER=1.72e-02
2025-10-16 07:07:48,554 | INFO | Epoch 126 Train Time 92.93829846382141s

2025-10-16 07:09:31,369 | INFO | Training epoch 127, Batch 1000/1000: LR=9.90e-05, Loss=4.71e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 07:09:31,445 | INFO | Epoch 127 Train Time 102.88927626609802s

2025-10-16 07:11:07,974 | INFO | Training epoch 128, Batch 1000/1000: LR=9.90e-05, Loss=4.74e-03 BER=1.93e-03 FER=1.74e-02
2025-10-16 07:11:08,021 | INFO | Epoch 128 Train Time 96.57431125640869s

2025-10-16 07:12:42,740 | INFO | Training epoch 129, Batch 1000/1000: LR=9.90e-05, Loss=4.58e-03 BER=1.90e-03 FER=1.71e-02
2025-10-16 07:12:42,795 | INFO | Epoch 129 Train Time 94.77280402183533s

2025-10-16 07:14:21,348 | INFO | Training epoch 130, Batch 1000/1000: LR=9.90e-05, Loss=4.76e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 07:14:21,411 | INFO | Epoch 130 Train Time 98.61319518089294s

2025-10-16 07:15:57,767 | INFO | Training epoch 131, Batch 1000/1000: LR=9.90e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.71e-02
2025-10-16 07:15:57,819 | INFO | Epoch 131 Train Time 96.4071409702301s

2025-10-16 07:17:39,615 | INFO | Training epoch 132, Batch 1000/1000: LR=9.90e-05, Loss=4.78e-03 BER=1.98e-03 FER=1.76e-02
2025-10-16 07:17:39,671 | INFO | Epoch 132 Train Time 101.84955596923828s

2025-10-16 07:19:08,750 | INFO | Training epoch 133, Batch 1000/1000: LR=9.89e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.69e-02
2025-10-16 07:19:08,795 | INFO | Epoch 133 Train Time 89.12231659889221s

2025-10-16 07:20:38,766 | INFO | Training epoch 134, Batch 1000/1000: LR=9.89e-05, Loss=4.65e-03 BER=1.93e-03 FER=1.73e-02
2025-10-16 07:20:38,818 | INFO | Epoch 134 Train Time 90.02042841911316s

2025-10-16 07:22:12,000 | INFO | Training epoch 135, Batch 1000/1000: LR=9.89e-05, Loss=4.80e-03 BER=1.96e-03 FER=1.72e-02
2025-10-16 07:22:12,062 | INFO | Epoch 135 Train Time 93.24165201187134s

2025-10-16 07:23:40,849 | INFO | Training epoch 136, Batch 1000/1000: LR=9.89e-05, Loss=4.59e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 07:23:40,911 | INFO | Epoch 136 Train Time 88.84742999076843s

2025-10-16 07:25:12,029 | INFO | Training epoch 137, Batch 1000/1000: LR=9.89e-05, Loss=4.53e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 07:25:12,099 | INFO | Epoch 137 Train Time 91.18544578552246s

2025-10-16 07:26:36,821 | INFO | Training epoch 138, Batch 1000/1000: LR=9.89e-05, Loss=4.78e-03 BER=2.01e-03 FER=1.77e-02
2025-10-16 07:26:36,872 | INFO | Epoch 138 Train Time 84.77122473716736s

2025-10-16 07:28:13,152 | INFO | Training epoch 139, Batch 1000/1000: LR=9.88e-05, Loss=4.79e-03 BER=1.98e-03 FER=1.76e-02
2025-10-16 07:28:13,209 | INFO | Epoch 139 Train Time 96.3364839553833s

2025-10-16 07:29:52,131 | INFO | Training epoch 140, Batch 1000/1000: LR=9.88e-05, Loss=4.76e-03 BER=1.96e-03 FER=1.77e-02
2025-10-16 07:29:52,181 | INFO | Epoch 140 Train Time 98.97087407112122s

2025-10-16 07:31:26,510 | INFO | Training epoch 141, Batch 1000/1000: LR=9.88e-05, Loss=4.56e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 07:31:26,555 | INFO | Epoch 141 Train Time 94.37175679206848s

2025-10-16 07:33:00,350 | INFO | Training epoch 142, Batch 1000/1000: LR=9.88e-05, Loss=4.55e-03 BER=1.87e-03 FER=1.69e-02
2025-10-16 07:33:00,405 | INFO | Epoch 142 Train Time 93.84884333610535s

2025-10-16 07:34:31,939 | INFO | Training epoch 143, Batch 1000/1000: LR=9.88e-05, Loss=4.72e-03 BER=1.95e-03 FER=1.72e-02
2025-10-16 07:34:31,990 | INFO | Epoch 143 Train Time 91.58435726165771s

2025-10-16 07:36:03,238 | INFO | Training epoch 144, Batch 1000/1000: LR=9.88e-05, Loss=4.74e-03 BER=1.92e-03 FER=1.74e-02
2025-10-16 07:36:03,307 | INFO | Epoch 144 Train Time 91.31489109992981s

2025-10-16 07:37:39,397 | INFO | Training epoch 145, Batch 1000/1000: LR=9.87e-05, Loss=4.67e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 07:37:39,450 | INFO | Epoch 145 Train Time 96.14133024215698s

2025-10-16 07:39:16,633 | INFO | Training epoch 146, Batch 1000/1000: LR=9.87e-05, Loss=4.54e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 07:39:16,704 | INFO | Epoch 146 Train Time 97.25279998779297s

2025-10-16 07:40:44,470 | INFO | Training epoch 147, Batch 1000/1000: LR=9.87e-05, Loss=4.72e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 07:40:44,529 | INFO | Epoch 147 Train Time 87.82184433937073s

2025-10-16 07:42:16,304 | INFO | Training epoch 148, Batch 1000/1000: LR=9.87e-05, Loss=4.70e-03 BER=1.93e-03 FER=1.72e-02
2025-10-16 07:42:16,363 | INFO | Epoch 148 Train Time 91.8326780796051s

2025-10-16 07:43:57,925 | INFO | Training epoch 149, Batch 1000/1000: LR=9.87e-05, Loss=4.75e-03 BER=1.95e-03 FER=1.73e-02
2025-10-16 07:43:57,974 | INFO | Epoch 149 Train Time 101.60981178283691s

2025-10-16 07:45:28,966 | INFO | Training epoch 150, Batch 1000/1000: LR=9.87e-05, Loss=4.69e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 07:45:29,029 | INFO | Epoch 150 Train Time 91.05383038520813s

2025-10-16 07:47:07,335 | INFO | Training epoch 151, Batch 1000/1000: LR=9.86e-05, Loss=4.89e-03 BER=2.00e-03 FER=1.78e-02
2025-10-16 07:47:07,393 | INFO | Epoch 151 Train Time 98.36254072189331s

2025-10-16 07:48:36,031 | INFO | Training epoch 152, Batch 1000/1000: LR=9.86e-05, Loss=4.52e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 07:48:36,094 | INFO | Epoch 152 Train Time 88.69897842407227s

2025-10-16 07:50:06,664 | INFO | Training epoch 153, Batch 1000/1000: LR=9.86e-05, Loss=4.52e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 07:50:06,734 | INFO | Epoch 153 Train Time 90.63933968544006s

2025-10-16 07:51:46,220 | INFO | Training epoch 154, Batch 1000/1000: LR=9.86e-05, Loss=4.63e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 07:51:46,281 | INFO | Epoch 154 Train Time 99.54488396644592s

2025-10-16 07:53:18,164 | INFO | Training epoch 155, Batch 1000/1000: LR=9.86e-05, Loss=4.71e-03 BER=1.95e-03 FER=1.75e-02
2025-10-16 07:53:18,214 | INFO | Epoch 155 Train Time 91.93088030815125s

2025-10-16 07:54:59,648 | INFO | Training epoch 156, Batch 1000/1000: LR=9.85e-05, Loss=4.70e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 07:54:59,701 | INFO | Epoch 156 Train Time 101.48595571517944s

2025-10-16 07:56:37,651 | INFO | Training epoch 157, Batch 1000/1000: LR=9.85e-05, Loss=4.54e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 07:56:37,692 | INFO | Epoch 157 Train Time 97.99028611183167s

2025-10-16 07:58:08,868 | INFO | Training epoch 158, Batch 1000/1000: LR=9.85e-05, Loss=4.77e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 07:58:08,912 | INFO | Epoch 158 Train Time 91.2189531326294s

2025-10-16 07:59:42,348 | INFO | Training epoch 159, Batch 1000/1000: LR=9.85e-05, Loss=4.76e-03 BER=1.93e-03 FER=1.73e-02
2025-10-16 07:59:42,399 | INFO | Epoch 159 Train Time 93.48450565338135s

2025-10-16 08:01:08,653 | INFO | Training epoch 160, Batch 1000/1000: LR=9.85e-05, Loss=4.63e-03 BER=1.90e-03 FER=1.71e-02
2025-10-16 08:01:08,698 | INFO | Epoch 160 Train Time 86.2971031665802s

2025-10-16 08:02:47,433 | INFO | Training epoch 161, Batch 1000/1000: LR=9.84e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.68e-02
2025-10-16 08:02:47,474 | INFO | Epoch 161 Train Time 98.77444982528687s

2025-10-16 08:04:18,179 | INFO | Training epoch 162, Batch 1000/1000: LR=9.84e-05, Loss=4.64e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 08:04:18,224 | INFO | Epoch 162 Train Time 90.74884724617004s

2025-10-16 08:05:53,872 | INFO | Training epoch 163, Batch 1000/1000: LR=9.84e-05, Loss=4.59e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 08:05:53,948 | INFO | Epoch 163 Train Time 95.72236585617065s

2025-10-16 08:07:31,928 | INFO | Training epoch 164, Batch 1000/1000: LR=9.84e-05, Loss=4.90e-03 BER=1.99e-03 FER=1.76e-02
2025-10-16 08:07:31,977 | INFO | Epoch 164 Train Time 98.02823829650879s

2025-10-16 08:09:16,056 | INFO | Training epoch 165, Batch 1000/1000: LR=9.84e-05, Loss=4.78e-03 BER=1.94e-03 FER=1.72e-02
2025-10-16 08:09:16,116 | INFO | Epoch 165 Train Time 104.13642144203186s

2025-10-16 08:10:58,112 | INFO | Training epoch 166, Batch 1000/1000: LR=9.83e-05, Loss=4.59e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 08:10:58,162 | INFO | Epoch 166 Train Time 102.04361486434937s

2025-10-16 08:12:31,171 | INFO | Training epoch 167, Batch 1000/1000: LR=9.83e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 08:12:31,216 | INFO | Epoch 167 Train Time 93.05221772193909s

2025-10-16 08:14:13,339 | INFO | Training epoch 168, Batch 1000/1000: LR=9.83e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.68e-02
2025-10-16 08:14:13,387 | INFO | Epoch 168 Train Time 102.16993808746338s

2025-10-16 08:15:50,660 | INFO | Training epoch 169, Batch 1000/1000: LR=9.83e-05, Loss=4.59e-03 BER=1.88e-03 FER=1.69e-02
2025-10-16 08:15:50,709 | INFO | Epoch 169 Train Time 97.32151103019714s

2025-10-16 08:17:28,603 | INFO | Training epoch 170, Batch 1000/1000: LR=9.83e-05, Loss=4.64e-03 BER=1.91e-03 FER=1.73e-02
2025-10-16 08:17:28,664 | INFO | Epoch 170 Train Time 97.95309066772461s

2025-10-16 08:19:02,722 | INFO | Training epoch 171, Batch 1000/1000: LR=9.82e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.67e-02
2025-10-16 08:19:02,790 | INFO | Epoch 171 Train Time 94.1248459815979s

2025-10-16 08:20:39,051 | INFO | Training epoch 172, Batch 1000/1000: LR=9.82e-05, Loss=4.61e-03 BER=1.88e-03 FER=1.67e-02
2025-10-16 08:20:39,100 | INFO | Epoch 172 Train Time 96.30432868003845s

2025-10-16 08:22:21,325 | INFO | Training epoch 173, Batch 1000/1000: LR=9.82e-05, Loss=4.77e-03 BER=1.94e-03 FER=1.75e-02
2025-10-16 08:22:21,377 | INFO | Epoch 173 Train Time 102.2748293876648s

2025-10-16 08:24:08,237 | INFO | Training epoch 174, Batch 1000/1000: LR=9.82e-05, Loss=4.80e-03 BER=1.94e-03 FER=1.72e-02
2025-10-16 08:24:08,298 | INFO | Epoch 174 Train Time 106.91876888275146s

2025-10-16 08:25:40,409 | INFO | Training epoch 175, Batch 1000/1000: LR=9.82e-05, Loss=4.65e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 08:25:40,471 | INFO | Epoch 175 Train Time 92.17164087295532s

2025-10-16 08:27:14,371 | INFO | Training epoch 176, Batch 1000/1000: LR=9.81e-05, Loss=4.76e-03 BER=1.94e-03 FER=1.72e-02
2025-10-16 08:27:14,430 | INFO | Epoch 176 Train Time 93.95775055885315s

2025-10-16 08:28:53,726 | INFO | Training epoch 177, Batch 1000/1000: LR=9.81e-05, Loss=4.61e-03 BER=1.88e-03 FER=1.71e-02
2025-10-16 08:28:53,768 | INFO | Epoch 177 Train Time 99.33601331710815s

2025-10-16 08:30:24,028 | INFO | Training epoch 178, Batch 1000/1000: LR=9.81e-05, Loss=4.67e-03 BER=1.90e-03 FER=1.73e-02
2025-10-16 08:30:24,081 | INFO | Epoch 178 Train Time 90.31100726127625s

2025-10-16 08:32:03,173 | INFO | Training epoch 179, Batch 1000/1000: LR=9.81e-05, Loss=4.83e-03 BER=1.96e-03 FER=1.75e-02
2025-10-16 08:32:03,238 | INFO | Epoch 179 Train Time 99.1557126045227s

2025-10-16 08:33:45,522 | INFO | Training epoch 180, Batch 1000/1000: LR=9.81e-05, Loss=4.73e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 08:33:45,575 | INFO | Epoch 180 Train Time 102.33550691604614s

2025-10-16 08:35:18,224 | INFO | Training epoch 181, Batch 1000/1000: LR=9.80e-05, Loss=4.72e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 08:35:18,284 | INFO | Epoch 181 Train Time 92.70783972740173s

2025-10-16 08:36:52,776 | INFO | Training epoch 182, Batch 1000/1000: LR=9.80e-05, Loss=4.73e-03 BER=1.90e-03 FER=1.71e-02
2025-10-16 08:36:52,826 | INFO | Epoch 182 Train Time 94.54098582267761s

2025-10-16 08:38:35,605 | INFO | Training epoch 183, Batch 1000/1000: LR=9.80e-05, Loss=4.61e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 08:38:35,659 | INFO | Epoch 183 Train Time 102.83183550834656s

2025-10-16 08:40:04,151 | INFO | Training epoch 184, Batch 1000/1000: LR=9.80e-05, Loss=4.54e-03 BER=1.85e-03 FER=1.65e-02
2025-10-16 08:40:04,197 | INFO | Epoch 184 Train Time 88.53669333457947s

2025-10-16 08:41:35,431 | INFO | Training epoch 185, Batch 1000/1000: LR=9.79e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.69e-02
2025-10-16 08:41:35,477 | INFO | Epoch 185 Train Time 91.27915000915527s

2025-10-16 08:43:16,456 | INFO | Training epoch 186, Batch 1000/1000: LR=9.79e-05, Loss=4.77e-03 BER=1.95e-03 FER=1.75e-02
2025-10-16 08:43:16,518 | INFO | Epoch 186 Train Time 101.03970265388489s

2025-10-16 08:44:54,437 | INFO | Training epoch 187, Batch 1000/1000: LR=9.79e-05, Loss=4.57e-03 BER=1.86e-03 FER=1.67e-02
2025-10-16 08:44:54,491 | INFO | Epoch 187 Train Time 97.97133255004883s

2025-10-16 08:46:22,772 | INFO | Training epoch 188, Batch 1000/1000: LR=9.79e-05, Loss=4.59e-03 BER=1.91e-03 FER=1.72e-02
2025-10-16 08:46:22,827 | INFO | Epoch 188 Train Time 88.33531451225281s

2025-10-16 08:48:01,270 | INFO | Training epoch 189, Batch 1000/1000: LR=9.79e-05, Loss=4.62e-03 BER=1.88e-03 FER=1.73e-02
2025-10-16 08:48:01,316 | INFO | Epoch 189 Train Time 98.48738622665405s

2025-10-16 08:49:33,143 | INFO | Training epoch 190, Batch 1000/1000: LR=9.78e-05, Loss=4.75e-03 BER=1.98e-03 FER=1.74e-02
2025-10-16 08:49:33,195 | INFO | Epoch 190 Train Time 91.87740993499756s

2025-10-16 08:51:02,538 | INFO | Training epoch 191, Batch 1000/1000: LR=9.78e-05, Loss=4.68e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 08:51:02,592 | INFO | Epoch 191 Train Time 89.39564371109009s

2025-10-16 08:52:38,362 | INFO | Training epoch 192, Batch 1000/1000: LR=9.78e-05, Loss=4.87e-03 BER=1.99e-03 FER=1.79e-02
2025-10-16 08:52:38,413 | INFO | Epoch 192 Train Time 95.81971001625061s

2025-10-16 08:54:09,767 | INFO | Training epoch 193, Batch 1000/1000: LR=9.78e-05, Loss=4.85e-03 BER=1.99e-03 FER=1.74e-02
2025-10-16 08:54:09,830 | INFO | Epoch 193 Train Time 91.41664719581604s

2025-10-16 08:55:47,413 | INFO | Training epoch 194, Batch 1000/1000: LR=9.77e-05, Loss=4.60e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 08:55:47,470 | INFO | Epoch 194 Train Time 97.63944983482361s

2025-10-16 08:57:28,612 | INFO | Training epoch 195, Batch 1000/1000: LR=9.77e-05, Loss=4.79e-03 BER=1.97e-03 FER=1.76e-02
2025-10-16 08:57:28,675 | INFO | Epoch 195 Train Time 101.20333290100098s

2025-10-16 08:59:10,021 | INFO | Training epoch 196, Batch 1000/1000: LR=9.77e-05, Loss=4.64e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 08:59:10,094 | INFO | Epoch 196 Train Time 101.41725897789001s

2025-10-16 09:00:42,039 | INFO | Training epoch 197, Batch 1000/1000: LR=9.77e-05, Loss=4.68e-03 BER=1.94e-03 FER=1.72e-02
2025-10-16 09:00:42,087 | INFO | Epoch 197 Train Time 91.99027156829834s

2025-10-16 09:02:09,879 | INFO | Training epoch 198, Batch 1000/1000: LR=9.76e-05, Loss=4.83e-03 BER=1.99e-03 FER=1.79e-02
2025-10-16 09:02:09,921 | INFO | Epoch 198 Train Time 87.83279490470886s

2025-10-16 09:03:43,740 | INFO | Training epoch 199, Batch 1000/1000: LR=9.76e-05, Loss=4.87e-03 BER=2.00e-03 FER=1.78e-02
2025-10-16 09:03:43,803 | INFO | Epoch 199 Train Time 93.88052749633789s

2025-10-16 09:05:18,493 | INFO | Training epoch 200, Batch 1000/1000: LR=9.76e-05, Loss=4.71e-03 BER=1.90e-03 FER=1.71e-02
2025-10-16 09:05:18,569 | INFO | Epoch 200 Train Time 94.76472163200378s

2025-10-16 09:06:55,332 | INFO | Training epoch 201, Batch 1000/1000: LR=9.76e-05, Loss=4.86e-03 BER=1.99e-03 FER=1.80e-02
2025-10-16 09:06:55,383 | INFO | Epoch 201 Train Time 96.8128547668457s

2025-10-16 09:08:34,444 | INFO | Training epoch 202, Batch 1000/1000: LR=9.76e-05, Loss=4.72e-03 BER=1.96e-03 FER=1.72e-02
2025-10-16 09:08:34,489 | INFO | Epoch 202 Train Time 99.10355234146118s

2025-10-16 09:10:11,816 | INFO | Training epoch 203, Batch 1000/1000: LR=9.75e-05, Loss=4.60e-03 BER=1.89e-03 FER=1.67e-02
2025-10-16 09:10:11,877 | INFO | Epoch 203 Train Time 97.38628768920898s

2025-10-16 09:11:44,461 | INFO | Training epoch 204, Batch 1000/1000: LR=9.75e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 09:11:44,505 | INFO | Epoch 204 Train Time 92.62737011909485s

2025-10-16 09:13:21,652 | INFO | Training epoch 205, Batch 1000/1000: LR=9.75e-05, Loss=4.82e-03 BER=1.98e-03 FER=1.77e-02
2025-10-16 09:13:21,722 | INFO | Epoch 205 Train Time 97.21531581878662s

2025-10-16 09:14:54,901 | INFO | Training epoch 206, Batch 1000/1000: LR=9.75e-05, Loss=4.66e-03 BER=1.88e-03 FER=1.69e-02
2025-10-16 09:14:54,959 | INFO | Epoch 206 Train Time 93.23608803749084s

2025-10-16 09:16:33,485 | INFO | Training epoch 207, Batch 1000/1000: LR=9.74e-05, Loss=4.68e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 09:16:33,559 | INFO | Epoch 207 Train Time 98.59757375717163s

2025-10-16 09:18:06,034 | INFO | Training epoch 208, Batch 1000/1000: LR=9.74e-05, Loss=4.63e-03 BER=1.91e-03 FER=1.70e-02
2025-10-16 09:18:06,079 | INFO | Epoch 208 Train Time 92.51880884170532s

2025-10-16 09:19:33,053 | INFO | Training epoch 209, Batch 1000/1000: LR=9.74e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.65e-02
2025-10-16 09:19:33,103 | INFO | Epoch 209 Train Time 87.02290630340576s

2025-10-16 09:21:08,822 | INFO | Training epoch 210, Batch 1000/1000: LR=9.74e-05, Loss=4.81e-03 BER=1.96e-03 FER=1.73e-02
2025-10-16 09:21:08,892 | INFO | Epoch 210 Train Time 95.78747415542603s

2025-10-16 09:22:45,427 | INFO | Training epoch 211, Batch 1000/1000: LR=9.73e-05, Loss=4.72e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 09:22:45,505 | INFO | Epoch 211 Train Time 96.61042833328247s

2025-10-16 09:24:24,746 | INFO | Training epoch 212, Batch 1000/1000: LR=9.73e-05, Loss=4.54e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 09:24:24,787 | INFO | Epoch 212 Train Time 99.2801628112793s

2025-10-16 09:25:52,046 | INFO | Training epoch 213, Batch 1000/1000: LR=9.73e-05, Loss=4.59e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 09:25:52,110 | INFO | Epoch 213 Train Time 87.3208909034729s

2025-10-16 09:27:30,068 | INFO | Training epoch 214, Batch 1000/1000: LR=9.73e-05, Loss=4.51e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 09:27:30,130 | INFO | Epoch 214 Train Time 98.01876449584961s

2025-10-16 09:29:11,748 | INFO | Training epoch 215, Batch 1000/1000: LR=9.72e-05, Loss=4.69e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 09:29:11,816 | INFO | Epoch 215 Train Time 101.68409776687622s

2025-10-16 09:30:41,634 | INFO | Training epoch 216, Batch 1000/1000: LR=9.72e-05, Loss=4.57e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 09:30:41,681 | INFO | Epoch 216 Train Time 89.86299133300781s

2025-10-16 09:32:13,234 | INFO | Training epoch 217, Batch 1000/1000: LR=9.72e-05, Loss=4.67e-03 BER=1.92e-03 FER=1.73e-02
2025-10-16 09:32:13,279 | INFO | Epoch 217 Train Time 91.59591746330261s

2025-10-16 09:33:48,479 | INFO | Training epoch 218, Batch 1000/1000: LR=9.72e-05, Loss=4.68e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 09:33:48,533 | INFO | Epoch 218 Train Time 95.25333714485168s

2025-10-16 09:35:19,147 | INFO | Training epoch 219, Batch 1000/1000: LR=9.71e-05, Loss=4.61e-03 BER=1.85e-03 FER=1.69e-02
2025-10-16 09:35:19,192 | INFO | Epoch 219 Train Time 90.65719985961914s

2025-10-16 09:36:52,948 | INFO | Training epoch 220, Batch 1000/1000: LR=9.71e-05, Loss=4.75e-03 BER=1.97e-03 FER=1.76e-02
2025-10-16 09:36:52,991 | INFO | Epoch 220 Train Time 93.7976450920105s

2025-10-16 09:38:30,588 | INFO | Training epoch 221, Batch 1000/1000: LR=9.71e-05, Loss=4.60e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 09:38:30,631 | INFO | Epoch 221 Train Time 97.6380124092102s

2025-10-16 09:40:04,635 | INFO | Training epoch 222, Batch 1000/1000: LR=9.70e-05, Loss=4.67e-03 BER=1.91e-03 FER=1.67e-02
2025-10-16 09:40:04,690 | INFO | Epoch 222 Train Time 94.05773568153381s

2025-10-16 09:41:38,459 | INFO | Training epoch 223, Batch 1000/1000: LR=9.70e-05, Loss=4.77e-03 BER=1.97e-03 FER=1.72e-02
2025-10-16 09:41:38,504 | INFO | Epoch 223 Train Time 93.8126540184021s

2025-10-16 09:43:16,203 | INFO | Training epoch 224, Batch 1000/1000: LR=9.70e-05, Loss=4.68e-03 BER=1.95e-03 FER=1.75e-02
2025-10-16 09:43:16,273 | INFO | Epoch 224 Train Time 97.76713800430298s

2025-10-16 09:44:55,720 | INFO | Training epoch 225, Batch 1000/1000: LR=9.70e-05, Loss=4.71e-03 BER=1.93e-03 FER=1.70e-02
2025-10-16 09:44:55,777 | INFO | Epoch 225 Train Time 99.50172162055969s

2025-10-16 09:46:29,471 | INFO | Training epoch 226, Batch 1000/1000: LR=9.69e-05, Loss=4.50e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 09:46:29,532 | INFO | Epoch 226 Train Time 93.75250887870789s

2025-10-16 09:48:00,438 | INFO | Training epoch 227, Batch 1000/1000: LR=9.69e-05, Loss=4.64e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 09:48:00,485 | INFO | Epoch 227 Train Time 90.95237421989441s

2025-10-16 09:49:32,480 | INFO | Training epoch 228, Batch 1000/1000: LR=9.69e-05, Loss=4.70e-03 BER=1.95e-03 FER=1.72e-02
2025-10-16 09:49:32,535 | INFO | Epoch 228 Train Time 92.04755687713623s

2025-10-16 09:51:02,135 | INFO | Training epoch 229, Batch 1000/1000: LR=9.69e-05, Loss=4.66e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 09:51:02,190 | INFO | Epoch 229 Train Time 89.65362310409546s

2025-10-16 09:52:37,560 | INFO | Training epoch 230, Batch 1000/1000: LR=9.68e-05, Loss=4.56e-03 BER=1.88e-03 FER=1.65e-02
2025-10-16 09:52:37,607 | INFO | Epoch 230 Train Time 95.4160704612732s

2025-10-16 09:54:05,965 | INFO | Training epoch 231, Batch 1000/1000: LR=9.68e-05, Loss=4.64e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 09:54:06,029 | INFO | Epoch 231 Train Time 88.4204773902893s

2025-10-16 09:55:41,875 | INFO | Training epoch 232, Batch 1000/1000: LR=9.68e-05, Loss=4.62e-03 BER=1.88e-03 FER=1.70e-02
2025-10-16 09:55:41,922 | INFO | Epoch 232 Train Time 95.89189338684082s

2025-10-16 09:57:10,769 | INFO | Training epoch 233, Batch 1000/1000: LR=9.67e-05, Loss=4.61e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 09:57:10,813 | INFO | Epoch 233 Train Time 88.88917422294617s

2025-10-16 09:58:49,983 | INFO | Training epoch 234, Batch 1000/1000: LR=9.67e-05, Loss=4.63e-03 BER=1.90e-03 FER=1.70e-02
2025-10-16 09:58:50,044 | INFO | Epoch 234 Train Time 99.22877168655396s

2025-10-16 10:00:29,963 | INFO | Training epoch 235, Batch 1000/1000: LR=9.67e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.65e-02
2025-10-16 10:00:30,025 | INFO | Epoch 235 Train Time 99.97900652885437s

2025-10-16 10:01:57,611 | INFO | Training epoch 236, Batch 1000/1000: LR=9.67e-05, Loss=4.62e-03 BER=1.86e-03 FER=1.67e-02
2025-10-16 10:01:57,665 | INFO | Epoch 236 Train Time 87.63919448852539s

2025-10-16 10:03:33,404 | INFO | Training epoch 237, Batch 1000/1000: LR=9.66e-05, Loss=4.74e-03 BER=1.91e-03 FER=1.73e-02
2025-10-16 10:03:33,464 | INFO | Epoch 237 Train Time 95.79694271087646s

2025-10-16 10:05:09,315 | INFO | Training epoch 238, Batch 1000/1000: LR=9.66e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.68e-02
2025-10-16 10:05:09,365 | INFO | Epoch 238 Train Time 95.89963483810425s

2025-10-16 10:06:39,348 | INFO | Training epoch 239, Batch 1000/1000: LR=9.66e-05, Loss=4.52e-03 BER=1.88e-03 FER=1.67e-02
2025-10-16 10:06:39,408 | INFO | Epoch 239 Train Time 90.04197883605957s

2025-10-16 10:08:08,957 | INFO | Training epoch 240, Batch 1000/1000: LR=9.66e-05, Loss=4.74e-03 BER=1.93e-03 FER=1.74e-02
2025-10-16 10:08:09,003 | INFO | Epoch 240 Train Time 89.59377765655518s

2025-10-16 10:09:46,630 | INFO | Training epoch 241, Batch 1000/1000: LR=9.65e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 10:09:46,699 | INFO | Epoch 241 Train Time 97.69500994682312s

2025-10-16 10:11:20,979 | INFO | Training epoch 242, Batch 1000/1000: LR=9.65e-05, Loss=4.70e-03 BER=1.93e-03 FER=1.70e-02
2025-10-16 10:11:21,032 | INFO | Epoch 242 Train Time 94.3314516544342s

2025-10-16 10:12:50,929 | INFO | Training epoch 243, Batch 1000/1000: LR=9.65e-05, Loss=4.72e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 10:12:50,983 | INFO | Epoch 243 Train Time 89.94971179962158s

2025-10-16 10:14:21,382 | INFO | Training epoch 244, Batch 1000/1000: LR=9.64e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 10:14:21,439 | INFO | Epoch 244 Train Time 90.45360398292542s

2025-10-16 10:15:51,149 | INFO | Training epoch 245, Batch 1000/1000: LR=9.64e-05, Loss=4.65e-03 BER=1.89e-03 FER=1.67e-02
2025-10-16 10:15:51,197 | INFO | Epoch 245 Train Time 89.75693321228027s

2025-10-16 10:17:22,836 | INFO | Training epoch 246, Batch 1000/1000: LR=9.64e-05, Loss=4.82e-03 BER=1.98e-03 FER=1.76e-02
2025-10-16 10:17:22,889 | INFO | Epoch 246 Train Time 91.69045543670654s

2025-10-16 10:18:55,097 | INFO | Training epoch 247, Batch 1000/1000: LR=9.64e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 10:18:55,144 | INFO | Epoch 247 Train Time 92.25263381004333s

2025-10-16 10:20:32,551 | INFO | Training epoch 248, Batch 1000/1000: LR=9.63e-05, Loss=4.44e-03 BER=1.81e-03 FER=1.63e-02
2025-10-16 10:20:32,600 | INFO | Epoch 248 Train Time 97.45444178581238s

2025-10-16 10:22:09,833 | INFO | Training epoch 249, Batch 1000/1000: LR=9.63e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.63e-02
2025-10-16 10:22:09,884 | INFO | Epoch 249 Train Time 97.2817554473877s

2025-10-16 10:23:46,817 | INFO | Training epoch 250, Batch 1000/1000: LR=9.63e-05, Loss=4.78e-03 BER=1.95e-03 FER=1.73e-02
2025-10-16 10:23:46,875 | INFO | Epoch 250 Train Time 96.98885750770569s

2025-10-16 10:25:23,249 | INFO | Training epoch 251, Batch 1000/1000: LR=9.62e-05, Loss=4.67e-03 BER=1.93e-03 FER=1.71e-02
2025-10-16 10:25:23,294 | INFO | Epoch 251 Train Time 96.41438317298889s

2025-10-16 10:26:59,317 | INFO | Training epoch 252, Batch 1000/1000: LR=9.62e-05, Loss=4.72e-03 BER=1.93e-03 FER=1.70e-02
2025-10-16 10:26:59,369 | INFO | Epoch 252 Train Time 96.07329201698303s

2025-10-16 10:28:40,277 | INFO | Training epoch 253, Batch 1000/1000: LR=9.62e-05, Loss=4.52e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 10:28:40,325 | INFO | Epoch 253 Train Time 100.95364665985107s

2025-10-16 10:30:08,321 | INFO | Training epoch 254, Batch 1000/1000: LR=9.61e-05, Loss=4.68e-03 BER=1.89e-03 FER=1.74e-02
2025-10-16 10:30:08,386 | INFO | Epoch 254 Train Time 88.05972576141357s

2025-10-16 10:31:44,044 | INFO | Training epoch 255, Batch 1000/1000: LR=9.61e-05, Loss=4.86e-03 BER=1.96e-03 FER=1.75e-02
2025-10-16 10:31:44,093 | INFO | Epoch 255 Train Time 95.70557856559753s

2025-10-16 10:33:29,471 | INFO | Training epoch 256, Batch 1000/1000: LR=9.61e-05, Loss=4.58e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 10:33:29,533 | INFO | Epoch 256 Train Time 105.43912053108215s

2025-10-16 10:35:10,221 | INFO | Training epoch 257, Batch 1000/1000: LR=9.61e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 10:35:10,272 | INFO | Epoch 257 Train Time 100.73857140541077s

2025-10-16 10:36:48,790 | INFO | Training epoch 258, Batch 1000/1000: LR=9.60e-05, Loss=4.51e-03 BER=1.86e-03 FER=1.67e-02
2025-10-16 10:36:48,843 | INFO | Epoch 258 Train Time 98.56827235221863s

2025-10-16 10:38:27,010 | INFO | Training epoch 259, Batch 1000/1000: LR=9.60e-05, Loss=4.61e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 10:38:27,062 | INFO | Epoch 259 Train Time 98.2185046672821s

2025-10-16 10:40:00,206 | INFO | Training epoch 260, Batch 1000/1000: LR=9.60e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.70e-02
2025-10-16 10:40:00,263 | INFO | Epoch 260 Train Time 93.19961881637573s

2025-10-16 10:41:30,099 | INFO | Training epoch 261, Batch 1000/1000: LR=9.59e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 10:41:30,153 | INFO | Epoch 261 Train Time 89.88812494277954s

2025-10-16 10:43:05,896 | INFO | Training epoch 262, Batch 1000/1000: LR=9.59e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 10:43:05,955 | INFO | Epoch 262 Train Time 95.80077600479126s

2025-10-16 10:44:44,894 | INFO | Training epoch 263, Batch 1000/1000: LR=9.59e-05, Loss=4.45e-03 BER=1.83e-03 FER=1.64e-02
2025-10-16 10:44:44,942 | INFO | Epoch 263 Train Time 98.98542857170105s

2025-10-16 10:46:20,931 | INFO | Training epoch 264, Batch 1000/1000: LR=9.58e-05, Loss=4.69e-03 BER=1.93e-03 FER=1.73e-02
2025-10-16 10:46:20,978 | INFO | Epoch 264 Train Time 96.0346372127533s

2025-10-16 10:47:53,435 | INFO | Training epoch 265, Batch 1000/1000: LR=9.58e-05, Loss=4.51e-03 BER=1.83e-03 FER=1.62e-02
2025-10-16 10:47:53,503 | INFO | Epoch 265 Train Time 92.523766040802s

2025-10-16 10:49:33,392 | INFO | Training epoch 266, Batch 1000/1000: LR=9.58e-05, Loss=4.77e-03 BER=1.95e-03 FER=1.75e-02
2025-10-16 10:49:33,457 | INFO | Epoch 266 Train Time 99.95176577568054s

2025-10-16 10:51:10,507 | INFO | Training epoch 267, Batch 1000/1000: LR=9.57e-05, Loss=4.64e-03 BER=1.88e-03 FER=1.71e-02
2025-10-16 10:51:10,569 | INFO | Epoch 267 Train Time 97.10984230041504s

2025-10-16 10:52:44,040 | INFO | Training epoch 268, Batch 1000/1000: LR=9.57e-05, Loss=4.74e-03 BER=1.94e-03 FER=1.74e-02
2025-10-16 10:52:44,102 | INFO | Epoch 268 Train Time 93.53159308433533s

2025-10-16 10:54:15,152 | INFO | Training epoch 269, Batch 1000/1000: LR=9.57e-05, Loss=4.52e-03 BER=1.83e-03 FER=1.64e-02
2025-10-16 10:54:15,206 | INFO | Epoch 269 Train Time 91.10267901420593s

2025-10-16 10:55:48,451 | INFO | Training epoch 270, Batch 1000/1000: LR=9.56e-05, Loss=4.43e-03 BER=1.81e-03 FER=1.64e-02
2025-10-16 10:55:48,493 | INFO | Epoch 270 Train Time 93.28485631942749s

2025-10-16 10:57:12,762 | INFO | Training epoch 271, Batch 1000/1000: LR=9.56e-05, Loss=4.59e-03 BER=1.90e-03 FER=1.72e-02
2025-10-16 10:57:12,808 | INFO | Epoch 271 Train Time 84.31403493881226s

2025-10-16 10:58:48,433 | INFO | Training epoch 272, Batch 1000/1000: LR=9.56e-05, Loss=4.51e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 10:58:48,476 | INFO | Epoch 272 Train Time 95.66600012779236s

2025-10-16 11:00:23,549 | INFO | Training epoch 273, Batch 1000/1000: LR=9.56e-05, Loss=4.72e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 11:00:23,601 | INFO | Epoch 273 Train Time 95.12476444244385s

2025-10-16 11:01:55,780 | INFO | Training epoch 274, Batch 1000/1000: LR=9.55e-05, Loss=4.74e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 11:01:55,857 | INFO | Epoch 274 Train Time 92.25430512428284s

2025-10-16 11:03:37,130 | INFO | Training epoch 275, Batch 1000/1000: LR=9.55e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 11:03:37,171 | INFO | Epoch 275 Train Time 101.31266355514526s

2025-10-16 11:05:09,705 | INFO | Training epoch 276, Batch 1000/1000: LR=9.55e-05, Loss=4.43e-03 BER=1.81e-03 FER=1.62e-02
2025-10-16 11:05:09,779 | INFO | Epoch 276 Train Time 92.60481309890747s

2025-10-16 11:06:47,530 | INFO | Training epoch 277, Batch 1000/1000: LR=9.54e-05, Loss=4.54e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 11:06:47,577 | INFO | Epoch 277 Train Time 97.79602408409119s

2025-10-16 11:08:15,272 | INFO | Training epoch 278, Batch 1000/1000: LR=9.54e-05, Loss=4.61e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 11:08:15,327 | INFO | Epoch 278 Train Time 87.74849367141724s

2025-10-16 11:09:51,932 | INFO | Training epoch 279, Batch 1000/1000: LR=9.54e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.61e-02
2025-10-16 11:09:51,984 | INFO | Epoch 279 Train Time 96.65400242805481s

2025-10-16 11:09:51,984 | INFO | [P2] saving best_model (QAT) with loss 0.004373 at epoch 279
2025-10-16 11:11:23,669 | INFO | Training epoch 280, Batch 1000/1000: LR=9.53e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.69e-02
2025-10-16 11:11:23,757 | INFO | Epoch 280 Train Time 91.6824803352356s

2025-10-16 11:12:53,764 | INFO | Training epoch 281, Batch 1000/1000: LR=9.53e-05, Loss=4.76e-03 BER=1.96e-03 FER=1.74e-02
2025-10-16 11:12:53,825 | INFO | Epoch 281 Train Time 90.06620192527771s

2025-10-16 11:14:26,718 | INFO | Training epoch 282, Batch 1000/1000: LR=9.53e-05, Loss=4.61e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 11:14:26,764 | INFO | Epoch 282 Train Time 92.93683290481567s

2025-10-16 11:16:03,896 | INFO | Training epoch 283, Batch 1000/1000: LR=9.52e-05, Loss=4.61e-03 BER=1.92e-03 FER=1.68e-02
2025-10-16 11:16:03,946 | INFO | Epoch 283 Train Time 97.18017196655273s

2025-10-16 11:17:30,131 | INFO | Training epoch 284, Batch 1000/1000: LR=9.52e-05, Loss=4.51e-03 BER=1.85e-03 FER=1.62e-02
2025-10-16 11:17:30,178 | INFO | Epoch 284 Train Time 86.23067784309387s

2025-10-16 11:19:04,729 | INFO | Training epoch 285, Batch 1000/1000: LR=9.52e-05, Loss=4.59e-03 BER=1.91e-03 FER=1.70e-02
2025-10-16 11:19:04,778 | INFO | Epoch 285 Train Time 94.59816932678223s

2025-10-16 11:20:47,794 | INFO | Training epoch 286, Batch 1000/1000: LR=9.51e-05, Loss=4.76e-03 BER=1.98e-03 FER=1.75e-02
2025-10-16 11:20:47,864 | INFO | Epoch 286 Train Time 103.08442974090576s

2025-10-16 11:22:22,717 | INFO | Training epoch 287, Batch 1000/1000: LR=9.51e-05, Loss=4.72e-03 BER=1.95e-03 FER=1.74e-02
2025-10-16 11:22:22,776 | INFO | Epoch 287 Train Time 94.90837216377258s

2025-10-16 11:23:59,050 | INFO | Training epoch 288, Batch 1000/1000: LR=9.51e-05, Loss=4.48e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 11:23:59,100 | INFO | Epoch 288 Train Time 96.32139182090759s

2025-10-16 11:25:28,835 | INFO | Training epoch 289, Batch 1000/1000: LR=9.50e-05, Loss=4.64e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 11:25:28,890 | INFO | Epoch 289 Train Time 89.78858947753906s

2025-10-16 11:27:03,247 | INFO | Training epoch 290, Batch 1000/1000: LR=9.50e-05, Loss=4.71e-03 BER=1.94e-03 FER=1.73e-02
2025-10-16 11:27:03,302 | INFO | Epoch 290 Train Time 94.40810871124268s

2025-10-16 11:28:35,485 | INFO | Training epoch 291, Batch 1000/1000: LR=9.50e-05, Loss=4.50e-03 BER=1.84e-03 FER=1.66e-02
2025-10-16 11:28:35,537 | INFO | Epoch 291 Train Time 92.23352313041687s

2025-10-16 11:30:19,330 | INFO | Training epoch 292, Batch 1000/1000: LR=9.49e-05, Loss=4.86e-03 BER=1.95e-03 FER=1.76e-02
2025-10-16 11:30:19,375 | INFO | Epoch 292 Train Time 103.8370053768158s

2025-10-16 11:31:51,734 | INFO | Training epoch 293, Batch 1000/1000: LR=9.49e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.68e-02
2025-10-16 11:31:51,795 | INFO | Epoch 293 Train Time 92.4190320968628s

2025-10-16 11:33:25,615 | INFO | Training epoch 294, Batch 1000/1000: LR=9.48e-05, Loss=4.72e-03 BER=1.92e-03 FER=1.72e-02
2025-10-16 11:33:25,679 | INFO | Epoch 294 Train Time 93.88330578804016s

2025-10-16 11:34:54,269 | INFO | Training epoch 295, Batch 1000/1000: LR=9.48e-05, Loss=4.79e-03 BER=1.96e-03 FER=1.69e-02
2025-10-16 11:34:54,310 | INFO | Epoch 295 Train Time 88.63012099266052s

2025-10-16 11:36:29,911 | INFO | Training epoch 296, Batch 1000/1000: LR=9.48e-05, Loss=4.79e-03 BER=1.96e-03 FER=1.75e-02
2025-10-16 11:36:29,969 | INFO | Epoch 296 Train Time 95.65767168998718s

2025-10-16 11:38:06,763 | INFO | Training epoch 297, Batch 1000/1000: LR=9.47e-05, Loss=4.67e-03 BER=1.90e-03 FER=1.71e-02
2025-10-16 11:38:06,807 | INFO | Epoch 297 Train Time 96.83583545684814s

2025-10-16 11:39:41,549 | INFO | Training epoch 298, Batch 1000/1000: LR=9.47e-05, Loss=4.56e-03 BER=1.90e-03 FER=1.67e-02
2025-10-16 11:39:41,601 | INFO | Epoch 298 Train Time 94.79295825958252s

2025-10-16 11:41:18,053 | INFO | Training epoch 299, Batch 1000/1000: LR=9.47e-05, Loss=4.52e-03 BER=1.85e-03 FER=1.65e-02
2025-10-16 11:41:18,102 | INFO | Epoch 299 Train Time 96.50008463859558s

2025-10-16 11:42:49,812 | INFO | Training epoch 300, Batch 1000/1000: LR=9.46e-05, Loss=4.86e-03 BER=1.98e-03 FER=1.76e-02
2025-10-16 11:42:49,876 | INFO | Epoch 300 Train Time 91.77296543121338s

2025-10-16 11:44:30,143 | INFO | Training epoch 301, Batch 1000/1000: LR=9.46e-05, Loss=4.44e-03 BER=1.81e-03 FER=1.61e-02
2025-10-16 11:44:30,202 | INFO | Epoch 301 Train Time 100.32341384887695s

2025-10-16 11:46:02,866 | INFO | Training epoch 302, Batch 1000/1000: LR=9.46e-05, Loss=6.90e-03 BER=2.49e-03 FER=2.51e-02
2025-10-16 11:46:02,925 | INFO | Epoch 302 Train Time 92.7226037979126s

2025-10-16 11:47:38,018 | INFO | Training epoch 303, Batch 1000/1000: LR=9.45e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 11:47:38,065 | INFO | Epoch 303 Train Time 95.13832259178162s

2025-10-16 11:49:12,805 | INFO | Training epoch 304, Batch 1000/1000: LR=9.45e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.66e-02
2025-10-16 11:49:12,866 | INFO | Epoch 304 Train Time 94.80035829544067s

2025-10-16 11:50:47,032 | INFO | Training epoch 305, Batch 1000/1000: LR=9.45e-05, Loss=4.36e-03 BER=1.78e-03 FER=1.59e-02
2025-10-16 11:50:47,079 | INFO | Epoch 305 Train Time 94.21107721328735s

2025-10-16 11:50:47,079 | INFO | [P2] saving best_model (QAT) with loss 0.004360 at epoch 305
2025-10-16 11:52:22,300 | INFO | Training epoch 306, Batch 1000/1000: LR=9.44e-05, Loss=4.59e-03 BER=1.88e-03 FER=1.62e-02
2025-10-16 11:52:22,362 | INFO | Epoch 306 Train Time 95.17606925964355s

2025-10-16 11:54:03,353 | INFO | Training epoch 307, Batch 1000/1000: LR=9.44e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.61e-02
2025-10-16 11:54:03,418 | INFO | Epoch 307 Train Time 101.05473923683167s

2025-10-16 11:55:39,199 | INFO | Training epoch 308, Batch 1000/1000: LR=9.44e-05, Loss=4.60e-03 BER=1.90e-03 FER=1.66e-02
2025-10-16 11:55:39,252 | INFO | Epoch 308 Train Time 95.83308601379395s

2025-10-16 11:57:10,612 | INFO | Training epoch 309, Batch 1000/1000: LR=9.43e-05, Loss=4.51e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 11:57:10,664 | INFO | Epoch 309 Train Time 91.40963459014893s

2025-10-16 11:58:42,729 | INFO | Training epoch 310, Batch 1000/1000: LR=9.43e-05, Loss=4.58e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 11:58:42,786 | INFO | Epoch 310 Train Time 92.11994457244873s

2025-10-16 12:00:20,634 | INFO | Training epoch 311, Batch 1000/1000: LR=9.42e-05, Loss=4.44e-03 BER=1.84e-03 FER=1.63e-02
2025-10-16 12:00:20,688 | INFO | Epoch 311 Train Time 97.90063381195068s

2025-10-16 12:01:49,338 | INFO | Training epoch 312, Batch 1000/1000: LR=9.42e-05, Loss=4.52e-03 BER=1.85e-03 FER=1.65e-02
2025-10-16 12:01:49,396 | INFO | Epoch 312 Train Time 88.70608377456665s

2025-10-16 12:03:27,534 | INFO | Training epoch 313, Batch 1000/1000: LR=9.42e-05, Loss=4.60e-03 BER=1.89e-03 FER=1.67e-02
2025-10-16 12:03:27,588 | INFO | Epoch 313 Train Time 98.1900806427002s

2025-10-16 12:05:07,437 | INFO | Training epoch 314, Batch 1000/1000: LR=9.41e-05, Loss=4.71e-03 BER=1.93e-03 FER=1.69e-02
2025-10-16 12:05:07,486 | INFO | Epoch 314 Train Time 99.89662957191467s

2025-10-16 12:06:48,385 | INFO | Training epoch 315, Batch 1000/1000: LR=9.41e-05, Loss=4.60e-03 BER=1.91e-03 FER=1.73e-02
2025-10-16 12:06:48,444 | INFO | Epoch 315 Train Time 100.95597672462463s

2025-10-16 12:08:23,794 | INFO | Training epoch 316, Batch 1000/1000: LR=9.41e-05, Loss=4.64e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 12:08:23,855 | INFO | Epoch 316 Train Time 95.41017198562622s

2025-10-16 12:09:53,214 | INFO | Training epoch 317, Batch 1000/1000: LR=9.40e-05, Loss=4.69e-03 BER=1.92e-03 FER=1.69e-02
2025-10-16 12:09:53,253 | INFO | Epoch 317 Train Time 89.39577579498291s

2025-10-16 12:11:31,227 | INFO | Training epoch 318, Batch 1000/1000: LR=9.40e-05, Loss=4.42e-03 BER=1.81e-03 FER=1.59e-02
2025-10-16 12:11:31,282 | INFO | Epoch 318 Train Time 98.02644920349121s

2025-10-16 12:13:05,415 | INFO | Training epoch 319, Batch 1000/1000: LR=9.40e-05, Loss=4.60e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 12:13:05,460 | INFO | Epoch 319 Train Time 94.17638993263245s

2025-10-16 12:14:45,023 | INFO | Training epoch 320, Batch 1000/1000: LR=9.39e-05, Loss=4.64e-03 BER=1.91e-03 FER=1.71e-02
2025-10-16 12:14:45,087 | INFO | Epoch 320 Train Time 99.62524938583374s

2025-10-16 12:16:22,265 | INFO | Training epoch 321, Batch 1000/1000: LR=9.39e-05, Loss=4.50e-03 BER=1.83e-03 FER=1.65e-02
2025-10-16 12:16:22,320 | INFO | Epoch 321 Train Time 97.2309160232544s

2025-10-16 12:17:45,475 | INFO | Training epoch 322, Batch 1000/1000: LR=9.38e-05, Loss=4.56e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 12:17:45,523 | INFO | Epoch 322 Train Time 83.20175623893738s

2025-10-16 12:19:15,564 | INFO | Training epoch 323, Batch 1000/1000: LR=9.38e-05, Loss=4.50e-03 BER=1.84e-03 FER=1.67e-02
2025-10-16 12:19:15,612 | INFO | Epoch 323 Train Time 90.08685255050659s

2025-10-16 12:20:46,398 | INFO | Training epoch 324, Batch 1000/1000: LR=9.38e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 12:20:46,462 | INFO | Epoch 324 Train Time 90.848881483078s

2025-10-16 12:22:25,406 | INFO | Training epoch 325, Batch 1000/1000: LR=9.37e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.65e-02
2025-10-16 12:22:25,456 | INFO | Epoch 325 Train Time 98.99240946769714s

2025-10-16 12:23:57,749 | INFO | Training epoch 326, Batch 1000/1000: LR=9.37e-05, Loss=4.63e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 12:23:57,805 | INFO | Epoch 326 Train Time 92.34705400466919s

2025-10-16 12:25:31,564 | INFO | Training epoch 327, Batch 1000/1000: LR=9.37e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 12:25:31,615 | INFO | Epoch 327 Train Time 93.80833888053894s

2025-10-16 12:27:08,561 | INFO | Training epoch 328, Batch 1000/1000: LR=9.36e-05, Loss=4.68e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 12:27:08,599 | INFO | Epoch 328 Train Time 96.98158860206604s

2025-10-16 12:28:46,032 | INFO | Training epoch 329, Batch 1000/1000: LR=9.36e-05, Loss=4.61e-03 BER=1.91e-03 FER=1.69e-02
2025-10-16 12:28:46,096 | INFO | Epoch 329 Train Time 97.49655508995056s

2025-10-16 12:30:22,761 | INFO | Training epoch 330, Batch 1000/1000: LR=9.35e-05, Loss=4.68e-03 BER=1.91e-03 FER=1.72e-02
2025-10-16 12:30:22,800 | INFO | Epoch 330 Train Time 96.7020857334137s

2025-10-16 12:31:59,553 | INFO | Training epoch 331, Batch 1000/1000: LR=9.35e-05, Loss=4.69e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 12:31:59,625 | INFO | Epoch 331 Train Time 96.82381176948547s

2025-10-16 12:33:39,858 | INFO | Training epoch 332, Batch 1000/1000: LR=9.35e-05, Loss=4.60e-03 BER=1.88e-03 FER=1.67e-02
2025-10-16 12:33:39,930 | INFO | Epoch 332 Train Time 100.30335974693298s

2025-10-16 12:35:15,500 | INFO | Training epoch 333, Batch 1000/1000: LR=9.34e-05, Loss=4.61e-03 BER=1.90e-03 FER=1.68e-02
2025-10-16 12:35:15,561 | INFO | Epoch 333 Train Time 95.63055849075317s

2025-10-16 12:36:49,178 | INFO | Training epoch 334, Batch 1000/1000: LR=9.34e-05, Loss=4.65e-03 BER=1.91e-03 FER=1.69e-02
2025-10-16 12:36:49,231 | INFO | Epoch 334 Train Time 93.66774487495422s

2025-10-16 12:38:35,833 | INFO | Training epoch 335, Batch 1000/1000: LR=9.33e-05, Loss=4.69e-03 BER=1.91e-03 FER=1.70e-02
2025-10-16 12:38:35,886 | INFO | Epoch 335 Train Time 106.65306377410889s

2025-10-16 12:40:09,034 | INFO | Training epoch 336, Batch 1000/1000: LR=9.33e-05, Loss=4.82e-03 BER=1.96e-03 FER=1.74e-02
2025-10-16 12:40:09,085 | INFO | Epoch 336 Train Time 93.19680523872375s

2025-10-16 12:41:38,335 | INFO | Training epoch 337, Batch 1000/1000: LR=9.33e-05, Loss=4.64e-03 BER=1.91e-03 FER=1.69e-02
2025-10-16 12:41:38,384 | INFO | Epoch 337 Train Time 89.29882192611694s

2025-10-16 12:43:10,657 | INFO | Training epoch 338, Batch 1000/1000: LR=9.32e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 12:43:10,710 | INFO | Epoch 338 Train Time 92.3243682384491s

2025-10-16 12:44:39,857 | INFO | Training epoch 339, Batch 1000/1000: LR=9.32e-05, Loss=4.46e-03 BER=1.84e-03 FER=1.65e-02
2025-10-16 12:44:39,897 | INFO | Epoch 339 Train Time 89.1862940788269s

2025-10-16 12:46:14,901 | INFO | Training epoch 340, Batch 1000/1000: LR=9.31e-05, Loss=4.41e-03 BER=1.83e-03 FER=1.62e-02
2025-10-16 12:46:14,952 | INFO | Epoch 340 Train Time 95.05377292633057s

2025-10-16 12:47:47,710 | INFO | Training epoch 341, Batch 1000/1000: LR=9.31e-05, Loss=4.41e-03 BER=1.81e-03 FER=1.62e-02
2025-10-16 12:47:47,774 | INFO | Epoch 341 Train Time 92.82040929794312s

2025-10-16 12:49:22,933 | INFO | Training epoch 342, Batch 1000/1000: LR=9.31e-05, Loss=4.54e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 12:49:22,974 | INFO | Epoch 342 Train Time 95.19825863838196s

2025-10-16 12:50:59,428 | INFO | Training epoch 343, Batch 1000/1000: LR=9.30e-05, Loss=4.51e-03 BER=1.82e-03 FER=1.63e-02
2025-10-16 12:50:59,479 | INFO | Epoch 343 Train Time 96.50405478477478s

2025-10-16 12:52:35,451 | INFO | Training epoch 344, Batch 1000/1000: LR=9.30e-05, Loss=4.40e-03 BER=1.82e-03 FER=1.63e-02
2025-10-16 12:52:35,497 | INFO | Epoch 344 Train Time 96.01668167114258s

2025-10-16 12:54:19,013 | INFO | Training epoch 345, Batch 1000/1000: LR=9.29e-05, Loss=4.64e-03 BER=1.91e-03 FER=1.70e-02
2025-10-16 12:54:19,060 | INFO | Epoch 345 Train Time 103.56181478500366s

2025-10-16 12:55:47,546 | INFO | Training epoch 346, Batch 1000/1000: LR=9.29e-05, Loss=4.84e-03 BER=1.96e-03 FER=1.76e-02
2025-10-16 12:55:47,606 | INFO | Epoch 346 Train Time 88.54524493217468s

2025-10-16 12:57:15,608 | INFO | Training epoch 347, Batch 1000/1000: LR=9.29e-05, Loss=4.58e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 12:57:15,660 | INFO | Epoch 347 Train Time 88.05166482925415s

2025-10-16 12:58:43,248 | INFO | Training epoch 348, Batch 1000/1000: LR=9.28e-05, Loss=4.65e-03 BER=1.89e-03 FER=1.69e-02
2025-10-16 12:58:43,289 | INFO | Epoch 348 Train Time 87.6280312538147s

2025-10-16 13:00:27,013 | INFO | Training epoch 349, Batch 1000/1000: LR=9.28e-05, Loss=4.60e-03 BER=1.88e-03 FER=1.70e-02
2025-10-16 13:00:27,060 | INFO | Epoch 349 Train Time 103.77039098739624s

2025-10-16 13:01:53,945 | INFO | Training epoch 350, Batch 1000/1000: LR=9.27e-05, Loss=4.50e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 13:01:53,990 | INFO | Epoch 350 Train Time 86.92799496650696s

2025-10-16 13:03:35,464 | INFO | Training epoch 351, Batch 1000/1000: LR=9.27e-05, Loss=4.81e-03 BER=1.98e-03 FER=1.75e-02
2025-10-16 13:03:35,513 | INFO | Epoch 351 Train Time 101.52119183540344s

2025-10-16 13:05:07,749 | INFO | Training epoch 352, Batch 1000/1000: LR=9.27e-05, Loss=4.46e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 13:05:07,801 | INFO | Epoch 352 Train Time 92.28640365600586s

2025-10-16 13:06:44,205 | INFO | Training epoch 353, Batch 1000/1000: LR=9.26e-05, Loss=4.90e-03 BER=2.00e-03 FER=1.77e-02
2025-10-16 13:06:44,252 | INFO | Epoch 353 Train Time 96.4492769241333s

2025-10-16 13:08:25,831 | INFO | Training epoch 354, Batch 1000/1000: LR=9.26e-05, Loss=4.69e-03 BER=1.93e-03 FER=1.70e-02
2025-10-16 13:08:25,895 | INFO | Epoch 354 Train Time 101.64087510108948s

2025-10-16 13:10:00,506 | INFO | Training epoch 355, Batch 1000/1000: LR=9.25e-05, Loss=4.50e-03 BER=1.83e-03 FER=1.64e-02
2025-10-16 13:10:00,559 | INFO | Epoch 355 Train Time 94.6623146533966s

2025-10-16 13:11:35,231 | INFO | Training epoch 356, Batch 1000/1000: LR=9.25e-05, Loss=4.68e-03 BER=1.90e-03 FER=1.70e-02
2025-10-16 13:11:35,289 | INFO | Epoch 356 Train Time 94.72881770133972s

2025-10-16 13:13:10,375 | INFO | Training epoch 357, Batch 1000/1000: LR=9.25e-05, Loss=4.55e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 13:13:10,419 | INFO | Epoch 357 Train Time 95.12779474258423s

2025-10-16 13:14:50,650 | INFO | Training epoch 358, Batch 1000/1000: LR=9.24e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.69e-02
2025-10-16 13:14:50,702 | INFO | Epoch 358 Train Time 100.2810800075531s

2025-10-16 13:16:31,410 | INFO | Training epoch 359, Batch 1000/1000: LR=9.24e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 13:16:31,459 | INFO | Epoch 359 Train Time 100.75643610954285s

2025-10-16 13:18:02,568 | INFO | Training epoch 360, Batch 1000/1000: LR=9.23e-05, Loss=4.61e-03 BER=1.90e-03 FER=1.68e-02
2025-10-16 13:18:02,664 | INFO | Epoch 360 Train Time 91.20263051986694s

2025-10-16 13:19:41,961 | INFO | Training epoch 361, Batch 1000/1000: LR=9.23e-05, Loss=4.59e-03 BER=1.88e-03 FER=1.70e-02
2025-10-16 13:19:42,006 | INFO | Epoch 361 Train Time 99.34056091308594s

2025-10-16 13:21:15,528 | INFO | Training epoch 362, Batch 1000/1000: LR=9.23e-05, Loss=4.51e-03 BER=1.82e-03 FER=1.64e-02
2025-10-16 13:21:15,567 | INFO | Epoch 362 Train Time 93.56031632423401s

2025-10-16 13:22:49,356 | INFO | Training epoch 363, Batch 1000/1000: LR=9.22e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.69e-02
2025-10-16 13:22:49,404 | INFO | Epoch 363 Train Time 93.83539414405823s

2025-10-16 13:24:24,319 | INFO | Training epoch 364, Batch 1000/1000: LR=9.22e-05, Loss=4.56e-03 BER=1.84e-03 FER=1.67e-02
2025-10-16 13:24:24,370 | INFO | Epoch 364 Train Time 94.96455955505371s

2025-10-16 13:25:56,958 | INFO | Training epoch 365, Batch 1000/1000: LR=9.21e-05, Loss=4.59e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 13:25:57,007 | INFO | Epoch 365 Train Time 92.63531947135925s

2025-10-16 13:27:35,952 | INFO | Training epoch 366, Batch 1000/1000: LR=9.21e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.69e-02
2025-10-16 13:27:36,001 | INFO | Epoch 366 Train Time 98.99230170249939s

2025-10-16 13:29:10,121 | INFO | Training epoch 367, Batch 1000/1000: LR=9.20e-05, Loss=4.63e-03 BER=1.91e-03 FER=1.69e-02
2025-10-16 13:29:10,177 | INFO | Epoch 367 Train Time 94.17412209510803s

2025-10-16 13:30:46,656 | INFO | Training epoch 368, Batch 1000/1000: LR=9.20e-05, Loss=4.54e-03 BER=1.86e-03 FER=1.63e-02
2025-10-16 13:30:46,699 | INFO | Epoch 368 Train Time 96.51982760429382s

2025-10-16 13:32:22,330 | INFO | Training epoch 369, Batch 1000/1000: LR=9.20e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 13:32:22,382 | INFO | Epoch 369 Train Time 95.68289542198181s

2025-10-16 13:33:49,045 | INFO | Training epoch 370, Batch 1000/1000: LR=9.19e-05, Loss=4.73e-03 BER=1.95e-03 FER=1.72e-02
2025-10-16 13:33:49,091 | INFO | Epoch 370 Train Time 86.70799922943115s

2025-10-16 13:35:25,626 | INFO | Training epoch 371, Batch 1000/1000: LR=9.19e-05, Loss=4.69e-03 BER=1.92e-03 FER=1.68e-02
2025-10-16 13:35:25,686 | INFO | Epoch 371 Train Time 96.59327960014343s

2025-10-16 13:36:59,561 | INFO | Training epoch 372, Batch 1000/1000: LR=9.18e-05, Loss=4.67e-03 BER=1.92e-03 FER=1.70e-02
2025-10-16 13:36:59,633 | INFO | Epoch 372 Train Time 93.94606804847717s

2025-10-16 13:38:34,519 | INFO | Training epoch 373, Batch 1000/1000: LR=9.18e-05, Loss=4.68e-03 BER=1.88e-03 FER=1.71e-02
2025-10-16 13:38:34,565 | INFO | Epoch 373 Train Time 94.92988181114197s

2025-10-16 13:40:07,395 | INFO | Training epoch 374, Batch 1000/1000: LR=9.17e-05, Loss=4.44e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 13:40:07,440 | INFO | Epoch 374 Train Time 92.87334489822388s

2025-10-16 13:41:38,931 | INFO | Training epoch 375, Batch 1000/1000: LR=9.17e-05, Loss=4.46e-03 BER=1.80e-03 FER=1.63e-02
2025-10-16 13:41:38,982 | INFO | Epoch 375 Train Time 91.54128861427307s

2025-10-16 13:43:17,431 | INFO | Training epoch 376, Batch 1000/1000: LR=9.17e-05, Loss=4.40e-03 BER=1.82e-03 FER=1.58e-02
2025-10-16 13:43:17,480 | INFO | Epoch 376 Train Time 98.49687123298645s

2025-10-16 13:44:53,236 | INFO | Training epoch 377, Batch 1000/1000: LR=9.16e-05, Loss=4.53e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 13:44:53,288 | INFO | Epoch 377 Train Time 95.80675315856934s

2025-10-16 13:46:37,249 | INFO | Training epoch 378, Batch 1000/1000: LR=9.16e-05, Loss=4.54e-03 BER=1.86e-03 FER=1.66e-02
2025-10-16 13:46:37,293 | INFO | Epoch 378 Train Time 104.00380754470825s

2025-10-16 13:48:13,772 | INFO | Training epoch 379, Batch 1000/1000: LR=9.15e-05, Loss=4.33e-03 BER=1.77e-03 FER=1.61e-02
2025-10-16 13:48:13,827 | INFO | Epoch 379 Train Time 96.53116703033447s

2025-10-16 13:48:13,827 | INFO | [P2] saving best_model (QAT) with loss 0.004333 at epoch 379
2025-10-16 13:49:39,581 | INFO | Training epoch 380, Batch 1000/1000: LR=9.15e-05, Loss=4.54e-03 BER=1.85e-03 FER=1.66e-02
2025-10-16 13:49:39,625 | INFO | Epoch 380 Train Time 85.68787741661072s

2025-10-16 13:51:09,658 | INFO | Training epoch 381, Batch 1000/1000: LR=9.14e-05, Loss=4.35e-03 BER=1.80e-03 FER=1.61e-02
2025-10-16 13:51:09,717 | INFO | Epoch 381 Train Time 90.09006643295288s

2025-10-16 13:52:40,015 | INFO | Training epoch 382, Batch 1000/1000: LR=9.14e-05, Loss=4.71e-03 BER=1.92e-03 FER=1.71e-02
2025-10-16 13:52:40,077 | INFO | Epoch 382 Train Time 90.35758233070374s

2025-10-16 13:54:25,124 | INFO | Training epoch 383, Batch 1000/1000: LR=9.14e-05, Loss=4.38e-03 BER=1.79e-03 FER=1.59e-02
2025-10-16 13:54:25,170 | INFO | Epoch 383 Train Time 105.09172940254211s

2025-10-16 13:56:03,170 | INFO | Training epoch 384, Batch 1000/1000: LR=9.13e-05, Loss=4.50e-03 BER=1.80e-03 FER=1.63e-02
2025-10-16 13:56:03,222 | INFO | Epoch 384 Train Time 98.05125308036804s

2025-10-16 13:57:35,836 | INFO | Training epoch 385, Batch 1000/1000: LR=9.13e-05, Loss=4.46e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 13:57:35,887 | INFO | Epoch 385 Train Time 92.66361474990845s

2025-10-16 13:59:14,000 | INFO | Training epoch 386, Batch 1000/1000: LR=9.12e-05, Loss=4.46e-03 BER=1.83e-03 FER=1.62e-02
2025-10-16 13:59:14,061 | INFO | Epoch 386 Train Time 98.17331218719482s

2025-10-16 14:00:44,519 | INFO | Training epoch 387, Batch 1000/1000: LR=9.12e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.66e-02
2025-10-16 14:00:44,565 | INFO | Epoch 387 Train Time 90.50164651870728s

2025-10-16 14:02:17,811 | INFO | Training epoch 388, Batch 1000/1000: LR=9.11e-05, Loss=4.49e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 14:02:17,860 | INFO | Epoch 388 Train Time 93.29228258132935s

2025-10-16 14:03:55,050 | INFO | Training epoch 389, Batch 1000/1000: LR=9.11e-05, Loss=4.71e-03 BER=1.94e-03 FER=1.71e-02
2025-10-16 14:03:55,105 | INFO | Epoch 389 Train Time 97.24377822875977s

2025-10-16 14:05:38,629 | INFO | Training epoch 390, Batch 1000/1000: LR=9.10e-05, Loss=4.70e-03 BER=1.94e-03 FER=1.72e-02
2025-10-16 14:05:38,673 | INFO | Epoch 390 Train Time 103.56590867042542s

2025-10-16 14:07:16,971 | INFO | Training epoch 391, Batch 1000/1000: LR=9.10e-05, Loss=4.51e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 14:07:17,036 | INFO | Epoch 391 Train Time 98.36101412773132s

2025-10-16 14:08:50,436 | INFO | Training epoch 392, Batch 1000/1000: LR=9.10e-05, Loss=4.47e-03 BER=1.83e-03 FER=1.61e-02
2025-10-16 14:08:50,481 | INFO | Epoch 392 Train Time 93.44321250915527s

2025-10-16 14:10:29,553 | INFO | Training epoch 393, Batch 1000/1000: LR=9.09e-05, Loss=4.46e-03 BER=1.84e-03 FER=1.65e-02
2025-10-16 14:10:29,609 | INFO | Epoch 393 Train Time 99.12628698348999s

2025-10-16 14:12:00,059 | INFO | Training epoch 394, Batch 1000/1000: LR=9.09e-05, Loss=4.58e-03 BER=1.90e-03 FER=1.68e-02
2025-10-16 14:12:00,124 | INFO | Epoch 394 Train Time 90.51136565208435s

2025-10-16 14:13:37,262 | INFO | Training epoch 395, Batch 1000/1000: LR=9.08e-05, Loss=4.61e-03 BER=1.90e-03 FER=1.67e-02
2025-10-16 14:13:37,307 | INFO | Epoch 395 Train Time 97.18054389953613s

2025-10-16 14:15:08,166 | INFO | Training epoch 396, Batch 1000/1000: LR=9.08e-05, Loss=4.63e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 14:15:08,219 | INFO | Epoch 396 Train Time 90.91011810302734s

2025-10-16 14:16:45,707 | INFO | Training epoch 397, Batch 1000/1000: LR=9.07e-05, Loss=4.64e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 14:16:45,789 | INFO | Epoch 397 Train Time 97.56911182403564s

2025-10-16 14:18:19,516 | INFO | Training epoch 398, Batch 1000/1000: LR=9.07e-05, Loss=4.46e-03 BER=1.83e-03 FER=1.59e-02
2025-10-16 14:18:19,558 | INFO | Epoch 398 Train Time 93.76804280281067s

2025-10-16 14:19:55,748 | INFO | Training epoch 399, Batch 1000/1000: LR=9.06e-05, Loss=4.61e-03 BER=1.89e-03 FER=1.71e-02
2025-10-16 14:19:55,818 | INFO | Epoch 399 Train Time 96.25824189186096s

2025-10-16 14:21:28,087 | INFO | Training epoch 400, Batch 1000/1000: LR=9.06e-05, Loss=4.58e-03 BER=1.85e-03 FER=1.65e-02
2025-10-16 14:21:28,140 | INFO | Epoch 400 Train Time 92.32045316696167s

2025-10-16 14:23:01,103 | INFO | Training epoch 401, Batch 1000/1000: LR=9.05e-05, Loss=4.72e-03 BER=1.94e-03 FER=1.71e-02
2025-10-16 14:23:01,171 | INFO | Epoch 401 Train Time 93.02948689460754s

2025-10-16 14:24:43,270 | INFO | Training epoch 402, Batch 1000/1000: LR=9.05e-05, Loss=4.53e-03 BER=1.85e-03 FER=1.67e-02
2025-10-16 14:24:43,315 | INFO | Epoch 402 Train Time 102.14268136024475s

2025-10-16 14:26:19,347 | INFO | Training epoch 403, Batch 1000/1000: LR=9.05e-05, Loss=4.68e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 14:26:19,394 | INFO | Epoch 403 Train Time 96.07789754867554s

2025-10-16 14:27:53,422 | INFO | Training epoch 404, Batch 1000/1000: LR=9.04e-05, Loss=4.64e-03 BER=1.91e-03 FER=1.70e-02
2025-10-16 14:27:53,482 | INFO | Epoch 404 Train Time 94.08616709709167s

2025-10-16 14:29:25,942 | INFO | Training epoch 405, Batch 1000/1000: LR=9.04e-05, Loss=4.57e-03 BER=1.83e-03 FER=1.63e-02
2025-10-16 14:29:25,991 | INFO | Epoch 405 Train Time 92.50787925720215s

2025-10-16 14:30:57,239 | INFO | Training epoch 406, Batch 1000/1000: LR=9.03e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 14:30:57,309 | INFO | Epoch 406 Train Time 91.31722474098206s

2025-10-16 14:32:26,543 | INFO | Training epoch 407, Batch 1000/1000: LR=9.03e-05, Loss=4.51e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 14:32:26,602 | INFO | Epoch 407 Train Time 89.29195308685303s

2025-10-16 14:34:02,827 | INFO | Training epoch 408, Batch 1000/1000: LR=9.02e-05, Loss=4.40e-03 BER=1.79e-03 FER=1.63e-02
2025-10-16 14:34:02,873 | INFO | Epoch 408 Train Time 96.26924300193787s

2025-10-16 14:35:49,544 | INFO | Training epoch 409, Batch 1000/1000: LR=9.02e-05, Loss=4.46e-03 BER=1.84e-03 FER=1.66e-02
2025-10-16 14:35:49,620 | INFO | Epoch 409 Train Time 106.74524140357971s

2025-10-16 14:37:24,537 | INFO | Training epoch 410, Batch 1000/1000: LR=9.01e-05, Loss=4.62e-03 BER=1.91e-03 FER=1.72e-02
2025-10-16 14:37:24,601 | INFO | Epoch 410 Train Time 94.97942280769348s

2025-10-16 14:39:01,636 | INFO | Training epoch 411, Batch 1000/1000: LR=9.01e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 14:39:01,695 | INFO | Epoch 411 Train Time 97.09274291992188s

2025-10-16 14:40:40,085 | INFO | Training epoch 412, Batch 1000/1000: LR=9.00e-05, Loss=4.50e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 14:40:40,135 | INFO | Epoch 412 Train Time 98.4392340183258s

2025-10-16 14:42:21,086 | INFO | Training epoch 413, Batch 1000/1000: LR=9.00e-05, Loss=4.58e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 14:42:21,128 | INFO | Epoch 413 Train Time 100.99112868309021s

2025-10-16 14:43:57,423 | INFO | Training epoch 414, Batch 1000/1000: LR=8.99e-05, Loss=4.48e-03 BER=1.82e-03 FER=1.62e-02
2025-10-16 14:43:57,497 | INFO | Epoch 414 Train Time 96.36828351020813s

2025-10-16 14:45:35,809 | INFO | Training epoch 415, Batch 1000/1000: LR=8.99e-05, Loss=4.48e-03 BER=1.82e-03 FER=1.62e-02
2025-10-16 14:45:35,856 | INFO | Epoch 415 Train Time 98.35647201538086s

2025-10-16 14:47:20,760 | INFO | Training epoch 416, Batch 1000/1000: LR=8.98e-05, Loss=4.49e-03 BER=1.85e-03 FER=1.65e-02
2025-10-16 14:47:20,807 | INFO | Epoch 416 Train Time 104.9497377872467s

2025-10-16 14:48:48,641 | INFO | Training epoch 417, Batch 1000/1000: LR=8.98e-05, Loss=4.41e-03 BER=1.81e-03 FER=1.59e-02
2025-10-16 14:48:48,693 | INFO | Epoch 417 Train Time 87.88376903533936s

2025-10-16 14:50:31,087 | INFO | Training epoch 418, Batch 1000/1000: LR=8.98e-05, Loss=4.43e-03 BER=1.83e-03 FER=1.65e-02
2025-10-16 14:50:31,132 | INFO | Epoch 418 Train Time 102.43777346611023s

2025-10-16 14:52:13,043 | INFO | Training epoch 419, Batch 1000/1000: LR=8.97e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.68e-02
2025-10-16 14:52:13,088 | INFO | Epoch 419 Train Time 101.95460724830627s

2025-10-16 14:53:46,551 | INFO | Training epoch 420, Batch 1000/1000: LR=8.97e-05, Loss=4.43e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 14:53:46,621 | INFO | Epoch 420 Train Time 93.5313491821289s

2025-10-16 14:55:19,133 | INFO | Training epoch 421, Batch 1000/1000: LR=8.96e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.63e-02
2025-10-16 14:55:19,195 | INFO | Epoch 421 Train Time 92.57297563552856s

2025-10-16 14:56:47,709 | INFO | Training epoch 422, Batch 1000/1000: LR=8.96e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.61e-02
2025-10-16 14:56:47,765 | INFO | Epoch 422 Train Time 88.56847524642944s

2025-10-16 14:58:21,433 | INFO | Training epoch 423, Batch 1000/1000: LR=8.95e-05, Loss=4.38e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 14:58:21,479 | INFO | Epoch 423 Train Time 93.71267557144165s

2025-10-16 14:59:53,367 | INFO | Training epoch 424, Batch 1000/1000: LR=8.95e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.65e-02
2025-10-16 14:59:53,406 | INFO | Epoch 424 Train Time 91.92528653144836s

2025-10-16 15:01:24,435 | INFO | Training epoch 425, Batch 1000/1000: LR=8.94e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 15:01:24,495 | INFO | Epoch 425 Train Time 91.08709645271301s

2025-10-16 15:02:57,717 | INFO | Training epoch 426, Batch 1000/1000: LR=8.94e-05, Loss=4.50e-03 BER=1.87e-03 FER=1.68e-02
2025-10-16 15:02:57,777 | INFO | Epoch 426 Train Time 93.28064918518066s

2025-10-16 15:04:37,127 | INFO | Training epoch 427, Batch 1000/1000: LR=8.93e-05, Loss=4.56e-03 BER=1.89e-03 FER=1.67e-02
2025-10-16 15:04:37,177 | INFO | Epoch 427 Train Time 99.39762377738953s

2025-10-16 15:06:20,328 | INFO | Training epoch 428, Batch 1000/1000: LR=8.93e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.66e-02
2025-10-16 15:06:20,389 | INFO | Epoch 428 Train Time 103.21095132827759s

2025-10-16 15:07:59,254 | INFO | Training epoch 429, Batch 1000/1000: LR=8.92e-05, Loss=4.70e-03 BER=1.92e-03 FER=1.69e-02
2025-10-16 15:07:59,299 | INFO | Epoch 429 Train Time 98.90859794616699s

2025-10-16 15:09:48,930 | INFO | Training epoch 430, Batch 1000/1000: LR=8.92e-05, Loss=4.52e-03 BER=1.88e-03 FER=1.65e-02
2025-10-16 15:09:48,984 | INFO | Epoch 430 Train Time 109.68304181098938s

2025-10-16 15:11:21,079 | INFO | Training epoch 431, Batch 1000/1000: LR=8.91e-05, Loss=4.45e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 15:11:21,137 | INFO | Epoch 431 Train Time 92.15184450149536s

2025-10-16 15:12:49,064 | INFO | Training epoch 432, Batch 1000/1000: LR=8.91e-05, Loss=4.61e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 15:12:49,113 | INFO | Epoch 432 Train Time 87.97420310974121s

2025-10-16 15:14:23,695 | INFO | Training epoch 433, Batch 1000/1000: LR=8.90e-05, Loss=4.45e-03 BER=1.79e-03 FER=1.59e-02
2025-10-16 15:14:23,755 | INFO | Epoch 433 Train Time 94.63990712165833s

2025-10-16 15:15:53,646 | INFO | Training epoch 434, Batch 1000/1000: LR=8.90e-05, Loss=4.42e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 15:15:53,705 | INFO | Epoch 434 Train Time 89.9490749835968s

2025-10-16 15:17:25,518 | INFO | Training epoch 435, Batch 1000/1000: LR=8.89e-05, Loss=4.64e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 15:17:25,570 | INFO | Epoch 435 Train Time 91.86407327651978s

2025-10-16 15:18:57,131 | INFO | Training epoch 436, Batch 1000/1000: LR=8.89e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.63e-02
2025-10-16 15:18:57,181 | INFO | Epoch 436 Train Time 91.6093099117279s

2025-10-16 15:20:34,972 | INFO | Training epoch 437, Batch 1000/1000: LR=8.88e-05, Loss=4.72e-03 BER=1.93e-03 FER=1.74e-02
2025-10-16 15:20:35,025 | INFO | Epoch 437 Train Time 97.84257936477661s

2025-10-16 15:22:04,212 | INFO | Training epoch 438, Batch 1000/1000: LR=8.88e-05, Loss=4.44e-03 BER=1.83e-03 FER=1.62e-02
2025-10-16 15:22:04,265 | INFO | Epoch 438 Train Time 89.23756861686707s

2025-10-16 15:23:41,930 | INFO | Training epoch 439, Batch 1000/1000: LR=8.87e-05, Loss=4.24e-03 BER=1.76e-03 FER=1.55e-02
2025-10-16 15:23:41,975 | INFO | Epoch 439 Train Time 97.70938372612s

2025-10-16 15:23:41,975 | INFO | [P2] saving best_model (QAT) with loss 0.004238 at epoch 439
2025-10-16 15:25:16,436 | INFO | Training epoch 440, Batch 1000/1000: LR=8.87e-05, Loss=4.54e-03 BER=1.86e-03 FER=1.66e-02
2025-10-16 15:25:16,503 | INFO | Epoch 440 Train Time 94.45299649238586s

2025-10-16 15:26:55,235 | INFO | Training epoch 441, Batch 1000/1000: LR=8.86e-05, Loss=4.61e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 15:26:55,299 | INFO | Epoch 441 Train Time 98.79501080513s

2025-10-16 15:28:31,109 | INFO | Training epoch 442, Batch 1000/1000: LR=8.86e-05, Loss=4.69e-03 BER=1.93e-03 FER=1.72e-02
2025-10-16 15:28:31,169 | INFO | Epoch 442 Train Time 95.8688006401062s

2025-10-16 15:30:05,822 | INFO | Training epoch 443, Batch 1000/1000: LR=8.85e-05, Loss=4.50e-03 BER=1.82e-03 FER=1.62e-02
2025-10-16 15:30:05,872 | INFO | Epoch 443 Train Time 94.70161724090576s

2025-10-16 15:31:39,041 | INFO | Training epoch 444, Batch 1000/1000: LR=8.85e-05, Loss=4.30e-03 BER=1.75e-03 FER=1.57e-02
2025-10-16 15:31:39,080 | INFO | Epoch 444 Train Time 93.20489120483398s

2025-10-16 15:33:19,070 | INFO | Training epoch 445, Batch 1000/1000: LR=8.84e-05, Loss=4.42e-03 BER=1.78e-03 FER=1.60e-02
2025-10-16 15:33:19,119 | INFO | Epoch 445 Train Time 100.03842806816101s

2025-10-16 15:34:50,932 | INFO | Training epoch 446, Batch 1000/1000: LR=8.84e-05, Loss=4.49e-03 BER=1.83e-03 FER=1.61e-02
2025-10-16 15:34:50,978 | INFO | Epoch 446 Train Time 91.85745859146118s

2025-10-16 15:36:16,625 | INFO | Training epoch 447, Batch 1000/1000: LR=8.83e-05, Loss=4.52e-03 BER=1.87e-03 FER=1.65e-02
2025-10-16 15:36:16,689 | INFO | Epoch 447 Train Time 85.70958471298218s

2025-10-16 15:38:01,073 | INFO | Training epoch 448, Batch 1000/1000: LR=8.83e-05, Loss=4.45e-03 BER=1.81e-03 FER=1.62e-02
2025-10-16 15:38:01,142 | INFO | Epoch 448 Train Time 104.45047903060913s

2025-10-16 15:39:43,346 | INFO | Training epoch 449, Batch 1000/1000: LR=8.82e-05, Loss=4.31e-03 BER=1.78e-03 FER=1.58e-02
2025-10-16 15:39:43,396 | INFO | Epoch 449 Train Time 102.25354099273682s

2025-10-16 15:41:22,353 | INFO | Training epoch 450, Batch 1000/1000: LR=8.82e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 15:41:22,396 | INFO | Epoch 450 Train Time 98.99868369102478s

2025-10-16 15:42:55,035 | INFO | Training epoch 451, Batch 1000/1000: LR=8.81e-05, Loss=4.59e-03 BER=1.88e-03 FER=1.68e-02
2025-10-16 15:42:55,089 | INFO | Epoch 451 Train Time 92.6922972202301s

2025-10-16 15:44:29,878 | INFO | Training epoch 452, Batch 1000/1000: LR=8.81e-05, Loss=4.76e-03 BER=1.97e-03 FER=1.74e-02
2025-10-16 15:44:29,922 | INFO | Epoch 452 Train Time 94.83163475990295s

2025-10-16 15:46:04,366 | INFO | Training epoch 453, Batch 1000/1000: LR=8.80e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.60e-02
2025-10-16 15:46:04,426 | INFO | Epoch 453 Train Time 94.50274085998535s

2025-10-16 15:47:40,559 | INFO | Training epoch 454, Batch 1000/1000: LR=8.80e-05, Loss=4.57e-03 BER=1.87e-03 FER=1.64e-02
2025-10-16 15:47:40,605 | INFO | Epoch 454 Train Time 96.17828631401062s

2025-10-16 15:49:16,856 | INFO | Training epoch 455, Batch 1000/1000: LR=8.79e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.57e-02
2025-10-16 15:49:16,903 | INFO | Epoch 455 Train Time 96.29642796516418s

2025-10-16 15:50:51,046 | INFO | Training epoch 456, Batch 1000/1000: LR=8.79e-05, Loss=4.57e-03 BER=1.84e-03 FER=1.67e-02
2025-10-16 15:50:51,086 | INFO | Epoch 456 Train Time 94.18131494522095s

2025-10-16 15:52:37,311 | INFO | Training epoch 457, Batch 1000/1000: LR=8.78e-05, Loss=4.49e-03 BER=1.82e-03 FER=1.63e-02
2025-10-16 15:52:37,363 | INFO | Epoch 457 Train Time 106.27556991577148s

2025-10-16 15:54:16,608 | INFO | Training epoch 458, Batch 1000/1000: LR=8.78e-05, Loss=4.40e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 15:54:16,656 | INFO | Epoch 458 Train Time 99.29184770584106s

2025-10-16 15:55:44,971 | INFO | Training epoch 459, Batch 1000/1000: LR=8.77e-05, Loss=4.53e-03 BER=1.88e-03 FER=1.65e-02
2025-10-16 15:55:45,026 | INFO | Epoch 459 Train Time 88.36800980567932s

2025-10-16 15:57:15,849 | INFO | Training epoch 460, Batch 1000/1000: LR=8.77e-05, Loss=4.53e-03 BER=1.84e-03 FER=1.63e-02
2025-10-16 15:57:15,894 | INFO | Epoch 460 Train Time 90.86647033691406s

2025-10-16 15:58:54,310 | INFO | Training epoch 461, Batch 1000/1000: LR=8.76e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 15:58:54,363 | INFO | Epoch 461 Train Time 98.46777558326721s

2025-10-16 16:00:25,908 | INFO | Training epoch 462, Batch 1000/1000: LR=8.76e-05, Loss=4.46e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 16:00:25,964 | INFO | Epoch 462 Train Time 91.59801268577576s

2025-10-16 16:01:59,857 | INFO | Training epoch 463, Batch 1000/1000: LR=8.75e-05, Loss=4.52e-03 BER=1.85e-03 FER=1.62e-02
2025-10-16 16:01:59,901 | INFO | Epoch 463 Train Time 93.93634843826294s

2025-10-16 16:03:39,025 | INFO | Training epoch 464, Batch 1000/1000: LR=8.75e-05, Loss=4.45e-03 BER=1.88e-03 FER=1.65e-02
2025-10-16 16:03:39,075 | INFO | Epoch 464 Train Time 99.17268061637878s

2025-10-16 16:05:09,947 | INFO | Training epoch 465, Batch 1000/1000: LR=8.74e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.63e-02
2025-10-16 16:05:09,997 | INFO | Epoch 465 Train Time 90.91937208175659s

2025-10-16 16:06:48,120 | INFO | Training epoch 466, Batch 1000/1000: LR=8.74e-05, Loss=4.46e-03 BER=1.83e-03 FER=1.64e-02
2025-10-16 16:06:48,165 | INFO | Epoch 466 Train Time 98.16677451133728s

2025-10-16 16:08:19,916 | INFO | Training epoch 467, Batch 1000/1000: LR=8.73e-05, Loss=4.55e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 16:08:19,963 | INFO | Epoch 467 Train Time 91.79624271392822s

2025-10-16 16:09:59,208 | INFO | Training epoch 468, Batch 1000/1000: LR=8.73e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 16:09:59,265 | INFO | Epoch 468 Train Time 99.30054688453674s

2025-10-16 16:11:32,874 | INFO | Training epoch 469, Batch 1000/1000: LR=8.72e-05, Loss=4.49e-03 BER=1.85e-03 FER=1.65e-02
2025-10-16 16:11:32,919 | INFO | Epoch 469 Train Time 93.65218091011047s

2025-10-16 16:13:04,257 | INFO | Training epoch 470, Batch 1000/1000: LR=8.72e-05, Loss=4.33e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 16:13:04,315 | INFO | Epoch 470 Train Time 91.39418196678162s

2025-10-16 16:14:51,132 | INFO | Training epoch 471, Batch 1000/1000: LR=8.71e-05, Loss=4.38e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 16:14:51,185 | INFO | Epoch 471 Train Time 106.8695547580719s

2025-10-16 16:16:25,300 | INFO | Training epoch 472, Batch 1000/1000: LR=8.71e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 16:16:25,344 | INFO | Epoch 472 Train Time 94.1566390991211s

2025-10-16 16:17:57,201 | INFO | Training epoch 473, Batch 1000/1000: LR=8.70e-05, Loss=4.39e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 16:17:57,253 | INFO | Epoch 473 Train Time 91.90767025947571s

2025-10-16 16:19:32,458 | INFO | Training epoch 474, Batch 1000/1000: LR=8.70e-05, Loss=4.42e-03 BER=1.81e-03 FER=1.59e-02
2025-10-16 16:19:32,519 | INFO | Epoch 474 Train Time 95.26494264602661s

2025-10-16 16:21:07,005 | INFO | Training epoch 475, Batch 1000/1000: LR=8.69e-05, Loss=4.41e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 16:21:07,057 | INFO | Epoch 475 Train Time 94.53687334060669s

2025-10-16 16:22:36,324 | INFO | Training epoch 476, Batch 1000/1000: LR=8.68e-05, Loss=4.51e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 16:22:36,389 | INFO | Epoch 476 Train Time 89.32920217514038s

2025-10-16 16:24:14,006 | INFO | Training epoch 477, Batch 1000/1000: LR=8.68e-05, Loss=4.58e-03 BER=1.87e-03 FER=1.67e-02
2025-10-16 16:24:14,052 | INFO | Epoch 477 Train Time 97.66094517707825s

2025-10-16 16:25:45,824 | INFO | Training epoch 478, Batch 1000/1000: LR=8.67e-05, Loss=4.35e-03 BER=1.74e-03 FER=1.54e-02
2025-10-16 16:25:45,877 | INFO | Epoch 478 Train Time 91.82374548912048s

2025-10-16 16:27:27,452 | INFO | Training epoch 479, Batch 1000/1000: LR=8.67e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.61e-02
2025-10-16 16:27:27,495 | INFO | Epoch 479 Train Time 101.61704540252686s

2025-10-16 16:29:17,852 | INFO | Training epoch 480, Batch 1000/1000: LR=8.66e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 16:29:17,896 | INFO | Epoch 480 Train Time 110.39951920509338s

2025-10-16 16:30:53,893 | INFO | Training epoch 481, Batch 1000/1000: LR=8.66e-05, Loss=4.49e-03 BER=1.85e-03 FER=1.63e-02
2025-10-16 16:30:53,944 | INFO | Epoch 481 Train Time 96.04573607444763s

2025-10-16 16:32:35,059 | INFO | Training epoch 482, Batch 1000/1000: LR=8.65e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 16:32:35,125 | INFO | Epoch 482 Train Time 101.17932724952698s

2025-10-16 16:34:10,324 | INFO | Training epoch 483, Batch 1000/1000: LR=8.65e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.64e-02
2025-10-16 16:34:10,368 | INFO | Epoch 483 Train Time 95.2421293258667s

2025-10-16 16:35:47,878 | INFO | Training epoch 484, Batch 1000/1000: LR=8.64e-05, Loss=4.43e-03 BER=1.79e-03 FER=1.61e-02
2025-10-16 16:35:47,922 | INFO | Epoch 484 Train Time 97.55284094810486s

2025-10-16 16:37:18,599 | INFO | Training epoch 485, Batch 1000/1000: LR=8.64e-05, Loss=4.55e-03 BER=1.86e-03 FER=1.67e-02
2025-10-16 16:37:18,649 | INFO | Epoch 485 Train Time 90.72586822509766s

2025-10-16 16:38:55,803 | INFO | Training epoch 486, Batch 1000/1000: LR=8.63e-05, Loss=4.45e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 16:38:55,855 | INFO | Epoch 486 Train Time 97.20500636100769s

2025-10-16 16:40:34,944 | INFO | Training epoch 487, Batch 1000/1000: LR=8.63e-05, Loss=4.45e-03 BER=1.81e-03 FER=1.61e-02
2025-10-16 16:40:34,994 | INFO | Epoch 487 Train Time 99.13768577575684s

2025-10-16 16:42:10,097 | INFO | Training epoch 488, Batch 1000/1000: LR=8.62e-05, Loss=4.54e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 16:42:10,147 | INFO | Epoch 488 Train Time 95.14995336532593s

2025-10-16 16:43:45,269 | INFO | Training epoch 489, Batch 1000/1000: LR=8.62e-05, Loss=4.43e-03 BER=1.81e-03 FER=1.62e-02
2025-10-16 16:43:45,320 | INFO | Epoch 489 Train Time 95.17232656478882s

2025-10-16 16:45:25,830 | INFO | Training epoch 490, Batch 1000/1000: LR=8.61e-05, Loss=4.57e-03 BER=1.88e-03 FER=1.69e-02
2025-10-16 16:45:25,890 | INFO | Epoch 490 Train Time 100.56796431541443s

2025-10-16 16:46:57,159 | INFO | Training epoch 491, Batch 1000/1000: LR=8.60e-05, Loss=4.37e-03 BER=1.78e-03 FER=1.62e-02
2025-10-16 16:46:57,211 | INFO | Epoch 491 Train Time 91.31907033920288s

2025-10-16 16:48:35,047 | INFO | Training epoch 492, Batch 1000/1000: LR=8.60e-05, Loss=4.40e-03 BER=1.80e-03 FER=1.63e-02
2025-10-16 16:48:35,090 | INFO | Epoch 492 Train Time 97.87856364250183s

2025-10-16 16:50:10,457 | INFO | Training epoch 493, Batch 1000/1000: LR=8.59e-05, Loss=4.55e-03 BER=1.86e-03 FER=1.65e-02
2025-10-16 16:50:10,527 | INFO | Epoch 493 Train Time 95.43520069122314s

2025-10-16 16:51:47,789 | INFO | Training epoch 494, Batch 1000/1000: LR=8.59e-05, Loss=4.47e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 16:51:47,839 | INFO | Epoch 494 Train Time 97.31072735786438s

2025-10-16 16:53:28,769 | INFO | Training epoch 495, Batch 1000/1000: LR=8.58e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 16:53:28,815 | INFO | Epoch 495 Train Time 100.97381234169006s

2025-10-16 16:55:07,833 | INFO | Training epoch 496, Batch 1000/1000: LR=8.58e-05, Loss=4.68e-03 BER=1.92e-03 FER=1.66e-02
2025-10-16 16:55:07,881 | INFO | Epoch 496 Train Time 99.06484055519104s

2025-10-16 16:56:44,631 | INFO | Training epoch 497, Batch 1000/1000: LR=8.57e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.67e-02
2025-10-16 16:56:44,681 | INFO | Epoch 497 Train Time 96.79944491386414s

2025-10-16 16:58:19,039 | INFO | Training epoch 498, Batch 1000/1000: LR=8.57e-05, Loss=4.62e-03 BER=1.90e-03 FER=1.69e-02
2025-10-16 16:58:19,096 | INFO | Epoch 498 Train Time 94.41301441192627s

2025-10-16 16:59:58,499 | INFO | Training epoch 499, Batch 1000/1000: LR=8.56e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.63e-02
2025-10-16 16:59:58,568 | INFO | Epoch 499 Train Time 99.4704077243805s

2025-10-16 17:01:35,436 | INFO | Training epoch 500, Batch 1000/1000: LR=8.56e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.61e-02
2025-10-16 17:01:35,487 | INFO | Epoch 500 Train Time 96.91530895233154s

2025-10-16 17:03:07,740 | INFO | Training epoch 501, Batch 1000/1000: LR=8.55e-05, Loss=4.49e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 17:03:07,779 | INFO | Epoch 501 Train Time 92.29053831100464s

2025-10-16 17:04:48,830 | INFO | Training epoch 502, Batch 1000/1000: LR=8.54e-05, Loss=4.73e-03 BER=1.92e-03 FER=1.75e-02
2025-10-16 17:04:48,887 | INFO | Epoch 502 Train Time 101.10744118690491s

2025-10-16 17:06:23,372 | INFO | Training epoch 503, Batch 1000/1000: LR=8.54e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.58e-02
2025-10-16 17:06:23,431 | INFO | Epoch 503 Train Time 94.54243111610413s

2025-10-16 17:07:59,418 | INFO | Training epoch 504, Batch 1000/1000: LR=8.53e-05, Loss=4.52e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 17:07:59,457 | INFO | Epoch 504 Train Time 96.0244779586792s

2025-10-16 17:09:25,357 | INFO | Training epoch 505, Batch 1000/1000: LR=8.53e-05, Loss=4.39e-03 BER=1.82e-03 FER=1.58e-02
2025-10-16 17:09:25,408 | INFO | Epoch 505 Train Time 85.9496557712555s

2025-10-16 17:10:53,948 | INFO | Training epoch 506, Batch 1000/1000: LR=8.52e-05, Loss=4.41e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 17:10:53,995 | INFO | Epoch 506 Train Time 88.58523893356323s

2025-10-16 17:12:30,225 | INFO | Training epoch 507, Batch 1000/1000: LR=8.52e-05, Loss=4.47e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 17:12:30,286 | INFO | Epoch 507 Train Time 96.29048943519592s

2025-10-16 17:14:04,701 | INFO | Training epoch 508, Batch 1000/1000: LR=8.51e-05, Loss=4.58e-03 BER=1.85e-03 FER=1.63e-02
2025-10-16 17:14:04,766 | INFO | Epoch 508 Train Time 94.4788932800293s

2025-10-16 17:15:49,537 | INFO | Training epoch 509, Batch 1000/1000: LR=8.51e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.56e-02
2025-10-16 17:15:49,587 | INFO | Epoch 509 Train Time 104.8173885345459s

2025-10-16 17:17:21,418 | INFO | Training epoch 510, Batch 1000/1000: LR=8.50e-05, Loss=4.54e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 17:17:21,471 | INFO | Epoch 510 Train Time 91.88285160064697s

2025-10-16 17:19:03,045 | INFO | Training epoch 511, Batch 1000/1000: LR=8.49e-05, Loss=4.41e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 17:19:03,087 | INFO | Epoch 511 Train Time 101.61393404006958s

2025-10-16 17:20:37,953 | INFO | Training epoch 512, Batch 1000/1000: LR=8.49e-05, Loss=4.34e-03 BER=1.75e-03 FER=1.57e-02
2025-10-16 17:20:38,007 | INFO | Epoch 512 Train Time 94.91988849639893s

2025-10-16 17:22:17,245 | INFO | Training epoch 513, Batch 1000/1000: LR=8.48e-05, Loss=4.53e-03 BER=1.87e-03 FER=1.64e-02
2025-10-16 17:22:17,295 | INFO | Epoch 513 Train Time 99.28671622276306s

2025-10-16 17:23:55,198 | INFO | Training epoch 514, Batch 1000/1000: LR=8.48e-05, Loss=4.46e-03 BER=1.83e-03 FER=1.61e-02
2025-10-16 17:23:55,240 | INFO | Epoch 514 Train Time 97.94300842285156s

2025-10-16 17:25:36,837 | INFO | Training epoch 515, Batch 1000/1000: LR=8.47e-05, Loss=4.34e-03 BER=1.79e-03 FER=1.57e-02
2025-10-16 17:25:36,880 | INFO | Epoch 515 Train Time 101.63995146751404s

2025-10-16 17:27:10,691 | INFO | Training epoch 516, Batch 1000/1000: LR=8.47e-05, Loss=4.34e-03 BER=1.79e-03 FER=1.58e-02
2025-10-16 17:27:10,734 | INFO | Epoch 516 Train Time 93.85273790359497s

2025-10-16 17:28:48,558 | INFO | Training epoch 517, Batch 1000/1000: LR=8.46e-05, Loss=4.28e-03 BER=1.77e-03 FER=1.56e-02
2025-10-16 17:28:48,609 | INFO | Epoch 517 Train Time 97.87393045425415s

2025-10-16 17:30:25,804 | INFO | Training epoch 518, Batch 1000/1000: LR=8.46e-05, Loss=4.52e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 17:30:25,852 | INFO | Epoch 518 Train Time 97.24090433120728s

2025-10-16 17:32:06,892 | INFO | Training epoch 519, Batch 1000/1000: LR=8.45e-05, Loss=4.46e-03 BER=1.81e-03 FER=1.62e-02
2025-10-16 17:32:06,935 | INFO | Epoch 519 Train Time 101.08234882354736s

2025-10-16 17:33:46,681 | INFO | Training epoch 520, Batch 1000/1000: LR=8.44e-05, Loss=4.41e-03 BER=1.80e-03 FER=1.57e-02
2025-10-16 17:33:46,732 | INFO | Epoch 520 Train Time 99.79518532752991s

2025-10-16 17:35:19,917 | INFO | Training epoch 521, Batch 1000/1000: LR=8.44e-05, Loss=4.43e-03 BER=1.84e-03 FER=1.60e-02
2025-10-16 17:35:19,980 | INFO | Epoch 521 Train Time 93.24690437316895s

2025-10-16 17:37:02,164 | INFO | Training epoch 522, Batch 1000/1000: LR=8.43e-05, Loss=4.54e-03 BER=1.87e-03 FER=1.65e-02
2025-10-16 17:37:02,218 | INFO | Epoch 522 Train Time 102.23641419410706s

2025-10-16 17:38:30,717 | INFO | Training epoch 523, Batch 1000/1000: LR=8.43e-05, Loss=4.65e-03 BER=1.94e-03 FER=1.71e-02
2025-10-16 17:38:30,769 | INFO | Epoch 523 Train Time 88.54964804649353s

2025-10-16 17:40:14,537 | INFO | Training epoch 524, Batch 1000/1000: LR=8.42e-05, Loss=4.44e-03 BER=1.84e-03 FER=1.61e-02
2025-10-16 17:40:14,584 | INFO | Epoch 524 Train Time 103.81206941604614s

2025-10-16 17:41:45,201 | INFO | Training epoch 525, Batch 1000/1000: LR=8.42e-05, Loss=4.37e-03 BER=1.78e-03 FER=1.59e-02
2025-10-16 17:41:45,252 | INFO | Epoch 525 Train Time 90.66678619384766s

2025-10-16 17:43:17,427 | INFO | Training epoch 526, Batch 1000/1000: LR=8.41e-05, Loss=4.47e-03 BER=1.84e-03 FER=1.60e-02
2025-10-16 17:43:17,470 | INFO | Epoch 526 Train Time 92.21678185462952s

2025-10-16 17:44:48,559 | INFO | Training epoch 527, Batch 1000/1000: LR=8.40e-05, Loss=4.39e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 17:44:48,613 | INFO | Epoch 527 Train Time 91.14189171791077s

2025-10-16 17:46:29,245 | INFO | Training epoch 528, Batch 1000/1000: LR=8.40e-05, Loss=4.38e-03 BER=1.79e-03 FER=1.57e-02
2025-10-16 17:46:29,287 | INFO | Epoch 528 Train Time 100.6728777885437s

2025-10-16 17:48:05,267 | INFO | Training epoch 529, Batch 1000/1000: LR=8.39e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 17:48:05,311 | INFO | Epoch 529 Train Time 96.02232217788696s

2025-10-16 17:49:48,099 | INFO | Training epoch 530, Batch 1000/1000: LR=8.39e-05, Loss=4.51e-03 BER=1.85e-03 FER=1.64e-02
2025-10-16 17:49:48,153 | INFO | Epoch 530 Train Time 102.84137773513794s

2025-10-16 17:51:22,523 | INFO | Training epoch 531, Batch 1000/1000: LR=8.38e-05, Loss=4.40e-03 BER=1.77e-03 FER=1.59e-02
2025-10-16 17:51:22,577 | INFO | Epoch 531 Train Time 94.42334079742432s

2025-10-16 17:53:02,201 | INFO | Training epoch 532, Batch 1000/1000: LR=8.38e-05, Loss=4.21e-03 BER=1.72e-03 FER=1.55e-02
2025-10-16 17:53:02,252 | INFO | Epoch 532 Train Time 99.67297911643982s

2025-10-16 17:53:02,253 | INFO | [P2] saving best_model (QAT) with loss 0.004215 at epoch 532
2025-10-16 17:54:33,937 | INFO | Training epoch 533, Batch 1000/1000: LR=8.37e-05, Loss=4.25e-03 BER=1.72e-03 FER=1.53e-02
2025-10-16 17:54:34,002 | INFO | Epoch 533 Train Time 91.64774084091187s

2025-10-16 17:56:03,937 | INFO | Training epoch 534, Batch 1000/1000: LR=8.36e-05, Loss=4.40e-03 BER=1.79e-03 FER=1.61e-02
2025-10-16 17:56:03,988 | INFO | Epoch 534 Train Time 89.98536920547485s

2025-10-16 17:57:48,423 | INFO | Training epoch 535, Batch 1000/1000: LR=8.36e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.61e-02
2025-10-16 17:57:48,485 | INFO | Epoch 535 Train Time 104.4958028793335s

2025-10-16 17:59:27,915 | INFO | Training epoch 536, Batch 1000/1000: LR=8.35e-05, Loss=4.35e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 17:59:27,971 | INFO | Epoch 536 Train Time 99.48515605926514s

2025-10-16 18:01:07,479 | INFO | Training epoch 537, Batch 1000/1000: LR=8.35e-05, Loss=4.47e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 18:01:07,524 | INFO | Epoch 537 Train Time 99.55120086669922s

2025-10-16 18:02:43,559 | INFO | Training epoch 538, Batch 1000/1000: LR=8.34e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.59e-02
2025-10-16 18:02:43,608 | INFO | Epoch 538 Train Time 96.08287501335144s

2025-10-16 18:04:26,467 | INFO | Training epoch 539, Batch 1000/1000: LR=8.34e-05, Loss=4.51e-03 BER=1.86e-03 FER=1.64e-02
2025-10-16 18:04:26,539 | INFO | Epoch 539 Train Time 102.93053793907166s

2025-10-16 18:06:08,392 | INFO | Training epoch 540, Batch 1000/1000: LR=8.33e-05, Loss=4.43e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 18:06:08,434 | INFO | Epoch 540 Train Time 101.89278554916382s

2025-10-16 18:07:53,297 | INFO | Training epoch 541, Batch 1000/1000: LR=8.32e-05, Loss=4.44e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 18:07:53,347 | INFO | Epoch 541 Train Time 104.91142725944519s

2025-10-16 18:09:27,721 | INFO | Training epoch 542, Batch 1000/1000: LR=8.32e-05, Loss=4.26e-03 BER=1.74e-03 FER=1.55e-02
2025-10-16 18:09:27,764 | INFO | Epoch 542 Train Time 94.41570448875427s

2025-10-16 18:11:01,300 | INFO | Training epoch 543, Batch 1000/1000: LR=8.31e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 18:11:01,360 | INFO | Epoch 543 Train Time 93.59490656852722s

2025-10-16 18:12:34,782 | INFO | Training epoch 544, Batch 1000/1000: LR=8.31e-05, Loss=4.49e-03 BER=1.83e-03 FER=1.63e-02
2025-10-16 18:12:34,828 | INFO | Epoch 544 Train Time 93.46555042266846s

2025-10-16 18:14:02,690 | INFO | Training epoch 545, Batch 1000/1000: LR=8.30e-05, Loss=4.16e-03 BER=1.69e-03 FER=1.53e-02
2025-10-16 18:14:02,742 | INFO | Epoch 545 Train Time 87.91281270980835s

2025-10-16 18:14:02,743 | INFO | [P2] saving best_model (QAT) with loss 0.004155 at epoch 545
2025-10-16 18:15:46,783 | INFO | Training epoch 546, Batch 1000/1000: LR=8.29e-05, Loss=4.19e-03 BER=1.72e-03 FER=1.58e-02
2025-10-16 18:15:46,823 | INFO | Epoch 546 Train Time 103.96845436096191s

2025-10-16 18:17:15,712 | INFO | Training epoch 547, Batch 1000/1000: LR=8.29e-05, Loss=4.49e-03 BER=1.79e-03 FER=1.60e-02
2025-10-16 18:17:15,765 | INFO | Epoch 547 Train Time 88.93885517120361s

2025-10-16 18:18:56,323 | INFO | Training epoch 548, Batch 1000/1000: LR=8.28e-05, Loss=4.43e-03 BER=1.81e-03 FER=1.62e-02
2025-10-16 18:18:56,374 | INFO | Epoch 548 Train Time 100.6076762676239s

2025-10-16 18:20:33,431 | INFO | Training epoch 549, Batch 1000/1000: LR=8.28e-05, Loss=4.43e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 18:20:33,527 | INFO | Epoch 549 Train Time 97.15234780311584s

2025-10-16 18:22:06,121 | INFO | Training epoch 550, Batch 1000/1000: LR=8.27e-05, Loss=4.39e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 18:22:06,185 | INFO | Epoch 550 Train Time 92.6563196182251s

2025-10-16 18:23:43,089 | INFO | Training epoch 551, Batch 1000/1000: LR=8.26e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.61e-02
2025-10-16 18:23:43,129 | INFO | Epoch 551 Train Time 96.94208598136902s

2025-10-16 18:25:17,124 | INFO | Training epoch 552, Batch 1000/1000: LR=8.26e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 18:25:17,167 | INFO | Epoch 552 Train Time 94.03731608390808s

2025-10-16 18:26:52,794 | INFO | Training epoch 553, Batch 1000/1000: LR=8.25e-05, Loss=4.43e-03 BER=1.80e-03 FER=1.62e-02
2025-10-16 18:26:52,870 | INFO | Epoch 553 Train Time 95.70156073570251s

2025-10-16 18:28:26,611 | INFO | Training epoch 554, Batch 1000/1000: LR=8.25e-05, Loss=4.55e-03 BER=1.87e-03 FER=1.66e-02
2025-10-16 18:28:26,675 | INFO | Epoch 554 Train Time 93.80360507965088s

2025-10-16 18:29:59,468 | INFO | Training epoch 555, Batch 1000/1000: LR=8.24e-05, Loss=4.47e-03 BER=1.83e-03 FER=1.62e-02
2025-10-16 18:29:59,532 | INFO | Epoch 555 Train Time 92.8538761138916s

2025-10-16 18:31:30,628 | INFO | Training epoch 556, Batch 1000/1000: LR=8.24e-05, Loss=4.40e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 18:31:30,694 | INFO | Epoch 556 Train Time 91.16155743598938s

2025-10-16 18:33:04,951 | INFO | Training epoch 557, Batch 1000/1000: LR=8.23e-05, Loss=4.23e-03 BER=1.73e-03 FER=1.56e-02
2025-10-16 18:33:04,994 | INFO | Epoch 557 Train Time 94.29786372184753s

2025-10-16 18:34:39,834 | INFO | Training epoch 558, Batch 1000/1000: LR=8.22e-05, Loss=4.24e-03 BER=1.73e-03 FER=1.53e-02
2025-10-16 18:34:39,886 | INFO | Epoch 558 Train Time 94.89122033119202s

2025-10-16 18:36:14,298 | INFO | Training epoch 559, Batch 1000/1000: LR=8.22e-05, Loss=4.24e-03 BER=1.74e-03 FER=1.55e-02
2025-10-16 18:36:14,362 | INFO | Epoch 559 Train Time 94.47386431694031s

2025-10-16 18:37:48,521 | INFO | Training epoch 560, Batch 1000/1000: LR=8.21e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 18:37:48,572 | INFO | Epoch 560 Train Time 94.20893716812134s

2025-10-16 18:39:19,532 | INFO | Training epoch 561, Batch 1000/1000: LR=8.21e-05, Loss=4.58e-03 BER=1.88e-03 FER=1.64e-02
2025-10-16 18:39:19,591 | INFO | Epoch 561 Train Time 91.01787996292114s

2025-10-16 18:40:50,995 | INFO | Training epoch 562, Batch 1000/1000: LR=8.20e-05, Loss=4.68e-03 BER=1.93e-03 FER=1.67e-02
2025-10-16 18:40:51,038 | INFO | Epoch 562 Train Time 91.44486999511719s

2025-10-16 18:42:24,205 | INFO | Training epoch 563, Batch 1000/1000: LR=8.19e-05, Loss=4.49e-03 BER=1.84e-03 FER=1.61e-02
2025-10-16 18:42:24,259 | INFO | Epoch 563 Train Time 93.22036457061768s

2025-10-16 18:44:03,272 | INFO | Training epoch 564, Batch 1000/1000: LR=8.19e-05, Loss=4.56e-03 BER=1.87e-03 FER=1.63e-02
2025-10-16 18:44:03,324 | INFO | Epoch 564 Train Time 99.06429100036621s

2025-10-16 18:45:39,266 | INFO | Training epoch 565, Batch 1000/1000: LR=8.18e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.58e-02
2025-10-16 18:45:39,309 | INFO | Epoch 565 Train Time 95.98342776298523s

2025-10-16 18:47:11,894 | INFO | Training epoch 566, Batch 1000/1000: LR=8.18e-05, Loss=4.31e-03 BER=1.78e-03 FER=1.58e-02
2025-10-16 18:47:11,948 | INFO | Epoch 566 Train Time 92.63785481452942s

2025-10-16 18:48:39,595 | INFO | Training epoch 567, Batch 1000/1000: LR=8.17e-05, Loss=4.46e-03 BER=1.83e-03 FER=1.61e-02
2025-10-16 18:48:39,650 | INFO | Epoch 567 Train Time 87.69946765899658s

2025-10-16 18:50:21,287 | INFO | Training epoch 568, Batch 1000/1000: LR=8.16e-05, Loss=4.25e-03 BER=1.76e-03 FER=1.57e-02
2025-10-16 18:50:21,330 | INFO | Epoch 568 Train Time 101.6778576374054s

2025-10-16 18:51:57,341 | INFO | Training epoch 569, Batch 1000/1000: LR=8.16e-05, Loss=4.53e-03 BER=1.84e-03 FER=1.63e-02
2025-10-16 18:51:57,408 | INFO | Epoch 569 Train Time 96.0768346786499s

2025-10-16 18:53:32,676 | INFO | Training epoch 570, Batch 1000/1000: LR=8.15e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 18:53:32,741 | INFO | Epoch 570 Train Time 95.33116888999939s

2025-10-16 18:55:10,662 | INFO | Training epoch 571, Batch 1000/1000: LR=8.14e-05, Loss=4.34e-03 BER=1.75e-03 FER=1.56e-02
2025-10-16 18:55:10,702 | INFO | Epoch 571 Train Time 97.96031379699707s

2025-10-16 18:56:44,348 | INFO | Training epoch 572, Batch 1000/1000: LR=8.14e-05, Loss=4.36e-03 BER=1.78e-03 FER=1.58e-02
2025-10-16 18:56:44,406 | INFO | Epoch 572 Train Time 93.70214891433716s

2025-10-16 18:58:24,939 | INFO | Training epoch 573, Batch 1000/1000: LR=8.13e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.58e-02
2025-10-16 18:58:24,993 | INFO | Epoch 573 Train Time 100.58534049987793s

2025-10-16 18:59:55,696 | INFO | Training epoch 574, Batch 1000/1000: LR=8.13e-05, Loss=4.16e-03 BER=1.70e-03 FER=1.50e-02
2025-10-16 18:59:55,740 | INFO | Epoch 574 Train Time 90.7463390827179s

2025-10-16 19:01:28,500 | INFO | Training epoch 575, Batch 1000/1000: LR=8.12e-05, Loss=4.56e-03 BER=1.87e-03 FER=1.64e-02
2025-10-16 19:01:28,566 | INFO | Epoch 575 Train Time 92.8251085281372s

2025-10-16 19:03:04,626 | INFO | Training epoch 576, Batch 1000/1000: LR=8.11e-05, Loss=4.62e-03 BER=1.89e-03 FER=1.70e-02
2025-10-16 19:03:04,688 | INFO | Epoch 576 Train Time 96.119637966156s

2025-10-16 19:04:43,362 | INFO | Training epoch 577, Batch 1000/1000: LR=8.11e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.57e-02
2025-10-16 19:04:43,408 | INFO | Epoch 577 Train Time 98.71819734573364s

2025-10-16 19:06:15,846 | INFO | Training epoch 578, Batch 1000/1000: LR=8.10e-05, Loss=4.41e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 19:06:15,891 | INFO | Epoch 578 Train Time 92.48222827911377s

2025-10-16 19:07:48,979 | INFO | Training epoch 579, Batch 1000/1000: LR=8.10e-05, Loss=4.33e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 19:07:49,026 | INFO | Epoch 579 Train Time 93.13318371772766s

2025-10-16 19:09:21,950 | INFO | Training epoch 580, Batch 1000/1000: LR=8.09e-05, Loss=4.45e-03 BER=1.87e-03 FER=1.61e-02
2025-10-16 19:09:21,995 | INFO | Epoch 580 Train Time 92.96666169166565s

2025-10-16 19:11:00,148 | INFO | Training epoch 581, Batch 1000/1000: LR=8.08e-05, Loss=4.39e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 19:11:00,226 | INFO | Epoch 581 Train Time 98.22830939292908s

2025-10-16 19:12:35,285 | INFO | Training epoch 582, Batch 1000/1000: LR=8.08e-05, Loss=4.29e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 19:12:35,326 | INFO | Epoch 582 Train Time 95.09749865531921s

2025-10-16 19:14:10,308 | INFO | Training epoch 583, Batch 1000/1000: LR=8.07e-05, Loss=4.19e-03 BER=1.74e-03 FER=1.53e-02
2025-10-16 19:14:10,359 | INFO | Epoch 583 Train Time 95.03230786323547s

2025-10-16 19:15:48,053 | INFO | Training epoch 584, Batch 1000/1000: LR=8.07e-05, Loss=4.45e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 19:15:48,106 | INFO | Epoch 584 Train Time 97.74455666542053s

2025-10-16 19:17:21,740 | INFO | Training epoch 585, Batch 1000/1000: LR=8.06e-05, Loss=4.44e-03 BER=1.81e-03 FER=1.61e-02
2025-10-16 19:17:21,792 | INFO | Epoch 585 Train Time 93.6840877532959s

2025-10-16 19:19:01,302 | INFO | Training epoch 586, Batch 1000/1000: LR=8.05e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.66e-02
2025-10-16 19:19:01,349 | INFO | Epoch 586 Train Time 99.55546283721924s

2025-10-16 19:20:38,886 | INFO | Training epoch 587, Batch 1000/1000: LR=8.05e-05, Loss=4.50e-03 BER=1.83e-03 FER=1.63e-02
2025-10-16 19:20:38,952 | INFO | Epoch 587 Train Time 97.60156607627869s

2025-10-16 19:22:19,683 | INFO | Training epoch 588, Batch 1000/1000: LR=8.04e-05, Loss=4.55e-03 BER=1.84e-03 FER=1.64e-02
2025-10-16 19:22:19,752 | INFO | Epoch 588 Train Time 100.79864478111267s

2025-10-16 19:23:56,135 | INFO | Training epoch 589, Batch 1000/1000: LR=8.03e-05, Loss=4.33e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 19:23:56,185 | INFO | Epoch 589 Train Time 96.43196201324463s

2025-10-16 19:25:31,526 | INFO | Training epoch 590, Batch 1000/1000: LR=8.03e-05, Loss=4.25e-03 BER=1.76e-03 FER=1.55e-02
2025-10-16 19:25:31,577 | INFO | Epoch 590 Train Time 95.39024686813354s

2025-10-16 19:27:06,648 | INFO | Training epoch 591, Batch 1000/1000: LR=8.02e-05, Loss=4.37e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 19:27:06,702 | INFO | Epoch 591 Train Time 95.1242458820343s

2025-10-16 19:28:36,776 | INFO | Training epoch 592, Batch 1000/1000: LR=8.02e-05, Loss=4.40e-03 BER=1.78e-03 FER=1.58e-02
2025-10-16 19:28:36,824 | INFO | Epoch 592 Train Time 90.12045741081238s

2025-10-16 19:30:12,452 | INFO | Training epoch 593, Batch 1000/1000: LR=8.01e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 19:30:12,517 | INFO | Epoch 593 Train Time 95.69202327728271s

2025-10-16 19:31:52,939 | INFO | Training epoch 594, Batch 1000/1000: LR=8.00e-05, Loss=4.45e-03 BER=1.79e-03 FER=1.60e-02
2025-10-16 19:31:52,983 | INFO | Epoch 594 Train Time 100.46277904510498s

2025-10-16 19:33:30,617 | INFO | Training epoch 595, Batch 1000/1000: LR=8.00e-05, Loss=4.29e-03 BER=1.75e-03 FER=1.55e-02
2025-10-16 19:33:30,668 | INFO | Epoch 595 Train Time 97.6835389137268s

2025-10-16 19:35:04,942 | INFO | Training epoch 596, Batch 1000/1000: LR=7.99e-05, Loss=4.46e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 19:35:04,992 | INFO | Epoch 596 Train Time 94.3231770992279s

2025-10-16 19:36:40,420 | INFO | Training epoch 597, Batch 1000/1000: LR=7.98e-05, Loss=4.19e-03 BER=1.73e-03 FER=1.54e-02
2025-10-16 19:36:40,482 | INFO | Epoch 597 Train Time 95.48672723770142s

2025-10-16 19:38:15,708 | INFO | Training epoch 598, Batch 1000/1000: LR=7.98e-05, Loss=4.33e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 19:38:15,764 | INFO | Epoch 598 Train Time 95.2804787158966s

2025-10-16 19:39:51,123 | INFO | Training epoch 599, Batch 1000/1000: LR=7.97e-05, Loss=4.26e-03 BER=1.74e-03 FER=1.55e-02
2025-10-16 19:39:51,176 | INFO | Epoch 599 Train Time 95.41062307357788s

2025-10-16 19:41:38,105 | INFO | Training epoch 600, Batch 1000/1000: LR=7.97e-05, Loss=4.40e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 19:41:38,162 | INFO | Epoch 600 Train Time 106.98339223861694s

2025-10-16 19:43:17,047 | INFO | Training epoch 601, Batch 1000/1000: LR=7.96e-05, Loss=4.40e-03 BER=1.81e-03 FER=1.58e-02
2025-10-16 19:43:17,112 | INFO | Epoch 601 Train Time 98.94902801513672s

2025-10-16 19:44:54,501 | INFO | Training epoch 602, Batch 1000/1000: LR=7.95e-05, Loss=4.30e-03 BER=1.75e-03 FER=1.56e-02
2025-10-16 19:44:54,559 | INFO | Epoch 602 Train Time 97.44509434700012s

2025-10-16 19:46:39,056 | INFO | Training epoch 603, Batch 1000/1000: LR=7.95e-05, Loss=4.44e-03 BER=1.84e-03 FER=1.61e-02
2025-10-16 19:46:39,105 | INFO | Epoch 603 Train Time 104.5437798500061s

2025-10-16 19:48:19,972 | INFO | Training epoch 604, Batch 1000/1000: LR=7.94e-05, Loss=4.43e-03 BER=1.84e-03 FER=1.63e-02
2025-10-16 19:48:20,030 | INFO | Epoch 604 Train Time 100.92318916320801s

2025-10-16 19:49:53,738 | INFO | Training epoch 605, Batch 1000/1000: LR=7.93e-05, Loss=4.38e-03 BER=1.77e-03 FER=1.57e-02
2025-10-16 19:49:53,802 | INFO | Epoch 605 Train Time 93.76982545852661s

2025-10-16 19:51:26,223 | INFO | Training epoch 606, Batch 1000/1000: LR=7.93e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 19:51:26,277 | INFO | Epoch 606 Train Time 92.47327780723572s

2025-10-16 19:53:01,998 | INFO | Training epoch 607, Batch 1000/1000: LR=7.92e-05, Loss=4.49e-03 BER=1.85e-03 FER=1.62e-02
2025-10-16 19:53:02,058 | INFO | Epoch 607 Train Time 95.77985787391663s

2025-10-16 19:54:36,073 | INFO | Training epoch 608, Batch 1000/1000: LR=7.92e-05, Loss=4.45e-03 BER=1.84e-03 FER=1.59e-02
2025-10-16 19:54:36,116 | INFO | Epoch 608 Train Time 94.05629062652588s

2025-10-16 19:56:09,362 | INFO | Training epoch 609, Batch 1000/1000: LR=7.91e-05, Loss=4.24e-03 BER=1.75e-03 FER=1.55e-02
2025-10-16 19:56:09,422 | INFO | Epoch 609 Train Time 93.30411100387573s

2025-10-16 19:57:44,304 | INFO | Training epoch 610, Batch 1000/1000: LR=7.90e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.55e-02
2025-10-16 19:57:44,361 | INFO | Epoch 610 Train Time 94.93784284591675s

2025-10-16 19:59:18,537 | INFO | Training epoch 611, Batch 1000/1000: LR=7.90e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.63e-02
2025-10-16 19:59:18,588 | INFO | Epoch 611 Train Time 94.22426009178162s

2025-10-16 20:00:50,697 | INFO | Training epoch 612, Batch 1000/1000: LR=7.89e-05, Loss=4.34e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 20:00:50,754 | INFO | Epoch 612 Train Time 92.16528034210205s

2025-10-16 20:02:24,967 | INFO | Training epoch 613, Batch 1000/1000: LR=7.88e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.58e-02
2025-10-16 20:02:25,014 | INFO | Epoch 613 Train Time 94.25694799423218s

2025-10-16 20:03:59,533 | INFO | Training epoch 614, Batch 1000/1000: LR=7.88e-05, Loss=4.27e-03 BER=1.77e-03 FER=1.55e-02
2025-10-16 20:03:59,583 | INFO | Epoch 614 Train Time 94.5673611164093s

2025-10-16 20:05:29,872 | INFO | Training epoch 615, Batch 1000/1000: LR=7.87e-05, Loss=4.17e-03 BER=1.69e-03 FER=1.48e-02
2025-10-16 20:05:29,938 | INFO | Epoch 615 Train Time 90.35359334945679s

2025-10-16 20:07:04,859 | INFO | Training epoch 616, Batch 1000/1000: LR=7.86e-05, Loss=4.30e-03 BER=1.75e-03 FER=1.55e-02
2025-10-16 20:07:04,911 | INFO | Epoch 616 Train Time 94.9717013835907s

2025-10-16 20:08:35,448 | INFO | Training epoch 617, Batch 1000/1000: LR=7.86e-05, Loss=4.42e-03 BER=1.83e-03 FER=1.60e-02
2025-10-16 20:08:35,492 | INFO | Epoch 617 Train Time 90.57970023155212s

2025-10-16 20:10:03,932 | INFO | Training epoch 618, Batch 1000/1000: LR=7.85e-05, Loss=4.18e-03 BER=1.71e-03 FER=1.52e-02
2025-10-16 20:10:03,984 | INFO | Epoch 618 Train Time 88.49145841598511s

2025-10-16 20:11:40,331 | INFO | Training epoch 619, Batch 1000/1000: LR=7.85e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.56e-02
2025-10-16 20:11:40,370 | INFO | Epoch 619 Train Time 96.38397932052612s

2025-10-16 20:13:13,620 | INFO | Training epoch 620, Batch 1000/1000: LR=7.84e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.54e-02
2025-10-16 20:13:13,674 | INFO | Epoch 620 Train Time 93.30204319953918s

2025-10-16 20:15:03,045 | INFO | Training epoch 621, Batch 1000/1000: LR=7.83e-05, Loss=4.47e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 20:15:03,104 | INFO | Epoch 621 Train Time 109.42888069152832s

2025-10-16 20:16:40,079 | INFO | Training epoch 622, Batch 1000/1000: LR=7.83e-05, Loss=4.48e-03 BER=1.87e-03 FER=1.62e-02
2025-10-16 20:16:40,127 | INFO | Epoch 622 Train Time 97.02106857299805s

2025-10-16 20:18:14,527 | INFO | Training epoch 623, Batch 1000/1000: LR=7.82e-05, Loss=4.40e-03 BER=1.80e-03 FER=1.57e-02
2025-10-16 20:18:14,579 | INFO | Epoch 623 Train Time 94.45124673843384s

2025-10-16 20:19:51,978 | INFO | Training epoch 624, Batch 1000/1000: LR=7.81e-05, Loss=4.19e-03 BER=1.72e-03 FER=1.54e-02
2025-10-16 20:19:52,028 | INFO | Epoch 624 Train Time 97.44730544090271s

2025-10-16 20:21:26,066 | INFO | Training epoch 625, Batch 1000/1000: LR=7.81e-05, Loss=4.51e-03 BER=1.84e-03 FER=1.60e-02
2025-10-16 20:21:26,115 | INFO | Epoch 625 Train Time 94.0866060256958s

2025-10-16 20:22:57,721 | INFO | Training epoch 626, Batch 1000/1000: LR=7.80e-05, Loss=4.27e-03 BER=1.77e-03 FER=1.58e-02
2025-10-16 20:22:57,786 | INFO | Epoch 626 Train Time 91.66967582702637s

2025-10-16 20:24:37,315 | INFO | Training epoch 627, Batch 1000/1000: LR=7.79e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.56e-02
2025-10-16 20:24:37,364 | INFO | Epoch 627 Train Time 99.57521915435791s

2025-10-16 20:26:17,328 | INFO | Training epoch 628, Batch 1000/1000: LR=7.79e-05, Loss=4.56e-03 BER=1.87e-03 FER=1.63e-02
2025-10-16 20:26:17,372 | INFO | Epoch 628 Train Time 100.00653624534607s

2025-10-16 20:27:49,621 | INFO | Training epoch 629, Batch 1000/1000: LR=7.78e-05, Loss=4.30e-03 BER=1.78e-03 FER=1.57e-02
2025-10-16 20:27:49,692 | INFO | Epoch 629 Train Time 92.31817698478699s

2025-10-16 20:29:28,349 | INFO | Training epoch 630, Batch 1000/1000: LR=7.77e-05, Loss=4.43e-03 BER=1.83e-03 FER=1.62e-02
2025-10-16 20:29:28,407 | INFO | Epoch 630 Train Time 98.71436834335327s

2025-10-16 20:31:03,239 | INFO | Training epoch 631, Batch 1000/1000: LR=7.77e-05, Loss=4.28e-03 BER=1.76e-03 FER=1.53e-02
2025-10-16 20:31:03,303 | INFO | Epoch 631 Train Time 94.89479994773865s

2025-10-16 20:32:47,244 | INFO | Training epoch 632, Batch 1000/1000: LR=7.76e-05, Loss=4.20e-03 BER=1.73e-03 FER=1.54e-02
2025-10-16 20:32:47,304 | INFO | Epoch 632 Train Time 103.99764513969421s

2025-10-16 20:34:24,556 | INFO | Training epoch 633, Batch 1000/1000: LR=7.75e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.57e-02
2025-10-16 20:34:24,601 | INFO | Epoch 633 Train Time 97.29608249664307s

2025-10-16 20:36:04,870 | INFO | Training epoch 634, Batch 1000/1000: LR=7.75e-05, Loss=4.39e-03 BER=1.77e-03 FER=1.60e-02
2025-10-16 20:36:04,925 | INFO | Epoch 634 Train Time 100.32191681861877s

2025-10-16 20:37:46,114 | INFO | Training epoch 635, Batch 1000/1000: LR=7.74e-05, Loss=4.50e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 20:37:46,189 | INFO | Epoch 635 Train Time 101.26314663887024s

2025-10-16 20:39:16,568 | INFO | Training epoch 636, Batch 1000/1000: LR=7.74e-05, Loss=4.37e-03 BER=1.78e-03 FER=1.58e-02
2025-10-16 20:39:16,614 | INFO | Epoch 636 Train Time 90.42359042167664s

2025-10-16 20:40:56,536 | INFO | Training epoch 637, Batch 1000/1000: LR=7.73e-05, Loss=4.47e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 20:40:56,585 | INFO | Epoch 637 Train Time 99.97049689292908s

2025-10-16 20:42:29,635 | INFO | Training epoch 638, Batch 1000/1000: LR=7.72e-05, Loss=4.22e-03 BER=1.70e-03 FER=1.51e-02
2025-10-16 20:42:29,684 | INFO | Epoch 638 Train Time 93.09721255302429s

2025-10-16 20:43:59,412 | INFO | Training epoch 639, Batch 1000/1000: LR=7.72e-05, Loss=4.48e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 20:43:59,457 | INFO | Epoch 639 Train Time 89.77109503746033s

2025-10-16 20:45:31,895 | INFO | Training epoch 640, Batch 1000/1000: LR=7.71e-05, Loss=4.46e-03 BER=1.85e-03 FER=1.61e-02
2025-10-16 20:45:31,971 | INFO | Epoch 640 Train Time 92.51274490356445s

2025-10-16 20:47:19,078 | INFO | Training epoch 641, Batch 1000/1000: LR=7.70e-05, Loss=4.35e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 20:47:19,137 | INFO | Epoch 641 Train Time 107.1631429195404s

2025-10-16 20:48:56,572 | INFO | Training epoch 642, Batch 1000/1000: LR=7.70e-05, Loss=4.28e-03 BER=1.76e-03 FER=1.55e-02
2025-10-16 20:48:56,617 | INFO | Epoch 642 Train Time 97.47992324829102s

2025-10-16 20:50:32,263 | INFO | Training epoch 643, Batch 1000/1000: LR=7.69e-05, Loss=4.36e-03 BER=1.77e-03 FER=1.58e-02
2025-10-16 20:50:32,306 | INFO | Epoch 643 Train Time 95.68732738494873s

2025-10-16 20:52:08,069 | INFO | Training epoch 644, Batch 1000/1000: LR=7.68e-05, Loss=4.30e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 20:52:08,109 | INFO | Epoch 644 Train Time 95.80057621002197s

2025-10-16 20:53:37,935 | INFO | Training epoch 645, Batch 1000/1000: LR=7.68e-05, Loss=4.35e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 20:53:37,994 | INFO | Epoch 645 Train Time 89.88442373275757s

2025-10-16 20:55:12,428 | INFO | Training epoch 646, Batch 1000/1000: LR=7.67e-05, Loss=4.49e-03 BER=1.83e-03 FER=1.65e-02
2025-10-16 20:55:12,496 | INFO | Epoch 646 Train Time 94.49945282936096s

2025-10-16 20:56:46,724 | INFO | Training epoch 647, Batch 1000/1000: LR=7.66e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.59e-02
2025-10-16 20:56:46,770 | INFO | Epoch 647 Train Time 94.27359890937805s

2025-10-16 20:58:25,050 | INFO | Training epoch 648, Batch 1000/1000: LR=7.66e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 20:58:25,089 | INFO | Epoch 648 Train Time 98.31807518005371s

2025-10-16 20:59:58,824 | INFO | Training epoch 649, Batch 1000/1000: LR=7.65e-05, Loss=4.38e-03 BER=1.76e-03 FER=1.56e-02
2025-10-16 20:59:58,885 | INFO | Epoch 649 Train Time 93.79378056526184s

2025-10-16 21:01:29,084 | INFO | Training epoch 650, Batch 1000/1000: LR=7.64e-05, Loss=4.34e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 21:01:29,143 | INFO | Epoch 650 Train Time 90.25547504425049s

2025-10-16 21:03:02,381 | INFO | Training epoch 651, Batch 1000/1000: LR=7.64e-05, Loss=4.46e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 21:03:02,431 | INFO | Epoch 651 Train Time 93.28682661056519s

2025-10-16 21:04:44,410 | INFO | Training epoch 652, Batch 1000/1000: LR=7.63e-05, Loss=4.33e-03 BER=1.77e-03 FER=1.56e-02
2025-10-16 21:04:44,464 | INFO | Epoch 652 Train Time 102.03115224838257s

2025-10-16 21:06:19,336 | INFO | Training epoch 653, Batch 1000/1000: LR=7.62e-05, Loss=4.46e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 21:06:19,375 | INFO | Epoch 653 Train Time 94.90988159179688s

2025-10-16 21:07:58,001 | INFO | Training epoch 654, Batch 1000/1000: LR=7.62e-05, Loss=4.36e-03 BER=1.79e-03 FER=1.57e-02
2025-10-16 21:07:58,047 | INFO | Epoch 654 Train Time 98.67031764984131s

2025-10-16 21:09:36,338 | INFO | Training epoch 655, Batch 1000/1000: LR=7.61e-05, Loss=4.17e-03 BER=1.70e-03 FER=1.51e-02
2025-10-16 21:09:36,385 | INFO | Epoch 655 Train Time 98.33745646476746s

2025-10-16 21:11:13,583 | INFO | Training epoch 656, Batch 1000/1000: LR=7.60e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.58e-02
2025-10-16 21:11:13,646 | INFO | Epoch 656 Train Time 97.25906991958618s

2025-10-16 21:12:46,077 | INFO | Training epoch 657, Batch 1000/1000: LR=7.60e-05, Loss=4.29e-03 BER=1.77e-03 FER=1.57e-02
2025-10-16 21:12:46,127 | INFO | Epoch 657 Train Time 92.47952079772949s

2025-10-16 21:14:26,616 | INFO | Training epoch 658, Batch 1000/1000: LR=7.59e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.62e-02
2025-10-16 21:14:26,667 | INFO | Epoch 658 Train Time 100.53777503967285s

2025-10-16 21:16:06,066 | INFO | Training epoch 659, Batch 1000/1000: LR=7.58e-05, Loss=4.26e-03 BER=1.74e-03 FER=1.56e-02
2025-10-16 21:16:06,114 | INFO | Epoch 659 Train Time 99.44610071182251s

2025-10-16 21:17:44,306 | INFO | Training epoch 660, Batch 1000/1000: LR=7.58e-05, Loss=4.36e-03 BER=1.81e-03 FER=1.57e-02
2025-10-16 21:17:44,362 | INFO | Epoch 660 Train Time 98.24648952484131s

2025-10-16 21:19:14,952 | INFO | Training epoch 661, Batch 1000/1000: LR=7.57e-05, Loss=4.26e-03 BER=1.73e-03 FER=1.56e-02
2025-10-16 21:19:14,995 | INFO | Epoch 661 Train Time 90.63120651245117s

2025-10-16 21:20:43,380 | INFO | Training epoch 662, Batch 1000/1000: LR=7.56e-05, Loss=4.41e-03 BER=1.82e-03 FER=1.59e-02
2025-10-16 21:20:43,421 | INFO | Epoch 662 Train Time 88.42399215698242s

2025-10-16 21:22:18,730 | INFO | Training epoch 663, Batch 1000/1000: LR=7.56e-05, Loss=4.26e-03 BER=1.75e-03 FER=1.57e-02
2025-10-16 21:22:18,784 | INFO | Epoch 663 Train Time 95.36166715621948s

2025-10-16 21:23:56,998 | INFO | Training epoch 664, Batch 1000/1000: LR=7.55e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.61e-02
2025-10-16 21:23:57,050 | INFO | Epoch 664 Train Time 98.26485848426819s

2025-10-16 21:25:29,675 | INFO | Training epoch 665, Batch 1000/1000: LR=7.54e-05, Loss=4.38e-03 BER=1.83e-03 FER=1.60e-02
2025-10-16 21:25:29,716 | INFO | Epoch 665 Train Time 92.66451239585876s

2025-10-16 21:27:04,975 | INFO | Training epoch 666, Batch 1000/1000: LR=7.54e-05, Loss=4.24e-03 BER=1.73e-03 FER=1.53e-02
2025-10-16 21:27:05,039 | INFO | Epoch 666 Train Time 95.32067823410034s

2025-10-16 21:28:40,904 | INFO | Training epoch 667, Batch 1000/1000: LR=7.53e-05, Loss=4.33e-03 BER=1.77e-03 FER=1.58e-02
2025-10-16 21:28:40,947 | INFO | Epoch 667 Train Time 95.90712285041809s

2025-10-16 21:30:26,541 | INFO | Training epoch 668, Batch 1000/1000: LR=7.52e-05, Loss=4.37e-03 BER=1.78e-03 FER=1.57e-02
2025-10-16 21:30:26,585 | INFO | Epoch 668 Train Time 105.63601779937744s

2025-10-16 21:32:02,704 | INFO | Training epoch 669, Batch 1000/1000: LR=7.52e-05, Loss=4.34e-03 BER=1.80e-03 FER=1.59e-02
2025-10-16 21:32:02,773 | INFO | Epoch 669 Train Time 96.18613028526306s

2025-10-16 21:33:42,366 | INFO | Training epoch 670, Batch 1000/1000: LR=7.51e-05, Loss=4.27e-03 BER=1.76e-03 FER=1.53e-02
2025-10-16 21:33:42,415 | INFO | Epoch 670 Train Time 99.64080810546875s

2025-10-16 21:35:16,077 | INFO | Training epoch 671, Batch 1000/1000: LR=7.50e-05, Loss=4.43e-03 BER=1.83e-03 FER=1.60e-02
2025-10-16 21:35:16,138 | INFO | Epoch 671 Train Time 93.72288370132446s

2025-10-16 21:36:51,408 | INFO | Training epoch 672, Batch 1000/1000: LR=7.50e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.51e-02
2025-10-16 21:36:51,461 | INFO | Epoch 672 Train Time 95.32135820388794s

2025-10-16 21:36:51,463 | INFO | [P2] saving best_model (QAT) with loss 0.004138 at epoch 672
2025-10-16 21:38:28,750 | INFO | Training epoch 673, Batch 1000/1000: LR=7.49e-05, Loss=4.34e-03 BER=1.77e-03 FER=1.55e-02
2025-10-16 21:38:28,796 | INFO | Epoch 673 Train Time 97.22621512413025s

2025-10-16 21:40:05,845 | INFO | Training epoch 674, Batch 1000/1000: LR=7.48e-05, Loss=4.46e-03 BER=1.84e-03 FER=1.61e-02
2025-10-16 21:40:05,900 | INFO | Epoch 674 Train Time 97.10315585136414s

2025-10-16 21:41:33,811 | INFO | Training epoch 675, Batch 1000/1000: LR=7.48e-05, Loss=4.30e-03 BER=1.75e-03 FER=1.54e-02
2025-10-16 21:41:33,861 | INFO | Epoch 675 Train Time 87.95961833000183s

2025-10-16 21:43:11,073 | INFO | Training epoch 676, Batch 1000/1000: LR=7.47e-05, Loss=4.25e-03 BER=1.77e-03 FER=1.56e-02
2025-10-16 21:43:11,135 | INFO | Epoch 676 Train Time 97.27158999443054s

2025-10-16 21:44:40,168 | INFO | Training epoch 677, Batch 1000/1000: LR=7.46e-05, Loss=4.21e-03 BER=1.73e-03 FER=1.52e-02
2025-10-16 21:44:40,214 | INFO | Epoch 677 Train Time 89.0775694847107s

2025-10-16 21:46:18,924 | INFO | Training epoch 678, Batch 1000/1000: LR=7.46e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.60e-02
2025-10-16 21:46:18,997 | INFO | Epoch 678 Train Time 98.78051376342773s

2025-10-16 21:47:49,997 | INFO | Training epoch 679, Batch 1000/1000: LR=7.45e-05, Loss=4.26e-03 BER=1.72e-03 FER=1.54e-02
2025-10-16 21:47:50,045 | INFO | Epoch 679 Train Time 91.04762387275696s

2025-10-16 21:49:28,161 | INFO | Training epoch 680, Batch 1000/1000: LR=7.44e-05, Loss=4.54e-03 BER=1.84e-03 FER=1.66e-02
2025-10-16 21:49:28,223 | INFO | Epoch 680 Train Time 98.17575907707214s

2025-10-16 21:51:00,253 | INFO | Training epoch 681, Batch 1000/1000: LR=7.43e-05, Loss=4.34e-03 BER=1.79e-03 FER=1.54e-02
2025-10-16 21:51:00,297 | INFO | Epoch 681 Train Time 92.07345128059387s

2025-10-16 21:52:37,815 | INFO | Training epoch 682, Batch 1000/1000: LR=7.43e-05, Loss=4.46e-03 BER=1.85e-03 FER=1.62e-02
2025-10-16 21:52:37,880 | INFO | Epoch 682 Train Time 97.58206343650818s

2025-10-16 21:54:00,966 | INFO | Training epoch 683, Batch 1000/1000: LR=7.42e-05, Loss=4.38e-03 BER=1.79e-03 FER=1.60e-02
2025-10-16 21:54:01,013 | INFO | Epoch 683 Train Time 83.1309916973114s

2025-10-16 21:55:43,203 | INFO | Training epoch 684, Batch 1000/1000: LR=7.41e-05, Loss=4.36e-03 BER=1.76e-03 FER=1.56e-02
2025-10-16 21:55:43,254 | INFO | Epoch 684 Train Time 102.23942494392395s

2025-10-16 21:57:12,231 | INFO | Training epoch 685, Batch 1000/1000: LR=7.41e-05, Loss=4.29e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 21:57:12,274 | INFO | Epoch 685 Train Time 89.01853919029236s

2025-10-16 21:58:45,021 | INFO | Training epoch 686, Batch 1000/1000: LR=7.40e-05, Loss=4.15e-03 BER=1.72e-03 FER=1.49e-02
2025-10-16 21:58:45,082 | INFO | Epoch 686 Train Time 92.80607843399048s

2025-10-16 22:00:16,027 | INFO | Training epoch 687, Batch 1000/1000: LR=7.39e-05, Loss=4.37e-03 BER=1.79e-03 FER=1.56e-02
2025-10-16 22:00:16,073 | INFO | Epoch 687 Train Time 90.98762702941895s

2025-10-16 22:01:52,716 | INFO | Training epoch 688, Batch 1000/1000: LR=7.39e-05, Loss=4.39e-03 BER=1.83e-03 FER=1.61e-02
2025-10-16 22:01:52,764 | INFO | Epoch 688 Train Time 96.69002223014832s

2025-10-16 22:03:27,917 | INFO | Training epoch 689, Batch 1000/1000: LR=7.38e-05, Loss=4.32e-03 BER=1.77e-03 FER=1.57e-02
2025-10-16 22:03:27,964 | INFO | Epoch 689 Train Time 95.1989357471466s

2025-10-16 22:05:12,538 | INFO | Training epoch 690, Batch 1000/1000: LR=7.37e-05, Loss=4.41e-03 BER=1.78e-03 FER=1.57e-02
2025-10-16 22:05:12,588 | INFO | Epoch 690 Train Time 104.62245988845825s

2025-10-16 22:06:50,634 | INFO | Training epoch 691, Batch 1000/1000: LR=7.37e-05, Loss=4.19e-03 BER=1.72e-03 FER=1.53e-02
2025-10-16 22:06:50,681 | INFO | Epoch 691 Train Time 98.09142756462097s

2025-10-16 22:08:25,593 | INFO | Training epoch 692, Batch 1000/1000: LR=7.36e-05, Loss=4.35e-03 BER=1.78e-03 FER=1.57e-02
2025-10-16 22:08:25,639 | INFO | Epoch 692 Train Time 94.9572057723999s

2025-10-16 22:09:56,525 | INFO | Training epoch 693, Batch 1000/1000: LR=7.35e-05, Loss=4.32e-03 BER=1.79e-03 FER=1.59e-02
2025-10-16 22:09:56,580 | INFO | Epoch 693 Train Time 90.93900394439697s

2025-10-16 22:11:37,022 | INFO | Training epoch 694, Batch 1000/1000: LR=7.35e-05, Loss=4.38e-03 BER=1.81e-03 FER=1.61e-02
2025-10-16 22:11:37,079 | INFO | Epoch 694 Train Time 100.4978518486023s

2025-10-16 22:13:15,937 | INFO | Training epoch 695, Batch 1000/1000: LR=7.34e-05, Loss=4.46e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 22:13:16,004 | INFO | Epoch 695 Train Time 98.9215919971466s

2025-10-16 22:14:56,303 | INFO | Training epoch 696, Batch 1000/1000: LR=7.33e-05, Loss=4.36e-03 BER=1.81e-03 FER=1.58e-02
2025-10-16 22:14:56,355 | INFO | Epoch 696 Train Time 100.34859347343445s

2025-10-16 22:16:26,952 | INFO | Training epoch 697, Batch 1000/1000: LR=7.32e-05, Loss=4.13e-03 BER=1.70e-03 FER=1.50e-02
2025-10-16 22:16:26,997 | INFO | Epoch 697 Train Time 90.64119839668274s

2025-10-16 22:16:26,997 | INFO | [P2] saving best_model (QAT) with loss 0.004134 at epoch 697
2025-10-16 22:18:01,894 | INFO | Training epoch 698, Batch 1000/1000: LR=7.32e-05, Loss=4.18e-03 BER=1.76e-03 FER=1.52e-02
2025-10-16 22:18:01,948 | INFO | Epoch 698 Train Time 94.87587261199951s

2025-10-16 22:19:43,658 | INFO | Training epoch 699, Batch 1000/1000: LR=7.31e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.58e-02
2025-10-16 22:19:43,697 | INFO | Epoch 699 Train Time 101.74697041511536s

2025-10-16 22:21:27,932 | INFO | Training epoch 700, Batch 1000/1000: LR=7.30e-05, Loss=4.53e-03 BER=1.86e-03 FER=1.62e-02
2025-10-16 22:21:27,988 | INFO | Epoch 700 Train Time 104.2903163433075s

2025-10-16 22:22:57,442 | INFO | Training epoch 701, Batch 1000/1000: LR=7.30e-05, Loss=4.32e-03 BER=1.74e-03 FER=1.55e-02
2025-10-16 22:22:57,502 | INFO | Epoch 701 Train Time 89.51188826560974s

2025-10-16 22:24:30,394 | INFO | Training epoch 702, Batch 1000/1000: LR=7.29e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.56e-02
2025-10-16 22:24:30,458 | INFO | Epoch 702 Train Time 92.95487999916077s

2025-10-16 22:26:15,187 | INFO | Training epoch 703, Batch 1000/1000: LR=7.28e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.53e-02
2025-10-16 22:26:15,237 | INFO | Epoch 703 Train Time 104.77708077430725s

2025-10-16 22:27:54,319 | INFO | Training epoch 704, Batch 1000/1000: LR=7.28e-05, Loss=4.28e-03 BER=1.76e-03 FER=1.55e-02
2025-10-16 22:27:54,380 | INFO | Epoch 704 Train Time 99.14079570770264s

2025-10-16 22:29:31,087 | INFO | Training epoch 705, Batch 1000/1000: LR=7.27e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.54e-02
2025-10-16 22:29:31,142 | INFO | Epoch 705 Train Time 96.75976777076721s

2025-10-16 22:31:06,406 | INFO | Training epoch 706, Batch 1000/1000: LR=7.26e-05, Loss=4.18e-03 BER=1.70e-03 FER=1.50e-02
2025-10-16 22:31:06,465 | INFO | Epoch 706 Train Time 95.32217955589294s

2025-10-16 22:32:46,017 | INFO | Training epoch 707, Batch 1000/1000: LR=7.26e-05, Loss=4.18e-03 BER=1.70e-03 FER=1.48e-02
2025-10-16 22:32:46,072 | INFO | Epoch 707 Train Time 99.60479927062988s

2025-10-16 22:34:18,529 | INFO | Training epoch 708, Batch 1000/1000: LR=7.25e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.52e-02
2025-10-16 22:34:18,589 | INFO | Epoch 708 Train Time 92.51510810852051s

2025-10-16 22:35:59,563 | INFO | Training epoch 709, Batch 1000/1000: LR=7.24e-05, Loss=4.31e-03 BER=1.76e-03 FER=1.52e-02
2025-10-16 22:35:59,617 | INFO | Epoch 709 Train Time 101.02639245986938s

2025-10-16 22:37:34,323 | INFO | Training epoch 710, Batch 1000/1000: LR=7.23e-05, Loss=4.23e-03 BER=1.73e-03 FER=1.53e-02
2025-10-16 22:37:34,375 | INFO | Epoch 710 Train Time 94.75598788261414s

2025-10-16 22:39:16,469 | INFO | Training epoch 711, Batch 1000/1000: LR=7.23e-05, Loss=4.30e-03 BER=1.73e-03 FER=1.52e-02
2025-10-16 22:39:16,521 | INFO | Epoch 711 Train Time 102.14505624771118s

2025-10-16 22:40:54,834 | INFO | Training epoch 712, Batch 1000/1000: LR=7.22e-05, Loss=4.37e-03 BER=1.80e-03 FER=1.56e-02
2025-10-16 22:40:54,874 | INFO | Epoch 712 Train Time 98.35137557983398s

2025-10-16 22:42:28,177 | INFO | Training epoch 713, Batch 1000/1000: LR=7.21e-05, Loss=4.20e-03 BER=1.73e-03 FER=1.54e-02
2025-10-16 22:42:28,226 | INFO | Epoch 713 Train Time 93.3496470451355s

2025-10-16 22:44:06,879 | INFO | Training epoch 714, Batch 1000/1000: LR=7.21e-05, Loss=4.20e-03 BER=1.73e-03 FER=1.53e-02
2025-10-16 22:44:06,930 | INFO | Epoch 714 Train Time 98.7034375667572s

2025-10-16 22:45:47,931 | INFO | Training epoch 715, Batch 1000/1000: LR=7.20e-05, Loss=4.47e-03 BER=1.85e-03 FER=1.62e-02
2025-10-16 22:45:47,972 | INFO | Epoch 715 Train Time 101.03964614868164s

2025-10-16 22:47:18,499 | INFO | Training epoch 716, Batch 1000/1000: LR=7.19e-05, Loss=4.35e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 22:47:18,552 | INFO | Epoch 716 Train Time 90.57877779006958s

2025-10-16 22:48:57,856 | INFO | Training epoch 717, Batch 1000/1000: LR=7.19e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.57e-02
2025-10-16 22:48:57,900 | INFO | Epoch 717 Train Time 99.34709167480469s

2025-10-16 22:50:34,217 | INFO | Training epoch 718, Batch 1000/1000: LR=7.18e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.61e-02
2025-10-16 22:50:34,269 | INFO | Epoch 718 Train Time 96.36710548400879s

2025-10-16 22:52:09,172 | INFO | Training epoch 719, Batch 1000/1000: LR=7.17e-05, Loss=4.33e-03 BER=1.75e-03 FER=1.57e-02
2025-10-16 22:52:09,214 | INFO | Epoch 719 Train Time 94.94409680366516s

2025-10-16 22:53:44,541 | INFO | Training epoch 720, Batch 1000/1000: LR=7.16e-05, Loss=4.18e-03 BER=1.73e-03 FER=1.53e-02
2025-10-16 22:53:44,600 | INFO | Epoch 720 Train Time 95.38397979736328s

2025-10-16 22:55:25,334 | INFO | Training epoch 721, Batch 1000/1000: LR=7.16e-05, Loss=4.35e-03 BER=1.77e-03 FER=1.59e-02
2025-10-16 22:55:25,393 | INFO | Epoch 721 Train Time 100.79161214828491s

2025-10-16 22:57:06,309 | INFO | Training epoch 722, Batch 1000/1000: LR=7.15e-05, Loss=4.33e-03 BER=1.77e-03 FER=1.57e-02
2025-10-16 22:57:06,351 | INFO | Epoch 722 Train Time 100.95663213729858s

2025-10-16 22:58:51,292 | INFO | Training epoch 723, Batch 1000/1000: LR=7.14e-05, Loss=4.44e-03 BER=1.82e-03 FER=1.60e-02
2025-10-16 22:58:51,365 | INFO | Epoch 723 Train Time 105.01236653327942s

2025-10-16 23:00:23,134 | INFO | Training epoch 724, Batch 1000/1000: LR=7.14e-05, Loss=4.53e-03 BER=1.87e-03 FER=1.60e-02
2025-10-16 23:00:23,177 | INFO | Epoch 724 Train Time 91.80864262580872s

2025-10-16 23:02:10,123 | INFO | Training epoch 725, Batch 1000/1000: LR=7.13e-05, Loss=4.42e-03 BER=1.83e-03 FER=1.63e-02
2025-10-16 23:02:10,178 | INFO | Epoch 725 Train Time 107.00000834465027s

2025-10-16 23:03:52,831 | INFO | Training epoch 726, Batch 1000/1000: LR=7.12e-05, Loss=4.50e-03 BER=1.84e-03 FER=1.62e-02
2025-10-16 23:03:52,880 | INFO | Epoch 726 Train Time 102.70003533363342s

2025-10-16 23:05:26,842 | INFO | Training epoch 727, Batch 1000/1000: LR=7.12e-05, Loss=4.24e-03 BER=1.71e-03 FER=1.51e-02
2025-10-16 23:05:26,884 | INFO | Epoch 727 Train Time 94.00257992744446s

2025-10-16 23:07:11,419 | INFO | Training epoch 728, Batch 1000/1000: LR=7.11e-05, Loss=4.38e-03 BER=1.80e-03 FER=1.56e-02
2025-10-16 23:07:11,470 | INFO | Epoch 728 Train Time 104.58356952667236s

2025-10-16 23:08:50,919 | INFO | Training epoch 729, Batch 1000/1000: LR=7.10e-05, Loss=4.10e-03 BER=1.66e-03 FER=1.47e-02
2025-10-16 23:08:50,962 | INFO | Epoch 729 Train Time 99.49048280715942s

2025-10-16 23:08:50,963 | INFO | [P2] saving best_model (QAT) with loss 0.004100 at epoch 729
2025-10-16 23:10:26,925 | INFO | Training epoch 730, Batch 1000/1000: LR=7.09e-05, Loss=4.23e-03 BER=1.74e-03 FER=1.52e-02
2025-10-16 23:10:26,969 | INFO | Epoch 730 Train Time 95.90658640861511s

2025-10-16 23:11:59,530 | INFO | Training epoch 731, Batch 1000/1000: LR=7.09e-05, Loss=4.48e-03 BER=1.84e-03 FER=1.61e-02
2025-10-16 23:11:59,585 | INFO | Epoch 731 Train Time 92.61414051055908s

2025-10-16 23:13:37,011 | INFO | Training epoch 732, Batch 1000/1000: LR=7.08e-05, Loss=4.27e-03 BER=1.77e-03 FER=1.56e-02
2025-10-16 23:13:37,062 | INFO | Epoch 732 Train Time 97.47537803649902s

2025-10-16 23:15:18,439 | INFO | Training epoch 733, Batch 1000/1000: LR=7.07e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.58e-02
2025-10-16 23:15:18,484 | INFO | Epoch 733 Train Time 101.41998410224915s

2025-10-16 23:17:04,509 | INFO | Training epoch 734, Batch 1000/1000: LR=7.07e-05, Loss=4.22e-03 BER=1.76e-03 FER=1.55e-02
2025-10-16 23:17:04,570 | INFO | Epoch 734 Train Time 106.0844829082489s

2025-10-16 23:18:36,372 | INFO | Training epoch 735, Batch 1000/1000: LR=7.06e-05, Loss=4.35e-03 BER=1.75e-03 FER=1.55e-02
2025-10-16 23:18:36,423 | INFO | Epoch 735 Train Time 91.85181713104248s

2025-10-16 23:20:11,548 | INFO | Training epoch 736, Batch 1000/1000: LR=7.05e-05, Loss=4.42e-03 BER=1.82e-03 FER=1.62e-02
2025-10-16 23:20:11,597 | INFO | Epoch 736 Train Time 95.1723735332489s

2025-10-16 23:21:44,554 | INFO | Training epoch 737, Batch 1000/1000: LR=7.04e-05, Loss=4.34e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 23:21:44,631 | INFO | Epoch 737 Train Time 93.03365659713745s

2025-10-16 23:23:22,334 | INFO | Training epoch 738, Batch 1000/1000: LR=7.04e-05, Loss=4.24e-03 BER=1.76e-03 FER=1.54e-02
2025-10-16 23:23:22,383 | INFO | Epoch 738 Train Time 97.75071883201599s

2025-10-16 23:25:01,766 | INFO | Training epoch 739, Batch 1000/1000: LR=7.03e-05, Loss=4.35e-03 BER=1.79e-03 FER=1.56e-02
2025-10-16 23:25:01,816 | INFO | Epoch 739 Train Time 99.43205404281616s

2025-10-16 23:26:43,224 | INFO | Training epoch 740, Batch 1000/1000: LR=7.02e-05, Loss=4.20e-03 BER=1.75e-03 FER=1.53e-02
2025-10-16 23:26:43,275 | INFO | Epoch 740 Train Time 101.4575502872467s

2025-10-16 23:28:22,177 | INFO | Training epoch 741, Batch 1000/1000: LR=7.02e-05, Loss=4.26e-03 BER=1.74e-03 FER=1.53e-02
2025-10-16 23:28:22,216 | INFO | Epoch 741 Train Time 98.93999719619751s

2025-10-16 23:30:00,402 | INFO | Training epoch 742, Batch 1000/1000: LR=7.01e-05, Loss=4.27e-03 BER=1.78e-03 FER=1.56e-02
2025-10-16 23:30:00,478 | INFO | Epoch 742 Train Time 98.26035952568054s

2025-10-16 23:31:47,406 | INFO | Training epoch 743, Batch 1000/1000: LR=7.00e-05, Loss=4.32e-03 BER=1.77e-03 FER=1.55e-02
2025-10-16 23:31:47,457 | INFO | Epoch 743 Train Time 106.97649025917053s

2025-10-16 23:33:22,011 | INFO | Training epoch 744, Batch 1000/1000: LR=6.99e-05, Loss=4.38e-03 BER=1.80e-03 FER=1.58e-02
2025-10-16 23:33:22,060 | INFO | Epoch 744 Train Time 94.60132193565369s

2025-10-16 23:35:07,981 | INFO | Training epoch 745, Batch 1000/1000: LR=6.99e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.55e-02
2025-10-16 23:35:08,034 | INFO | Epoch 745 Train Time 105.97204303741455s

2025-10-16 23:36:43,706 | INFO | Training epoch 746, Batch 1000/1000: LR=6.98e-05, Loss=4.35e-03 BER=1.77e-03 FER=1.56e-02
2025-10-16 23:36:43,767 | INFO | Epoch 746 Train Time 95.7320499420166s

2025-10-16 23:38:23,451 | INFO | Training epoch 747, Batch 1000/1000: LR=6.97e-05, Loss=4.19e-03 BER=1.70e-03 FER=1.51e-02
2025-10-16 23:38:23,496 | INFO | Epoch 747 Train Time 99.7280957698822s

2025-10-16 23:39:56,029 | INFO | Training epoch 748, Batch 1000/1000: LR=6.97e-05, Loss=4.38e-03 BER=1.81e-03 FER=1.60e-02
2025-10-16 23:39:56,092 | INFO | Epoch 748 Train Time 92.5944652557373s

2025-10-16 23:41:33,023 | INFO | Training epoch 749, Batch 1000/1000: LR=6.96e-05, Loss=4.36e-03 BER=1.79e-03 FER=1.56e-02
2025-10-16 23:41:33,082 | INFO | Epoch 749 Train Time 96.98919200897217s

2025-10-16 23:43:04,050 | INFO | Training epoch 750, Batch 1000/1000: LR=6.95e-05, Loss=4.26e-03 BER=1.77e-03 FER=1.53e-02
2025-10-16 23:43:04,093 | INFO | Epoch 750 Train Time 91.00900030136108s

2025-10-16 23:44:38,649 | INFO | Training epoch 751, Batch 1000/1000: LR=6.94e-05, Loss=4.18e-03 BER=1.70e-03 FER=1.52e-02
2025-10-16 23:44:38,700 | INFO | Epoch 751 Train Time 94.60542511940002s

2025-10-16 23:46:11,468 | INFO | Training epoch 752, Batch 1000/1000: LR=6.94e-05, Loss=4.25e-03 BER=1.76e-03 FER=1.54e-02
2025-10-16 23:46:11,511 | INFO | Epoch 752 Train Time 92.8093991279602s

2025-10-16 23:47:52,032 | INFO | Training epoch 753, Batch 1000/1000: LR=6.93e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.52e-02
2025-10-16 23:47:52,082 | INFO | Epoch 753 Train Time 100.56985592842102s

2025-10-16 23:49:31,701 | INFO | Training epoch 754, Batch 1000/1000: LR=6.92e-05, Loss=4.41e-03 BER=1.81e-03 FER=1.59e-02
2025-10-16 23:49:31,758 | INFO | Epoch 754 Train Time 99.67455506324768s

2025-10-16 23:51:14,325 | INFO | Training epoch 755, Batch 1000/1000: LR=6.92e-05, Loss=4.35e-03 BER=1.77e-03 FER=1.55e-02
2025-10-16 23:51:14,377 | INFO | Epoch 755 Train Time 102.61593914031982s

2025-10-16 23:52:45,429 | INFO | Training epoch 756, Batch 1000/1000: LR=6.91e-05, Loss=4.21e-03 BER=1.71e-03 FER=1.51e-02
2025-10-16 23:52:45,481 | INFO | Epoch 756 Train Time 91.10215187072754s

2025-10-16 23:54:16,205 | INFO | Training epoch 757, Batch 1000/1000: LR=6.90e-05, Loss=4.31e-03 BER=1.76e-03 FER=1.54e-02
2025-10-16 23:54:16,260 | INFO | Epoch 757 Train Time 90.777752161026s

2025-10-16 23:56:00,130 | INFO | Training epoch 758, Batch 1000/1000: LR=6.89e-05, Loss=4.36e-03 BER=1.80e-03 FER=1.56e-02
2025-10-16 23:56:00,189 | INFO | Epoch 758 Train Time 103.92699933052063s

2025-10-16 23:57:31,650 | INFO | Training epoch 759, Batch 1000/1000: LR=6.89e-05, Loss=4.27e-03 BER=1.77e-03 FER=1.53e-02
2025-10-16 23:57:31,710 | INFO | Epoch 759 Train Time 91.51893544197083s

2025-10-16 23:58:58,692 | INFO | Training epoch 760, Batch 1000/1000: LR=6.88e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.52e-02
2025-10-16 23:58:58,745 | INFO | Epoch 760 Train Time 87.03441786766052s

2025-10-17 00:00:31,650 | INFO | Training epoch 761, Batch 1000/1000: LR=6.87e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.56e-02
2025-10-17 00:00:31,695 | INFO | Epoch 761 Train Time 92.94788646697998s

2025-10-17 00:02:02,851 | INFO | Training epoch 762, Batch 1000/1000: LR=6.86e-05, Loss=4.27e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 00:02:02,905 | INFO | Epoch 762 Train Time 91.20876884460449s

2025-10-17 00:03:41,818 | INFO | Training epoch 763, Batch 1000/1000: LR=6.86e-05, Loss=4.56e-03 BER=1.86e-03 FER=1.62e-02
2025-10-17 00:03:41,882 | INFO | Epoch 763 Train Time 98.97574877738953s

2025-10-17 00:05:18,778 | INFO | Training epoch 764, Batch 1000/1000: LR=6.85e-05, Loss=4.40e-03 BER=1.83e-03 FER=1.59e-02
2025-10-17 00:05:18,818 | INFO | Epoch 764 Train Time 96.9346911907196s

2025-10-17 00:06:49,246 | INFO | Training epoch 765, Batch 1000/1000: LR=6.84e-05, Loss=4.26e-03 BER=1.78e-03 FER=1.54e-02
2025-10-17 00:06:49,288 | INFO | Epoch 765 Train Time 90.468994140625s

2025-10-17 00:08:29,678 | INFO | Training epoch 766, Batch 1000/1000: LR=6.84e-05, Loss=4.27e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 00:08:29,734 | INFO | Epoch 766 Train Time 100.4455304145813s

2025-10-17 00:09:57,150 | INFO | Training epoch 767, Batch 1000/1000: LR=6.83e-05, Loss=4.40e-03 BER=1.81e-03 FER=1.59e-02
2025-10-17 00:09:57,201 | INFO | Epoch 767 Train Time 87.46558260917664s

2025-10-17 00:11:36,559 | INFO | Training epoch 768, Batch 1000/1000: LR=6.82e-05, Loss=4.17e-03 BER=1.69e-03 FER=1.50e-02
2025-10-17 00:11:36,630 | INFO | Epoch 768 Train Time 99.42723965644836s

2025-10-17 00:13:12,625 | INFO | Training epoch 769, Batch 1000/1000: LR=6.81e-05, Loss=4.26e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 00:13:12,674 | INFO | Epoch 769 Train Time 96.04272556304932s

2025-10-17 00:14:54,332 | INFO | Training epoch 770, Batch 1000/1000: LR=6.81e-05, Loss=4.29e-03 BER=1.74e-03 FER=1.53e-02
2025-10-17 00:14:54,379 | INFO | Epoch 770 Train Time 101.70360994338989s

2025-10-17 00:16:32,745 | INFO | Training epoch 771, Batch 1000/1000: LR=6.80e-05, Loss=4.11e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 00:16:32,787 | INFO | Epoch 771 Train Time 98.407386302948s

2025-10-17 00:18:04,934 | INFO | Training epoch 772, Batch 1000/1000: LR=6.79e-05, Loss=4.27e-03 BER=1.77e-03 FER=1.57e-02
2025-10-17 00:18:04,985 | INFO | Epoch 772 Train Time 92.19659757614136s

2025-10-17 00:19:47,264 | INFO | Training epoch 773, Batch 1000/1000: LR=6.79e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.53e-02
2025-10-17 00:19:47,314 | INFO | Epoch 773 Train Time 102.32776141166687s

2025-10-17 00:21:24,153 | INFO | Training epoch 774, Batch 1000/1000: LR=6.78e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.60e-02
2025-10-17 00:21:24,196 | INFO | Epoch 774 Train Time 96.88118314743042s

2025-10-17 00:23:00,642 | INFO | Training epoch 775, Batch 1000/1000: LR=6.77e-05, Loss=4.28e-03 BER=1.74e-03 FER=1.56e-02
2025-10-17 00:23:00,702 | INFO | Epoch 775 Train Time 96.50519371032715s

2025-10-17 00:24:33,621 | INFO | Training epoch 776, Batch 1000/1000: LR=6.76e-05, Loss=4.18e-03 BER=1.71e-03 FER=1.49e-02
2025-10-17 00:24:33,678 | INFO | Epoch 776 Train Time 92.9730179309845s

2025-10-17 00:26:13,916 | INFO | Training epoch 777, Batch 1000/1000: LR=6.76e-05, Loss=4.28e-03 BER=1.76e-03 FER=1.54e-02
2025-10-17 00:26:13,974 | INFO | Epoch 777 Train Time 100.29511666297913s

2025-10-17 00:27:54,500 | INFO | Training epoch 778, Batch 1000/1000: LR=6.75e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 00:27:54,560 | INFO | Epoch 778 Train Time 100.5841805934906s

2025-10-17 00:29:31,004 | INFO | Training epoch 779, Batch 1000/1000: LR=6.74e-05, Loss=4.30e-03 BER=1.78e-03 FER=1.55e-02
2025-10-17 00:29:31,054 | INFO | Epoch 779 Train Time 96.49241590499878s

2025-10-17 00:31:06,340 | INFO | Training epoch 780, Batch 1000/1000: LR=6.73e-05, Loss=4.27e-03 BER=1.76e-03 FER=1.56e-02
2025-10-17 00:31:06,380 | INFO | Epoch 780 Train Time 95.3238730430603s

2025-10-17 00:32:51,550 | INFO | Training epoch 781, Batch 1000/1000: LR=6.73e-05, Loss=4.38e-03 BER=1.77e-03 FER=1.58e-02
2025-10-17 00:32:51,603 | INFO | Epoch 781 Train Time 105.22097587585449s

2025-10-17 00:34:32,027 | INFO | Training epoch 782, Batch 1000/1000: LR=6.72e-05, Loss=4.34e-03 BER=1.78e-03 FER=1.55e-02
2025-10-17 00:34:32,102 | INFO | Epoch 782 Train Time 100.49706935882568s

2025-10-17 00:36:03,677 | INFO | Training epoch 783, Batch 1000/1000: LR=6.71e-05, Loss=4.31e-03 BER=1.78e-03 FER=1.53e-02
2025-10-17 00:36:03,722 | INFO | Epoch 783 Train Time 91.61852884292603s

2025-10-17 00:37:45,281 | INFO | Training epoch 784, Batch 1000/1000: LR=6.70e-05, Loss=4.14e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 00:37:45,348 | INFO | Epoch 784 Train Time 101.6256411075592s

2025-10-17 00:39:24,047 | INFO | Training epoch 785, Batch 1000/1000: LR=6.70e-05, Loss=4.22e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 00:39:24,101 | INFO | Epoch 785 Train Time 98.75059843063354s

2025-10-17 00:40:59,662 | INFO | Training epoch 786, Batch 1000/1000: LR=6.69e-05, Loss=4.32e-03 BER=1.76e-03 FER=1.55e-02
2025-10-17 00:40:59,722 | INFO | Epoch 786 Train Time 95.6206922531128s

2025-10-17 00:42:28,269 | INFO | Training epoch 787, Batch 1000/1000: LR=6.68e-05, Loss=4.22e-03 BER=1.70e-03 FER=1.51e-02
2025-10-17 00:42:28,323 | INFO | Epoch 787 Train Time 88.598317861557s

2025-10-17 00:44:04,096 | INFO | Training epoch 788, Batch 1000/1000: LR=6.68e-05, Loss=4.21e-03 BER=1.74e-03 FER=1.53e-02
2025-10-17 00:44:04,154 | INFO | Epoch 788 Train Time 95.8305413722992s

2025-10-17 00:45:44,226 | INFO | Training epoch 789, Batch 1000/1000: LR=6.67e-05, Loss=4.25e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 00:45:44,266 | INFO | Epoch 789 Train Time 100.11034417152405s

2025-10-17 00:47:23,397 | INFO | Training epoch 790, Batch 1000/1000: LR=6.66e-05, Loss=4.35e-03 BER=1.77e-03 FER=1.55e-02
2025-10-17 00:47:23,468 | INFO | Epoch 790 Train Time 99.20016169548035s

2025-10-17 00:48:53,117 | INFO | Training epoch 791, Batch 1000/1000: LR=6.65e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.51e-02
2025-10-17 00:48:53,162 | INFO | Epoch 791 Train Time 89.69187784194946s

2025-10-17 00:50:35,363 | INFO | Training epoch 792, Batch 1000/1000: LR=6.65e-05, Loss=4.12e-03 BER=1.71e-03 FER=1.52e-02
2025-10-17 00:50:35,406 | INFO | Epoch 792 Train Time 102.24322080612183s

2025-10-17 00:52:05,351 | INFO | Training epoch 793, Batch 1000/1000: LR=6.64e-05, Loss=4.15e-03 BER=1.72e-03 FER=1.50e-02
2025-10-17 00:52:05,404 | INFO | Epoch 793 Train Time 89.99718761444092s

2025-10-17 00:53:38,669 | INFO | Training epoch 794, Batch 1000/1000: LR=6.63e-05, Loss=4.23e-03 BER=1.75e-03 FER=1.52e-02
2025-10-17 00:53:38,709 | INFO | Epoch 794 Train Time 93.3036572933197s

2025-10-17 00:55:15,644 | INFO | Training epoch 795, Batch 1000/1000: LR=6.62e-05, Loss=4.24e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 00:55:15,694 | INFO | Epoch 795 Train Time 96.98373341560364s

2025-10-17 00:56:53,538 | INFO | Training epoch 796, Batch 1000/1000: LR=6.62e-05, Loss=3.98e-03 BER=1.65e-03 FER=1.48e-02
2025-10-17 00:56:53,595 | INFO | Epoch 796 Train Time 97.89999461174011s

2025-10-17 00:56:53,596 | INFO | [P2] saving best_model (QAT) with loss 0.003976 at epoch 796
2025-10-17 00:58:31,159 | INFO | Training epoch 797, Batch 1000/1000: LR=6.61e-05, Loss=4.21e-03 BER=1.73e-03 FER=1.53e-02
2025-10-17 00:58:31,213 | INFO | Epoch 797 Train Time 97.54513907432556s

2025-10-17 01:00:13,021 | INFO | Training epoch 798, Batch 1000/1000: LR=6.60e-05, Loss=4.26e-03 BER=1.77e-03 FER=1.55e-02
2025-10-17 01:00:13,067 | INFO | Epoch 798 Train Time 101.85198545455933s

2025-10-17 01:01:54,111 | INFO | Training epoch 799, Batch 1000/1000: LR=6.59e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.55e-02
2025-10-17 01:01:54,163 | INFO | Epoch 799 Train Time 101.09434509277344s

2025-10-17 01:03:23,754 | INFO | Training epoch 800, Batch 1000/1000: LR=6.59e-05, Loss=4.26e-03 BER=1.73e-03 FER=1.53e-02
2025-10-17 01:03:23,814 | INFO | Epoch 800 Train Time 89.64962458610535s

2025-10-17 01:05:03,060 | INFO | Training epoch 801, Batch 1000/1000: LR=6.58e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.53e-02
2025-10-17 01:05:03,107 | INFO | Epoch 801 Train Time 99.29225850105286s

2025-10-17 01:06:37,786 | INFO | Training epoch 802, Batch 1000/1000: LR=6.57e-05, Loss=4.16e-03 BER=1.70e-03 FER=1.52e-02
2025-10-17 01:06:37,846 | INFO | Epoch 802 Train Time 94.73809766769409s

2025-10-17 01:08:11,437 | INFO | Training epoch 803, Batch 1000/1000: LR=6.56e-05, Loss=4.28e-03 BER=1.74e-03 FER=1.54e-02
2025-10-17 01:08:11,481 | INFO | Epoch 803 Train Time 93.63371348381042s

2025-10-17 01:09:55,095 | INFO | Training epoch 804, Batch 1000/1000: LR=6.56e-05, Loss=4.42e-03 BER=1.80e-03 FER=1.56e-02
2025-10-17 01:09:55,164 | INFO | Epoch 804 Train Time 103.68127155303955s

2025-10-17 01:11:29,860 | INFO | Training epoch 805, Batch 1000/1000: LR=6.55e-05, Loss=4.09e-03 BER=1.67e-03 FER=1.48e-02
2025-10-17 01:11:29,918 | INFO | Epoch 805 Train Time 94.75238871574402s

2025-10-17 01:13:01,646 | INFO | Training epoch 806, Batch 1000/1000: LR=6.54e-05, Loss=4.41e-03 BER=1.80e-03 FER=1.57e-02
2025-10-17 01:13:01,695 | INFO | Epoch 806 Train Time 91.77439999580383s

2025-10-17 01:14:42,877 | INFO | Training epoch 807, Batch 1000/1000: LR=6.54e-05, Loss=4.22e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 01:14:42,932 | INFO | Epoch 807 Train Time 101.23260736465454s

2025-10-17 01:16:11,472 | INFO | Training epoch 808, Batch 1000/1000: LR=6.53e-05, Loss=4.20e-03 BER=1.71e-03 FER=1.50e-02
2025-10-17 01:16:11,527 | INFO | Epoch 808 Train Time 88.59281802177429s

2025-10-17 01:17:42,308 | INFO | Training epoch 809, Batch 1000/1000: LR=6.52e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.54e-02
2025-10-17 01:17:42,364 | INFO | Epoch 809 Train Time 90.83611154556274s

2025-10-17 01:19:30,540 | INFO | Training epoch 810, Batch 1000/1000: LR=6.51e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.53e-02
2025-10-17 01:19:30,594 | INFO | Epoch 810 Train Time 108.22680425643921s

2025-10-17 01:21:09,411 | INFO | Training epoch 811, Batch 1000/1000: LR=6.51e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 01:21:09,467 | INFO | Epoch 811 Train Time 98.87090826034546s

2025-10-17 01:22:47,741 | INFO | Training epoch 812, Batch 1000/1000: LR=6.50e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.49e-02
2025-10-17 01:22:47,781 | INFO | Epoch 812 Train Time 98.31224012374878s

2025-10-17 01:24:30,201 | INFO | Training epoch 813, Batch 1000/1000: LR=6.49e-05, Loss=4.29e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 01:24:30,259 | INFO | Epoch 813 Train Time 102.47710990905762s

2025-10-17 01:26:10,039 | INFO | Training epoch 814, Batch 1000/1000: LR=6.48e-05, Loss=4.48e-03 BER=1.83e-03 FER=1.62e-02
2025-10-17 01:26:10,090 | INFO | Epoch 814 Train Time 99.829261302948s

2025-10-17 01:27:48,870 | INFO | Training epoch 815, Batch 1000/1000: LR=6.48e-05, Loss=4.18e-03 BER=1.71e-03 FER=1.53e-02
2025-10-17 01:27:48,937 | INFO | Epoch 815 Train Time 98.8454942703247s

2025-10-17 01:29:32,062 | INFO | Training epoch 816, Batch 1000/1000: LR=6.47e-05, Loss=4.20e-03 BER=1.70e-03 FER=1.51e-02
2025-10-17 01:29:32,106 | INFO | Epoch 816 Train Time 103.1675672531128s

2025-10-17 01:31:00,213 | INFO | Training epoch 817, Batch 1000/1000: LR=6.46e-05, Loss=4.48e-03 BER=1.80e-03 FER=1.58e-02
2025-10-17 01:31:00,257 | INFO | Epoch 817 Train Time 88.14995980262756s

2025-10-17 01:32:32,640 | INFO | Training epoch 818, Batch 1000/1000: LR=6.45e-05, Loss=4.23e-03 BER=1.71e-03 FER=1.53e-02
2025-10-17 01:32:32,689 | INFO | Epoch 818 Train Time 92.43023228645325s

2025-10-17 01:34:06,299 | INFO | Training epoch 819, Batch 1000/1000: LR=6.45e-05, Loss=4.35e-03 BER=1.78e-03 FER=1.55e-02
2025-10-17 01:34:06,350 | INFO | Epoch 819 Train Time 93.65925025939941s

2025-10-17 01:35:52,145 | INFO | Training epoch 820, Batch 1000/1000: LR=6.44e-05, Loss=4.17e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 01:35:52,197 | INFO | Epoch 820 Train Time 105.8442862033844s

2025-10-17 01:37:30,905 | INFO | Training epoch 821, Batch 1000/1000: LR=6.43e-05, Loss=4.18e-03 BER=1.68e-03 FER=1.51e-02
2025-10-17 01:37:30,960 | INFO | Epoch 821 Train Time 98.76225423812866s

2025-10-17 01:39:11,067 | INFO | Training epoch 822, Batch 1000/1000: LR=6.42e-05, Loss=4.35e-03 BER=1.78e-03 FER=1.57e-02
2025-10-17 01:39:11,125 | INFO | Epoch 822 Train Time 100.16113448143005s

2025-10-17 01:40:44,773 | INFO | Training epoch 823, Batch 1000/1000: LR=6.42e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.47e-02
2025-10-17 01:40:44,820 | INFO | Epoch 823 Train Time 93.69272184371948s

2025-10-17 01:42:20,930 | INFO | Training epoch 824, Batch 1000/1000: LR=6.41e-05, Loss=4.36e-03 BER=1.77e-03 FER=1.57e-02
2025-10-17 01:42:20,975 | INFO | Epoch 824 Train Time 96.15357160568237s

2025-10-17 01:43:55,164 | INFO | Training epoch 825, Batch 1000/1000: LR=6.40e-05, Loss=4.17e-03 BER=1.75e-03 FER=1.52e-02
2025-10-17 01:43:55,222 | INFO | Epoch 825 Train Time 94.2460606098175s

2025-10-17 01:45:33,599 | INFO | Training epoch 826, Batch 1000/1000: LR=6.39e-05, Loss=4.41e-03 BER=1.79e-03 FER=1.56e-02
2025-10-17 01:45:33,650 | INFO | Epoch 826 Train Time 98.42579579353333s

2025-10-17 01:47:13,424 | INFO | Training epoch 827, Batch 1000/1000: LR=6.39e-05, Loss=4.34e-03 BER=1.75e-03 FER=1.56e-02
2025-10-17 01:47:13,491 | INFO | Epoch 827 Train Time 99.83970975875854s

2025-10-17 01:48:47,315 | INFO | Training epoch 828, Batch 1000/1000: LR=6.38e-05, Loss=4.10e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 01:48:47,386 | INFO | Epoch 828 Train Time 93.89302039146423s

2025-10-17 01:50:29,163 | INFO | Training epoch 829, Batch 1000/1000: LR=6.37e-05, Loss=4.11e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 01:50:29,203 | INFO | Epoch 829 Train Time 101.81631088256836s

2025-10-17 01:52:04,794 | INFO | Training epoch 830, Batch 1000/1000: LR=6.36e-05, Loss=4.27e-03 BER=1.77e-03 FER=1.54e-02
2025-10-17 01:52:04,847 | INFO | Epoch 830 Train Time 95.64290046691895s

2025-10-17 01:53:42,200 | INFO | Training epoch 831, Batch 1000/1000: LR=6.36e-05, Loss=4.40e-03 BER=1.80e-03 FER=1.57e-02
2025-10-17 01:53:42,251 | INFO | Epoch 831 Train Time 97.40150952339172s

2025-10-17 01:55:22,758 | INFO | Training epoch 832, Batch 1000/1000: LR=6.35e-05, Loss=4.22e-03 BER=1.72e-03 FER=1.52e-02
2025-10-17 01:55:22,805 | INFO | Epoch 832 Train Time 100.55272102355957s

2025-10-17 01:56:57,423 | INFO | Training epoch 833, Batch 1000/1000: LR=6.34e-05, Loss=4.40e-03 BER=1.79e-03 FER=1.56e-02
2025-10-17 01:56:57,467 | INFO | Epoch 833 Train Time 94.66111397743225s

2025-10-17 01:58:37,932 | INFO | Training epoch 834, Batch 1000/1000: LR=6.33e-05, Loss=4.35e-03 BER=1.77e-03 FER=1.55e-02
2025-10-17 01:58:37,994 | INFO | Epoch 834 Train Time 100.52530932426453s

2025-10-17 02:00:17,407 | INFO | Training epoch 835, Batch 1000/1000: LR=6.33e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.49e-02
2025-10-17 02:00:17,469 | INFO | Epoch 835 Train Time 99.47385215759277s

2025-10-17 02:01:48,227 | INFO | Training epoch 836, Batch 1000/1000: LR=6.32e-05, Loss=4.18e-03 BER=1.72e-03 FER=1.53e-02
2025-10-17 02:01:48,273 | INFO | Epoch 836 Train Time 90.80204057693481s

2025-10-17 02:03:27,724 | INFO | Training epoch 837, Batch 1000/1000: LR=6.31e-05, Loss=4.07e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 02:03:27,782 | INFO | Epoch 837 Train Time 99.50823330879211s

2025-10-17 02:05:11,827 | INFO | Training epoch 838, Batch 1000/1000: LR=6.30e-05, Loss=4.05e-03 BER=1.65e-03 FER=1.46e-02
2025-10-17 02:05:11,873 | INFO | Epoch 838 Train Time 104.088534116745s

2025-10-17 02:06:53,135 | INFO | Training epoch 839, Batch 1000/1000: LR=6.30e-05, Loss=3.91e-03 BER=1.58e-03 FER=1.39e-02
2025-10-17 02:06:53,185 | INFO | Epoch 839 Train Time 101.3112051486969s

2025-10-17 02:06:53,186 | INFO | [P2] saving best_model (QAT) with loss 0.003908 at epoch 839
2025-10-17 02:08:33,832 | INFO | Training epoch 840, Batch 1000/1000: LR=6.29e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.54e-02
2025-10-17 02:08:33,882 | INFO | Epoch 840 Train Time 100.59574723243713s

2025-10-17 02:10:09,363 | INFO | Training epoch 841, Batch 1000/1000: LR=6.28e-05, Loss=4.04e-03 BER=1.64e-03 FER=1.45e-02
2025-10-17 02:10:09,414 | INFO | Epoch 841 Train Time 95.53095412254333s

2025-10-17 02:11:45,943 | INFO | Training epoch 842, Batch 1000/1000: LR=6.27e-05, Loss=4.30e-03 BER=1.76e-03 FER=1.55e-02
2025-10-17 02:11:45,991 | INFO | Epoch 842 Train Time 96.57542896270752s

2025-10-17 02:13:14,504 | INFO | Training epoch 843, Batch 1000/1000: LR=6.27e-05, Loss=4.32e-03 BER=1.77e-03 FER=1.56e-02
2025-10-17 02:13:14,548 | INFO | Epoch 843 Train Time 88.55555129051208s

2025-10-17 02:14:53,168 | INFO | Training epoch 844, Batch 1000/1000: LR=6.26e-05, Loss=4.20e-03 BER=1.71e-03 FER=1.51e-02
2025-10-17 02:14:53,222 | INFO | Epoch 844 Train Time 98.67107486724854s

2025-10-17 02:16:31,436 | INFO | Training epoch 845, Batch 1000/1000: LR=6.25e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.47e-02
2025-10-17 02:16:31,478 | INFO | Epoch 845 Train Time 98.25408744812012s

2025-10-17 02:18:08,540 | INFO | Training epoch 846, Batch 1000/1000: LR=6.24e-05, Loss=4.21e-03 BER=1.75e-03 FER=1.52e-02
2025-10-17 02:18:08,592 | INFO | Epoch 846 Train Time 97.11361694335938s

2025-10-17 02:19:56,520 | INFO | Training epoch 847, Batch 1000/1000: LR=6.24e-05, Loss=4.28e-03 BER=1.78e-03 FER=1.54e-02
2025-10-17 02:19:56,574 | INFO | Epoch 847 Train Time 107.97968983650208s

2025-10-17 02:21:38,537 | INFO | Training epoch 848, Batch 1000/1000: LR=6.23e-05, Loss=4.12e-03 BER=1.68e-03 FER=1.49e-02
2025-10-17 02:21:38,587 | INFO | Epoch 848 Train Time 102.01160144805908s

2025-10-17 02:23:19,404 | INFO | Training epoch 849, Batch 1000/1000: LR=6.22e-05, Loss=3.89e-03 BER=1.61e-03 FER=1.40e-02
2025-10-17 02:23:19,461 | INFO | Epoch 849 Train Time 100.87195634841919s

2025-10-17 02:23:19,463 | INFO | [P2] saving best_model (QAT) with loss 0.003886 at epoch 849
2025-10-17 02:24:56,309 | INFO | Training epoch 850, Batch 1000/1000: LR=6.21e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.51e-02
2025-10-17 02:24:56,361 | INFO | Epoch 850 Train Time 96.79762268066406s

2025-10-17 02:26:35,543 | INFO | Training epoch 851, Batch 1000/1000: LR=6.21e-05, Loss=4.11e-03 BER=1.69e-03 FER=1.49e-02
2025-10-17 02:26:35,594 | INFO | Epoch 851 Train Time 99.23158478736877s

2025-10-17 02:28:26,099 | INFO | Training epoch 852, Batch 1000/1000: LR=6.20e-05, Loss=4.29e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 02:28:26,155 | INFO | Epoch 852 Train Time 110.55963182449341s

2025-10-17 02:30:04,313 | INFO | Training epoch 853, Batch 1000/1000: LR=6.19e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.49e-02
2025-10-17 02:30:04,375 | INFO | Epoch 853 Train Time 98.21772980690002s

2025-10-17 02:31:50,385 | INFO | Training epoch 854, Batch 1000/1000: LR=6.18e-05, Loss=4.23e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 02:31:50,432 | INFO | Epoch 854 Train Time 106.05635023117065s

2025-10-17 02:33:22,602 | INFO | Training epoch 855, Batch 1000/1000: LR=6.18e-05, Loss=4.21e-03 BER=1.72e-03 FER=1.50e-02
2025-10-17 02:33:22,662 | INFO | Epoch 855 Train Time 92.22914814949036s

2025-10-17 02:34:55,928 | INFO | Training epoch 856, Batch 1000/1000: LR=6.17e-05, Loss=4.05e-03 BER=1.67e-03 FER=1.46e-02
2025-10-17 02:34:55,991 | INFO | Epoch 856 Train Time 93.32685995101929s

2025-10-17 02:36:27,943 | INFO | Training epoch 857, Batch 1000/1000: LR=6.16e-05, Loss=4.34e-03 BER=1.77e-03 FER=1.54e-02
2025-10-17 02:36:27,992 | INFO | Epoch 857 Train Time 91.99997782707214s

2025-10-17 02:38:07,517 | INFO | Training epoch 858, Batch 1000/1000: LR=6.15e-05, Loss=4.21e-03 BER=1.71e-03 FER=1.51e-02
2025-10-17 02:38:07,566 | INFO | Epoch 858 Train Time 99.57253813743591s

2025-10-17 02:39:42,734 | INFO | Training epoch 859, Batch 1000/1000: LR=6.14e-05, Loss=4.24e-03 BER=1.75e-03 FER=1.51e-02
2025-10-17 02:39:42,788 | INFO | Epoch 859 Train Time 95.22103714942932s

2025-10-17 02:41:21,811 | INFO | Training epoch 860, Batch 1000/1000: LR=6.14e-05, Loss=4.08e-03 BER=1.69e-03 FER=1.47e-02
2025-10-17 02:41:21,854 | INFO | Epoch 860 Train Time 99.06504654884338s

2025-10-17 02:42:55,035 | INFO | Training epoch 861, Batch 1000/1000: LR=6.13e-05, Loss=4.11e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 02:42:55,084 | INFO | Epoch 861 Train Time 93.22832441329956s

2025-10-17 02:44:31,425 | INFO | Training epoch 862, Batch 1000/1000: LR=6.12e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.54e-02
2025-10-17 02:44:31,479 | INFO | Epoch 862 Train Time 96.39385104179382s

2025-10-17 02:46:07,332 | INFO | Training epoch 863, Batch 1000/1000: LR=6.11e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.51e-02
2025-10-17 02:46:07,401 | INFO | Epoch 863 Train Time 95.9199640750885s

2025-10-17 02:47:42,442 | INFO | Training epoch 864, Batch 1000/1000: LR=6.11e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.54e-02
2025-10-17 02:47:42,490 | INFO | Epoch 864 Train Time 95.0884301662445s

2025-10-17 02:49:23,699 | INFO | Training epoch 865, Batch 1000/1000: LR=6.10e-05, Loss=4.29e-03 BER=1.74e-03 FER=1.53e-02
2025-10-17 02:49:23,748 | INFO | Epoch 865 Train Time 101.25541162490845s

2025-10-17 02:51:00,412 | INFO | Training epoch 866, Batch 1000/1000: LR=6.09e-05, Loss=4.13e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 02:51:00,485 | INFO | Epoch 866 Train Time 96.73485016822815s

2025-10-17 02:52:40,270 | INFO | Training epoch 867, Batch 1000/1000: LR=6.08e-05, Loss=4.28e-03 BER=1.75e-03 FER=1.54e-02
2025-10-17 02:52:40,331 | INFO | Epoch 867 Train Time 99.84407687187195s

2025-10-17 02:54:20,925 | INFO | Training epoch 868, Batch 1000/1000: LR=6.08e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.49e-02
2025-10-17 02:54:20,974 | INFO | Epoch 868 Train Time 100.64188194274902s

2025-10-17 02:55:49,555 | INFO | Training epoch 869, Batch 1000/1000: LR=6.07e-05, Loss=4.09e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 02:55:49,610 | INFO | Epoch 869 Train Time 88.63386917114258s

2025-10-17 02:57:23,823 | INFO | Training epoch 870, Batch 1000/1000: LR=6.06e-05, Loss=4.31e-03 BER=1.77e-03 FER=1.53e-02
2025-10-17 02:57:23,875 | INFO | Epoch 870 Train Time 94.2637848854065s

2025-10-17 02:59:00,330 | INFO | Training epoch 871, Batch 1000/1000: LR=6.05e-05, Loss=4.23e-03 BER=1.75e-03 FER=1.52e-02
2025-10-17 02:59:00,372 | INFO | Epoch 871 Train Time 96.49583888053894s

2025-10-17 03:00:36,039 | INFO | Training epoch 872, Batch 1000/1000: LR=6.05e-05, Loss=4.17e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 03:00:36,119 | INFO | Epoch 872 Train Time 95.74563455581665s

2025-10-17 03:02:14,292 | INFO | Training epoch 873, Batch 1000/1000: LR=6.04e-05, Loss=4.39e-03 BER=1.79e-03 FER=1.59e-02
2025-10-17 03:02:14,346 | INFO | Epoch 873 Train Time 98.22573971748352s

2025-10-17 03:03:47,055 | INFO | Training epoch 874, Batch 1000/1000: LR=6.03e-05, Loss=4.24e-03 BER=1.74e-03 FER=1.51e-02
2025-10-17 03:03:47,114 | INFO | Epoch 874 Train Time 92.76626777648926s

2025-10-17 03:05:21,031 | INFO | Training epoch 875, Batch 1000/1000: LR=6.02e-05, Loss=4.09e-03 BER=1.66e-03 FER=1.49e-02
2025-10-17 03:05:21,081 | INFO | Epoch 875 Train Time 93.9646348953247s

2025-10-17 03:06:53,251 | INFO | Training epoch 876, Batch 1000/1000: LR=6.02e-05, Loss=3.99e-03 BER=1.63e-03 FER=1.43e-02
2025-10-17 03:06:53,306 | INFO | Epoch 876 Train Time 92.22336483001709s

2025-10-17 03:08:30,598 | INFO | Training epoch 877, Batch 1000/1000: LR=6.01e-05, Loss=4.11e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 03:08:30,644 | INFO | Epoch 877 Train Time 97.33678412437439s

2025-10-17 03:10:02,132 | INFO | Training epoch 878, Batch 1000/1000: LR=6.00e-05, Loss=4.22e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 03:10:02,185 | INFO | Epoch 878 Train Time 91.53979229927063s

2025-10-17 03:11:37,616 | INFO | Training epoch 879, Batch 1000/1000: LR=5.99e-05, Loss=4.20e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 03:11:37,664 | INFO | Epoch 879 Train Time 95.47718071937561s

2025-10-17 03:13:08,302 | INFO | Training epoch 880, Batch 1000/1000: LR=5.99e-05, Loss=4.12e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 03:13:08,349 | INFO | Epoch 880 Train Time 90.68385624885559s

2025-10-17 03:14:44,245 | INFO | Training epoch 881, Batch 1000/1000: LR=5.98e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 03:14:44,309 | INFO | Epoch 881 Train Time 95.95618867874146s

2025-10-17 03:16:12,237 | INFO | Training epoch 882, Batch 1000/1000: LR=5.97e-05, Loss=4.13e-03 BER=1.71e-03 FER=1.52e-02
2025-10-17 03:16:12,281 | INFO | Epoch 882 Train Time 87.97040176391602s

2025-10-17 03:17:51,181 | INFO | Training epoch 883, Batch 1000/1000: LR=5.96e-05, Loss=4.17e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 03:17:51,250 | INFO | Epoch 883 Train Time 98.96799159049988s

2025-10-17 03:19:38,003 | INFO | Training epoch 884, Batch 1000/1000: LR=5.95e-05, Loss=4.26e-03 BER=1.73e-03 FER=1.53e-02
2025-10-17 03:19:38,066 | INFO | Epoch 884 Train Time 106.81397747993469s

2025-10-17 03:21:19,635 | INFO | Training epoch 885, Batch 1000/1000: LR=5.95e-05, Loss=4.29e-03 BER=1.77e-03 FER=1.55e-02
2025-10-17 03:21:19,685 | INFO | Epoch 885 Train Time 101.61813378334045s

2025-10-17 03:22:56,731 | INFO | Training epoch 886, Batch 1000/1000: LR=5.94e-05, Loss=4.13e-03 BER=1.66e-03 FER=1.47e-02
2025-10-17 03:22:56,778 | INFO | Epoch 886 Train Time 97.09143376350403s

2025-10-17 03:24:32,450 | INFO | Training epoch 887, Batch 1000/1000: LR=5.93e-05, Loss=4.30e-03 BER=1.74e-03 FER=1.54e-02
2025-10-17 03:24:32,506 | INFO | Epoch 887 Train Time 95.7257730960846s

2025-10-17 03:26:12,554 | INFO | Training epoch 888, Batch 1000/1000: LR=5.92e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.43e-02
2025-10-17 03:26:12,618 | INFO | Epoch 888 Train Time 100.11069846153259s

2025-10-17 03:27:48,435 | INFO | Training epoch 889, Batch 1000/1000: LR=5.92e-05, Loss=4.19e-03 BER=1.72e-03 FER=1.49e-02
2025-10-17 03:27:48,489 | INFO | Epoch 889 Train Time 95.86940622329712s

2025-10-17 03:29:22,199 | INFO | Training epoch 890, Batch 1000/1000: LR=5.91e-05, Loss=4.09e-03 BER=1.70e-03 FER=1.50e-02
2025-10-17 03:29:22,254 | INFO | Epoch 890 Train Time 93.7631208896637s

2025-10-17 03:30:58,937 | INFO | Training epoch 891, Batch 1000/1000: LR=5.90e-05, Loss=4.25e-03 BER=1.75e-03 FER=1.51e-02
2025-10-17 03:30:58,998 | INFO | Epoch 891 Train Time 96.7432165145874s

2025-10-17 03:32:36,112 | INFO | Training epoch 892, Batch 1000/1000: LR=5.89e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.52e-02
2025-10-17 03:32:36,156 | INFO | Epoch 892 Train Time 97.1571478843689s

2025-10-17 03:34:10,841 | INFO | Training epoch 893, Batch 1000/1000: LR=5.89e-05, Loss=4.21e-03 BER=1.72e-03 FER=1.49e-02
2025-10-17 03:34:10,881 | INFO | Epoch 893 Train Time 94.72386860847473s

2025-10-17 03:35:55,601 | INFO | Training epoch 894, Batch 1000/1000: LR=5.88e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.51e-02
2025-10-17 03:35:55,662 | INFO | Epoch 894 Train Time 104.77952551841736s

2025-10-17 03:37:32,002 | INFO | Training epoch 895, Batch 1000/1000: LR=5.87e-05, Loss=4.12e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 03:37:32,057 | INFO | Epoch 895 Train Time 96.39395141601562s

2025-10-17 03:39:04,992 | INFO | Training epoch 896, Batch 1000/1000: LR=5.86e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.57e-02
2025-10-17 03:39:05,056 | INFO | Epoch 896 Train Time 92.99786400794983s

2025-10-17 03:40:39,683 | INFO | Training epoch 897, Batch 1000/1000: LR=5.86e-05, Loss=4.32e-03 BER=1.74e-03 FER=1.54e-02
2025-10-17 03:40:39,727 | INFO | Epoch 897 Train Time 94.66816425323486s

2025-10-17 03:42:12,521 | INFO | Training epoch 898, Batch 1000/1000: LR=5.85e-05, Loss=4.21e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 03:42:12,591 | INFO | Epoch 898 Train Time 92.86204600334167s

2025-10-17 03:43:49,124 | INFO | Training epoch 899, Batch 1000/1000: LR=5.84e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.47e-02
2025-10-17 03:43:49,176 | INFO | Epoch 899 Train Time 96.58471512794495s

2025-10-17 03:45:29,099 | INFO | Training epoch 900, Batch 1000/1000: LR=5.83e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.42e-02
2025-10-17 03:45:29,156 | INFO | Epoch 900 Train Time 99.9780638217926s

2025-10-17 03:46:58,257 | INFO | Training epoch 901, Batch 1000/1000: LR=5.82e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.50e-02
2025-10-17 03:46:58,311 | INFO | Epoch 901 Train Time 89.15331220626831s

2025-10-17 03:48:41,317 | INFO | Training epoch 902, Batch 1000/1000: LR=5.82e-05, Loss=4.11e-03 BER=1.67e-03 FER=1.44e-02
2025-10-17 03:48:41,370 | INFO | Epoch 902 Train Time 103.05715274810791s

2025-10-17 03:50:19,110 | INFO | Training epoch 903, Batch 1000/1000: LR=5.81e-05, Loss=4.22e-03 BER=1.72e-03 FER=1.49e-02
2025-10-17 03:50:19,177 | INFO | Epoch 903 Train Time 97.80593800544739s

2025-10-17 03:51:53,964 | INFO | Training epoch 904, Batch 1000/1000: LR=5.80e-05, Loss=4.12e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 03:51:54,016 | INFO | Epoch 904 Train Time 94.83709263801575s

2025-10-17 03:53:32,756 | INFO | Training epoch 905, Batch 1000/1000: LR=5.79e-05, Loss=4.11e-03 BER=1.71e-03 FER=1.52e-02
2025-10-17 03:53:32,807 | INFO | Epoch 905 Train Time 98.7900161743164s

2025-10-17 03:55:18,022 | INFO | Training epoch 906, Batch 1000/1000: LR=5.79e-05, Loss=4.03e-03 BER=1.69e-03 FER=1.50e-02
2025-10-17 03:55:18,091 | INFO | Epoch 906 Train Time 105.28203868865967s

2025-10-17 03:56:51,479 | INFO | Training epoch 907, Batch 1000/1000: LR=5.78e-05, Loss=4.19e-03 BER=1.73e-03 FER=1.53e-02
2025-10-17 03:56:51,528 | INFO | Epoch 907 Train Time 93.436203956604s

2025-10-17 03:58:24,096 | INFO | Training epoch 908, Batch 1000/1000: LR=5.77e-05, Loss=4.22e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 03:58:24,149 | INFO | Epoch 908 Train Time 92.61906266212463s

2025-10-17 04:00:05,905 | INFO | Training epoch 909, Batch 1000/1000: LR=5.76e-05, Loss=4.19e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 04:00:05,978 | INFO | Epoch 909 Train Time 101.82772254943848s

2025-10-17 04:01:35,288 | INFO | Training epoch 910, Batch 1000/1000: LR=5.76e-05, Loss=4.21e-03 BER=1.71e-03 FER=1.50e-02
2025-10-17 04:01:35,334 | INFO | Epoch 910 Train Time 89.35359835624695s

2025-10-17 04:03:10,030 | INFO | Training epoch 911, Batch 1000/1000: LR=5.75e-05, Loss=4.15e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 04:03:10,089 | INFO | Epoch 911 Train Time 94.7544617652893s

2025-10-17 04:04:46,409 | INFO | Training epoch 912, Batch 1000/1000: LR=5.74e-05, Loss=4.28e-03 BER=1.74e-03 FER=1.53e-02
2025-10-17 04:04:46,469 | INFO | Epoch 912 Train Time 96.37675499916077s

2025-10-17 04:06:21,741 | INFO | Training epoch 913, Batch 1000/1000: LR=5.73e-05, Loss=4.17e-03 BER=1.71e-03 FER=1.50e-02
2025-10-17 04:06:21,809 | INFO | Epoch 913 Train Time 95.3389139175415s

2025-10-17 04:07:49,722 | INFO | Training epoch 914, Batch 1000/1000: LR=5.72e-05, Loss=4.10e-03 BER=1.71e-03 FER=1.46e-02
2025-10-17 04:07:49,780 | INFO | Epoch 914 Train Time 87.9683632850647s

2025-10-17 04:09:34,255 | INFO | Training epoch 915, Batch 1000/1000: LR=5.72e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.45e-02
2025-10-17 04:09:34,327 | INFO | Epoch 915 Train Time 104.54471945762634s

2025-10-17 04:11:15,562 | INFO | Training epoch 916, Batch 1000/1000: LR=5.71e-05, Loss=4.18e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 04:11:15,620 | INFO | Epoch 916 Train Time 101.29100012779236s

2025-10-17 04:12:50,890 | INFO | Training epoch 917, Batch 1000/1000: LR=5.70e-05, Loss=4.37e-03 BER=1.77e-03 FER=1.59e-02
2025-10-17 04:12:50,933 | INFO | Epoch 917 Train Time 95.31203174591064s

2025-10-17 04:14:27,362 | INFO | Training epoch 918, Batch 1000/1000: LR=5.69e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 04:14:27,404 | INFO | Epoch 918 Train Time 96.46849346160889s

2025-10-17 04:16:03,558 | INFO | Training epoch 919, Batch 1000/1000: LR=5.69e-05, Loss=4.11e-03 BER=1.67e-03 FER=1.43e-02
2025-10-17 04:16:03,608 | INFO | Epoch 919 Train Time 96.20232582092285s

2025-10-17 04:17:34,737 | INFO | Training epoch 920, Batch 1000/1000: LR=5.68e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 04:17:34,788 | INFO | Epoch 920 Train Time 91.17860889434814s

2025-10-17 04:19:06,516 | INFO | Training epoch 921, Batch 1000/1000: LR=5.67e-05, Loss=4.19e-03 BER=1.72e-03 FER=1.48e-02
2025-10-17 04:19:06,577 | INFO | Epoch 921 Train Time 91.78811836242676s

2025-10-17 04:20:44,241 | INFO | Training epoch 922, Batch 1000/1000: LR=5.66e-05, Loss=4.21e-03 BER=1.73e-03 FER=1.51e-02
2025-10-17 04:20:44,290 | INFO | Epoch 922 Train Time 97.71037602424622s

2025-10-17 04:22:12,566 | INFO | Training epoch 923, Batch 1000/1000: LR=5.65e-05, Loss=4.06e-03 BER=1.69e-03 FER=1.46e-02
2025-10-17 04:22:12,618 | INFO | Epoch 923 Train Time 88.32715702056885s

2025-10-17 04:23:55,858 | INFO | Training epoch 924, Batch 1000/1000: LR=5.65e-05, Loss=4.03e-03 BER=1.67e-03 FER=1.46e-02
2025-10-17 04:23:55,907 | INFO | Epoch 924 Train Time 103.28840637207031s

2025-10-17 04:25:35,817 | INFO | Training epoch 925, Batch 1000/1000: LR=5.64e-05, Loss=4.02e-03 BER=1.64e-03 FER=1.45e-02
2025-10-17 04:25:35,878 | INFO | Epoch 925 Train Time 99.96808934211731s

2025-10-17 04:27:12,328 | INFO | Training epoch 926, Batch 1000/1000: LR=5.63e-05, Loss=4.15e-03 BER=1.74e-03 FER=1.51e-02
2025-10-17 04:27:12,389 | INFO | Epoch 926 Train Time 96.51022791862488s

2025-10-17 04:28:54,611 | INFO | Training epoch 927, Batch 1000/1000: LR=5.62e-05, Loss=4.06e-03 BER=1.69e-03 FER=1.46e-02
2025-10-17 04:28:54,662 | INFO | Epoch 927 Train Time 102.27054572105408s

2025-10-17 04:30:37,769 | INFO | Training epoch 928, Batch 1000/1000: LR=5.62e-05, Loss=4.31e-03 BER=1.81e-03 FER=1.55e-02
2025-10-17 04:30:37,825 | INFO | Epoch 928 Train Time 103.1622531414032s

2025-10-17 04:32:16,433 | INFO | Training epoch 929, Batch 1000/1000: LR=5.61e-05, Loss=4.16e-03 BER=1.68e-03 FER=1.49e-02
2025-10-17 04:32:16,510 | INFO | Epoch 929 Train Time 98.6831202507019s

2025-10-17 04:33:52,035 | INFO | Training epoch 930, Batch 1000/1000: LR=5.60e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.47e-02
2025-10-17 04:33:52,078 | INFO | Epoch 930 Train Time 95.56744647026062s

2025-10-17 04:35:28,484 | INFO | Training epoch 931, Batch 1000/1000: LR=5.59e-05, Loss=4.19e-03 BER=1.72e-03 FER=1.49e-02
2025-10-17 04:35:28,537 | INFO | Epoch 931 Train Time 96.45695948600769s

2025-10-17 04:37:03,429 | INFO | Training epoch 932, Batch 1000/1000: LR=5.59e-05, Loss=4.03e-03 BER=1.69e-03 FER=1.49e-02
2025-10-17 04:37:03,492 | INFO | Epoch 932 Train Time 94.95459866523743s

2025-10-17 04:38:44,150 | INFO | Training epoch 933, Batch 1000/1000: LR=5.58e-05, Loss=4.16e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 04:38:44,206 | INFO | Epoch 933 Train Time 100.71262097358704s

2025-10-17 04:40:23,940 | INFO | Training epoch 934, Batch 1000/1000: LR=5.57e-05, Loss=3.89e-03 BER=1.59e-03 FER=1.43e-02
2025-10-17 04:40:23,996 | INFO | Epoch 934 Train Time 99.7872486114502s

2025-10-17 04:42:01,023 | INFO | Training epoch 935, Batch 1000/1000: LR=5.56e-05, Loss=4.34e-03 BER=1.77e-03 FER=1.56e-02
2025-10-17 04:42:01,085 | INFO | Epoch 935 Train Time 97.08690142631531s

2025-10-17 04:43:45,113 | INFO | Training epoch 936, Batch 1000/1000: LR=5.55e-05, Loss=4.10e-03 BER=1.68e-03 FER=1.46e-02
2025-10-17 04:43:45,177 | INFO | Epoch 936 Train Time 104.090815782547s

2025-10-17 04:45:21,101 | INFO | Training epoch 937, Batch 1000/1000: LR=5.55e-05, Loss=4.18e-03 BER=1.73e-03 FER=1.51e-02
2025-10-17 04:45:21,159 | INFO | Epoch 937 Train Time 95.97889971733093s

2025-10-17 04:46:55,698 | INFO | Training epoch 938, Batch 1000/1000: LR=5.54e-05, Loss=4.03e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 04:46:55,742 | INFO | Epoch 938 Train Time 94.58158326148987s

2025-10-17 04:48:36,433 | INFO | Training epoch 939, Batch 1000/1000: LR=5.53e-05, Loss=3.93e-03 BER=1.63e-03 FER=1.43e-02
2025-10-17 04:48:36,482 | INFO | Epoch 939 Train Time 100.73824977874756s

2025-10-17 04:50:11,449 | INFO | Training epoch 940, Batch 1000/1000: LR=5.52e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.45e-02
2025-10-17 04:50:11,515 | INFO | Epoch 940 Train Time 95.03131175041199s

2025-10-17 04:51:46,643 | INFO | Training epoch 941, Batch 1000/1000: LR=5.52e-05, Loss=4.35e-03 BER=1.80e-03 FER=1.56e-02
2025-10-17 04:51:46,695 | INFO | Epoch 941 Train Time 95.17945122718811s

2025-10-17 04:53:22,001 | INFO | Training epoch 942, Batch 1000/1000: LR=5.51e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 04:53:22,052 | INFO | Epoch 942 Train Time 95.3551926612854s

2025-10-17 04:54:57,503 | INFO | Training epoch 943, Batch 1000/1000: LR=5.50e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.54e-02
2025-10-17 04:54:57,551 | INFO | Epoch 943 Train Time 95.496994972229s

2025-10-17 04:56:31,533 | INFO | Training epoch 944, Batch 1000/1000: LR=5.49e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.47e-02
2025-10-17 04:56:31,581 | INFO | Epoch 944 Train Time 94.02831625938416s

2025-10-17 04:58:17,726 | INFO | Training epoch 945, Batch 1000/1000: LR=5.48e-05, Loss=4.30e-03 BER=1.75e-03 FER=1.54e-02
2025-10-17 04:58:17,780 | INFO | Epoch 945 Train Time 106.19586110115051s

2025-10-17 05:00:00,742 | INFO | Training epoch 946, Batch 1000/1000: LR=5.48e-05, Loss=4.25e-03 BER=1.72e-03 FER=1.51e-02
2025-10-17 05:00:00,800 | INFO | Epoch 946 Train Time 103.01777005195618s

2025-10-17 05:01:35,670 | INFO | Training epoch 947, Batch 1000/1000: LR=5.47e-05, Loss=4.21e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 05:01:35,726 | INFO | Epoch 947 Train Time 94.92431473731995s

2025-10-17 05:03:12,967 | INFO | Training epoch 948, Batch 1000/1000: LR=5.46e-05, Loss=4.34e-03 BER=1.79e-03 FER=1.57e-02
2025-10-17 05:03:13,014 | INFO | Epoch 948 Train Time 97.28659176826477s

2025-10-17 05:04:43,013 | INFO | Training epoch 949, Batch 1000/1000: LR=5.45e-05, Loss=4.21e-03 BER=1.72e-03 FER=1.50e-02
2025-10-17 05:04:43,063 | INFO | Epoch 949 Train Time 90.0475263595581s

2025-10-17 05:06:22,301 | INFO | Training epoch 950, Batch 1000/1000: LR=5.45e-05, Loss=4.39e-03 BER=1.80e-03 FER=1.56e-02
2025-10-17 05:06:22,359 | INFO | Epoch 950 Train Time 99.29352641105652s

2025-10-17 05:07:57,226 | INFO | Training epoch 951, Batch 1000/1000: LR=5.44e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.44e-02
2025-10-17 05:07:57,290 | INFO | Epoch 951 Train Time 94.92980718612671s

2025-10-17 05:09:34,858 | INFO | Training epoch 952, Batch 1000/1000: LR=5.43e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.46e-02
2025-10-17 05:09:34,898 | INFO | Epoch 952 Train Time 97.60565280914307s

2025-10-17 05:11:09,030 | INFO | Training epoch 953, Batch 1000/1000: LR=5.42e-05, Loss=4.18e-03 BER=1.73e-03 FER=1.52e-02
2025-10-17 05:11:09,090 | INFO | Epoch 953 Train Time 94.19178557395935s

2025-10-17 05:12:41,382 | INFO | Training epoch 954, Batch 1000/1000: LR=5.42e-05, Loss=4.15e-03 BER=1.69e-03 FER=1.49e-02
2025-10-17 05:12:41,434 | INFO | Epoch 954 Train Time 92.34254193305969s

2025-10-17 05:14:17,974 | INFO | Training epoch 955, Batch 1000/1000: LR=5.41e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.40e-02
2025-10-17 05:14:18,017 | INFO | Epoch 955 Train Time 96.581627368927s

2025-10-17 05:15:49,344 | INFO | Training epoch 956, Batch 1000/1000: LR=5.40e-05, Loss=4.19e-03 BER=1.71e-03 FER=1.51e-02
2025-10-17 05:15:49,388 | INFO | Epoch 956 Train Time 91.3692135810852s

2025-10-17 05:17:19,788 | INFO | Training epoch 957, Batch 1000/1000: LR=5.39e-05, Loss=4.26e-03 BER=1.76e-03 FER=1.53e-02
2025-10-17 05:17:19,849 | INFO | Epoch 957 Train Time 90.46002960205078s

2025-10-17 05:18:50,246 | INFO | Training epoch 958, Batch 1000/1000: LR=5.38e-05, Loss=4.26e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 05:18:50,287 | INFO | Epoch 958 Train Time 90.43680143356323s

2025-10-17 05:20:25,677 | INFO | Training epoch 959, Batch 1000/1000: LR=5.38e-05, Loss=4.16e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 05:20:25,742 | INFO | Epoch 959 Train Time 95.45334100723267s

2025-10-17 05:21:57,158 | INFO | Training epoch 960, Batch 1000/1000: LR=5.37e-05, Loss=4.05e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 05:21:57,207 | INFO | Epoch 960 Train Time 91.46398973464966s

2025-10-17 05:23:36,542 | INFO | Training epoch 961, Batch 1000/1000: LR=5.36e-05, Loss=4.23e-03 BER=1.74e-03 FER=1.51e-02
2025-10-17 05:23:36,586 | INFO | Epoch 961 Train Time 99.37873339653015s

2025-10-17 05:25:32,660 | INFO | Training epoch 962, Batch 1000/1000: LR=5.35e-05, Loss=4.11e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 05:25:32,725 | INFO | Epoch 962 Train Time 116.13759160041809s

2025-10-17 05:27:16,030 | INFO | Training epoch 963, Batch 1000/1000: LR=5.35e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.43e-02
2025-10-17 05:27:16,093 | INFO | Epoch 963 Train Time 103.36608576774597s

2025-10-17 05:28:50,147 | INFO | Training epoch 964, Batch 1000/1000: LR=5.34e-05, Loss=4.11e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 05:28:50,206 | INFO | Epoch 964 Train Time 94.10861253738403s

2025-10-17 05:30:25,079 | INFO | Training epoch 965, Batch 1000/1000: LR=5.33e-05, Loss=4.18e-03 BER=1.72e-03 FER=1.52e-02
2025-10-17 05:30:25,140 | INFO | Epoch 965 Train Time 94.93188238143921s

2025-10-17 05:32:08,205 | INFO | Training epoch 966, Batch 1000/1000: LR=5.32e-05, Loss=4.25e-03 BER=1.75e-03 FER=1.53e-02
2025-10-17 05:32:08,247 | INFO | Epoch 966 Train Time 103.10509324073792s

2025-10-17 05:33:44,950 | INFO | Training epoch 967, Batch 1000/1000: LR=5.31e-05, Loss=4.27e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 05:33:45,015 | INFO | Epoch 967 Train Time 96.76684403419495s

2025-10-17 05:35:24,466 | INFO | Training epoch 968, Batch 1000/1000: LR=5.31e-05, Loss=4.13e-03 BER=1.67e-03 FER=1.46e-02
2025-10-17 05:35:24,511 | INFO | Epoch 968 Train Time 99.49395227432251s

2025-10-17 05:36:59,408 | INFO | Training epoch 969, Batch 1000/1000: LR=5.30e-05, Loss=4.23e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 05:36:59,467 | INFO | Epoch 969 Train Time 94.95383620262146s

2025-10-17 05:38:34,995 | INFO | Training epoch 970, Batch 1000/1000: LR=5.29e-05, Loss=4.00e-03 BER=1.62e-03 FER=1.42e-02
2025-10-17 05:38:35,046 | INFO | Epoch 970 Train Time 95.5783052444458s

2025-10-17 05:40:07,906 | INFO | Training epoch 971, Batch 1000/1000: LR=5.28e-05, Loss=4.13e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 05:40:07,959 | INFO | Epoch 971 Train Time 92.90997409820557s

2025-10-17 05:41:48,770 | INFO | Training epoch 972, Batch 1000/1000: LR=5.28e-05, Loss=4.04e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 05:41:48,822 | INFO | Epoch 972 Train Time 100.86138319969177s

2025-10-17 05:43:19,061 | INFO | Training epoch 973, Batch 1000/1000: LR=5.27e-05, Loss=4.26e-03 BER=1.75e-03 FER=1.54e-02
2025-10-17 05:43:19,121 | INFO | Epoch 973 Train Time 90.2971260547638s

2025-10-17 05:44:57,714 | INFO | Training epoch 974, Batch 1000/1000: LR=5.26e-05, Loss=4.27e-03 BER=1.75e-03 FER=1.52e-02
2025-10-17 05:44:57,764 | INFO | Epoch 974 Train Time 98.64188551902771s

2025-10-17 05:46:37,113 | INFO | Training epoch 975, Batch 1000/1000: LR=5.25e-05, Loss=4.12e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 05:46:37,172 | INFO | Epoch 975 Train Time 99.40585279464722s

2025-10-17 05:48:17,634 | INFO | Training epoch 976, Batch 1000/1000: LR=5.24e-05, Loss=4.06e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 05:48:17,687 | INFO | Epoch 976 Train Time 100.51380729675293s

2025-10-17 05:49:53,902 | INFO | Training epoch 977, Batch 1000/1000: LR=5.24e-05, Loss=4.05e-03 BER=1.65e-03 FER=1.46e-02
2025-10-17 05:49:53,949 | INFO | Epoch 977 Train Time 96.26079869270325s

2025-10-17 05:51:31,002 | INFO | Training epoch 978, Batch 1000/1000: LR=5.23e-05, Loss=3.95e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 05:51:31,054 | INFO | Epoch 978 Train Time 97.10339689254761s

2025-10-17 05:53:03,016 | INFO | Training epoch 979, Batch 1000/1000: LR=5.22e-05, Loss=4.17e-03 BER=1.71e-03 FER=1.50e-02
2025-10-17 05:53:03,080 | INFO | Epoch 979 Train Time 92.02501940727234s

2025-10-17 05:54:43,075 | INFO | Training epoch 980, Batch 1000/1000: LR=5.21e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.50e-02
2025-10-17 05:54:43,136 | INFO | Epoch 980 Train Time 100.05514597892761s

2025-10-17 05:56:19,999 | INFO | Training epoch 981, Batch 1000/1000: LR=5.21e-05, Loss=4.12e-03 BER=1.65e-03 FER=1.47e-02
2025-10-17 05:56:20,048 | INFO | Epoch 981 Train Time 96.91025686264038s

2025-10-17 05:58:03,073 | INFO | Training epoch 982, Batch 1000/1000: LR=5.20e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 05:58:03,124 | INFO | Epoch 982 Train Time 103.07404851913452s

2025-10-17 05:59:44,254 | INFO | Training epoch 983, Batch 1000/1000: LR=5.19e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 05:59:44,298 | INFO | Epoch 983 Train Time 101.1720073223114s

2025-10-17 06:01:19,255 | INFO | Training epoch 984, Batch 1000/1000: LR=5.18e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.50e-02
2025-10-17 06:01:19,306 | INFO | Epoch 984 Train Time 95.0075216293335s

2025-10-17 06:02:55,238 | INFO | Training epoch 985, Batch 1000/1000: LR=5.17e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 06:02:55,290 | INFO | Epoch 985 Train Time 95.98178458213806s

2025-10-17 06:04:22,686 | INFO | Training epoch 986, Batch 1000/1000: LR=5.17e-05, Loss=4.17e-03 BER=1.72e-03 FER=1.52e-02
2025-10-17 06:04:22,742 | INFO | Epoch 986 Train Time 87.4506094455719s

2025-10-17 06:05:58,502 | INFO | Training epoch 987, Batch 1000/1000: LR=5.16e-05, Loss=4.26e-03 BER=1.76e-03 FER=1.54e-02
2025-10-17 06:05:58,554 | INFO | Epoch 987 Train Time 95.81170678138733s

2025-10-17 06:07:33,749 | INFO | Training epoch 988, Batch 1000/1000: LR=5.15e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.41e-02
2025-10-17 06:07:33,814 | INFO | Epoch 988 Train Time 95.25841903686523s

2025-10-17 06:09:13,269 | INFO | Training epoch 989, Batch 1000/1000: LR=5.14e-05, Loss=4.10e-03 BER=1.73e-03 FER=1.51e-02
2025-10-17 06:09:13,319 | INFO | Epoch 989 Train Time 99.50427651405334s

2025-10-17 06:10:47,171 | INFO | Training epoch 990, Batch 1000/1000: LR=5.14e-05, Loss=4.07e-03 BER=1.66e-03 FER=1.50e-02
2025-10-17 06:10:47,214 | INFO | Epoch 990 Train Time 93.89349246025085s

2025-10-17 06:12:19,642 | INFO | Training epoch 991, Batch 1000/1000: LR=5.13e-05, Loss=4.14e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 06:12:19,693 | INFO | Epoch 991 Train Time 92.47816491127014s

2025-10-17 06:13:51,569 | INFO | Training epoch 992, Batch 1000/1000: LR=5.12e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.45e-02
2025-10-17 06:13:51,615 | INFO | Epoch 992 Train Time 91.92058634757996s

2025-10-17 06:15:27,883 | INFO | Training epoch 993, Batch 1000/1000: LR=5.11e-05, Loss=4.02e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 06:15:27,936 | INFO | Epoch 993 Train Time 96.31848216056824s

2025-10-17 06:17:03,061 | INFO | Training epoch 994, Batch 1000/1000: LR=5.10e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.46e-02
2025-10-17 06:17:03,120 | INFO | Epoch 994 Train Time 95.18289470672607s

2025-10-17 06:18:40,402 | INFO | Training epoch 995, Batch 1000/1000: LR=5.10e-05, Loss=4.15e-03 BER=1.70e-03 FER=1.52e-02
2025-10-17 06:18:40,453 | INFO | Epoch 995 Train Time 97.33154487609863s

2025-10-17 06:20:14,317 | INFO | Training epoch 996, Batch 1000/1000: LR=5.09e-05, Loss=4.15e-03 BER=1.74e-03 FER=1.50e-02
2025-10-17 06:20:14,376 | INFO | Epoch 996 Train Time 93.92176055908203s

2025-10-17 06:21:50,749 | INFO | Training epoch 997, Batch 1000/1000: LR=5.08e-05, Loss=4.17e-03 BER=1.70e-03 FER=1.52e-02
2025-10-17 06:21:50,791 | INFO | Epoch 997 Train Time 96.41219806671143s

2025-10-17 06:23:22,268 | INFO | Training epoch 998, Batch 1000/1000: LR=5.07e-05, Loss=4.05e-03 BER=1.67e-03 FER=1.43e-02
2025-10-17 06:23:22,320 | INFO | Epoch 998 Train Time 91.52842497825623s

2025-10-17 06:24:55,820 | INFO | Training epoch 999, Batch 1000/1000: LR=5.07e-05, Loss=4.13e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 06:24:55,873 | INFO | Epoch 999 Train Time 93.55115818977356s

2025-10-17 06:26:30,109 | INFO | Training epoch 1000, Batch 1000/1000: LR=5.06e-05, Loss=4.12e-03 BER=1.67e-03 FER=1.47e-02
2025-10-17 06:26:30,159 | INFO | Epoch 1000 Train Time 94.28330445289612s

2025-10-17 06:27:57,926 | INFO | Training epoch 1001, Batch 1000/1000: LR=5.05e-05, Loss=4.17e-03 BER=1.73e-03 FER=1.48e-02
2025-10-17 06:27:57,975 | INFO | Epoch 1001 Train Time 87.81456637382507s

2025-10-17 06:29:32,804 | INFO | Training epoch 1002, Batch 1000/1000: LR=5.04e-05, Loss=4.11e-03 BER=1.67e-03 FER=1.47e-02
2025-10-17 06:29:32,858 | INFO | Epoch 1002 Train Time 94.88069939613342s

2025-10-17 06:31:06,907 | INFO | Training epoch 1003, Batch 1000/1000: LR=5.03e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 06:31:06,958 | INFO | Epoch 1003 Train Time 94.0990219116211s

2025-10-17 06:32:40,633 | INFO | Training epoch 1004, Batch 1000/1000: LR=5.03e-05, Loss=4.02e-03 BER=1.67e-03 FER=1.47e-02
2025-10-17 06:32:40,677 | INFO | Epoch 1004 Train Time 93.71641206741333s

2025-10-17 06:34:16,434 | INFO | Training epoch 1005, Batch 1000/1000: LR=5.02e-05, Loss=4.25e-03 BER=1.72e-03 FER=1.54e-02
2025-10-17 06:34:16,485 | INFO | Epoch 1005 Train Time 95.8066337108612s

2025-10-17 06:35:51,900 | INFO | Training epoch 1006, Batch 1000/1000: LR=5.01e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 06:35:51,951 | INFO | Epoch 1006 Train Time 95.46547746658325s

2025-10-17 06:37:31,885 | INFO | Training epoch 1007, Batch 1000/1000: LR=5.00e-05, Loss=3.95e-03 BER=1.61e-03 FER=1.44e-02
2025-10-17 06:37:31,947 | INFO | Epoch 1007 Train Time 99.99418950080872s

2025-10-17 06:39:03,448 | INFO | Training epoch 1008, Batch 1000/1000: LR=5.00e-05, Loss=4.25e-03 BER=1.75e-03 FER=1.49e-02
2025-10-17 06:39:03,500 | INFO | Epoch 1008 Train Time 91.55189275741577s

2025-10-17 06:40:37,413 | INFO | Training epoch 1009, Batch 1000/1000: LR=4.99e-05, Loss=3.97e-03 BER=1.63e-03 FER=1.42e-02
2025-10-17 06:40:37,462 | INFO | Epoch 1009 Train Time 93.96071481704712s

2025-10-17 06:42:14,843 | INFO | Training epoch 1010, Batch 1000/1000: LR=4.98e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 06:42:14,908 | INFO | Epoch 1010 Train Time 97.44332671165466s

2025-10-17 06:43:49,552 | INFO | Training epoch 1011, Batch 1000/1000: LR=4.97e-05, Loss=3.87e-03 BER=1.61e-03 FER=1.36e-02
2025-10-17 06:43:49,605 | INFO | Epoch 1011 Train Time 94.6928060054779s

2025-10-17 06:43:49,605 | INFO | [P2] saving best_model (QAT) with loss 0.003868 at epoch 1011
2025-10-17 06:45:25,601 | INFO | Training epoch 1012, Batch 1000/1000: LR=4.96e-05, Loss=4.16e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 06:45:25,646 | INFO | Epoch 1012 Train Time 95.95650959014893s

2025-10-17 06:47:01,012 | INFO | Training epoch 1013, Batch 1000/1000: LR=4.96e-05, Loss=4.17e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 06:47:01,069 | INFO | Epoch 1013 Train Time 95.42102456092834s

2025-10-17 06:48:36,843 | INFO | Training epoch 1014, Batch 1000/1000: LR=4.95e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.43e-02
2025-10-17 06:48:36,901 | INFO | Epoch 1014 Train Time 95.83065629005432s

2025-10-17 06:50:12,237 | INFO | Training epoch 1015, Batch 1000/1000: LR=4.94e-05, Loss=4.01e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 06:50:12,287 | INFO | Epoch 1015 Train Time 95.38372850418091s

2025-10-17 06:51:54,416 | INFO | Training epoch 1016, Batch 1000/1000: LR=4.93e-05, Loss=4.06e-03 BER=1.68e-03 FER=1.45e-02
2025-10-17 06:51:54,484 | INFO | Epoch 1016 Train Time 102.1947648525238s

2025-10-17 06:53:35,260 | INFO | Training epoch 1017, Batch 1000/1000: LR=4.93e-05, Loss=4.02e-03 BER=1.67e-03 FER=1.48e-02
2025-10-17 06:53:35,309 | INFO | Epoch 1017 Train Time 100.82301688194275s

2025-10-17 06:55:15,874 | INFO | Training epoch 1018, Batch 1000/1000: LR=4.92e-05, Loss=4.04e-03 BER=1.68e-03 FER=1.48e-02
2025-10-17 06:55:15,923 | INFO | Epoch 1018 Train Time 100.61198663711548s

2025-10-17 06:56:51,966 | INFO | Training epoch 1019, Batch 1000/1000: LR=4.91e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 06:56:52,021 | INFO | Epoch 1019 Train Time 96.0975296497345s

2025-10-17 06:58:36,864 | INFO | Training epoch 1020, Batch 1000/1000: LR=4.90e-05, Loss=4.31e-03 BER=1.75e-03 FER=1.52e-02
2025-10-17 06:58:36,912 | INFO | Epoch 1020 Train Time 104.88899374008179s

2025-10-17 07:00:11,945 | INFO | Training epoch 1021, Batch 1000/1000: LR=4.89e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.43e-02
2025-10-17 07:00:11,989 | INFO | Epoch 1021 Train Time 95.07540225982666s

2025-10-17 07:01:53,030 | INFO | Training epoch 1022, Batch 1000/1000: LR=4.89e-05, Loss=3.97e-03 BER=1.63e-03 FER=1.43e-02
2025-10-17 07:01:53,085 | INFO | Epoch 1022 Train Time 101.09494590759277s

2025-10-17 07:03:30,557 | INFO | Training epoch 1023, Batch 1000/1000: LR=4.88e-05, Loss=4.04e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 07:03:30,601 | INFO | Epoch 1023 Train Time 97.51150679588318s

2025-10-17 07:05:03,699 | INFO | Training epoch 1024, Batch 1000/1000: LR=4.87e-05, Loss=4.22e-03 BER=1.74e-03 FER=1.49e-02
2025-10-17 07:05:03,749 | INFO | Epoch 1024 Train Time 93.14711356163025s

2025-10-17 07:06:45,750 | INFO | Training epoch 1025, Batch 1000/1000: LR=4.86e-05, Loss=4.05e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 07:06:45,802 | INFO | Epoch 1025 Train Time 102.05028581619263s

2025-10-17 07:08:20,308 | INFO | Training epoch 1026, Batch 1000/1000: LR=4.86e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 07:08:20,358 | INFO | Epoch 1026 Train Time 94.55476427078247s

2025-10-17 07:09:51,384 | INFO | Training epoch 1027, Batch 1000/1000: LR=4.85e-05, Loss=3.99e-03 BER=1.63e-03 FER=1.43e-02
2025-10-17 07:09:51,433 | INFO | Epoch 1027 Train Time 91.0733847618103s

2025-10-17 07:11:27,519 | INFO | Training epoch 1028, Batch 1000/1000: LR=4.84e-05, Loss=4.10e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 07:11:27,582 | INFO | Epoch 1028 Train Time 96.1475031375885s

2025-10-17 07:13:07,554 | INFO | Training epoch 1029, Batch 1000/1000: LR=4.83e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 07:13:07,595 | INFO | Epoch 1029 Train Time 100.01139569282532s

2025-10-17 07:14:47,831 | INFO | Training epoch 1030, Batch 1000/1000: LR=4.82e-05, Loss=3.98e-03 BER=1.65e-03 FER=1.45e-02
2025-10-17 07:14:47,884 | INFO | Epoch 1030 Train Time 100.28822374343872s

2025-10-17 07:16:19,373 | INFO | Training epoch 1031, Batch 1000/1000: LR=4.82e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.47e-02
2025-10-17 07:16:19,423 | INFO | Epoch 1031 Train Time 91.53733396530151s

2025-10-17 07:17:51,207 | INFO | Training epoch 1032, Batch 1000/1000: LR=4.81e-05, Loss=3.98e-03 BER=1.66e-03 FER=1.41e-02
2025-10-17 07:17:51,259 | INFO | Epoch 1032 Train Time 91.83500599861145s

2025-10-17 07:19:27,317 | INFO | Training epoch 1033, Batch 1000/1000: LR=4.80e-05, Loss=4.03e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 07:19:27,365 | INFO | Epoch 1033 Train Time 96.10321235656738s

2025-10-17 07:21:05,361 | INFO | Training epoch 1034, Batch 1000/1000: LR=4.79e-05, Loss=3.98e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 07:21:05,408 | INFO | Epoch 1034 Train Time 98.04134249687195s

2025-10-17 07:22:42,412 | INFO | Training epoch 1035, Batch 1000/1000: LR=4.79e-05, Loss=4.08e-03 BER=1.69e-03 FER=1.49e-02
2025-10-17 07:22:42,464 | INFO | Epoch 1035 Train Time 97.05502963066101s

2025-10-17 07:24:26,334 | INFO | Training epoch 1036, Batch 1000/1000: LR=4.78e-05, Loss=4.19e-03 BER=1.73e-03 FER=1.50e-02
2025-10-17 07:24:26,390 | INFO | Epoch 1036 Train Time 103.92457056045532s

2025-10-17 07:26:02,087 | INFO | Training epoch 1037, Batch 1000/1000: LR=4.77e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.47e-02
2025-10-17 07:26:02,130 | INFO | Epoch 1037 Train Time 95.7377860546112s

2025-10-17 07:27:39,676 | INFO | Training epoch 1038, Batch 1000/1000: LR=4.76e-05, Loss=3.98e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 07:27:39,720 | INFO | Epoch 1038 Train Time 97.5893280506134s

2025-10-17 07:29:09,471 | INFO | Training epoch 1039, Batch 1000/1000: LR=4.75e-05, Loss=4.06e-03 BER=1.65e-03 FER=1.46e-02
2025-10-17 07:29:09,520 | INFO | Epoch 1039 Train Time 89.79910397529602s

2025-10-17 07:30:38,058 | INFO | Training epoch 1040, Batch 1000/1000: LR=4.75e-05, Loss=4.20e-03 BER=1.72e-03 FER=1.49e-02
2025-10-17 07:30:38,104 | INFO | Epoch 1040 Train Time 88.58254551887512s

2025-10-17 07:32:11,526 | INFO | Training epoch 1041, Batch 1000/1000: LR=4.74e-05, Loss=3.94e-03 BER=1.62e-03 FER=1.43e-02
2025-10-17 07:32:11,577 | INFO | Epoch 1041 Train Time 93.47138214111328s

2025-10-17 07:33:52,868 | INFO | Training epoch 1042, Batch 1000/1000: LR=4.73e-05, Loss=4.03e-03 BER=1.63e-03 FER=1.40e-02
2025-10-17 07:33:52,919 | INFO | Epoch 1042 Train Time 101.34068059921265s

2025-10-17 07:35:31,340 | INFO | Training epoch 1043, Batch 1000/1000: LR=4.72e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.51e-02
2025-10-17 07:35:31,391 | INFO | Epoch 1043 Train Time 98.47090792655945s

2025-10-17 07:37:03,518 | INFO | Training epoch 1044, Batch 1000/1000: LR=4.72e-05, Loss=4.06e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 07:37:03,567 | INFO | Epoch 1044 Train Time 92.17415452003479s

2025-10-17 07:38:37,230 | INFO | Training epoch 1045, Batch 1000/1000: LR=4.71e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 07:38:37,289 | INFO | Epoch 1045 Train Time 93.72088813781738s

2025-10-17 07:40:13,527 | INFO | Training epoch 1046, Batch 1000/1000: LR=4.70e-05, Loss=4.08e-03 BER=1.68e-03 FER=1.45e-02
2025-10-17 07:40:13,581 | INFO | Epoch 1046 Train Time 96.29058194160461s

2025-10-17 07:41:46,677 | INFO | Training epoch 1047, Batch 1000/1000: LR=4.69e-05, Loss=4.13e-03 BER=1.71e-03 FER=1.49e-02
2025-10-17 07:41:46,720 | INFO | Epoch 1047 Train Time 93.13703918457031s

2025-10-17 07:43:17,169 | INFO | Training epoch 1048, Batch 1000/1000: LR=4.68e-05, Loss=4.07e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 07:43:17,227 | INFO | Epoch 1048 Train Time 90.50622725486755s

2025-10-17 07:44:51,519 | INFO | Training epoch 1049, Batch 1000/1000: LR=4.68e-05, Loss=4.03e-03 BER=1.63e-03 FER=1.40e-02
2025-10-17 07:44:51,563 | INFO | Epoch 1049 Train Time 94.33489179611206s

2025-10-17 07:46:30,019 | INFO | Training epoch 1050, Batch 1000/1000: LR=4.67e-05, Loss=4.20e-03 BER=1.74e-03 FER=1.52e-02
2025-10-17 07:46:30,069 | INFO | Epoch 1050 Train Time 98.50476598739624s

2025-10-17 07:48:07,275 | INFO | Training epoch 1051, Batch 1000/1000: LR=4.66e-05, Loss=4.09e-03 BER=1.68e-03 FER=1.44e-02
2025-10-17 07:48:07,321 | INFO | Epoch 1051 Train Time 97.24990773200989s

2025-10-17 07:49:41,562 | INFO | Training epoch 1052, Batch 1000/1000: LR=4.65e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 07:49:41,609 | INFO | Epoch 1052 Train Time 94.28702449798584s

2025-10-17 07:51:19,102 | INFO | Training epoch 1053, Batch 1000/1000: LR=4.65e-05, Loss=3.87e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 07:51:19,166 | INFO | Epoch 1053 Train Time 97.55580759048462s

2025-10-17 07:51:19,167 | INFO | [P2] saving best_model (QAT) with loss 0.003866 at epoch 1053
2025-10-17 07:52:53,129 | INFO | Training epoch 1054, Batch 1000/1000: LR=4.64e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 07:52:53,180 | INFO | Epoch 1054 Train Time 93.90135431289673s

2025-10-17 07:54:29,675 | INFO | Training epoch 1055, Batch 1000/1000: LR=4.63e-05, Loss=3.93e-03 BER=1.62e-03 FER=1.43e-02
2025-10-17 07:54:29,726 | INFO | Epoch 1055 Train Time 96.54442620277405s

2025-10-17 07:56:09,724 | INFO | Training epoch 1056, Batch 1000/1000: LR=4.62e-05, Loss=4.13e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 07:56:09,778 | INFO | Epoch 1056 Train Time 100.0504982471466s

2025-10-17 07:57:48,000 | INFO | Training epoch 1057, Batch 1000/1000: LR=4.62e-05, Loss=3.99e-03 BER=1.63e-03 FER=1.42e-02
2025-10-17 07:57:48,048 | INFO | Epoch 1057 Train Time 98.26759934425354s

2025-10-17 07:59:29,634 | INFO | Training epoch 1058, Batch 1000/1000: LR=4.61e-05, Loss=4.06e-03 BER=1.68e-03 FER=1.45e-02
2025-10-17 07:59:29,685 | INFO | Epoch 1058 Train Time 101.63611435890198s

2025-10-17 08:01:10,226 | INFO | Training epoch 1059, Batch 1000/1000: LR=4.60e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 08:01:10,277 | INFO | Epoch 1059 Train Time 100.59092426300049s

2025-10-17 08:02:41,162 | INFO | Training epoch 1060, Batch 1000/1000: LR=4.59e-05, Loss=4.07e-03 BER=1.66e-03 FER=1.44e-02
2025-10-17 08:02:41,217 | INFO | Epoch 1060 Train Time 90.93763256072998s

2025-10-17 08:04:17,485 | INFO | Training epoch 1061, Batch 1000/1000: LR=4.58e-05, Loss=4.22e-03 BER=1.73e-03 FER=1.53e-02
2025-10-17 08:04:17,541 | INFO | Epoch 1061 Train Time 96.32308053970337s

2025-10-17 08:05:45,781 | INFO | Training epoch 1062, Batch 1000/1000: LR=4.58e-05, Loss=4.29e-03 BER=1.76e-03 FER=1.53e-02
2025-10-17 08:05:45,836 | INFO | Epoch 1062 Train Time 88.29218435287476s

2025-10-17 08:07:16,730 | INFO | Training epoch 1063, Batch 1000/1000: LR=4.57e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 08:07:16,785 | INFO | Epoch 1063 Train Time 90.94806480407715s

2025-10-17 08:08:49,443 | INFO | Training epoch 1064, Batch 1000/1000: LR=4.56e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 08:08:49,486 | INFO | Epoch 1064 Train Time 92.69821763038635s

2025-10-17 08:10:17,738 | INFO | Training epoch 1065, Batch 1000/1000: LR=4.55e-05, Loss=4.06e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 08:10:17,783 | INFO | Epoch 1065 Train Time 88.29615068435669s

2025-10-17 08:11:50,372 | INFO | Training epoch 1066, Batch 1000/1000: LR=4.55e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.43e-02
2025-10-17 08:11:50,416 | INFO | Epoch 1066 Train Time 92.63179993629456s

2025-10-17 08:13:18,900 | INFO | Training epoch 1067, Batch 1000/1000: LR=4.54e-05, Loss=3.82e-03 BER=1.55e-03 FER=1.37e-02
2025-10-17 08:13:18,947 | INFO | Epoch 1067 Train Time 88.5287983417511s

2025-10-17 08:13:18,948 | INFO | [P2] saving best_model (QAT) with loss 0.003819 at epoch 1067
2025-10-17 08:14:49,774 | INFO | Training epoch 1068, Batch 1000/1000: LR=4.53e-05, Loss=4.17e-03 BER=1.71e-03 FER=1.50e-02
2025-10-17 08:14:49,823 | INFO | Epoch 1068 Train Time 90.77393960952759s

2025-10-17 08:16:24,883 | INFO | Training epoch 1069, Batch 1000/1000: LR=4.52e-05, Loss=4.12e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 08:16:24,926 | INFO | Epoch 1069 Train Time 95.10103273391724s

2025-10-17 08:18:07,827 | INFO | Training epoch 1070, Batch 1000/1000: LR=4.51e-05, Loss=3.98e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 08:18:07,887 | INFO | Epoch 1070 Train Time 102.95896458625793s

2025-10-17 08:19:43,636 | INFO | Training epoch 1071, Batch 1000/1000: LR=4.51e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.41e-02
2025-10-17 08:19:43,681 | INFO | Epoch 1071 Train Time 95.79291701316833s

2025-10-17 08:21:20,133 | INFO | Training epoch 1072, Batch 1000/1000: LR=4.50e-05, Loss=4.12e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 08:21:20,187 | INFO | Epoch 1072 Train Time 96.50372982025146s

2025-10-17 08:22:51,545 | INFO | Training epoch 1073, Batch 1000/1000: LR=4.49e-05, Loss=4.00e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 08:22:51,597 | INFO | Epoch 1073 Train Time 91.40858960151672s

2025-10-17 08:24:26,849 | INFO | Training epoch 1074, Batch 1000/1000: LR=4.48e-05, Loss=4.01e-03 BER=1.67e-03 FER=1.46e-02
2025-10-17 08:24:26,896 | INFO | Epoch 1074 Train Time 95.29777574539185s

2025-10-17 08:26:05,650 | INFO | Training epoch 1075, Batch 1000/1000: LR=4.48e-05, Loss=4.26e-03 BER=1.77e-03 FER=1.53e-02
2025-10-17 08:26:05,707 | INFO | Epoch 1075 Train Time 98.80978059768677s

2025-10-17 08:27:48,176 | INFO | Training epoch 1076, Batch 1000/1000: LR=4.47e-05, Loss=4.09e-03 BER=1.66e-03 FER=1.43e-02
2025-10-17 08:27:48,238 | INFO | Epoch 1076 Train Time 102.52855181694031s

2025-10-17 08:29:24,014 | INFO | Training epoch 1077, Batch 1000/1000: LR=4.46e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 08:29:24,072 | INFO | Epoch 1077 Train Time 95.83189940452576s

2025-10-17 08:30:52,248 | INFO | Training epoch 1078, Batch 1000/1000: LR=4.45e-05, Loss=4.01e-03 BER=1.68e-03 FER=1.45e-02
2025-10-17 08:30:52,290 | INFO | Epoch 1078 Train Time 88.21738624572754s

2025-10-17 08:32:28,849 | INFO | Training epoch 1079, Batch 1000/1000: LR=4.45e-05, Loss=4.13e-03 BER=1.72e-03 FER=1.48e-02
2025-10-17 08:32:28,905 | INFO | Epoch 1079 Train Time 96.61375117301941s

2025-10-17 08:34:14,242 | INFO | Training epoch 1080, Batch 1000/1000: LR=4.44e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 08:34:14,281 | INFO | Epoch 1080 Train Time 105.37479615211487s

2025-10-17 08:35:49,445 | INFO | Training epoch 1081, Batch 1000/1000: LR=4.43e-05, Loss=4.09e-03 BER=1.71e-03 FER=1.49e-02
2025-10-17 08:35:49,500 | INFO | Epoch 1081 Train Time 95.2180597782135s

2025-10-17 08:37:24,681 | INFO | Training epoch 1082, Batch 1000/1000: LR=4.42e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.44e-02
2025-10-17 08:37:24,740 | INFO | Epoch 1082 Train Time 95.2387626171112s

2025-10-17 08:39:00,852 | INFO | Training epoch 1083, Batch 1000/1000: LR=4.41e-05, Loss=4.03e-03 BER=1.63e-03 FER=1.45e-02
2025-10-17 08:39:00,894 | INFO | Epoch 1083 Train Time 96.15207839012146s

2025-10-17 08:40:34,705 | INFO | Training epoch 1084, Batch 1000/1000: LR=4.41e-05, Loss=4.09e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 08:40:34,764 | INFO | Epoch 1084 Train Time 93.86851358413696s

2025-10-17 08:42:10,664 | INFO | Training epoch 1085, Batch 1000/1000: LR=4.40e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.40e-02
2025-10-17 08:42:10,703 | INFO | Epoch 1085 Train Time 95.93736338615417s

2025-10-17 08:43:50,168 | INFO | Training epoch 1086, Batch 1000/1000: LR=4.39e-05, Loss=3.97e-03 BER=1.61e-03 FER=1.41e-02
2025-10-17 08:43:50,211 | INFO | Epoch 1086 Train Time 99.50785231590271s

2025-10-17 08:45:20,007 | INFO | Training epoch 1087, Batch 1000/1000: LR=4.38e-05, Loss=4.01e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 08:45:20,055 | INFO | Epoch 1087 Train Time 89.84294247627258s

2025-10-17 08:46:53,690 | INFO | Training epoch 1088, Batch 1000/1000: LR=4.38e-05, Loss=3.94e-03 BER=1.61e-03 FER=1.42e-02
2025-10-17 08:46:53,748 | INFO | Epoch 1088 Train Time 93.69071292877197s

2025-10-17 08:48:31,428 | INFO | Training epoch 1089, Batch 1000/1000: LR=4.37e-05, Loss=4.09e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 08:48:31,490 | INFO | Epoch 1089 Train Time 97.74165487289429s

2025-10-17 08:50:05,458 | INFO | Training epoch 1090, Batch 1000/1000: LR=4.36e-05, Loss=4.10e-03 BER=1.68e-03 FER=1.45e-02
2025-10-17 08:50:05,514 | INFO | Epoch 1090 Train Time 94.02160000801086s

2025-10-17 08:51:46,437 | INFO | Training epoch 1091, Batch 1000/1000: LR=4.35e-05, Loss=4.03e-03 BER=1.68e-03 FER=1.46e-02
2025-10-17 08:51:46,497 | INFO | Epoch 1091 Train Time 100.98159146308899s

2025-10-17 08:53:34,040 | INFO | Training epoch 1092, Batch 1000/1000: LR=4.34e-05, Loss=3.97e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 08:53:34,088 | INFO | Epoch 1092 Train Time 107.59012484550476s

2025-10-17 08:55:08,183 | INFO | Training epoch 1093, Batch 1000/1000: LR=4.34e-05, Loss=4.17e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 08:55:08,237 | INFO | Epoch 1093 Train Time 94.14635705947876s

2025-10-17 08:56:42,407 | INFO | Training epoch 1094, Batch 1000/1000: LR=4.33e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 08:56:42,475 | INFO | Epoch 1094 Train Time 94.23689126968384s

2025-10-17 08:58:17,655 | INFO | Training epoch 1095, Batch 1000/1000: LR=4.32e-05, Loss=4.07e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 08:58:17,710 | INFO | Epoch 1095 Train Time 95.23319482803345s

2025-10-17 08:59:51,402 | INFO | Training epoch 1096, Batch 1000/1000: LR=4.31e-05, Loss=4.12e-03 BER=1.70e-03 FER=1.48e-02
2025-10-17 08:59:51,458 | INFO | Epoch 1096 Train Time 93.74621987342834s

2025-10-17 09:01:31,472 | INFO | Training epoch 1097, Batch 1000/1000: LR=4.31e-05, Loss=4.16e-03 BER=1.69e-03 FER=1.49e-02
2025-10-17 09:01:31,515 | INFO | Epoch 1097 Train Time 100.05551862716675s

2025-10-17 09:03:11,255 | INFO | Training epoch 1098, Batch 1000/1000: LR=4.30e-05, Loss=3.92e-03 BER=1.60e-03 FER=1.42e-02
2025-10-17 09:03:11,317 | INFO | Epoch 1098 Train Time 99.80092334747314s

2025-10-17 09:04:47,115 | INFO | Training epoch 1099, Batch 1000/1000: LR=4.29e-05, Loss=4.01e-03 BER=1.63e-03 FER=1.44e-02
2025-10-17 09:04:47,180 | INFO | Epoch 1099 Train Time 95.8618311882019s

2025-10-17 09:06:24,703 | INFO | Training epoch 1100, Batch 1000/1000: LR=4.28e-05, Loss=4.09e-03 BER=1.67e-03 FER=1.47e-02
2025-10-17 09:06:24,766 | INFO | Epoch 1100 Train Time 97.58536267280579s

2025-10-17 09:08:10,139 | INFO | Training epoch 1101, Batch 1000/1000: LR=4.28e-05, Loss=4.08e-03 BER=1.68e-03 FER=1.47e-02
2025-10-17 09:08:10,175 | INFO | Epoch 1101 Train Time 105.40761542320251s

2025-10-17 09:09:42,435 | INFO | Training epoch 1102, Batch 1000/1000: LR=4.27e-05, Loss=4.05e-03 BER=1.66e-03 FER=1.44e-02
2025-10-17 09:09:42,486 | INFO | Epoch 1102 Train Time 92.3096387386322s

2025-10-17 09:11:22,050 | INFO | Training epoch 1103, Batch 1000/1000: LR=4.26e-05, Loss=4.25e-03 BER=1.74e-03 FER=1.51e-02
2025-10-17 09:11:22,106 | INFO | Epoch 1103 Train Time 99.61925649642944s

2025-10-17 09:12:54,037 | INFO | Training epoch 1104, Batch 1000/1000: LR=4.25e-05, Loss=4.05e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 09:12:54,083 | INFO | Epoch 1104 Train Time 91.97448134422302s

2025-10-17 09:14:31,157 | INFO | Training epoch 1105, Batch 1000/1000: LR=4.24e-05, Loss=3.87e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 09:14:31,204 | INFO | Epoch 1105 Train Time 97.11981773376465s

2025-10-17 09:16:01,850 | INFO | Training epoch 1106, Batch 1000/1000: LR=4.24e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 09:16:01,891 | INFO | Epoch 1106 Train Time 90.68565344810486s

2025-10-17 09:17:32,345 | INFO | Training epoch 1107, Batch 1000/1000: LR=4.23e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.45e-02
2025-10-17 09:17:32,413 | INFO | Epoch 1107 Train Time 90.52058601379395s

2025-10-17 09:19:04,137 | INFO | Training epoch 1108, Batch 1000/1000: LR=4.22e-05, Loss=3.94e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 09:19:04,202 | INFO | Epoch 1108 Train Time 91.78810143470764s

2025-10-17 09:20:41,802 | INFO | Training epoch 1109, Batch 1000/1000: LR=4.21e-05, Loss=3.96e-03 BER=1.66e-03 FER=1.44e-02
2025-10-17 09:20:41,865 | INFO | Epoch 1109 Train Time 97.66087055206299s

2025-10-17 09:22:22,828 | INFO | Training epoch 1110, Batch 1000/1000: LR=4.21e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.45e-02
2025-10-17 09:22:22,876 | INFO | Epoch 1110 Train Time 101.00927329063416s

2025-10-17 09:23:58,062 | INFO | Training epoch 1111, Batch 1000/1000: LR=4.20e-05, Loss=3.96e-03 BER=1.61e-03 FER=1.43e-02
2025-10-17 09:23:58,109 | INFO | Epoch 1111 Train Time 95.2320327758789s

2025-10-17 09:25:26,042 | INFO | Training epoch 1112, Batch 1000/1000: LR=4.19e-05, Loss=4.06e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 09:25:26,105 | INFO | Epoch 1112 Train Time 87.99328398704529s

2025-10-17 09:26:55,936 | INFO | Training epoch 1113, Batch 1000/1000: LR=4.18e-05, Loss=4.12e-03 BER=1.71e-03 FER=1.52e-02
2025-10-17 09:26:55,986 | INFO | Epoch 1113 Train Time 89.88019466400146s

2025-10-17 09:28:28,952 | INFO | Training epoch 1114, Batch 1000/1000: LR=4.18e-05, Loss=3.94e-03 BER=1.61e-03 FER=1.41e-02
2025-10-17 09:28:28,997 | INFO | Epoch 1114 Train Time 93.00857758522034s

2025-10-17 09:30:04,686 | INFO | Training epoch 1115, Batch 1000/1000: LR=4.17e-05, Loss=4.13e-03 BER=1.70e-03 FER=1.46e-02
2025-10-17 09:30:04,748 | INFO | Epoch 1115 Train Time 95.74903607368469s

2025-10-17 09:31:30,763 | INFO | Training epoch 1116, Batch 1000/1000: LR=4.16e-05, Loss=3.88e-03 BER=1.62e-03 FER=1.38e-02
2025-10-17 09:31:30,805 | INFO | Epoch 1116 Train Time 86.05575966835022s

2025-10-17 09:33:13,351 | INFO | Training epoch 1117, Batch 1000/1000: LR=4.15e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.41e-02
2025-10-17 09:33:13,403 | INFO | Epoch 1117 Train Time 102.59622502326965s

2025-10-17 09:34:51,137 | INFO | Training epoch 1118, Batch 1000/1000: LR=4.15e-05, Loss=4.10e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 09:34:51,190 | INFO | Epoch 1118 Train Time 97.78439712524414s

2025-10-17 09:36:33,360 | INFO | Training epoch 1119, Batch 1000/1000: LR=4.14e-05, Loss=3.89e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 09:36:33,410 | INFO | Epoch 1119 Train Time 102.21908187866211s

2025-10-17 09:38:01,786 | INFO | Training epoch 1120, Batch 1000/1000: LR=4.13e-05, Loss=4.12e-03 BER=1.71e-03 FER=1.47e-02
2025-10-17 09:38:01,840 | INFO | Epoch 1120 Train Time 88.42868566513062s

2025-10-17 09:39:38,534 | INFO | Training epoch 1121, Batch 1000/1000: LR=4.12e-05, Loss=3.88e-03 BER=1.61e-03 FER=1.39e-02
2025-10-17 09:39:38,583 | INFO | Epoch 1121 Train Time 96.74150156974792s

2025-10-17 09:41:14,285 | INFO | Training epoch 1122, Batch 1000/1000: LR=4.11e-05, Loss=3.99e-03 BER=1.67e-03 FER=1.43e-02
2025-10-17 09:41:14,341 | INFO | Epoch 1122 Train Time 95.75567650794983s

2025-10-17 09:42:54,638 | INFO | Training epoch 1123, Batch 1000/1000: LR=4.11e-05, Loss=4.13e-03 BER=1.70e-03 FER=1.47e-02
2025-10-17 09:42:54,684 | INFO | Epoch 1123 Train Time 100.34095859527588s

2025-10-17 09:44:23,472 | INFO | Training epoch 1124, Batch 1000/1000: LR=4.10e-05, Loss=3.95e-03 BER=1.61e-03 FER=1.41e-02
2025-10-17 09:44:23,530 | INFO | Epoch 1124 Train Time 88.84445929527283s

2025-10-17 09:45:56,772 | INFO | Training epoch 1125, Batch 1000/1000: LR=4.09e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 09:45:56,821 | INFO | Epoch 1125 Train Time 93.29019093513489s

2025-10-17 09:47:38,296 | INFO | Training epoch 1126, Batch 1000/1000: LR=4.08e-05, Loss=4.08e-03 BER=1.67e-03 FER=1.47e-02
2025-10-17 09:47:38,349 | INFO | Epoch 1126 Train Time 101.52659678459167s

2025-10-17 09:49:20,595 | INFO | Training epoch 1127, Batch 1000/1000: LR=4.08e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 09:49:20,652 | INFO | Epoch 1127 Train Time 102.30176544189453s

2025-10-17 09:50:58,441 | INFO | Training epoch 1128, Batch 1000/1000: LR=4.07e-05, Loss=3.93e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 09:50:58,486 | INFO | Epoch 1128 Train Time 97.83257699012756s

2025-10-17 09:52:24,858 | INFO | Training epoch 1129, Batch 1000/1000: LR=4.06e-05, Loss=4.18e-03 BER=1.72e-03 FER=1.49e-02
2025-10-17 09:52:24,910 | INFO | Epoch 1129 Train Time 86.42239594459534s

2025-10-17 09:54:02,055 | INFO | Training epoch 1130, Batch 1000/1000: LR=4.05e-05, Loss=4.06e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 09:54:02,116 | INFO | Epoch 1130 Train Time 97.20381259918213s

2025-10-17 09:55:35,902 | INFO | Training epoch 1131, Batch 1000/1000: LR=4.05e-05, Loss=4.13e-03 BER=1.71e-03 FER=1.47e-02
2025-10-17 09:55:35,963 | INFO | Epoch 1131 Train Time 93.84589433670044s

2025-10-17 09:57:09,536 | INFO | Training epoch 1132, Batch 1000/1000: LR=4.04e-05, Loss=4.22e-03 BER=1.78e-03 FER=1.51e-02
2025-10-17 09:57:09,595 | INFO | Epoch 1132 Train Time 93.6311993598938s

2025-10-17 09:58:44,712 | INFO | Training epoch 1133, Batch 1000/1000: LR=4.03e-05, Loss=4.08e-03 BER=1.68e-03 FER=1.46e-02
2025-10-17 09:58:44,761 | INFO | Epoch 1133 Train Time 95.16360688209534s

2025-10-17 10:00:17,380 | INFO | Training epoch 1134, Batch 1000/1000: LR=4.02e-05, Loss=4.12e-03 BER=1.68e-03 FER=1.44e-02
2025-10-17 10:00:17,442 | INFO | Epoch 1134 Train Time 92.6785569190979s

2025-10-17 10:01:51,888 | INFO | Training epoch 1135, Batch 1000/1000: LR=4.02e-05, Loss=3.84e-03 BER=1.58e-03 FER=1.36e-02
2025-10-17 10:01:51,970 | INFO | Epoch 1135 Train Time 94.52650475502014s

2025-10-17 10:03:25,760 | INFO | Training epoch 1136, Batch 1000/1000: LR=4.01e-05, Loss=4.14e-03 BER=1.68e-03 FER=1.46e-02
2025-10-17 10:03:25,803 | INFO | Epoch 1136 Train Time 93.83126449584961s

2025-10-17 10:05:00,638 | INFO | Training epoch 1137, Batch 1000/1000: LR=4.00e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.45e-02
2025-10-17 10:05:00,688 | INFO | Epoch 1137 Train Time 94.8836350440979s

2025-10-17 10:06:34,832 | INFO | Training epoch 1138, Batch 1000/1000: LR=3.99e-05, Loss=4.08e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 10:06:34,878 | INFO | Epoch 1138 Train Time 94.18852162361145s

2025-10-17 10:08:16,192 | INFO | Training epoch 1139, Batch 1000/1000: LR=3.99e-05, Loss=4.01e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 10:08:16,245 | INFO | Epoch 1139 Train Time 101.36608457565308s

2025-10-17 10:09:51,202 | INFO | Training epoch 1140, Batch 1000/1000: LR=3.98e-05, Loss=3.97e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 10:09:51,247 | INFO | Epoch 1140 Train Time 94.99987244606018s

2025-10-17 10:11:20,063 | INFO | Training epoch 1141, Batch 1000/1000: LR=3.97e-05, Loss=4.00e-03 BER=1.66e-03 FER=1.41e-02
2025-10-17 10:11:20,115 | INFO | Epoch 1141 Train Time 88.86642980575562s

2025-10-17 10:12:59,271 | INFO | Training epoch 1142, Batch 1000/1000: LR=3.96e-05, Loss=4.03e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 10:12:59,337 | INFO | Epoch 1142 Train Time 99.22161531448364s

2025-10-17 10:14:45,187 | INFO | Training epoch 1143, Batch 1000/1000: LR=3.96e-05, Loss=4.00e-03 BER=1.61e-03 FER=1.41e-02
2025-10-17 10:14:45,248 | INFO | Epoch 1143 Train Time 105.90856218338013s

2025-10-17 10:16:18,026 | INFO | Training epoch 1144, Batch 1000/1000: LR=3.95e-05, Loss=3.99e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 10:16:18,079 | INFO | Epoch 1144 Train Time 92.82897543907166s

2025-10-17 10:17:46,499 | INFO | Training epoch 1145, Batch 1000/1000: LR=3.94e-05, Loss=4.09e-03 BER=1.70e-03 FER=1.50e-02
2025-10-17 10:17:46,550 | INFO | Epoch 1145 Train Time 88.4690248966217s

2025-10-17 10:19:26,943 | INFO | Training epoch 1146, Batch 1000/1000: LR=3.93e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.46e-02
2025-10-17 10:19:27,000 | INFO | Epoch 1146 Train Time 100.44801568984985s

2025-10-17 10:21:05,753 | INFO | Training epoch 1147, Batch 1000/1000: LR=3.92e-05, Loss=4.14e-03 BER=1.71e-03 FER=1.48e-02
2025-10-17 10:21:05,808 | INFO | Epoch 1147 Train Time 98.80636382102966s

2025-10-17 10:22:39,622 | INFO | Training epoch 1148, Batch 1000/1000: LR=3.92e-05, Loss=4.16e-03 BER=1.70e-03 FER=1.50e-02
2025-10-17 10:22:39,683 | INFO | Epoch 1148 Train Time 93.87351107597351s

2025-10-17 10:24:19,709 | INFO | Training epoch 1149, Batch 1000/1000: LR=3.91e-05, Loss=3.92e-03 BER=1.61e-03 FER=1.40e-02
2025-10-17 10:24:19,766 | INFO | Epoch 1149 Train Time 100.08158993721008s

2025-10-17 10:26:05,250 | INFO | Training epoch 1150, Batch 1000/1000: LR=3.90e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 10:26:05,306 | INFO | Epoch 1150 Train Time 105.53782868385315s

2025-10-17 10:27:37,360 | INFO | Training epoch 1151, Batch 1000/1000: LR=3.89e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.46e-02
2025-10-17 10:27:37,406 | INFO | Epoch 1151 Train Time 92.0991518497467s

2025-10-17 10:29:13,008 | INFO | Training epoch 1152, Batch 1000/1000: LR=3.89e-05, Loss=4.06e-03 BER=1.65e-03 FER=1.46e-02
2025-10-17 10:29:13,081 | INFO | Epoch 1152 Train Time 95.67249369621277s

2025-10-17 10:30:52,938 | INFO | Training epoch 1153, Batch 1000/1000: LR=3.88e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.38e-02
2025-10-17 10:30:53,000 | INFO | Epoch 1153 Train Time 99.91712093353271s

2025-10-17 10:30:53,000 | INFO | [P2] saving best_model (QAT) with loss 0.003812 at epoch 1153
2025-10-17 10:32:30,037 | INFO | Training epoch 1154, Batch 1000/1000: LR=3.87e-05, Loss=3.91e-03 BER=1.58e-03 FER=1.39e-02
2025-10-17 10:32:30,097 | INFO | Epoch 1154 Train Time 96.98761773109436s

2025-10-17 10:34:13,728 | INFO | Training epoch 1155, Batch 1000/1000: LR=3.86e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.40e-02
2025-10-17 10:34:13,788 | INFO | Epoch 1155 Train Time 103.68933296203613s

2025-10-17 10:35:44,235 | INFO | Training epoch 1156, Batch 1000/1000: LR=3.86e-05, Loss=3.93e-03 BER=1.63e-03 FER=1.44e-02
2025-10-17 10:35:44,328 | INFO | Epoch 1156 Train Time 90.53853058815002s

2025-10-17 10:37:21,241 | INFO | Training epoch 1157, Batch 1000/1000: LR=3.85e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.45e-02
2025-10-17 10:37:21,314 | INFO | Epoch 1157 Train Time 96.98344206809998s

2025-10-17 10:38:56,441 | INFO | Training epoch 1158, Batch 1000/1000: LR=3.84e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.44e-02
2025-10-17 10:38:56,486 | INFO | Epoch 1158 Train Time 95.17053604125977s

2025-10-17 10:40:29,205 | INFO | Training epoch 1159, Batch 1000/1000: LR=3.83e-05, Loss=3.96e-03 BER=1.61e-03 FER=1.42e-02
2025-10-17 10:40:29,278 | INFO | Epoch 1159 Train Time 92.79183554649353s

2025-10-17 10:41:58,350 | INFO | Training epoch 1160, Batch 1000/1000: LR=3.83e-05, Loss=3.98e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 10:41:58,395 | INFO | Epoch 1160 Train Time 89.11560153961182s

2025-10-17 10:43:39,159 | INFO | Training epoch 1161, Batch 1000/1000: LR=3.82e-05, Loss=4.10e-03 BER=1.69e-03 FER=1.45e-02
2025-10-17 10:43:39,203 | INFO | Epoch 1161 Train Time 100.80601739883423s

2025-10-17 10:45:19,471 | INFO | Training epoch 1162, Batch 1000/1000: LR=3.81e-05, Loss=4.06e-03 BER=1.64e-03 FER=1.42e-02
2025-10-17 10:45:19,508 | INFO | Epoch 1162 Train Time 100.30376410484314s

2025-10-17 10:46:52,270 | INFO | Training epoch 1163, Batch 1000/1000: LR=3.80e-05, Loss=3.99e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 10:46:52,314 | INFO | Epoch 1163 Train Time 92.80456948280334s

2025-10-17 10:48:26,217 | INFO | Training epoch 1164, Batch 1000/1000: LR=3.80e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 10:48:26,271 | INFO | Epoch 1164 Train Time 93.95569610595703s

2025-10-17 10:49:53,815 | INFO | Training epoch 1165, Batch 1000/1000: LR=3.79e-05, Loss=3.95e-03 BER=1.61e-03 FER=1.43e-02
2025-10-17 10:49:53,866 | INFO | Epoch 1165 Train Time 87.59414839744568s

2025-10-17 10:51:31,616 | INFO | Training epoch 1166, Batch 1000/1000: LR=3.78e-05, Loss=4.26e-03 BER=1.77e-03 FER=1.52e-02
2025-10-17 10:51:31,677 | INFO | Epoch 1166 Train Time 97.80833268165588s

2025-10-17 10:53:03,544 | INFO | Training epoch 1167, Batch 1000/1000: LR=3.77e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.43e-02
2025-10-17 10:53:03,590 | INFO | Epoch 1167 Train Time 91.91141510009766s

2025-10-17 10:54:34,651 | INFO | Training epoch 1168, Batch 1000/1000: LR=3.77e-05, Loss=3.91e-03 BER=1.61e-03 FER=1.39e-02
2025-10-17 10:54:34,703 | INFO | Epoch 1168 Train Time 91.11306691169739s

2025-10-17 10:56:05,930 | INFO | Training epoch 1169, Batch 1000/1000: LR=3.76e-05, Loss=3.93e-03 BER=1.62e-03 FER=1.42e-02
2025-10-17 10:56:05,977 | INFO | Epoch 1169 Train Time 91.27274990081787s

2025-10-17 10:57:32,605 | INFO | Training epoch 1170, Batch 1000/1000: LR=3.75e-05, Loss=3.97e-03 BER=1.62e-03 FER=1.42e-02
2025-10-17 10:57:32,651 | INFO | Epoch 1170 Train Time 86.67267990112305s

2025-10-17 10:59:07,769 | INFO | Training epoch 1171, Batch 1000/1000: LR=3.74e-05, Loss=3.97e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 10:59:07,809 | INFO | Epoch 1171 Train Time 95.15747261047363s

2025-10-17 11:00:44,035 | INFO | Training epoch 1172, Batch 1000/1000: LR=3.74e-05, Loss=4.16e-03 BER=1.69e-03 FER=1.47e-02
2025-10-17 11:00:44,110 | INFO | Epoch 1172 Train Time 96.29989814758301s

2025-10-17 11:02:23,341 | INFO | Training epoch 1173, Batch 1000/1000: LR=3.73e-05, Loss=4.19e-03 BER=1.73e-03 FER=1.50e-02
2025-10-17 11:02:23,395 | INFO | Epoch 1173 Train Time 99.2832682132721s

2025-10-17 11:03:57,347 | INFO | Training epoch 1174, Batch 1000/1000: LR=3.72e-05, Loss=4.07e-03 BER=1.68e-03 FER=1.45e-02
2025-10-17 11:03:57,405 | INFO | Epoch 1174 Train Time 94.00776886940002s

2025-10-17 11:05:37,784 | INFO | Training epoch 1175, Batch 1000/1000: LR=3.71e-05, Loss=4.13e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 11:05:37,844 | INFO | Epoch 1175 Train Time 100.43770241737366s

2025-10-17 11:07:07,745 | INFO | Training epoch 1176, Batch 1000/1000: LR=3.71e-05, Loss=4.02e-03 BER=1.63e-03 FER=1.44e-02
2025-10-17 11:07:07,807 | INFO | Epoch 1176 Train Time 89.961110830307s

2025-10-17 11:08:39,197 | INFO | Training epoch 1177, Batch 1000/1000: LR=3.70e-05, Loss=3.96e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 11:08:39,251 | INFO | Epoch 1177 Train Time 91.44165802001953s

2025-10-17 11:10:18,287 | INFO | Training epoch 1178, Batch 1000/1000: LR=3.69e-05, Loss=4.05e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 11:10:18,333 | INFO | Epoch 1178 Train Time 99.08081221580505s

2025-10-17 11:11:48,005 | INFO | Training epoch 1179, Batch 1000/1000: LR=3.68e-05, Loss=4.16e-03 BER=1.70e-03 FER=1.49e-02
2025-10-17 11:11:48,057 | INFO | Epoch 1179 Train Time 89.72249364852905s

2025-10-17 11:13:30,441 | INFO | Training epoch 1180, Batch 1000/1000: LR=3.68e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 11:13:30,520 | INFO | Epoch 1180 Train Time 102.46139121055603s

2025-10-17 11:14:56,911 | INFO | Training epoch 1181, Batch 1000/1000: LR=3.67e-05, Loss=3.99e-03 BER=1.65e-03 FER=1.41e-02
2025-10-17 11:14:56,955 | INFO | Epoch 1181 Train Time 86.43330311775208s

2025-10-17 11:16:33,302 | INFO | Training epoch 1182, Batch 1000/1000: LR=3.66e-05, Loss=4.02e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 11:16:33,356 | INFO | Epoch 1182 Train Time 96.39999961853027s

2025-10-17 11:18:03,809 | INFO | Training epoch 1183, Batch 1000/1000: LR=3.65e-05, Loss=3.97e-03 BER=1.63e-03 FER=1.44e-02
2025-10-17 11:18:03,863 | INFO | Epoch 1183 Train Time 90.50556015968323s

2025-10-17 11:19:37,852 | INFO | Training epoch 1184, Batch 1000/1000: LR=3.65e-05, Loss=4.20e-03 BER=1.74e-03 FER=1.50e-02
2025-10-17 11:19:37,917 | INFO | Epoch 1184 Train Time 94.05196619033813s

2025-10-17 11:21:19,166 | INFO | Training epoch 1185, Batch 1000/1000: LR=3.64e-05, Loss=3.93e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 11:21:19,227 | INFO | Epoch 1185 Train Time 101.30815100669861s

2025-10-17 11:23:00,111 | INFO | Training epoch 1186, Batch 1000/1000: LR=3.63e-05, Loss=4.02e-03 BER=1.62e-03 FER=1.43e-02
2025-10-17 11:23:00,164 | INFO | Epoch 1186 Train Time 100.93496227264404s

2025-10-17 11:24:34,129 | INFO | Training epoch 1187, Batch 1000/1000: LR=3.62e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 11:24:34,205 | INFO | Epoch 1187 Train Time 94.04014945030212s

2025-10-17 11:26:14,686 | INFO | Training epoch 1188, Batch 1000/1000: LR=3.62e-05, Loss=3.87e-03 BER=1.57e-03 FER=1.38e-02
2025-10-17 11:26:14,737 | INFO | Epoch 1188 Train Time 100.52995777130127s

2025-10-17 11:28:00,530 | INFO | Training epoch 1189, Batch 1000/1000: LR=3.61e-05, Loss=3.86e-03 BER=1.62e-03 FER=1.38e-02
2025-10-17 11:28:00,573 | INFO | Epoch 1189 Train Time 105.8347225189209s

2025-10-17 11:29:45,746 | INFO | Training epoch 1190, Batch 1000/1000: LR=3.60e-05, Loss=3.97e-03 BER=1.65e-03 FER=1.40e-02
2025-10-17 11:29:45,813 | INFO | Epoch 1190 Train Time 105.23824381828308s

2025-10-17 11:31:27,218 | INFO | Training epoch 1191, Batch 1000/1000: LR=3.59e-05, Loss=3.83e-03 BER=1.59e-03 FER=1.41e-02
2025-10-17 11:31:27,288 | INFO | Epoch 1191 Train Time 101.47368049621582s

2025-10-17 11:33:11,746 | INFO | Training epoch 1192, Batch 1000/1000: LR=3.59e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 11:33:11,799 | INFO | Epoch 1192 Train Time 104.50824427604675s

2025-10-17 11:33:11,799 | INFO | [P2] saving best_model (QAT) with loss 0.003761 at epoch 1192
2025-10-17 11:34:49,123 | INFO | Training epoch 1193, Batch 1000/1000: LR=3.58e-05, Loss=3.87e-03 BER=1.58e-03 FER=1.38e-02
2025-10-17 11:34:49,176 | INFO | Epoch 1193 Train Time 97.28828287124634s

2025-10-17 11:36:20,347 | INFO | Training epoch 1194, Batch 1000/1000: LR=3.57e-05, Loss=4.01e-03 BER=1.66e-03 FER=1.45e-02
2025-10-17 11:36:20,411 | INFO | Epoch 1194 Train Time 91.23298716545105s

2025-10-17 11:37:52,511 | INFO | Training epoch 1195, Batch 1000/1000: LR=3.56e-05, Loss=3.90e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 11:37:52,560 | INFO | Epoch 1195 Train Time 92.14736008644104s

2025-10-17 11:39:28,763 | INFO | Training epoch 1196, Batch 1000/1000: LR=3.56e-05, Loss=4.07e-03 BER=1.69e-03 FER=1.46e-02
2025-10-17 11:39:28,805 | INFO | Epoch 1196 Train Time 96.24350643157959s

2025-10-17 11:41:04,691 | INFO | Training epoch 1197, Batch 1000/1000: LR=3.55e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 11:41:04,744 | INFO | Epoch 1197 Train Time 95.93727040290833s

2025-10-17 11:42:40,308 | INFO | Training epoch 1198, Batch 1000/1000: LR=3.54e-05, Loss=3.92e-03 BER=1.62e-03 FER=1.43e-02
2025-10-17 11:42:40,360 | INFO | Epoch 1198 Train Time 95.61507391929626s

2025-10-17 11:44:19,750 | INFO | Training epoch 1199, Batch 1000/1000: LR=3.54e-05, Loss=3.78e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 11:44:19,798 | INFO | Epoch 1199 Train Time 99.43682551383972s

2025-10-17 11:46:04,477 | INFO | Training epoch 1200, Batch 1000/1000: LR=3.53e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.43e-02
2025-10-17 11:46:04,524 | INFO | Epoch 1200 Train Time 104.7220287322998s

2025-10-17 11:47:44,552 | INFO | Training epoch 1201, Batch 1000/1000: LR=3.52e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 11:47:44,605 | INFO | Epoch 1201 Train Time 100.07973432540894s

2025-10-17 11:47:44,606 | INFO | [P2] saving best_model (QAT) with loss 0.003761 at epoch 1201
2025-10-17 11:49:25,122 | INFO | Training epoch 1202, Batch 1000/1000: LR=3.51e-05, Loss=4.02e-03 BER=1.67e-03 FER=1.45e-02
2025-10-17 11:49:25,167 | INFO | Epoch 1202 Train Time 100.48666739463806s

2025-10-17 11:51:02,411 | INFO | Training epoch 1203, Batch 1000/1000: LR=3.51e-05, Loss=3.96e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 11:51:02,468 | INFO | Epoch 1203 Train Time 97.29987859725952s

2025-10-17 11:52:38,523 | INFO | Training epoch 1204, Batch 1000/1000: LR=3.50e-05, Loss=4.02e-03 BER=1.66e-03 FER=1.44e-02
2025-10-17 11:52:38,582 | INFO | Epoch 1204 Train Time 96.1117193698883s

2025-10-17 11:54:16,641 | INFO | Training epoch 1205, Batch 1000/1000: LR=3.49e-05, Loss=4.07e-03 BER=1.65e-03 FER=1.46e-02
2025-10-17 11:54:16,690 | INFO | Epoch 1205 Train Time 98.10658979415894s

2025-10-17 11:55:49,211 | INFO | Training epoch 1206, Batch 1000/1000: LR=3.48e-05, Loss=4.01e-03 BER=1.63e-03 FER=1.42e-02
2025-10-17 11:55:49,270 | INFO | Epoch 1206 Train Time 92.57916808128357s

2025-10-17 11:57:35,417 | INFO | Training epoch 1207, Batch 1000/1000: LR=3.48e-05, Loss=4.10e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 11:57:35,468 | INFO | Epoch 1207 Train Time 106.19557881355286s

2025-10-17 11:59:11,757 | INFO | Training epoch 1208, Batch 1000/1000: LR=3.47e-05, Loss=4.04e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 11:59:11,810 | INFO | Epoch 1208 Train Time 96.3404004573822s

2025-10-17 12:00:52,408 | INFO | Training epoch 1209, Batch 1000/1000: LR=3.46e-05, Loss=3.99e-03 BER=1.60e-03 FER=1.40e-02
2025-10-17 12:00:52,467 | INFO | Epoch 1209 Train Time 100.65604305267334s

2025-10-17 12:02:27,260 | INFO | Training epoch 1210, Batch 1000/1000: LR=3.45e-05, Loss=3.84e-03 BER=1.56e-03 FER=1.38e-02
2025-10-17 12:02:27,322 | INFO | Epoch 1210 Train Time 94.85324597358704s

2025-10-17 12:04:02,410 | INFO | Training epoch 1211, Batch 1000/1000: LR=3.45e-05, Loss=4.01e-03 BER=1.65e-03 FER=1.44e-02
2025-10-17 12:04:02,470 | INFO | Epoch 1211 Train Time 95.14705514907837s

2025-10-17 12:05:32,789 | INFO | Training epoch 1212, Batch 1000/1000: LR=3.44e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 12:05:32,830 | INFO | Epoch 1212 Train Time 90.35868859291077s

2025-10-17 12:07:04,671 | INFO | Training epoch 1213, Batch 1000/1000: LR=3.43e-05, Loss=4.14e-03 BER=1.69e-03 FER=1.46e-02
2025-10-17 12:07:04,710 | INFO | Epoch 1213 Train Time 91.87932920455933s

2025-10-17 12:08:43,016 | INFO | Training epoch 1214, Batch 1000/1000: LR=3.42e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 12:08:43,066 | INFO | Epoch 1214 Train Time 98.35423493385315s

2025-10-17 12:10:27,996 | INFO | Training epoch 1215, Batch 1000/1000: LR=3.42e-05, Loss=3.85e-03 BER=1.58e-03 FER=1.37e-02
2025-10-17 12:10:28,038 | INFO | Epoch 1215 Train Time 104.97028946876526s

2025-10-17 12:12:08,506 | INFO | Training epoch 1216, Batch 1000/1000: LR=3.41e-05, Loss=4.12e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 12:12:08,556 | INFO | Epoch 1216 Train Time 100.51656484603882s

2025-10-17 12:13:52,838 | INFO | Training epoch 1217, Batch 1000/1000: LR=3.40e-05, Loss=3.88e-03 BER=1.58e-03 FER=1.36e-02
2025-10-17 12:13:52,899 | INFO | Epoch 1217 Train Time 104.34030294418335s

2025-10-17 12:15:25,844 | INFO | Training epoch 1218, Batch 1000/1000: LR=3.40e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.41e-02
2025-10-17 12:15:25,888 | INFO | Epoch 1218 Train Time 92.98737001419067s

2025-10-17 12:17:05,007 | INFO | Training epoch 1219, Batch 1000/1000: LR=3.39e-05, Loss=3.96e-03 BER=1.63e-03 FER=1.44e-02
2025-10-17 12:17:05,053 | INFO | Epoch 1219 Train Time 99.16417789459229s

2025-10-17 12:18:42,873 | INFO | Training epoch 1220, Batch 1000/1000: LR=3.38e-05, Loss=3.92e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 12:18:42,927 | INFO | Epoch 1220 Train Time 97.8728654384613s

2025-10-17 12:20:16,805 | INFO | Training epoch 1221, Batch 1000/1000: LR=3.37e-05, Loss=4.13e-03 BER=1.70e-03 FER=1.46e-02
2025-10-17 12:20:16,845 | INFO | Epoch 1221 Train Time 93.91590023040771s

2025-10-17 12:21:46,362 | INFO | Training epoch 1222, Batch 1000/1000: LR=3.37e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.42e-02
2025-10-17 12:21:46,411 | INFO | Epoch 1222 Train Time 89.56367325782776s

2025-10-17 12:23:19,415 | INFO | Training epoch 1223, Batch 1000/1000: LR=3.36e-05, Loss=4.04e-03 BER=1.67e-03 FER=1.42e-02
2025-10-17 12:23:19,493 | INFO | Epoch 1223 Train Time 93.08174014091492s

2025-10-17 12:25:03,441 | INFO | Training epoch 1224, Batch 1000/1000: LR=3.35e-05, Loss=4.07e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 12:25:03,485 | INFO | Epoch 1224 Train Time 103.98998641967773s

2025-10-17 12:26:33,899 | INFO | Training epoch 1225, Batch 1000/1000: LR=3.34e-05, Loss=3.94e-03 BER=1.61e-03 FER=1.40e-02
2025-10-17 12:26:33,942 | INFO | Epoch 1225 Train Time 90.4562578201294s

2025-10-17 12:28:11,516 | INFO | Training epoch 1226, Batch 1000/1000: LR=3.34e-05, Loss=4.15e-03 BER=1.69e-03 FER=1.44e-02
2025-10-17 12:28:11,575 | INFO | Epoch 1226 Train Time 97.63133907318115s

2025-10-17 12:29:51,327 | INFO | Training epoch 1227, Batch 1000/1000: LR=3.33e-05, Loss=3.88e-03 BER=1.59e-03 FER=1.36e-02
2025-10-17 12:29:51,387 | INFO | Epoch 1227 Train Time 99.8108184337616s

2025-10-17 12:31:24,965 | INFO | Training epoch 1228, Batch 1000/1000: LR=3.32e-05, Loss=4.07e-03 BER=1.67e-03 FER=1.44e-02
2025-10-17 12:31:25,019 | INFO | Epoch 1228 Train Time 93.63031697273254s

2025-10-17 12:32:59,711 | INFO | Training epoch 1229, Batch 1000/1000: LR=3.31e-05, Loss=4.06e-03 BER=1.69e-03 FER=1.48e-02
2025-10-17 12:32:59,758 | INFO | Epoch 1229 Train Time 94.73718810081482s

2025-10-17 12:34:33,713 | INFO | Training epoch 1230, Batch 1000/1000: LR=3.31e-05, Loss=3.88e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 12:34:33,765 | INFO | Epoch 1230 Train Time 94.0059061050415s

2025-10-17 12:36:06,267 | INFO | Training epoch 1231, Batch 1000/1000: LR=3.30e-05, Loss=3.95e-03 BER=1.64e-03 FER=1.42e-02
2025-10-17 12:36:06,332 | INFO | Epoch 1231 Train Time 92.56423950195312s

2025-10-17 12:37:37,931 | INFO | Training epoch 1232, Batch 1000/1000: LR=3.29e-05, Loss=3.86e-03 BER=1.55e-03 FER=1.36e-02
2025-10-17 12:37:37,985 | INFO | Epoch 1232 Train Time 91.65134286880493s

2025-10-17 12:39:13,280 | INFO | Training epoch 1233, Batch 1000/1000: LR=3.29e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.40e-02
2025-10-17 12:39:13,327 | INFO | Epoch 1233 Train Time 95.3406298160553s

2025-10-17 12:40:49,777 | INFO | Training epoch 1234, Batch 1000/1000: LR=3.28e-05, Loss=3.86e-03 BER=1.58e-03 FER=1.38e-02
2025-10-17 12:40:49,844 | INFO | Epoch 1234 Train Time 96.51518321037292s

2025-10-17 12:42:23,960 | INFO | Training epoch 1235, Batch 1000/1000: LR=3.27e-05, Loss=4.08e-03 BER=1.68e-03 FER=1.42e-02
2025-10-17 12:42:24,009 | INFO | Epoch 1235 Train Time 94.16325545310974s

2025-10-17 12:44:02,079 | INFO | Training epoch 1236, Batch 1000/1000: LR=3.26e-05, Loss=3.84e-03 BER=1.58e-03 FER=1.37e-02
2025-10-17 12:44:02,128 | INFO | Epoch 1236 Train Time 98.11640286445618s

2025-10-17 12:45:38,916 | INFO | Training epoch 1237, Batch 1000/1000: LR=3.26e-05, Loss=4.24e-03 BER=1.75e-03 FER=1.49e-02
2025-10-17 12:45:38,958 | INFO | Epoch 1237 Train Time 96.82793164253235s

2025-10-17 12:47:14,241 | INFO | Training epoch 1238, Batch 1000/1000: LR=3.25e-05, Loss=3.98e-03 BER=1.62e-03 FER=1.42e-02
2025-10-17 12:47:14,286 | INFO | Epoch 1238 Train Time 95.32761096954346s

2025-10-17 12:48:52,922 | INFO | Training epoch 1239, Batch 1000/1000: LR=3.24e-05, Loss=3.94e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 12:48:52,965 | INFO | Epoch 1239 Train Time 98.67750215530396s

2025-10-17 12:50:27,549 | INFO | Training epoch 1240, Batch 1000/1000: LR=3.24e-05, Loss=3.83e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 12:50:27,604 | INFO | Epoch 1240 Train Time 94.63824677467346s

2025-10-17 12:52:05,850 | INFO | Training epoch 1241, Batch 1000/1000: LR=3.23e-05, Loss=3.94e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 12:52:05,902 | INFO | Epoch 1241 Train Time 98.29618811607361s

2025-10-17 12:53:47,050 | INFO | Training epoch 1242, Batch 1000/1000: LR=3.22e-05, Loss=4.03e-03 BER=1.67e-03 FER=1.44e-02
2025-10-17 12:53:47,102 | INFO | Epoch 1242 Train Time 101.1983323097229s

2025-10-17 12:55:16,474 | INFO | Training epoch 1243, Batch 1000/1000: LR=3.21e-05, Loss=3.92e-03 BER=1.62e-03 FER=1.38e-02
2025-10-17 12:55:16,535 | INFO | Epoch 1243 Train Time 89.43058490753174s

2025-10-17 12:56:59,366 | INFO | Training epoch 1244, Batch 1000/1000: LR=3.21e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 12:56:59,418 | INFO | Epoch 1244 Train Time 102.88213896751404s

2025-10-17 12:58:41,861 | INFO | Training epoch 1245, Batch 1000/1000: LR=3.20e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 12:58:41,920 | INFO | Epoch 1245 Train Time 102.4994125366211s

2025-10-17 13:00:19,542 | INFO | Training epoch 1246, Batch 1000/1000: LR=3.19e-05, Loss=3.92e-03 BER=1.64e-03 FER=1.40e-02
2025-10-17 13:00:19,601 | INFO | Epoch 1246 Train Time 97.68022966384888s

2025-10-17 13:01:54,053 | INFO | Training epoch 1247, Batch 1000/1000: LR=3.18e-05, Loss=3.86e-03 BER=1.58e-03 FER=1.38e-02
2025-10-17 13:01:54,106 | INFO | Epoch 1247 Train Time 94.50243473052979s

2025-10-17 13:03:32,970 | INFO | Training epoch 1248, Batch 1000/1000: LR=3.18e-05, Loss=4.15e-03 BER=1.71e-03 FER=1.46e-02
2025-10-17 13:03:33,014 | INFO | Epoch 1248 Train Time 98.90720987319946s

2025-10-17 13:05:09,087 | INFO | Training epoch 1249, Batch 1000/1000: LR=3.17e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.42e-02
2025-10-17 13:05:09,129 | INFO | Epoch 1249 Train Time 96.11270046234131s

2025-10-17 13:06:44,218 | INFO | Training epoch 1250, Batch 1000/1000: LR=3.16e-05, Loss=3.90e-03 BER=1.61e-03 FER=1.39e-02
2025-10-17 13:06:44,267 | INFO | Epoch 1250 Train Time 95.13643312454224s

2025-10-17 13:08:18,276 | INFO | Training epoch 1251, Batch 1000/1000: LR=3.16e-05, Loss=4.17e-03 BER=1.70e-03 FER=1.45e-02
2025-10-17 13:08:18,339 | INFO | Epoch 1251 Train Time 94.07074022293091s

2025-10-17 13:09:55,596 | INFO | Training epoch 1252, Batch 1000/1000: LR=3.15e-05, Loss=3.79e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 13:09:55,649 | INFO | Epoch 1252 Train Time 97.30938601493835s

2025-10-17 13:11:34,302 | INFO | Training epoch 1253, Batch 1000/1000: LR=3.14e-05, Loss=3.97e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 13:11:34,363 | INFO | Epoch 1253 Train Time 98.71296525001526s

2025-10-17 13:13:14,052 | INFO | Training epoch 1254, Batch 1000/1000: LR=3.13e-05, Loss=3.91e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 13:13:14,116 | INFO | Epoch 1254 Train Time 99.75095725059509s

2025-10-17 13:14:49,055 | INFO | Training epoch 1255, Batch 1000/1000: LR=3.13e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.37e-02
2025-10-17 13:14:49,101 | INFO | Epoch 1255 Train Time 94.98416137695312s

2025-10-17 13:16:32,400 | INFO | Training epoch 1256, Batch 1000/1000: LR=3.12e-05, Loss=4.09e-03 BER=1.68e-03 FER=1.46e-02
2025-10-17 13:16:32,460 | INFO | Epoch 1256 Train Time 103.3573534488678s

2025-10-17 13:18:14,284 | INFO | Training epoch 1257, Batch 1000/1000: LR=3.11e-05, Loss=4.02e-03 BER=1.67e-03 FER=1.42e-02
2025-10-17 13:18:14,333 | INFO | Epoch 1257 Train Time 101.87179040908813s

2025-10-17 13:19:57,993 | INFO | Training epoch 1258, Batch 1000/1000: LR=3.11e-05, Loss=3.89e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 13:19:58,053 | INFO | Epoch 1258 Train Time 103.71877717971802s

2025-10-17 13:21:41,202 | INFO | Training epoch 1259, Batch 1000/1000: LR=3.10e-05, Loss=3.81e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 13:21:41,264 | INFO | Epoch 1259 Train Time 103.20986771583557s

2025-10-17 13:23:12,669 | INFO | Training epoch 1260, Batch 1000/1000: LR=3.09e-05, Loss=3.96e-03 BER=1.61e-03 FER=1.42e-02
2025-10-17 13:23:12,728 | INFO | Epoch 1260 Train Time 91.46206283569336s

2025-10-17 13:24:51,366 | INFO | Training epoch 1261, Batch 1000/1000: LR=3.08e-05, Loss=3.86e-03 BER=1.58e-03 FER=1.40e-02
2025-10-17 13:24:51,417 | INFO | Epoch 1261 Train Time 98.6880521774292s

2025-10-17 13:26:34,545 | INFO | Training epoch 1262, Batch 1000/1000: LR=3.08e-05, Loss=3.65e-03 BER=1.49e-03 FER=1.27e-02
2025-10-17 13:26:34,594 | INFO | Epoch 1262 Train Time 103.17579388618469s

2025-10-17 13:26:34,595 | INFO | [P2] saving best_model (QAT) with loss 0.003651 at epoch 1262
2025-10-17 13:28:10,558 | INFO | Training epoch 1263, Batch 1000/1000: LR=3.07e-05, Loss=3.85e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 13:28:10,619 | INFO | Epoch 1263 Train Time 95.94573163986206s

2025-10-17 13:29:46,875 | INFO | Training epoch 1264, Batch 1000/1000: LR=3.06e-05, Loss=3.83e-03 BER=1.57e-03 FER=1.38e-02
2025-10-17 13:29:46,934 | INFO | Epoch 1264 Train Time 96.31374835968018s

2025-10-17 13:31:30,196 | INFO | Training epoch 1265, Batch 1000/1000: LR=3.06e-05, Loss=3.85e-03 BER=1.58e-03 FER=1.39e-02
2025-10-17 13:31:30,261 | INFO | Epoch 1265 Train Time 103.32631325721741s

2025-10-17 13:33:07,279 | INFO | Training epoch 1266, Batch 1000/1000: LR=3.05e-05, Loss=3.84e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 13:33:07,322 | INFO | Epoch 1266 Train Time 97.05918526649475s

2025-10-17 13:34:44,171 | INFO | Training epoch 1267, Batch 1000/1000: LR=3.04e-05, Loss=4.01e-03 BER=1.61e-03 FER=1.46e-02
2025-10-17 13:34:44,223 | INFO | Epoch 1267 Train Time 96.8995087146759s

2025-10-17 13:36:21,740 | INFO | Training epoch 1268, Batch 1000/1000: LR=3.03e-05, Loss=4.05e-03 BER=1.68e-03 FER=1.43e-02
2025-10-17 13:36:21,791 | INFO | Epoch 1268 Train Time 97.56740856170654s

2025-10-17 13:37:57,847 | INFO | Training epoch 1269, Batch 1000/1000: LR=3.03e-05, Loss=3.82e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 13:37:57,891 | INFO | Epoch 1269 Train Time 96.09851503372192s

2025-10-17 13:39:32,277 | INFO | Training epoch 1270, Batch 1000/1000: LR=3.02e-05, Loss=4.00e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 13:39:32,336 | INFO | Epoch 1270 Train Time 94.44416427612305s

2025-10-17 13:41:03,747 | INFO | Training epoch 1271, Batch 1000/1000: LR=3.01e-05, Loss=3.99e-03 BER=1.64e-03 FER=1.44e-02
2025-10-17 13:41:03,810 | INFO | Epoch 1271 Train Time 91.47201156616211s

2025-10-17 13:42:44,100 | INFO | Training epoch 1272, Batch 1000/1000: LR=3.01e-05, Loss=3.88e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 13:42:44,160 | INFO | Epoch 1272 Train Time 100.34879612922668s

2025-10-17 13:44:25,956 | INFO | Training epoch 1273, Batch 1000/1000: LR=3.00e-05, Loss=3.87e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 13:44:26,006 | INFO | Epoch 1273 Train Time 101.84468626976013s

2025-10-17 13:46:02,680 | INFO | Training epoch 1274, Batch 1000/1000: LR=2.99e-05, Loss=3.95e-03 BER=1.66e-03 FER=1.42e-02
2025-10-17 13:46:02,730 | INFO | Epoch 1274 Train Time 96.72337484359741s

2025-10-17 13:47:41,567 | INFO | Training epoch 1275, Batch 1000/1000: LR=2.98e-05, Loss=3.85e-03 BER=1.57e-03 FER=1.38e-02
2025-10-17 13:47:41,607 | INFO | Epoch 1275 Train Time 98.87468934059143s

2025-10-17 13:49:23,070 | INFO | Training epoch 1276, Batch 1000/1000: LR=2.98e-05, Loss=3.86e-03 BER=1.57e-03 FER=1.38e-02
2025-10-17 13:49:23,132 | INFO | Epoch 1276 Train Time 101.52322721481323s

2025-10-17 13:50:59,740 | INFO | Training epoch 1277, Batch 1000/1000: LR=2.97e-05, Loss=3.88e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 13:50:59,802 | INFO | Epoch 1277 Train Time 96.66937804222107s

2025-10-17 13:52:39,863 | INFO | Training epoch 1278, Batch 1000/1000: LR=2.96e-05, Loss=3.96e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 13:52:39,918 | INFO | Epoch 1278 Train Time 100.11412215232849s

2025-10-17 13:54:12,506 | INFO | Training epoch 1279, Batch 1000/1000: LR=2.96e-05, Loss=3.71e-03 BER=1.52e-03 FER=1.30e-02
2025-10-17 13:54:12,572 | INFO | Epoch 1279 Train Time 92.65269589424133s

2025-10-17 13:55:45,417 | INFO | Training epoch 1280, Batch 1000/1000: LR=2.95e-05, Loss=3.91e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 13:55:45,469 | INFO | Epoch 1280 Train Time 92.89598608016968s

2025-10-17 13:57:16,745 | INFO | Training epoch 1281, Batch 1000/1000: LR=2.94e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 13:57:16,788 | INFO | Epoch 1281 Train Time 91.3176736831665s

2025-10-17 13:58:54,453 | INFO | Training epoch 1282, Batch 1000/1000: LR=2.94e-05, Loss=4.07e-03 BER=1.71e-03 FER=1.47e-02
2025-10-17 13:58:54,500 | INFO | Epoch 1282 Train Time 97.71066331863403s

2025-10-17 14:00:37,053 | INFO | Training epoch 1283, Batch 1000/1000: LR=2.93e-05, Loss=3.93e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 14:00:37,097 | INFO | Epoch 1283 Train Time 102.59517407417297s

2025-10-17 14:02:17,243 | INFO | Training epoch 1284, Batch 1000/1000: LR=2.92e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 14:02:17,292 | INFO | Epoch 1284 Train Time 100.19324278831482s

2025-10-17 14:03:58,153 | INFO | Training epoch 1285, Batch 1000/1000: LR=2.91e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.43e-02
2025-10-17 14:03:58,203 | INFO | Epoch 1285 Train Time 100.90946507453918s

2025-10-17 14:05:36,097 | INFO | Training epoch 1286, Batch 1000/1000: LR=2.91e-05, Loss=3.74e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 14:05:36,139 | INFO | Epoch 1286 Train Time 97.93395328521729s

2025-10-17 14:07:10,258 | INFO | Training epoch 1287, Batch 1000/1000: LR=2.90e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 14:07:10,307 | INFO | Epoch 1287 Train Time 94.16744327545166s

2025-10-17 14:08:47,473 | INFO | Training epoch 1288, Batch 1000/1000: LR=2.89e-05, Loss=4.02e-03 BER=1.68e-03 FER=1.44e-02
2025-10-17 14:08:47,523 | INFO | Epoch 1288 Train Time 97.21465539932251s

2025-10-17 14:10:18,787 | INFO | Training epoch 1289, Batch 1000/1000: LR=2.89e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.41e-02
2025-10-17 14:10:18,835 | INFO | Epoch 1289 Train Time 91.31078577041626s

2025-10-17 14:11:56,767 | INFO | Training epoch 1290, Batch 1000/1000: LR=2.88e-05, Loss=4.00e-03 BER=1.62e-03 FER=1.41e-02
2025-10-17 14:11:56,830 | INFO | Epoch 1290 Train Time 97.99323081970215s

2025-10-17 14:13:34,643 | INFO | Training epoch 1291, Batch 1000/1000: LR=2.87e-05, Loss=3.89e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 14:13:34,698 | INFO | Epoch 1291 Train Time 97.86526870727539s

2025-10-17 14:15:12,770 | INFO | Training epoch 1292, Batch 1000/1000: LR=2.87e-05, Loss=4.05e-03 BER=1.65e-03 FER=1.43e-02
2025-10-17 14:15:12,821 | INFO | Epoch 1292 Train Time 98.12174272537231s

2025-10-17 14:16:53,411 | INFO | Training epoch 1293, Batch 1000/1000: LR=2.86e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.36e-02
2025-10-17 14:16:53,478 | INFO | Epoch 1293 Train Time 100.6542980670929s

2025-10-17 14:18:26,280 | INFO | Training epoch 1294, Batch 1000/1000: LR=2.85e-05, Loss=3.93e-03 BER=1.60e-03 FER=1.40e-02
2025-10-17 14:18:26,337 | INFO | Epoch 1294 Train Time 92.85780715942383s

2025-10-17 14:20:04,047 | INFO | Training epoch 1295, Batch 1000/1000: LR=2.84e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.45e-02
2025-10-17 14:20:04,122 | INFO | Epoch 1295 Train Time 97.78434181213379s

2025-10-17 14:21:46,775 | INFO | Training epoch 1296, Batch 1000/1000: LR=2.84e-05, Loss=3.94e-03 BER=1.60e-03 FER=1.40e-02
2025-10-17 14:21:46,849 | INFO | Epoch 1296 Train Time 102.7241690158844s

2025-10-17 14:23:31,531 | INFO | Training epoch 1297, Batch 1000/1000: LR=2.83e-05, Loss=3.79e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 14:23:31,595 | INFO | Epoch 1297 Train Time 104.7444577217102s

2025-10-17 14:25:10,533 | INFO | Training epoch 1298, Batch 1000/1000: LR=2.82e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 14:25:10,598 | INFO | Epoch 1298 Train Time 99.00126504898071s

2025-10-17 14:26:52,579 | INFO | Training epoch 1299, Batch 1000/1000: LR=2.82e-05, Loss=3.85e-03 BER=1.57e-03 FER=1.34e-02
2025-10-17 14:26:52,633 | INFO | Epoch 1299 Train Time 102.03441429138184s

2025-10-17 14:28:26,797 | INFO | Training epoch 1300, Batch 1000/1000: LR=2.81e-05, Loss=3.72e-03 BER=1.54e-03 FER=1.34e-02
2025-10-17 14:28:26,853 | INFO | Epoch 1300 Train Time 94.21903347969055s

2025-10-17 14:30:04,465 | INFO | Training epoch 1301, Batch 1000/1000: LR=2.80e-05, Loss=4.00e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 14:30:04,516 | INFO | Epoch 1301 Train Time 97.66148900985718s

2025-10-17 14:31:45,037 | INFO | Training epoch 1302, Batch 1000/1000: LR=2.80e-05, Loss=3.86e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 14:31:45,094 | INFO | Epoch 1302 Train Time 100.57633018493652s

2025-10-17 14:33:27,985 | INFO | Training epoch 1303, Batch 1000/1000: LR=2.79e-05, Loss=3.91e-03 BER=1.58e-03 FER=1.37e-02
2025-10-17 14:33:28,052 | INFO | Epoch 1303 Train Time 102.95673274993896s

2025-10-17 14:35:07,972 | INFO | Training epoch 1304, Batch 1000/1000: LR=2.78e-05, Loss=3.89e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 14:35:08,030 | INFO | Epoch 1304 Train Time 99.97514772415161s

2025-10-17 14:36:42,378 | INFO | Training epoch 1305, Batch 1000/1000: LR=2.78e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.36e-02
2025-10-17 14:36:42,450 | INFO | Epoch 1305 Train Time 94.41851234436035s

2025-10-17 14:38:17,546 | INFO | Training epoch 1306, Batch 1000/1000: LR=2.77e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.37e-02
2025-10-17 14:38:17,591 | INFO | Epoch 1306 Train Time 95.13976049423218s

2025-10-17 14:39:55,261 | INFO | Training epoch 1307, Batch 1000/1000: LR=2.76e-05, Loss=4.05e-03 BER=1.67e-03 FER=1.43e-02
2025-10-17 14:39:55,328 | INFO | Epoch 1307 Train Time 97.73474955558777s

2025-10-17 14:41:32,864 | INFO | Training epoch 1308, Batch 1000/1000: LR=2.75e-05, Loss=3.85e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 14:41:32,929 | INFO | Epoch 1308 Train Time 97.59935903549194s

2025-10-17 14:43:09,717 | INFO | Training epoch 1309, Batch 1000/1000: LR=2.75e-05, Loss=3.74e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 14:43:09,773 | INFO | Epoch 1309 Train Time 96.84333610534668s

2025-10-17 14:44:49,973 | INFO | Training epoch 1310, Batch 1000/1000: LR=2.74e-05, Loss=3.82e-03 BER=1.58e-03 FER=1.39e-02
2025-10-17 14:44:50,028 | INFO | Epoch 1310 Train Time 100.25328779220581s

2025-10-17 14:46:32,692 | INFO | Training epoch 1311, Batch 1000/1000: LR=2.73e-05, Loss=3.60e-03 BER=1.48e-03 FER=1.27e-02
2025-10-17 14:46:32,754 | INFO | Epoch 1311 Train Time 102.72476887702942s

2025-10-17 14:46:32,754 | INFO | [P2] saving best_model (QAT) with loss 0.003598 at epoch 1311
2025-10-17 14:48:11,357 | INFO | Training epoch 1312, Batch 1000/1000: LR=2.73e-05, Loss=4.01e-03 BER=1.64e-03 FER=1.44e-02
2025-10-17 14:48:11,426 | INFO | Epoch 1312 Train Time 98.5704038143158s

2025-10-17 14:49:48,956 | INFO | Training epoch 1313, Batch 1000/1000: LR=2.72e-05, Loss=4.03e-03 BER=1.68e-03 FER=1.43e-02
2025-10-17 14:49:49,016 | INFO | Epoch 1313 Train Time 97.58878421783447s

2025-10-17 14:51:23,489 | INFO | Training epoch 1314, Batch 1000/1000: LR=2.71e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 14:51:23,572 | INFO | Epoch 1314 Train Time 94.5546760559082s

2025-10-17 14:52:55,434 | INFO | Training epoch 1315, Batch 1000/1000: LR=2.71e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.40e-02
2025-10-17 14:52:55,508 | INFO | Epoch 1315 Train Time 91.93490242958069s

2025-10-17 14:54:26,974 | INFO | Training epoch 1316, Batch 1000/1000: LR=2.70e-05, Loss=3.83e-03 BER=1.58e-03 FER=1.38e-02
2025-10-17 14:54:27,030 | INFO | Epoch 1316 Train Time 91.52013945579529s

2025-10-17 14:56:04,782 | INFO | Training epoch 1317, Batch 1000/1000: LR=2.69e-05, Loss=3.94e-03 BER=1.62e-03 FER=1.42e-02
2025-10-17 14:56:04,838 | INFO | Epoch 1317 Train Time 97.80571269989014s

2025-10-17 14:57:42,888 | INFO | Training epoch 1318, Batch 1000/1000: LR=2.69e-05, Loss=3.77e-03 BER=1.57e-03 FER=1.37e-02
2025-10-17 14:57:42,946 | INFO | Epoch 1318 Train Time 98.10789895057678s

2025-10-17 14:59:18,795 | INFO | Training epoch 1319, Batch 1000/1000: LR=2.68e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 14:59:18,860 | INFO | Epoch 1319 Train Time 95.91187477111816s

2025-10-17 15:00:50,708 | INFO | Training epoch 1320, Batch 1000/1000: LR=2.67e-05, Loss=4.00e-03 BER=1.63e-03 FER=1.42e-02
2025-10-17 15:00:50,760 | INFO | Epoch 1320 Train Time 91.89932322502136s

2025-10-17 15:02:29,136 | INFO | Training epoch 1321, Batch 1000/1000: LR=2.67e-05, Loss=3.67e-03 BER=1.51e-03 FER=1.32e-02
2025-10-17 15:02:29,202 | INFO | Epoch 1321 Train Time 98.44092679023743s

2025-10-17 15:04:08,517 | INFO | Training epoch 1322, Batch 1000/1000: LR=2.66e-05, Loss=4.05e-03 BER=1.62e-03 FER=1.42e-02
2025-10-17 15:04:08,584 | INFO | Epoch 1322 Train Time 99.38068151473999s

2025-10-17 15:05:46,272 | INFO | Training epoch 1323, Batch 1000/1000: LR=2.65e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.35e-02
2025-10-17 15:05:46,332 | INFO | Epoch 1323 Train Time 97.7468409538269s

2025-10-17 15:07:17,106 | INFO | Training epoch 1324, Batch 1000/1000: LR=2.64e-05, Loss=4.03e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 15:07:17,163 | INFO | Epoch 1324 Train Time 90.82888197898865s

2025-10-17 15:08:49,806 | INFO | Training epoch 1325, Batch 1000/1000: LR=2.64e-05, Loss=3.95e-03 BER=1.59e-03 FER=1.38e-02
2025-10-17 15:08:49,874 | INFO | Epoch 1325 Train Time 92.70980954170227s

2025-10-17 15:10:26,302 | INFO | Training epoch 1326, Batch 1000/1000: LR=2.63e-05, Loss=3.91e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 15:10:26,364 | INFO | Epoch 1326 Train Time 96.48785877227783s

2025-10-17 15:11:57,588 | INFO | Training epoch 1327, Batch 1000/1000: LR=2.62e-05, Loss=4.06e-03 BER=1.66e-03 FER=1.43e-02
2025-10-17 15:11:57,647 | INFO | Epoch 1327 Train Time 91.28129267692566s

2025-10-17 15:13:33,202 | INFO | Training epoch 1328, Batch 1000/1000: LR=2.62e-05, Loss=3.77e-03 BER=1.54e-03 FER=1.34e-02
2025-10-17 15:13:33,267 | INFO | Epoch 1328 Train Time 95.61924695968628s

2025-10-17 15:15:12,070 | INFO | Training epoch 1329, Batch 1000/1000: LR=2.61e-05, Loss=3.87e-03 BER=1.58e-03 FER=1.35e-02
2025-10-17 15:15:12,135 | INFO | Epoch 1329 Train Time 98.86739206314087s

2025-10-17 15:16:49,717 | INFO | Training epoch 1330, Batch 1000/1000: LR=2.60e-05, Loss=3.84e-03 BER=1.57e-03 FER=1.37e-02
2025-10-17 15:16:49,777 | INFO | Epoch 1330 Train Time 97.64036655426025s

2025-10-17 15:18:29,124 | INFO | Training epoch 1331, Batch 1000/1000: LR=2.60e-05, Loss=3.85e-03 BER=1.60e-03 FER=1.37e-02
2025-10-17 15:18:29,187 | INFO | Epoch 1331 Train Time 99.40968775749207s

2025-10-17 15:20:02,157 | INFO | Training epoch 1332, Batch 1000/1000: LR=2.59e-05, Loss=3.97e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 15:20:02,215 | INFO | Epoch 1332 Train Time 93.02679371833801s

2025-10-17 15:21:42,007 | INFO | Training epoch 1333, Batch 1000/1000: LR=2.58e-05, Loss=3.93e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 15:21:42,089 | INFO | Epoch 1333 Train Time 99.87203407287598s

2025-10-17 15:23:24,258 | INFO | Training epoch 1334, Batch 1000/1000: LR=2.58e-05, Loss=3.96e-03 BER=1.60e-03 FER=1.39e-02
2025-10-17 15:23:24,317 | INFO | Epoch 1334 Train Time 102.22713971138s

2025-10-17 15:25:00,476 | INFO | Training epoch 1335, Batch 1000/1000: LR=2.57e-05, Loss=4.00e-03 BER=1.64e-03 FER=1.42e-02
2025-10-17 15:25:00,537 | INFO | Epoch 1335 Train Time 96.21814918518066s

2025-10-17 15:26:35,093 | INFO | Training epoch 1336, Batch 1000/1000: LR=2.56e-05, Loss=3.76e-03 BER=1.58e-03 FER=1.34e-02
2025-10-17 15:26:35,142 | INFO | Epoch 1336 Train Time 94.60338711738586s

2025-10-17 15:28:14,972 | INFO | Training epoch 1337, Batch 1000/1000: LR=2.56e-05, Loss=3.86e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 15:28:15,031 | INFO | Epoch 1337 Train Time 99.88811707496643s

2025-10-17 15:29:46,833 | INFO | Training epoch 1338, Batch 1000/1000: LR=2.55e-05, Loss=3.82e-03 BER=1.57e-03 FER=1.39e-02
2025-10-17 15:29:46,883 | INFO | Epoch 1338 Train Time 91.85127711296082s

2025-10-17 15:31:30,901 | INFO | Training epoch 1339, Batch 1000/1000: LR=2.54e-05, Loss=3.94e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 15:31:30,963 | INFO | Epoch 1339 Train Time 104.07887244224548s

2025-10-17 15:33:10,050 | INFO | Training epoch 1340, Batch 1000/1000: LR=2.54e-05, Loss=3.84e-03 BER=1.58e-03 FER=1.34e-02
2025-10-17 15:33:10,105 | INFO | Epoch 1340 Train Time 99.1401731967926s

2025-10-17 15:34:57,548 | INFO | Training epoch 1341, Batch 1000/1000: LR=2.53e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 15:34:57,609 | INFO | Epoch 1341 Train Time 107.50371623039246s

2025-10-17 15:36:42,070 | INFO | Training epoch 1342, Batch 1000/1000: LR=2.52e-05, Loss=3.83e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 15:36:42,134 | INFO | Epoch 1342 Train Time 104.52326107025146s

2025-10-17 15:38:20,509 | INFO | Training epoch 1343, Batch 1000/1000: LR=2.52e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 15:38:20,569 | INFO | Epoch 1343 Train Time 98.43382430076599s

2025-10-17 15:39:53,236 | INFO | Training epoch 1344, Batch 1000/1000: LR=2.51e-05, Loss=3.89e-03 BER=1.61e-03 FER=1.40e-02
2025-10-17 15:39:53,292 | INFO | Epoch 1344 Train Time 92.72130131721497s

2025-10-17 15:41:32,103 | INFO | Training epoch 1345, Batch 1000/1000: LR=2.50e-05, Loss=3.60e-03 BER=1.46e-03 FER=1.26e-02
2025-10-17 15:41:32,154 | INFO | Epoch 1345 Train Time 98.86100363731384s

2025-10-17 15:41:32,154 | INFO | [P2] saving best_model (QAT) with loss 0.003596 at epoch 1345
2025-10-17 15:43:09,097 | INFO | Training epoch 1346, Batch 1000/1000: LR=2.50e-05, Loss=3.79e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 15:43:09,154 | INFO | Epoch 1346 Train Time 96.92341089248657s

2025-10-17 15:44:43,526 | INFO | Training epoch 1347, Batch 1000/1000: LR=2.49e-05, Loss=3.98e-03 BER=1.64e-03 FER=1.43e-02
2025-10-17 15:44:43,590 | INFO | Epoch 1347 Train Time 94.4340353012085s

2025-10-17 15:46:16,480 | INFO | Training epoch 1348, Batch 1000/1000: LR=2.48e-05, Loss=3.83e-03 BER=1.59e-03 FER=1.35e-02
2025-10-17 15:46:16,539 | INFO | Epoch 1348 Train Time 92.94750809669495s

2025-10-17 15:47:58,365 | INFO | Training epoch 1349, Batch 1000/1000: LR=2.48e-05, Loss=3.96e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 15:47:58,417 | INFO | Epoch 1349 Train Time 101.87569737434387s

2025-10-17 15:49:39,768 | INFO | Training epoch 1350, Batch 1000/1000: LR=2.47e-05, Loss=3.88e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 15:49:39,831 | INFO | Epoch 1350 Train Time 101.413006067276s

2025-10-17 15:51:16,177 | INFO | Training epoch 1351, Batch 1000/1000: LR=2.46e-05, Loss=3.88e-03 BER=1.61e-03 FER=1.39e-02
2025-10-17 15:51:16,233 | INFO | Epoch 1351 Train Time 96.4015052318573s

2025-10-17 15:52:55,975 | INFO | Training epoch 1352, Batch 1000/1000: LR=2.46e-05, Loss=3.86e-03 BER=1.58e-03 FER=1.39e-02
2025-10-17 15:52:56,044 | INFO | Epoch 1352 Train Time 99.80922889709473s

2025-10-17 15:54:31,939 | INFO | Training epoch 1353, Batch 1000/1000: LR=2.45e-05, Loss=3.93e-03 BER=1.61e-03 FER=1.40e-02
2025-10-17 15:54:31,998 | INFO | Epoch 1353 Train Time 95.95330023765564s

2025-10-17 15:56:13,671 | INFO | Training epoch 1354, Batch 1000/1000: LR=2.44e-05, Loss=3.92e-03 BER=1.64e-03 FER=1.41e-02
2025-10-17 15:56:13,735 | INFO | Epoch 1354 Train Time 101.73504638671875s

2025-10-17 15:57:47,878 | INFO | Training epoch 1355, Batch 1000/1000: LR=2.44e-05, Loss=3.79e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 15:57:47,933 | INFO | Epoch 1355 Train Time 94.19706058502197s

2025-10-17 15:59:26,593 | INFO | Training epoch 1356, Batch 1000/1000: LR=2.43e-05, Loss=3.85e-03 BER=1.61e-03 FER=1.36e-02
2025-10-17 15:59:26,650 | INFO | Epoch 1356 Train Time 98.71550059318542s

2025-10-17 16:01:02,650 | INFO | Training epoch 1357, Batch 1000/1000: LR=2.42e-05, Loss=3.91e-03 BER=1.62e-03 FER=1.38e-02
2025-10-17 16:01:02,715 | INFO | Epoch 1357 Train Time 96.06358170509338s

2025-10-17 16:02:39,194 | INFO | Training epoch 1358, Batch 1000/1000: LR=2.42e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 16:02:39,253 | INFO | Epoch 1358 Train Time 96.5366153717041s

2025-10-17 16:04:10,910 | INFO | Training epoch 1359, Batch 1000/1000: LR=2.41e-05, Loss=3.93e-03 BER=1.59e-03 FER=1.38e-02
2025-10-17 16:04:10,963 | INFO | Epoch 1359 Train Time 91.70897197723389s

2025-10-17 16:05:49,194 | INFO | Training epoch 1360, Batch 1000/1000: LR=2.40e-05, Loss=3.81e-03 BER=1.58e-03 FER=1.37e-02
2025-10-17 16:05:49,255 | INFO | Epoch 1360 Train Time 98.2920081615448s

2025-10-17 16:07:32,000 | INFO | Training epoch 1361, Batch 1000/1000: LR=2.40e-05, Loss=3.91e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 16:07:32,065 | INFO | Epoch 1361 Train Time 102.80860304832458s

2025-10-17 16:09:11,231 | INFO | Training epoch 1362, Batch 1000/1000: LR=2.39e-05, Loss=3.63e-03 BER=1.47e-03 FER=1.27e-02
2025-10-17 16:09:11,292 | INFO | Epoch 1362 Train Time 99.22293591499329s

2025-10-17 16:10:50,081 | INFO | Training epoch 1363, Batch 1000/1000: LR=2.38e-05, Loss=3.79e-03 BER=1.57e-03 FER=1.34e-02
2025-10-17 16:10:50,144 | INFO | Epoch 1363 Train Time 98.85099124908447s

2025-10-17 16:12:25,471 | INFO | Training epoch 1364, Batch 1000/1000: LR=2.38e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.41e-02
2025-10-17 16:12:25,524 | INFO | Epoch 1364 Train Time 95.37888073921204s

2025-10-17 16:13:57,485 | INFO | Training epoch 1365, Batch 1000/1000: LR=2.37e-05, Loss=3.88e-03 BER=1.58e-03 FER=1.35e-02
2025-10-17 16:13:57,547 | INFO | Epoch 1365 Train Time 92.02093434333801s

2025-10-17 16:15:34,422 | INFO | Training epoch 1366, Batch 1000/1000: LR=2.36e-05, Loss=3.81e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 16:15:34,480 | INFO | Epoch 1366 Train Time 96.93132448196411s

2025-10-17 16:17:12,876 | INFO | Training epoch 1367, Batch 1000/1000: LR=2.36e-05, Loss=3.73e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 16:17:12,941 | INFO | Epoch 1367 Train Time 98.45947217941284s

2025-10-17 16:18:50,077 | INFO | Training epoch 1368, Batch 1000/1000: LR=2.35e-05, Loss=3.70e-03 BER=1.52e-03 FER=1.34e-02
2025-10-17 16:18:50,153 | INFO | Epoch 1368 Train Time 97.2107481956482s

2025-10-17 16:20:26,166 | INFO | Training epoch 1369, Batch 1000/1000: LR=2.35e-05, Loss=3.79e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 16:20:26,226 | INFO | Epoch 1369 Train Time 96.07193803787231s

2025-10-17 16:22:04,473 | INFO | Training epoch 1370, Batch 1000/1000: LR=2.34e-05, Loss=3.84e-03 BER=1.60e-03 FER=1.37e-02
2025-10-17 16:22:04,534 | INFO | Epoch 1370 Train Time 98.30656719207764s

2025-10-17 16:23:39,571 | INFO | Training epoch 1371, Batch 1000/1000: LR=2.33e-05, Loss=3.74e-03 BER=1.53e-03 FER=1.30e-02
2025-10-17 16:23:39,624 | INFO | Epoch 1371 Train Time 95.08899331092834s

2025-10-17 16:25:21,535 | INFO | Training epoch 1372, Batch 1000/1000: LR=2.33e-05, Loss=3.68e-03 BER=1.54e-03 FER=1.34e-02
2025-10-17 16:25:21,593 | INFO | Epoch 1372 Train Time 101.96783399581909s

2025-10-17 16:26:57,764 | INFO | Training epoch 1373, Batch 1000/1000: LR=2.32e-05, Loss=3.88e-03 BER=1.61e-03 FER=1.39e-02
2025-10-17 16:26:57,837 | INFO | Epoch 1373 Train Time 96.24200558662415s

2025-10-17 16:28:32,755 | INFO | Training epoch 1374, Batch 1000/1000: LR=2.31e-05, Loss=3.83e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 16:28:32,811 | INFO | Epoch 1374 Train Time 94.97247695922852s

2025-10-17 16:30:18,645 | INFO | Training epoch 1375, Batch 1000/1000: LR=2.31e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 16:30:18,705 | INFO | Epoch 1375 Train Time 105.89273047447205s

2025-10-17 16:32:01,480 | INFO | Training epoch 1376, Batch 1000/1000: LR=2.30e-05, Loss=3.83e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 16:32:01,537 | INFO | Epoch 1376 Train Time 102.83038878440857s

2025-10-17 16:33:39,428 | INFO | Training epoch 1377, Batch 1000/1000: LR=2.29e-05, Loss=3.62e-03 BER=1.49e-03 FER=1.30e-02
2025-10-17 16:33:39,492 | INFO | Epoch 1377 Train Time 97.95332479476929s

2025-10-17 16:35:15,177 | INFO | Training epoch 1378, Batch 1000/1000: LR=2.29e-05, Loss=3.73e-03 BER=1.51e-03 FER=1.33e-02
2025-10-17 16:35:15,230 | INFO | Epoch 1378 Train Time 95.73664355278015s

2025-10-17 16:36:48,578 | INFO | Training epoch 1379, Batch 1000/1000: LR=2.28e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 16:36:48,638 | INFO | Epoch 1379 Train Time 93.40724515914917s

2025-10-17 16:38:24,183 | INFO | Training epoch 1380, Batch 1000/1000: LR=2.27e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 16:38:24,244 | INFO | Epoch 1380 Train Time 95.60500621795654s

2025-10-17 16:39:58,389 | INFO | Training epoch 1381, Batch 1000/1000: LR=2.27e-05, Loss=3.77e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 16:39:58,460 | INFO | Epoch 1381 Train Time 94.21489000320435s

2025-10-17 16:41:36,247 | INFO | Training epoch 1382, Batch 1000/1000: LR=2.26e-05, Loss=3.95e-03 BER=1.63e-03 FER=1.40e-02
2025-10-17 16:41:36,309 | INFO | Epoch 1382 Train Time 97.84714221954346s

2025-10-17 16:43:11,298 | INFO | Training epoch 1383, Batch 1000/1000: LR=2.25e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 16:43:11,354 | INFO | Epoch 1383 Train Time 95.04400420188904s

2025-10-17 16:44:45,506 | INFO | Training epoch 1384, Batch 1000/1000: LR=2.25e-05, Loss=3.68e-03 BER=1.50e-03 FER=1.30e-02
2025-10-17 16:44:45,573 | INFO | Epoch 1384 Train Time 94.21772193908691s

2025-10-17 16:46:21,371 | INFO | Training epoch 1385, Batch 1000/1000: LR=2.24e-05, Loss=3.73e-03 BER=1.52e-03 FER=1.32e-02
2025-10-17 16:46:21,436 | INFO | Epoch 1385 Train Time 95.86233258247375s

2025-10-17 16:47:57,580 | INFO | Training epoch 1386, Batch 1000/1000: LR=2.24e-05, Loss=3.94e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 16:47:57,646 | INFO | Epoch 1386 Train Time 96.20888829231262s

2025-10-17 16:49:35,297 | INFO | Training epoch 1387, Batch 1000/1000: LR=2.23e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.37e-02
2025-10-17 16:49:35,365 | INFO | Epoch 1387 Train Time 97.71741914749146s

2025-10-17 16:51:13,362 | INFO | Training epoch 1388, Batch 1000/1000: LR=2.22e-05, Loss=3.67e-03 BER=1.52e-03 FER=1.30e-02
2025-10-17 16:51:13,426 | INFO | Epoch 1388 Train Time 98.06036925315857s

2025-10-17 16:52:51,791 | INFO | Training epoch 1389, Batch 1000/1000: LR=2.22e-05, Loss=3.93e-03 BER=1.59e-03 FER=1.39e-02
2025-10-17 16:52:51,849 | INFO | Epoch 1389 Train Time 98.42048335075378s

2025-10-17 16:54:33,564 | INFO | Training epoch 1390, Batch 1000/1000: LR=2.21e-05, Loss=3.96e-03 BER=1.63e-03 FER=1.38e-02
2025-10-17 16:54:33,624 | INFO | Epoch 1390 Train Time 101.77489256858826s

2025-10-17 16:56:06,527 | INFO | Training epoch 1391, Batch 1000/1000: LR=2.20e-05, Loss=3.85e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 16:56:06,588 | INFO | Epoch 1391 Train Time 92.96214556694031s

2025-10-17 16:57:48,368 | INFO | Training epoch 1392, Batch 1000/1000: LR=2.20e-05, Loss=3.73e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 16:57:48,440 | INFO | Epoch 1392 Train Time 101.85060119628906s

2025-10-17 16:59:26,380 | INFO | Training epoch 1393, Batch 1000/1000: LR=2.19e-05, Loss=3.79e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 16:59:26,454 | INFO | Epoch 1393 Train Time 98.01367139816284s

2025-10-17 17:01:02,993 | INFO | Training epoch 1394, Batch 1000/1000: LR=2.18e-05, Loss=3.94e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 17:01:03,048 | INFO | Epoch 1394 Train Time 96.59315800666809s

2025-10-17 17:02:43,589 | INFO | Training epoch 1395, Batch 1000/1000: LR=2.18e-05, Loss=3.80e-03 BER=1.58e-03 FER=1.36e-02
2025-10-17 17:02:43,652 | INFO | Epoch 1395 Train Time 100.60251808166504s

2025-10-17 17:04:17,380 | INFO | Training epoch 1396, Batch 1000/1000: LR=2.17e-05, Loss=3.79e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 17:04:17,426 | INFO | Epoch 1396 Train Time 93.77315974235535s

2025-10-17 17:06:01,298 | INFO | Training epoch 1397, Batch 1000/1000: LR=2.17e-05, Loss=3.90e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 17:06:01,376 | INFO | Epoch 1397 Train Time 103.94876337051392s

2025-10-17 17:07:42,870 | INFO | Training epoch 1398, Batch 1000/1000: LR=2.16e-05, Loss=3.93e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 17:07:42,935 | INFO | Epoch 1398 Train Time 101.55718946456909s

2025-10-17 17:09:21,990 | INFO | Training epoch 1399, Batch 1000/1000: LR=2.15e-05, Loss=3.65e-03 BER=1.47e-03 FER=1.28e-02
2025-10-17 17:09:22,074 | INFO | Epoch 1399 Train Time 99.1370153427124s

2025-10-17 17:11:01,501 | INFO | Training epoch 1400, Batch 1000/1000: LR=2.15e-05, Loss=3.73e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 17:11:01,560 | INFO | Epoch 1400 Train Time 99.48348903656006s

2025-10-17 17:12:41,551 | INFO | Training epoch 1401, Batch 1000/1000: LR=2.14e-05, Loss=3.68e-03 BER=1.51e-03 FER=1.31e-02
2025-10-17 17:12:41,613 | INFO | Epoch 1401 Train Time 100.0519073009491s

2025-10-17 17:14:16,907 | INFO | Training epoch 1402, Batch 1000/1000: LR=2.13e-05, Loss=3.68e-03 BER=1.51e-03 FER=1.30e-02
2025-10-17 17:14:16,975 | INFO | Epoch 1402 Train Time 95.36028790473938s

2025-10-17 17:15:50,705 | INFO | Training epoch 1403, Batch 1000/1000: LR=2.13e-05, Loss=3.58e-03 BER=1.47e-03 FER=1.28e-02
2025-10-17 17:15:50,767 | INFO | Epoch 1403 Train Time 93.79049706459045s

2025-10-17 17:15:50,768 | INFO | [P2] saving best_model (QAT) with loss 0.003578 at epoch 1403
2025-10-17 17:17:28,510 | INFO | Training epoch 1404, Batch 1000/1000: LR=2.12e-05, Loss=4.00e-03 BER=1.69e-03 FER=1.42e-02
2025-10-17 17:17:28,584 | INFO | Epoch 1404 Train Time 97.71199893951416s

2025-10-17 17:19:06,094 | INFO | Training epoch 1405, Batch 1000/1000: LR=2.12e-05, Loss=3.84e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 17:19:06,159 | INFO | Epoch 1405 Train Time 97.57158398628235s

2025-10-17 17:20:42,861 | INFO | Training epoch 1406, Batch 1000/1000: LR=2.11e-05, Loss=3.85e-03 BER=1.56e-03 FER=1.32e-02
2025-10-17 17:20:42,925 | INFO | Epoch 1406 Train Time 96.76422619819641s

2025-10-17 17:22:21,024 | INFO | Training epoch 1407, Batch 1000/1000: LR=2.10e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 17:22:21,091 | INFO | Epoch 1407 Train Time 98.16479897499084s

2025-10-17 17:23:59,592 | INFO | Training epoch 1408, Batch 1000/1000: LR=2.10e-05, Loss=3.69e-03 BER=1.54e-03 FER=1.31e-02
2025-10-17 17:23:59,659 | INFO | Epoch 1408 Train Time 98.56634974479675s

2025-10-17 17:25:38,071 | INFO | Training epoch 1409, Batch 1000/1000: LR=2.09e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.32e-02
2025-10-17 17:25:38,139 | INFO | Epoch 1409 Train Time 98.47806143760681s

2025-10-17 17:27:17,671 | INFO | Training epoch 1410, Batch 1000/1000: LR=2.08e-05, Loss=3.97e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 17:27:17,722 | INFO | Epoch 1410 Train Time 99.58229660987854s

2025-10-17 17:29:03,761 | INFO | Training epoch 1411, Batch 1000/1000: LR=2.08e-05, Loss=3.84e-03 BER=1.59e-03 FER=1.36e-02
2025-10-17 17:29:03,811 | INFO | Epoch 1411 Train Time 106.08449411392212s

2025-10-17 17:30:41,683 | INFO | Training epoch 1412, Batch 1000/1000: LR=2.07e-05, Loss=3.92e-03 BER=1.63e-03 FER=1.41e-02
2025-10-17 17:30:41,748 | INFO | Epoch 1412 Train Time 97.93495106697083s

2025-10-17 17:32:22,698 | INFO | Training epoch 1413, Batch 1000/1000: LR=2.07e-05, Loss=3.85e-03 BER=1.60e-03 FER=1.36e-02
2025-10-17 17:32:22,768 | INFO | Epoch 1413 Train Time 101.0185341835022s

2025-10-17 17:33:57,794 | INFO | Training epoch 1414, Batch 1000/1000: LR=2.06e-05, Loss=3.82e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 17:33:57,853 | INFO | Epoch 1414 Train Time 95.08386492729187s

2025-10-17 17:35:33,813 | INFO | Training epoch 1415, Batch 1000/1000: LR=2.05e-05, Loss=3.84e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 17:35:33,875 | INFO | Epoch 1415 Train Time 96.01995849609375s

2025-10-17 17:37:14,312 | INFO | Training epoch 1416, Batch 1000/1000: LR=2.05e-05, Loss=3.91e-03 BER=1.60e-03 FER=1.36e-02
2025-10-17 17:37:14,371 | INFO | Epoch 1416 Train Time 100.49528384208679s

2025-10-17 17:39:01,290 | INFO | Training epoch 1417, Batch 1000/1000: LR=2.04e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 17:39:01,351 | INFO | Epoch 1417 Train Time 106.97897243499756s

2025-10-17 17:40:41,989 | INFO | Training epoch 1418, Batch 1000/1000: LR=2.03e-05, Loss=3.59e-03 BER=1.48e-03 FER=1.28e-02
2025-10-17 17:40:42,066 | INFO | Epoch 1418 Train Time 100.71328139305115s

2025-10-17 17:42:26,999 | INFO | Training epoch 1419, Batch 1000/1000: LR=2.03e-05, Loss=3.69e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 17:42:27,061 | INFO | Epoch 1419 Train Time 104.99239015579224s

2025-10-17 17:44:07,351 | INFO | Training epoch 1420, Batch 1000/1000: LR=2.02e-05, Loss=3.88e-03 BER=1.60e-03 FER=1.37e-02
2025-10-17 17:44:07,396 | INFO | Epoch 1420 Train Time 100.33393883705139s

2025-10-17 17:45:45,775 | INFO | Training epoch 1421, Batch 1000/1000: LR=2.02e-05, Loss=3.82e-03 BER=1.57e-03 FER=1.37e-02
2025-10-17 17:45:45,836 | INFO | Epoch 1421 Train Time 98.43799591064453s

2025-10-17 17:47:18,282 | INFO | Training epoch 1422, Batch 1000/1000: LR=2.01e-05, Loss=3.80e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 17:47:18,333 | INFO | Epoch 1422 Train Time 92.49613428115845s

2025-10-17 17:48:56,586 | INFO | Training epoch 1423, Batch 1000/1000: LR=2.00e-05, Loss=3.78e-03 BER=1.58e-03 FER=1.36e-02
2025-10-17 17:48:56,637 | INFO | Epoch 1423 Train Time 98.30295252799988s

2025-10-17 17:50:26,377 | INFO | Training epoch 1424, Batch 1000/1000: LR=2.00e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 17:50:26,440 | INFO | Epoch 1424 Train Time 89.8013288974762s

2025-10-17 17:52:06,380 | INFO | Training epoch 1425, Batch 1000/1000: LR=1.99e-05, Loss=4.22e-03 BER=1.72e-03 FER=1.47e-02
2025-10-17 17:52:06,436 | INFO | Epoch 1425 Train Time 99.99512529373169s

2025-10-17 17:53:42,881 | INFO | Training epoch 1426, Batch 1000/1000: LR=1.99e-05, Loss=3.81e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 17:53:42,947 | INFO | Epoch 1426 Train Time 96.50943398475647s

2025-10-17 17:55:21,697 | INFO | Training epoch 1427, Batch 1000/1000: LR=1.98e-05, Loss=3.85e-03 BER=1.59e-03 FER=1.38e-02
2025-10-17 17:55:21,754 | INFO | Epoch 1427 Train Time 98.80587792396545s

2025-10-17 17:56:58,799 | INFO | Training epoch 1428, Batch 1000/1000: LR=1.97e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.38e-02
2025-10-17 17:56:58,858 | INFO | Epoch 1428 Train Time 97.10090613365173s

2025-10-17 17:58:40,008 | INFO | Training epoch 1429, Batch 1000/1000: LR=1.97e-05, Loss=3.70e-03 BER=1.51e-03 FER=1.32e-02
2025-10-17 17:58:40,067 | INFO | Epoch 1429 Train Time 101.2080590724945s

2025-10-17 18:00:18,759 | INFO | Training epoch 1430, Batch 1000/1000: LR=1.96e-05, Loss=3.80e-03 BER=1.58e-03 FER=1.37e-02
2025-10-17 18:00:18,816 | INFO | Epoch 1430 Train Time 98.74793410301208s

2025-10-17 18:02:02,305 | INFO | Training epoch 1431, Batch 1000/1000: LR=1.96e-05, Loss=3.79e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 18:02:02,363 | INFO | Epoch 1431 Train Time 103.54497647285461s

2025-10-17 18:03:44,614 | INFO | Training epoch 1432, Batch 1000/1000: LR=1.95e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.37e-02
2025-10-17 18:03:44,686 | INFO | Epoch 1432 Train Time 102.32160353660583s

2025-10-17 18:05:20,571 | INFO | Training epoch 1433, Batch 1000/1000: LR=1.94e-05, Loss=3.82e-03 BER=1.59e-03 FER=1.34e-02
2025-10-17 18:05:20,611 | INFO | Epoch 1433 Train Time 95.92208003997803s

2025-10-17 18:06:57,034 | INFO | Training epoch 1434, Batch 1000/1000: LR=1.94e-05, Loss=3.70e-03 BER=1.49e-03 FER=1.29e-02
2025-10-17 18:06:57,087 | INFO | Epoch 1434 Train Time 96.47503995895386s

2025-10-17 18:08:36,540 | INFO | Training epoch 1435, Batch 1000/1000: LR=1.93e-05, Loss=3.64e-03 BER=1.51e-03 FER=1.31e-02
2025-10-17 18:08:36,608 | INFO | Epoch 1435 Train Time 99.51924324035645s

2025-10-17 18:10:11,988 | INFO | Training epoch 1436, Batch 1000/1000: LR=1.92e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.35e-02
2025-10-17 18:10:12,057 | INFO | Epoch 1436 Train Time 95.44820499420166s

2025-10-17 18:11:45,491 | INFO | Training epoch 1437, Batch 1000/1000: LR=1.92e-05, Loss=3.67e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 18:11:45,542 | INFO | Epoch 1437 Train Time 93.48298859596252s

2025-10-17 18:13:25,865 | INFO | Training epoch 1438, Batch 1000/1000: LR=1.91e-05, Loss=3.97e-03 BER=1.61e-03 FER=1.37e-02
2025-10-17 18:13:25,936 | INFO | Epoch 1438 Train Time 100.39244222640991s

2025-10-17 18:15:03,335 | INFO | Training epoch 1439, Batch 1000/1000: LR=1.91e-05, Loss=3.63e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 18:15:03,393 | INFO | Epoch 1439 Train Time 97.45581293106079s

2025-10-17 18:16:50,152 | INFO | Training epoch 1440, Batch 1000/1000: LR=1.90e-05, Loss=3.94e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 18:16:50,212 | INFO | Epoch 1440 Train Time 106.81811213493347s

2025-10-17 18:18:22,286 | INFO | Training epoch 1441, Batch 1000/1000: LR=1.89e-05, Loss=3.64e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 18:18:22,347 | INFO | Epoch 1441 Train Time 92.13347053527832s

2025-10-17 18:20:00,935 | INFO | Training epoch 1442, Batch 1000/1000: LR=1.89e-05, Loss=3.94e-03 BER=1.59e-03 FER=1.38e-02
2025-10-17 18:20:00,995 | INFO | Epoch 1442 Train Time 98.64640879631042s

2025-10-17 18:21:47,941 | INFO | Training epoch 1443, Batch 1000/1000: LR=1.88e-05, Loss=3.78e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 18:21:48,012 | INFO | Epoch 1443 Train Time 107.01643800735474s

2025-10-17 18:23:23,675 | INFO | Training epoch 1444, Batch 1000/1000: LR=1.88e-05, Loss=3.71e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 18:23:23,739 | INFO | Epoch 1444 Train Time 95.7246458530426s

2025-10-17 18:24:59,713 | INFO | Training epoch 1445, Batch 1000/1000: LR=1.87e-05, Loss=3.74e-03 BER=1.56e-03 FER=1.35e-02
2025-10-17 18:24:59,769 | INFO | Epoch 1445 Train Time 96.02808904647827s

2025-10-17 18:26:38,352 | INFO | Training epoch 1446, Batch 1000/1000: LR=1.86e-05, Loss=3.65e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 18:26:38,416 | INFO | Epoch 1446 Train Time 98.6463303565979s

2025-10-17 18:28:12,671 | INFO | Training epoch 1447, Batch 1000/1000: LR=1.86e-05, Loss=3.67e-03 BER=1.49e-03 FER=1.31e-02
2025-10-17 18:28:12,733 | INFO | Epoch 1447 Train Time 94.31474590301514s

2025-10-17 18:29:46,899 | INFO | Training epoch 1448, Batch 1000/1000: LR=1.85e-05, Loss=3.76e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 18:29:46,947 | INFO | Epoch 1448 Train Time 94.21321487426758s

2025-10-17 18:31:30,002 | INFO | Training epoch 1449, Batch 1000/1000: LR=1.85e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 18:31:30,068 | INFO | Epoch 1449 Train Time 103.1202929019928s

2025-10-17 18:33:12,498 | INFO | Training epoch 1450, Batch 1000/1000: LR=1.84e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 18:33:12,560 | INFO | Epoch 1450 Train Time 102.49066925048828s

2025-10-17 18:34:50,420 | INFO | Training epoch 1451, Batch 1000/1000: LR=1.84e-05, Loss=3.96e-03 BER=1.64e-03 FER=1.41e-02
2025-10-17 18:34:50,480 | INFO | Epoch 1451 Train Time 97.91794776916504s

2025-10-17 18:36:31,939 | INFO | Training epoch 1452, Batch 1000/1000: LR=1.83e-05, Loss=3.98e-03 BER=1.63e-03 FER=1.39e-02
2025-10-17 18:36:32,020 | INFO | Epoch 1452 Train Time 101.53846549987793s

2025-10-17 18:38:11,791 | INFO | Training epoch 1453, Batch 1000/1000: LR=1.82e-05, Loss=3.76e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 18:38:11,837 | INFO | Epoch 1453 Train Time 99.81504154205322s

2025-10-17 18:39:49,432 | INFO | Training epoch 1454, Batch 1000/1000: LR=1.82e-05, Loss=3.84e-03 BER=1.57e-03 FER=1.34e-02
2025-10-17 18:39:49,487 | INFO | Epoch 1454 Train Time 97.64910197257996s

2025-10-17 18:41:26,897 | INFO | Training epoch 1455, Batch 1000/1000: LR=1.81e-05, Loss=3.95e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 18:41:26,957 | INFO | Epoch 1455 Train Time 97.46852827072144s

2025-10-17 18:43:00,369 | INFO | Training epoch 1456, Batch 1000/1000: LR=1.81e-05, Loss=3.76e-03 BER=1.52e-03 FER=1.32e-02
2025-10-17 18:43:00,421 | INFO | Epoch 1456 Train Time 93.46163058280945s

2025-10-17 18:44:36,353 | INFO | Training epoch 1457, Batch 1000/1000: LR=1.80e-05, Loss=3.53e-03 BER=1.46e-03 FER=1.26e-02
2025-10-17 18:44:36,413 | INFO | Epoch 1457 Train Time 95.98931813240051s

2025-10-17 18:44:36,414 | INFO | [P2] saving best_model (QAT) with loss 0.003528 at epoch 1457
2025-10-17 18:46:16,496 | INFO | Training epoch 1458, Batch 1000/1000: LR=1.79e-05, Loss=3.90e-03 BER=1.58e-03 FER=1.35e-02
2025-10-17 18:46:16,556 | INFO | Epoch 1458 Train Time 100.04022240638733s

2025-10-17 18:47:56,471 | INFO | Training epoch 1459, Batch 1000/1000: LR=1.79e-05, Loss=3.63e-03 BER=1.48e-03 FER=1.27e-02
2025-10-17 18:47:56,534 | INFO | Epoch 1459 Train Time 99.9758653640747s

2025-10-17 18:49:40,256 | INFO | Training epoch 1460, Batch 1000/1000: LR=1.78e-05, Loss=3.64e-03 BER=1.49e-03 FER=1.30e-02
2025-10-17 18:49:40,332 | INFO | Epoch 1460 Train Time 103.79720568656921s

2025-10-17 18:51:16,341 | INFO | Training epoch 1461, Batch 1000/1000: LR=1.78e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 18:51:16,393 | INFO | Epoch 1461 Train Time 96.06026029586792s

2025-10-17 18:53:01,100 | INFO | Training epoch 1462, Batch 1000/1000: LR=1.77e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 18:53:01,160 | INFO | Epoch 1462 Train Time 104.76468110084534s

2025-10-17 18:54:40,153 | INFO | Training epoch 1463, Batch 1000/1000: LR=1.76e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 18:54:40,213 | INFO | Epoch 1463 Train Time 99.05252361297607s

2025-10-17 18:56:24,048 | INFO | Training epoch 1464, Batch 1000/1000: LR=1.76e-05, Loss=3.85e-03 BER=1.59e-03 FER=1.38e-02
2025-10-17 18:56:24,101 | INFO | Epoch 1464 Train Time 103.88612699508667s

2025-10-17 18:58:00,894 | INFO | Training epoch 1465, Batch 1000/1000: LR=1.75e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.36e-02
2025-10-17 18:58:00,954 | INFO | Epoch 1465 Train Time 96.85243654251099s

2025-10-17 18:59:38,147 | INFO | Training epoch 1466, Batch 1000/1000: LR=1.75e-05, Loss=3.74e-03 BER=1.55e-03 FER=1.30e-02
2025-10-17 18:59:38,193 | INFO | Epoch 1466 Train Time 97.23737859725952s

2025-10-17 19:01:15,175 | INFO | Training epoch 1467, Batch 1000/1000: LR=1.74e-05, Loss=3.67e-03 BER=1.55e-03 FER=1.30e-02
2025-10-17 19:01:15,223 | INFO | Epoch 1467 Train Time 97.02841687202454s

2025-10-17 19:02:54,905 | INFO | Training epoch 1468, Batch 1000/1000: LR=1.74e-05, Loss=3.88e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 19:02:54,971 | INFO | Epoch 1468 Train Time 99.74704003334045s

2025-10-17 19:04:32,580 | INFO | Training epoch 1469, Batch 1000/1000: LR=1.73e-05, Loss=3.51e-03 BER=1.42e-03 FER=1.24e-02
2025-10-17 19:04:32,635 | INFO | Epoch 1469 Train Time 97.6633517742157s

2025-10-17 19:04:32,635 | INFO | [P2] saving best_model (QAT) with loss 0.003508 at epoch 1469
2025-10-17 19:06:12,804 | INFO | Training epoch 1470, Batch 1000/1000: LR=1.72e-05, Loss=3.85e-03 BER=1.58e-03 FER=1.36e-02
2025-10-17 19:06:12,846 | INFO | Epoch 1470 Train Time 100.14031147956848s

2025-10-17 19:07:55,948 | INFO | Training epoch 1471, Batch 1000/1000: LR=1.72e-05, Loss=3.68e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 19:07:55,999 | INFO | Epoch 1471 Train Time 103.15161490440369s

2025-10-17 19:09:28,175 | INFO | Training epoch 1472, Batch 1000/1000: LR=1.71e-05, Loss=3.75e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 19:09:28,249 | INFO | Epoch 1472 Train Time 92.24874448776245s

2025-10-17 19:11:09,192 | INFO | Training epoch 1473, Batch 1000/1000: LR=1.71e-05, Loss=3.93e-03 BER=1.62e-03 FER=1.40e-02
2025-10-17 19:11:09,250 | INFO | Epoch 1473 Train Time 100.99961590766907s

2025-10-17 19:12:45,375 | INFO | Training epoch 1474, Batch 1000/1000: LR=1.70e-05, Loss=3.86e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 19:12:45,438 | INFO | Epoch 1474 Train Time 96.1864800453186s

2025-10-17 19:14:19,554 | INFO | Training epoch 1475, Batch 1000/1000: LR=1.70e-05, Loss=3.57e-03 BER=1.47e-03 FER=1.25e-02
2025-10-17 19:14:19,606 | INFO | Epoch 1475 Train Time 94.16673159599304s

2025-10-17 19:15:51,389 | INFO | Training epoch 1476, Batch 1000/1000: LR=1.69e-05, Loss=3.74e-03 BER=1.55e-03 FER=1.32e-02
2025-10-17 19:15:51,449 | INFO | Epoch 1476 Train Time 91.8410632610321s

2025-10-17 19:17:30,300 | INFO | Training epoch 1477, Batch 1000/1000: LR=1.68e-05, Loss=3.78e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 19:17:30,354 | INFO | Epoch 1477 Train Time 98.90436506271362s

2025-10-17 19:19:08,988 | INFO | Training epoch 1478, Batch 1000/1000: LR=1.68e-05, Loss=3.68e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 19:19:09,046 | INFO | Epoch 1478 Train Time 98.6903326511383s

2025-10-17 19:20:46,270 | INFO | Training epoch 1479, Batch 1000/1000: LR=1.67e-05, Loss=3.63e-03 BER=1.49e-03 FER=1.28e-02
2025-10-17 19:20:46,330 | INFO | Epoch 1479 Train Time 97.28248238563538s

2025-10-17 19:22:24,798 | INFO | Training epoch 1480, Batch 1000/1000: LR=1.67e-05, Loss=3.63e-03 BER=1.51e-03 FER=1.30e-02
2025-10-17 19:22:24,851 | INFO | Epoch 1480 Train Time 98.52020955085754s

2025-10-17 19:24:02,542 | INFO | Training epoch 1481, Batch 1000/1000: LR=1.66e-05, Loss=3.82e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 19:24:02,599 | INFO | Epoch 1481 Train Time 97.74448418617249s

2025-10-17 19:25:38,090 | INFO | Training epoch 1482, Batch 1000/1000: LR=1.66e-05, Loss=3.87e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 19:25:38,151 | INFO | Epoch 1482 Train Time 95.55074214935303s

2025-10-17 19:27:12,312 | INFO | Training epoch 1483, Batch 1000/1000: LR=1.65e-05, Loss=3.83e-03 BER=1.59e-03 FER=1.35e-02
2025-10-17 19:27:12,378 | INFO | Epoch 1483 Train Time 94.22561287879944s

2025-10-17 19:28:51,214 | INFO | Training epoch 1484, Batch 1000/1000: LR=1.64e-05, Loss=3.77e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 19:28:51,274 | INFO | Epoch 1484 Train Time 98.89491033554077s

2025-10-17 19:30:27,245 | INFO | Training epoch 1485, Batch 1000/1000: LR=1.64e-05, Loss=3.79e-03 BER=1.58e-03 FER=1.37e-02
2025-10-17 19:30:27,308 | INFO | Epoch 1485 Train Time 96.03285217285156s

2025-10-17 19:32:05,115 | INFO | Training epoch 1486, Batch 1000/1000: LR=1.63e-05, Loss=3.63e-03 BER=1.49e-03 FER=1.30e-02
2025-10-17 19:32:05,171 | INFO | Epoch 1486 Train Time 97.86203837394714s

2025-10-17 19:33:38,593 | INFO | Training epoch 1487, Batch 1000/1000: LR=1.63e-05, Loss=3.85e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 19:33:38,662 | INFO | Epoch 1487 Train Time 93.48976397514343s

2025-10-17 19:35:18,051 | INFO | Training epoch 1488, Batch 1000/1000: LR=1.62e-05, Loss=3.63e-03 BER=1.47e-03 FER=1.28e-02
2025-10-17 19:35:18,120 | INFO | Epoch 1488 Train Time 99.45665884017944s

2025-10-17 19:36:58,523 | INFO | Training epoch 1489, Batch 1000/1000: LR=1.62e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 19:36:58,582 | INFO | Epoch 1489 Train Time 100.46013975143433s

2025-10-17 19:38:32,423 | INFO | Training epoch 1490, Batch 1000/1000: LR=1.61e-05, Loss=3.80e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 19:38:32,481 | INFO | Epoch 1490 Train Time 93.8979721069336s

2025-10-17 19:40:09,316 | INFO | Training epoch 1491, Batch 1000/1000: LR=1.61e-05, Loss=3.76e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 19:40:09,376 | INFO | Epoch 1491 Train Time 96.8930983543396s

2025-10-17 19:41:46,916 | INFO | Training epoch 1492, Batch 1000/1000: LR=1.60e-05, Loss=3.61e-03 BER=1.47e-03 FER=1.28e-02
2025-10-17 19:41:46,977 | INFO | Epoch 1492 Train Time 97.60046482086182s

2025-10-17 19:43:24,869 | INFO | Training epoch 1493, Batch 1000/1000: LR=1.59e-05, Loss=3.63e-03 BER=1.52e-03 FER=1.33e-02
2025-10-17 19:43:24,930 | INFO | Epoch 1493 Train Time 97.95115327835083s

2025-10-17 19:45:03,551 | INFO | Training epoch 1494, Batch 1000/1000: LR=1.59e-05, Loss=3.75e-03 BER=1.53e-03 FER=1.32e-02
2025-10-17 19:45:03,625 | INFO | Epoch 1494 Train Time 98.69420170783997s

2025-10-17 19:46:33,760 | INFO | Training epoch 1495, Batch 1000/1000: LR=1.58e-05, Loss=3.77e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 19:46:33,814 | INFO | Epoch 1495 Train Time 90.1850597858429s

2025-10-17 19:48:13,210 | INFO | Training epoch 1496, Batch 1000/1000: LR=1.58e-05, Loss=3.83e-03 BER=1.58e-03 FER=1.36e-02
2025-10-17 19:48:13,268 | INFO | Epoch 1496 Train Time 99.45275568962097s

2025-10-17 19:49:45,312 | INFO | Training epoch 1497, Batch 1000/1000: LR=1.57e-05, Loss=3.78e-03 BER=1.57e-03 FER=1.36e-02
2025-10-17 19:49:45,379 | INFO | Epoch 1497 Train Time 92.109050989151s

2025-10-17 19:51:18,845 | INFO | Training epoch 1498, Batch 1000/1000: LR=1.57e-05, Loss=3.68e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 19:51:18,906 | INFO | Epoch 1498 Train Time 93.52625870704651s

2025-10-17 19:52:53,987 | INFO | Training epoch 1499, Batch 1000/1000: LR=1.56e-05, Loss=3.80e-03 BER=1.58e-03 FER=1.35e-02
2025-10-17 19:52:54,045 | INFO | Epoch 1499 Train Time 95.13762021064758s

2025-10-17 19:54:28,915 | INFO | Training epoch 1500, Batch 1000/1000: LR=1.56e-05, Loss=3.66e-03 BER=1.53e-03 FER=1.29e-02
2025-10-17 19:54:28,981 | INFO | Epoch 1500 Train Time 94.93290495872498s

2025-10-17 19:56:04,072 | INFO | Training epoch 1501, Batch 1000/1000: LR=1.55e-05, Loss=3.78e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 19:56:04,137 | INFO | Epoch 1501 Train Time 95.15550017356873s

2025-10-17 19:57:48,984 | INFO | Training epoch 1502, Batch 1000/1000: LR=1.54e-05, Loss=3.73e-03 BER=1.54e-03 FER=1.31e-02
2025-10-17 19:57:49,046 | INFO | Epoch 1502 Train Time 104.90707159042358s

2025-10-17 19:59:29,200 | INFO | Training epoch 1503, Batch 1000/1000: LR=1.54e-05, Loss=3.79e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 19:59:29,260 | INFO | Epoch 1503 Train Time 100.21186351776123s

2025-10-17 20:01:05,832 | INFO | Training epoch 1504, Batch 1000/1000: LR=1.53e-05, Loss=3.64e-03 BER=1.48e-03 FER=1.29e-02
2025-10-17 20:01:05,895 | INFO | Epoch 1504 Train Time 96.63392519950867s

2025-10-17 20:02:44,412 | INFO | Training epoch 1505, Batch 1000/1000: LR=1.53e-05, Loss=3.73e-03 BER=1.53e-03 FER=1.32e-02
2025-10-17 20:02:44,475 | INFO | Epoch 1505 Train Time 98.57898783683777s

2025-10-17 20:04:24,640 | INFO | Training epoch 1506, Batch 1000/1000: LR=1.52e-05, Loss=3.57e-03 BER=1.46e-03 FER=1.28e-02
2025-10-17 20:04:24,684 | INFO | Epoch 1506 Train Time 100.20709705352783s

2025-10-17 20:05:53,791 | INFO | Training epoch 1507, Batch 1000/1000: LR=1.52e-05, Loss=3.85e-03 BER=1.59e-03 FER=1.38e-02
2025-10-17 20:05:53,857 | INFO | Epoch 1507 Train Time 89.17061734199524s

2025-10-17 20:07:38,200 | INFO | Training epoch 1508, Batch 1000/1000: LR=1.51e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 20:07:38,269 | INFO | Epoch 1508 Train Time 104.41143226623535s

2025-10-17 20:09:19,689 | INFO | Training epoch 1509, Batch 1000/1000: LR=1.51e-05, Loss=3.74e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 20:09:19,749 | INFO | Epoch 1509 Train Time 101.47915148735046s

2025-10-17 20:10:58,205 | INFO | Training epoch 1510, Batch 1000/1000: LR=1.50e-05, Loss=3.83e-03 BER=1.58e-03 FER=1.35e-02
2025-10-17 20:10:58,263 | INFO | Epoch 1510 Train Time 98.51081871986389s

2025-10-17 20:12:33,251 | INFO | Training epoch 1511, Batch 1000/1000: LR=1.50e-05, Loss=3.90e-03 BER=1.62e-03 FER=1.39e-02
2025-10-17 20:12:33,314 | INFO | Epoch 1511 Train Time 95.04996681213379s

2025-10-17 20:14:10,691 | INFO | Training epoch 1512, Batch 1000/1000: LR=1.49e-05, Loss=3.71e-03 BER=1.50e-03 FER=1.30e-02
2025-10-17 20:14:10,752 | INFO | Epoch 1512 Train Time 97.43727707862854s

2025-10-17 20:15:47,024 | INFO | Training epoch 1513, Batch 1000/1000: LR=1.48e-05, Loss=3.71e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 20:15:47,084 | INFO | Epoch 1513 Train Time 96.33084464073181s

2025-10-17 20:17:24,595 | INFO | Training epoch 1514, Batch 1000/1000: LR=1.48e-05, Loss=3.78e-03 BER=1.55e-03 FER=1.32e-02
2025-10-17 20:17:24,666 | INFO | Epoch 1514 Train Time 97.58002495765686s

2025-10-17 20:19:05,523 | INFO | Training epoch 1515, Batch 1000/1000: LR=1.47e-05, Loss=3.83e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 20:19:05,590 | INFO | Epoch 1515 Train Time 100.92182803153992s

2025-10-17 20:20:48,484 | INFO | Training epoch 1516, Batch 1000/1000: LR=1.47e-05, Loss=3.73e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 20:20:48,548 | INFO | Epoch 1516 Train Time 102.95688056945801s

2025-10-17 20:22:28,278 | INFO | Training epoch 1517, Batch 1000/1000: LR=1.46e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 20:22:28,354 | INFO | Epoch 1517 Train Time 99.80464029312134s

2025-10-17 20:24:07,419 | INFO | Training epoch 1518, Batch 1000/1000: LR=1.46e-05, Loss=3.91e-03 BER=1.59e-03 FER=1.37e-02
2025-10-17 20:24:07,497 | INFO | Epoch 1518 Train Time 99.14107537269592s

2025-10-17 20:25:47,142 | INFO | Training epoch 1519, Batch 1000/1000: LR=1.45e-05, Loss=3.69e-03 BER=1.51e-03 FER=1.28e-02
2025-10-17 20:25:47,198 | INFO | Epoch 1519 Train Time 99.69968438148499s

2025-10-17 20:27:30,596 | INFO | Training epoch 1520, Batch 1000/1000: LR=1.45e-05, Loss=3.72e-03 BER=1.52e-03 FER=1.32e-02
2025-10-17 20:27:30,660 | INFO | Epoch 1520 Train Time 103.45918035507202s

2025-10-17 20:29:08,958 | INFO | Training epoch 1521, Batch 1000/1000: LR=1.44e-05, Loss=3.83e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 20:29:09,004 | INFO | Epoch 1521 Train Time 98.34207010269165s

2025-10-17 20:30:44,486 | INFO | Training epoch 1522, Batch 1000/1000: LR=1.44e-05, Loss=3.65e-03 BER=1.51e-03 FER=1.31e-02
2025-10-17 20:30:44,538 | INFO | Epoch 1522 Train Time 95.53262448310852s

2025-10-17 20:32:18,571 | INFO | Training epoch 1523, Batch 1000/1000: LR=1.43e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.32e-02
2025-10-17 20:32:18,628 | INFO | Epoch 1523 Train Time 94.08828234672546s

2025-10-17 20:33:59,558 | INFO | Training epoch 1524, Batch 1000/1000: LR=1.43e-05, Loss=3.68e-03 BER=1.51e-03 FER=1.32e-02
2025-10-17 20:33:59,610 | INFO | Epoch 1524 Train Time 100.9796884059906s

2025-10-17 20:35:40,688 | INFO | Training epoch 1525, Batch 1000/1000: LR=1.42e-05, Loss=3.78e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 20:35:40,757 | INFO | Epoch 1525 Train Time 101.14501452445984s

2025-10-17 20:37:14,765 | INFO | Training epoch 1526, Batch 1000/1000: LR=1.42e-05, Loss=3.73e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 20:37:14,815 | INFO | Epoch 1526 Train Time 94.05681014060974s

2025-10-17 20:38:53,443 | INFO | Training epoch 1527, Batch 1000/1000: LR=1.41e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 20:38:53,495 | INFO | Epoch 1527 Train Time 98.67846012115479s

2025-10-17 20:40:35,304 | INFO | Training epoch 1528, Batch 1000/1000: LR=1.40e-05, Loss=3.81e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 20:40:35,367 | INFO | Epoch 1528 Train Time 101.87076878547668s

2025-10-17 20:42:14,158 | INFO | Training epoch 1529, Batch 1000/1000: LR=1.40e-05, Loss=3.72e-03 BER=1.54e-03 FER=1.34e-02
2025-10-17 20:42:14,234 | INFO | Epoch 1529 Train Time 98.86473321914673s

2025-10-17 20:43:53,573 | INFO | Training epoch 1530, Batch 1000/1000: LR=1.39e-05, Loss=3.75e-03 BER=1.54e-03 FER=1.34e-02
2025-10-17 20:43:53,642 | INFO | Epoch 1530 Train Time 99.40604567527771s

2025-10-17 20:45:37,051 | INFO | Training epoch 1531, Batch 1000/1000: LR=1.39e-05, Loss=3.89e-03 BER=1.60e-03 FER=1.38e-02
2025-10-17 20:45:37,115 | INFO | Epoch 1531 Train Time 103.47257971763611s

2025-10-17 20:47:17,703 | INFO | Training epoch 1532, Batch 1000/1000: LR=1.38e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.35e-02
2025-10-17 20:47:17,770 | INFO | Epoch 1532 Train Time 100.65203857421875s

2025-10-17 20:49:00,468 | INFO | Training epoch 1533, Batch 1000/1000: LR=1.38e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.34e-02
2025-10-17 20:49:00,537 | INFO | Epoch 1533 Train Time 102.7652735710144s

2025-10-17 20:50:47,314 | INFO | Training epoch 1534, Batch 1000/1000: LR=1.37e-05, Loss=3.73e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 20:50:47,378 | INFO | Epoch 1534 Train Time 106.84034419059753s

2025-10-17 20:52:23,880 | INFO | Training epoch 1535, Batch 1000/1000: LR=1.37e-05, Loss=3.77e-03 BER=1.56e-03 FER=1.32e-02
2025-10-17 20:52:23,921 | INFO | Epoch 1535 Train Time 96.54167056083679s

2025-10-17 20:54:04,792 | INFO | Training epoch 1536, Batch 1000/1000: LR=1.36e-05, Loss=3.73e-03 BER=1.50e-03 FER=1.32e-02
2025-10-17 20:54:04,846 | INFO | Epoch 1536 Train Time 100.92436385154724s

2025-10-17 20:55:52,080 | INFO | Training epoch 1537, Batch 1000/1000: LR=1.36e-05, Loss=3.81e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 20:55:52,136 | INFO | Epoch 1537 Train Time 107.28702092170715s

2025-10-17 20:57:35,352 | INFO | Training epoch 1538, Batch 1000/1000: LR=1.35e-05, Loss=3.57e-03 BER=1.46e-03 FER=1.24e-02
2025-10-17 20:57:35,410 | INFO | Epoch 1538 Train Time 103.27273297309875s

2025-10-17 20:59:10,412 | INFO | Training epoch 1539, Batch 1000/1000: LR=1.35e-05, Loss=3.65e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 20:59:10,470 | INFO | Epoch 1539 Train Time 95.05910968780518s

2025-10-17 21:00:49,435 | INFO | Training epoch 1540, Batch 1000/1000: LR=1.34e-05, Loss=3.62e-03 BER=1.49e-03 FER=1.30e-02
2025-10-17 21:00:49,483 | INFO | Epoch 1540 Train Time 99.01156806945801s

2025-10-17 21:02:33,219 | INFO | Training epoch 1541, Batch 1000/1000: LR=1.34e-05, Loss=3.63e-03 BER=1.49e-03 FER=1.26e-02
2025-10-17 21:02:33,289 | INFO | Epoch 1541 Train Time 103.80483436584473s

2025-10-17 21:04:08,389 | INFO | Training epoch 1542, Batch 1000/1000: LR=1.33e-05, Loss=3.72e-03 BER=1.51e-03 FER=1.29e-02
2025-10-17 21:04:08,462 | INFO | Epoch 1542 Train Time 95.17209911346436s

2025-10-17 21:05:44,423 | INFO | Training epoch 1543, Batch 1000/1000: LR=1.33e-05, Loss=3.56e-03 BER=1.49e-03 FER=1.25e-02
2025-10-17 21:05:44,482 | INFO | Epoch 1543 Train Time 96.01838040351868s

2025-10-17 21:07:20,672 | INFO | Training epoch 1544, Batch 1000/1000: LR=1.32e-05, Loss=3.70e-03 BER=1.54e-03 FER=1.29e-02
2025-10-17 21:07:20,734 | INFO | Epoch 1544 Train Time 96.2504198551178s

2025-10-17 21:09:03,334 | INFO | Training epoch 1545, Batch 1000/1000: LR=1.32e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 21:09:03,400 | INFO | Epoch 1545 Train Time 102.66459035873413s

2025-10-17 21:10:39,344 | INFO | Training epoch 1546, Batch 1000/1000: LR=1.31e-05, Loss=3.66e-03 BER=1.54e-03 FER=1.31e-02
2025-10-17 21:10:39,393 | INFO | Epoch 1546 Train Time 95.99053907394409s

2025-10-17 21:12:17,842 | INFO | Training epoch 1547, Batch 1000/1000: LR=1.31e-05, Loss=3.73e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 21:12:17,893 | INFO | Epoch 1547 Train Time 98.49862575531006s

2025-10-17 21:13:58,955 | INFO | Training epoch 1548, Batch 1000/1000: LR=1.30e-05, Loss=3.64e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 21:13:59,024 | INFO | Epoch 1548 Train Time 101.12975025177002s

2025-10-17 21:15:45,456 | INFO | Training epoch 1549, Batch 1000/1000: LR=1.30e-05, Loss=3.70e-03 BER=1.49e-03 FER=1.26e-02
2025-10-17 21:15:45,531 | INFO | Epoch 1549 Train Time 106.50511574745178s

2025-10-17 21:17:34,607 | INFO | Training epoch 1550, Batch 1000/1000: LR=1.29e-05, Loss=3.73e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 21:17:34,675 | INFO | Epoch 1550 Train Time 109.14037227630615s

2025-10-17 21:19:16,253 | INFO | Training epoch 1551, Batch 1000/1000: LR=1.29e-05, Loss=3.69e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 21:19:16,324 | INFO | Epoch 1551 Train Time 101.64781332015991s

2025-10-17 21:20:55,989 | INFO | Training epoch 1552, Batch 1000/1000: LR=1.28e-05, Loss=3.70e-03 BER=1.51e-03 FER=1.30e-02
2025-10-17 21:20:56,046 | INFO | Epoch 1552 Train Time 99.72077465057373s

2025-10-17 21:22:35,144 | INFO | Training epoch 1553, Batch 1000/1000: LR=1.28e-05, Loss=3.67e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 21:22:35,207 | INFO | Epoch 1553 Train Time 99.15965533256531s

2025-10-17 21:24:13,715 | INFO | Training epoch 1554, Batch 1000/1000: LR=1.27e-05, Loss=4.08e-03 BER=1.65e-03 FER=1.42e-02
2025-10-17 21:24:13,770 | INFO | Epoch 1554 Train Time 98.56107091903687s

2025-10-17 21:25:57,589 | INFO | Training epoch 1555, Batch 1000/1000: LR=1.27e-05, Loss=3.69e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 21:25:57,646 | INFO | Epoch 1555 Train Time 103.87440347671509s

2025-10-17 21:27:38,963 | INFO | Training epoch 1556, Batch 1000/1000: LR=1.26e-05, Loss=3.77e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 21:27:39,025 | INFO | Epoch 1556 Train Time 101.37782669067383s

2025-10-17 21:29:15,081 | INFO | Training epoch 1557, Batch 1000/1000: LR=1.26e-05, Loss=3.69e-03 BER=1.53e-03 FER=1.30e-02
2025-10-17 21:29:15,140 | INFO | Epoch 1557 Train Time 96.11362862586975s

2025-10-17 21:31:00,088 | INFO | Training epoch 1558, Batch 1000/1000: LR=1.25e-05, Loss=3.66e-03 BER=1.49e-03 FER=1.29e-02
2025-10-17 21:31:00,148 | INFO | Epoch 1558 Train Time 105.00752925872803s

2025-10-17 21:32:38,187 | INFO | Training epoch 1559, Batch 1000/1000: LR=1.25e-05, Loss=3.77e-03 BER=1.60e-03 FER=1.35e-02
2025-10-17 21:32:38,255 | INFO | Epoch 1559 Train Time 98.10447287559509s

2025-10-17 21:34:23,534 | INFO | Training epoch 1560, Batch 1000/1000: LR=1.24e-05, Loss=3.74e-03 BER=1.51e-03 FER=1.32e-02
2025-10-17 21:34:23,591 | INFO | Epoch 1560 Train Time 105.33362317085266s

2025-10-17 21:36:05,911 | INFO | Training epoch 1561, Batch 1000/1000: LR=1.24e-05, Loss=3.72e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 21:36:05,976 | INFO | Epoch 1561 Train Time 102.38262963294983s

2025-10-17 21:37:47,984 | INFO | Training epoch 1562, Batch 1000/1000: LR=1.23e-05, Loss=3.75e-03 BER=1.55e-03 FER=1.32e-02
2025-10-17 21:37:48,038 | INFO | Epoch 1562 Train Time 102.05928945541382s

2025-10-17 21:39:29,920 | INFO | Training epoch 1563, Batch 1000/1000: LR=1.23e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.34e-02
2025-10-17 21:39:29,964 | INFO | Epoch 1563 Train Time 101.92516374588013s

2025-10-17 21:41:10,831 | INFO | Training epoch 1564, Batch 1000/1000: LR=1.22e-05, Loss=3.78e-03 BER=1.57e-03 FER=1.35e-02
2025-10-17 21:41:10,896 | INFO | Epoch 1564 Train Time 100.93037986755371s

2025-10-17 21:42:52,494 | INFO | Training epoch 1565, Batch 1000/1000: LR=1.22e-05, Loss=3.76e-03 BER=1.55e-03 FER=1.31e-02
2025-10-17 21:42:52,565 | INFO | Epoch 1565 Train Time 101.66693449020386s

2025-10-17 21:44:40,731 | INFO | Training epoch 1566, Batch 1000/1000: LR=1.21e-05, Loss=3.58e-03 BER=1.44e-03 FER=1.23e-02
2025-10-17 21:44:40,787 | INFO | Epoch 1566 Train Time 108.2213842868805s

2025-10-17 21:46:26,093 | INFO | Training epoch 1567, Batch 1000/1000: LR=1.21e-05, Loss=3.77e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 21:46:26,177 | INFO | Epoch 1567 Train Time 105.38831448554993s

2025-10-17 21:48:02,807 | INFO | Training epoch 1568, Batch 1000/1000: LR=1.20e-05, Loss=3.86e-03 BER=1.61e-03 FER=1.38e-02
2025-10-17 21:48:02,857 | INFO | Epoch 1568 Train Time 96.67770433425903s

2025-10-17 21:49:41,982 | INFO | Training epoch 1569, Batch 1000/1000: LR=1.20e-05, Loss=3.69e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 21:49:42,054 | INFO | Epoch 1569 Train Time 99.19615149497986s

2025-10-17 21:51:27,682 | INFO | Training epoch 1570, Batch 1000/1000: LR=1.19e-05, Loss=3.81e-03 BER=1.57e-03 FER=1.34e-02
2025-10-17 21:51:27,739 | INFO | Epoch 1570 Train Time 105.68192458152771s

2025-10-17 21:53:15,530 | INFO | Training epoch 1571, Batch 1000/1000: LR=1.19e-05, Loss=3.64e-03 BER=1.50e-03 FER=1.28e-02
2025-10-17 21:53:15,598 | INFO | Epoch 1571 Train Time 107.8576328754425s

2025-10-17 21:55:04,758 | INFO | Training epoch 1572, Batch 1000/1000: LR=1.18e-05, Loss=3.57e-03 BER=1.47e-03 FER=1.25e-02
2025-10-17 21:55:04,819 | INFO | Epoch 1572 Train Time 109.21869111061096s

2025-10-17 21:56:43,171 | INFO | Training epoch 1573, Batch 1000/1000: LR=1.18e-05, Loss=3.87e-03 BER=1.61e-03 FER=1.39e-02
2025-10-17 21:56:43,255 | INFO | Epoch 1573 Train Time 98.43426394462585s

2025-10-17 21:58:23,494 | INFO | Training epoch 1574, Batch 1000/1000: LR=1.17e-05, Loss=3.76e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 21:58:23,558 | INFO | Epoch 1574 Train Time 100.30150604248047s

2025-10-17 22:00:00,993 | INFO | Training epoch 1575, Batch 1000/1000: LR=1.17e-05, Loss=3.75e-03 BER=1.58e-03 FER=1.34e-02
2025-10-17 22:00:01,052 | INFO | Epoch 1575 Train Time 97.49208354949951s

2025-10-17 22:01:39,093 | INFO | Training epoch 1576, Batch 1000/1000: LR=1.16e-05, Loss=3.66e-03 BER=1.52e-03 FER=1.29e-02
2025-10-17 22:01:39,155 | INFO | Epoch 1576 Train Time 98.10108971595764s

2025-10-17 22:03:22,105 | INFO | Training epoch 1577, Batch 1000/1000: LR=1.16e-05, Loss=3.82e-03 BER=1.57e-03 FER=1.33e-02
2025-10-17 22:03:22,188 | INFO | Epoch 1577 Train Time 103.03089356422424s

2025-10-17 22:04:59,526 | INFO | Training epoch 1578, Batch 1000/1000: LR=1.15e-05, Loss=3.74e-03 BER=1.53e-03 FER=1.31e-02
2025-10-17 22:04:59,585 | INFO | Epoch 1578 Train Time 97.39499139785767s

2025-10-17 22:06:39,981 | INFO | Training epoch 1579, Batch 1000/1000: LR=1.15e-05, Loss=3.70e-03 BER=1.51e-03 FER=1.31e-02
2025-10-17 22:06:40,046 | INFO | Epoch 1579 Train Time 100.46002221107483s

2025-10-17 22:08:14,588 | INFO | Training epoch 1580, Batch 1000/1000: LR=1.14e-05, Loss=3.61e-03 BER=1.45e-03 FER=1.27e-02
2025-10-17 22:08:14,654 | INFO | Epoch 1580 Train Time 94.60637068748474s

2025-10-17 22:09:56,495 | INFO | Training epoch 1581, Batch 1000/1000: LR=1.14e-05, Loss=3.71e-03 BER=1.51e-03 FER=1.30e-02
2025-10-17 22:09:56,559 | INFO | Epoch 1581 Train Time 101.90367245674133s

2025-10-17 22:11:37,176 | INFO | Training epoch 1582, Batch 1000/1000: LR=1.13e-05, Loss=3.66e-03 BER=1.52e-03 FER=1.29e-02
2025-10-17 22:11:37,234 | INFO | Epoch 1582 Train Time 100.67371916770935s

2025-10-17 22:13:25,896 | INFO | Training epoch 1583, Batch 1000/1000: LR=1.13e-05, Loss=3.65e-03 BER=1.49e-03 FER=1.30e-02
2025-10-17 22:13:25,956 | INFO | Epoch 1583 Train Time 108.720538854599s

2025-10-17 22:15:06,703 | INFO | Training epoch 1584, Batch 1000/1000: LR=1.12e-05, Loss=3.60e-03 BER=1.47e-03 FER=1.26e-02
2025-10-17 22:15:06,773 | INFO | Epoch 1584 Train Time 100.81579899787903s

2025-10-17 22:16:51,603 | INFO | Training epoch 1585, Batch 1000/1000: LR=1.12e-05, Loss=3.56e-03 BER=1.45e-03 FER=1.25e-02
2025-10-17 22:16:51,664 | INFO | Epoch 1585 Train Time 104.88904929161072s

2025-10-17 22:18:33,700 | INFO | Training epoch 1586, Batch 1000/1000: LR=1.12e-05, Loss=3.80e-03 BER=1.57e-03 FER=1.34e-02
2025-10-17 22:18:33,784 | INFO | Epoch 1586 Train Time 102.11820459365845s

2025-10-17 22:20:16,501 | INFO | Training epoch 1587, Batch 1000/1000: LR=1.11e-05, Loss=3.49e-03 BER=1.43e-03 FER=1.24e-02
2025-10-17 22:20:16,580 | INFO | Epoch 1587 Train Time 102.79466152191162s

2025-10-17 22:20:16,582 | INFO | [P2] saving best_model (QAT) with loss 0.003494 at epoch 1587
2025-10-17 22:21:55,792 | INFO | Training epoch 1588, Batch 1000/1000: LR=1.11e-05, Loss=3.80e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 22:21:55,859 | INFO | Epoch 1588 Train Time 99.1488311290741s

2025-10-17 22:23:36,590 | INFO | Training epoch 1589, Batch 1000/1000: LR=1.10e-05, Loss=3.57e-03 BER=1.46e-03 FER=1.24e-02
2025-10-17 22:23:36,658 | INFO | Epoch 1589 Train Time 100.79705905914307s

2025-10-17 22:25:14,465 | INFO | Training epoch 1590, Batch 1000/1000: LR=1.10e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.36e-02
2025-10-17 22:25:14,533 | INFO | Epoch 1590 Train Time 97.87382817268372s

2025-10-17 22:26:57,440 | INFO | Training epoch 1591, Batch 1000/1000: LR=1.09e-05, Loss=3.67e-03 BER=1.50e-03 FER=1.27e-02
2025-10-17 22:26:57,495 | INFO | Epoch 1591 Train Time 102.96025061607361s

2025-10-17 22:28:42,621 | INFO | Training epoch 1592, Batch 1000/1000: LR=1.09e-05, Loss=3.62e-03 BER=1.47e-03 FER=1.28e-02
2025-10-17 22:28:42,681 | INFO | Epoch 1592 Train Time 105.18499684333801s

2025-10-17 22:30:23,728 | INFO | Training epoch 1593, Batch 1000/1000: LR=1.08e-05, Loss=3.72e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 22:30:23,793 | INFO | Epoch 1593 Train Time 101.1090760231018s

2025-10-17 22:32:04,941 | INFO | Training epoch 1594, Batch 1000/1000: LR=1.08e-05, Loss=3.83e-03 BER=1.56e-03 FER=1.35e-02
2025-10-17 22:32:04,992 | INFO | Epoch 1594 Train Time 101.19724941253662s

2025-10-17 22:33:52,617 | INFO | Training epoch 1595, Batch 1000/1000: LR=1.07e-05, Loss=3.83e-03 BER=1.60e-03 FER=1.35e-02
2025-10-17 22:33:52,675 | INFO | Epoch 1595 Train Time 107.68193650245667s

2025-10-17 22:35:32,236 | INFO | Training epoch 1596, Batch 1000/1000: LR=1.07e-05, Loss=3.68e-03 BER=1.51e-03 FER=1.29e-02
2025-10-17 22:35:32,301 | INFO | Epoch 1596 Train Time 99.62537670135498s

2025-10-17 22:37:12,667 | INFO | Training epoch 1597, Batch 1000/1000: LR=1.06e-05, Loss=3.75e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 22:37:12,731 | INFO | Epoch 1597 Train Time 100.42690753936768s

2025-10-17 22:38:57,007 | INFO | Training epoch 1598, Batch 1000/1000: LR=1.06e-05, Loss=3.75e-03 BER=1.53e-03 FER=1.33e-02
2025-10-17 22:38:57,078 | INFO | Epoch 1598 Train Time 104.34496331214905s

2025-10-17 22:40:38,439 | INFO | Training epoch 1599, Batch 1000/1000: LR=1.05e-05, Loss=3.71e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 22:40:38,502 | INFO | Epoch 1599 Train Time 101.42267155647278s

2025-10-17 22:42:15,818 | INFO | Training epoch 1600, Batch 1000/1000: LR=1.05e-05, Loss=3.65e-03 BER=1.50e-03 FER=1.27e-02
2025-10-17 22:42:15,890 | INFO | Epoch 1600 Train Time 97.38659310340881s

2025-10-17 22:43:53,930 | INFO | Training epoch 1601, Batch 1000/1000: LR=1.05e-05, Loss=3.72e-03 BER=1.53e-03 FER=1.32e-02
2025-10-17 22:43:54,002 | INFO | Epoch 1601 Train Time 98.11001968383789s

2025-10-17 22:45:36,887 | INFO | Training epoch 1602, Batch 1000/1000: LR=1.04e-05, Loss=3.78e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 22:45:36,949 | INFO | Epoch 1602 Train Time 102.94552063941956s

2025-10-17 22:47:15,061 | INFO | Training epoch 1603, Batch 1000/1000: LR=1.04e-05, Loss=3.81e-03 BER=1.58e-03 FER=1.35e-02
2025-10-17 22:47:15,129 | INFO | Epoch 1603 Train Time 98.17945694923401s

2025-10-17 22:48:55,221 | INFO | Training epoch 1604, Batch 1000/1000: LR=1.03e-05, Loss=3.78e-03 BER=1.56e-03 FER=1.32e-02
2025-10-17 22:48:55,282 | INFO | Epoch 1604 Train Time 100.15178894996643s

2025-10-17 22:50:36,837 | INFO | Training epoch 1605, Batch 1000/1000: LR=1.03e-05, Loss=3.45e-03 BER=1.41e-03 FER=1.23e-02
2025-10-17 22:50:36,906 | INFO | Epoch 1605 Train Time 101.62204027175903s

2025-10-17 22:50:36,907 | INFO | [P2] saving best_model (QAT) with loss 0.003450 at epoch 1605
2025-10-17 22:52:11,569 | INFO | Training epoch 1606, Batch 1000/1000: LR=1.02e-05, Loss=3.74e-03 BER=1.51e-03 FER=1.30e-02
2025-10-17 22:52:11,635 | INFO | Epoch 1606 Train Time 94.61983489990234s

2025-10-17 22:53:53,191 | INFO | Training epoch 1607, Batch 1000/1000: LR=1.02e-05, Loss=3.64e-03 BER=1.51e-03 FER=1.28e-02
2025-10-17 22:53:53,265 | INFO | Epoch 1607 Train Time 101.62847423553467s

2025-10-17 22:55:39,313 | INFO | Training epoch 1608, Batch 1000/1000: LR=1.01e-05, Loss=3.71e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 22:55:39,370 | INFO | Epoch 1608 Train Time 106.10319542884827s

2025-10-17 22:57:21,202 | INFO | Training epoch 1609, Batch 1000/1000: LR=1.01e-05, Loss=3.56e-03 BER=1.46e-03 FER=1.28e-02
2025-10-17 22:57:21,254 | INFO | Epoch 1609 Train Time 101.88176369667053s

2025-10-17 22:58:58,746 | INFO | Training epoch 1610, Batch 1000/1000: LR=1.00e-05, Loss=3.58e-03 BER=1.47e-03 FER=1.26e-02
2025-10-17 22:58:58,796 | INFO | Epoch 1610 Train Time 97.53971219062805s

2025-10-17 23:00:43,139 | INFO | Training epoch 1611, Batch 1000/1000: LR=1.00e-05, Loss=3.72e-03 BER=1.54e-03 FER=1.30e-02
2025-10-17 23:00:43,203 | INFO | Epoch 1611 Train Time 104.40495610237122s

2025-10-17 23:02:22,705 | INFO | Training epoch 1612, Batch 1000/1000: LR=9.96e-06, Loss=3.81e-03 BER=1.59e-03 FER=1.34e-02
2025-10-17 23:02:22,750 | INFO | Epoch 1612 Train Time 99.5453360080719s

2025-10-17 23:04:04,413 | INFO | Training epoch 1613, Batch 1000/1000: LR=9.91e-06, Loss=3.77e-03 BER=1.55e-03 FER=1.34e-02
2025-10-17 23:04:04,453 | INFO | Epoch 1613 Train Time 101.70206308364868s

2025-10-17 23:05:51,321 | INFO | Training epoch 1614, Batch 1000/1000: LR=9.87e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 23:05:51,361 | INFO | Epoch 1614 Train Time 106.9068284034729s

2025-10-17 23:07:28,908 | INFO | Training epoch 1615, Batch 1000/1000: LR=9.82e-06, Loss=3.41e-03 BER=1.40e-03 FER=1.20e-02
2025-10-17 23:07:28,983 | INFO | Epoch 1615 Train Time 97.6204674243927s

2025-10-17 23:07:28,983 | INFO | [P2] saving best_model (QAT) with loss 0.003414 at epoch 1615
2025-10-17 23:09:11,934 | INFO | Training epoch 1616, Batch 1000/1000: LR=9.78e-06, Loss=3.77e-03 BER=1.55e-03 FER=1.31e-02
2025-10-17 23:09:11,987 | INFO | Epoch 1616 Train Time 102.89290952682495s

2025-10-17 23:10:55,326 | INFO | Training epoch 1617, Batch 1000/1000: LR=9.74e-06, Loss=4.00e-03 BER=1.66e-03 FER=1.41e-02
2025-10-17 23:10:55,394 | INFO | Epoch 1617 Train Time 103.40583372116089s

2025-10-17 23:12:35,297 | INFO | Training epoch 1618, Batch 1000/1000: LR=9.69e-06, Loss=3.70e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 23:12:35,364 | INFO | Epoch 1618 Train Time 99.96867346763611s

2025-10-17 23:14:11,593 | INFO | Training epoch 1619, Batch 1000/1000: LR=9.65e-06, Loss=3.72e-03 BER=1.52e-03 FER=1.31e-02
2025-10-17 23:14:11,647 | INFO | Epoch 1619 Train Time 96.28096389770508s

2025-10-17 23:15:50,608 | INFO | Training epoch 1620, Batch 1000/1000: LR=9.60e-06, Loss=3.70e-03 BER=1.51e-03 FER=1.28e-02
2025-10-17 23:15:50,682 | INFO | Epoch 1620 Train Time 99.03438544273376s

2025-10-17 23:17:36,797 | INFO | Training epoch 1621, Batch 1000/1000: LR=9.56e-06, Loss=3.69e-03 BER=1.50e-03 FER=1.29e-02
2025-10-17 23:17:36,863 | INFO | Epoch 1621 Train Time 106.17905783653259s

2025-10-17 23:19:19,192 | INFO | Training epoch 1622, Batch 1000/1000: LR=9.52e-06, Loss=3.74e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 23:19:19,259 | INFO | Epoch 1622 Train Time 102.39402198791504s

2025-10-17 23:21:00,508 | INFO | Training epoch 1623, Batch 1000/1000: LR=9.47e-06, Loss=3.81e-03 BER=1.56e-03 FER=1.33e-02
2025-10-17 23:21:00,567 | INFO | Epoch 1623 Train Time 101.30758023262024s

2025-10-17 23:22:42,907 | INFO | Training epoch 1624, Batch 1000/1000: LR=9.43e-06, Loss=3.82e-03 BER=1.57e-03 FER=1.33e-02
2025-10-17 23:22:42,971 | INFO | Epoch 1624 Train Time 102.40178418159485s

2025-10-17 23:24:27,194 | INFO | Training epoch 1625, Batch 1000/1000: LR=9.39e-06, Loss=3.60e-03 BER=1.50e-03 FER=1.28e-02
2025-10-17 23:24:27,250 | INFO | Epoch 1625 Train Time 104.27683210372925s

2025-10-17 23:26:10,683 | INFO | Training epoch 1626, Batch 1000/1000: LR=9.34e-06, Loss=3.66e-03 BER=1.52e-03 FER=1.30e-02
2025-10-17 23:26:10,746 | INFO | Epoch 1626 Train Time 103.49477100372314s

2025-10-17 23:28:02,711 | INFO | Training epoch 1627, Batch 1000/1000: LR=9.30e-06, Loss=3.62e-03 BER=1.47e-03 FER=1.26e-02
2025-10-17 23:28:02,777 | INFO | Epoch 1627 Train Time 112.0292980670929s

2025-10-17 23:29:43,438 | INFO | Training epoch 1628, Batch 1000/1000: LR=9.26e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.28e-02
2025-10-17 23:29:43,496 | INFO | Epoch 1628 Train Time 100.716952085495s

2025-10-17 23:31:25,728 | INFO | Training epoch 1629, Batch 1000/1000: LR=9.21e-06, Loss=3.74e-03 BER=1.54e-03 FER=1.32e-02
2025-10-17 23:31:25,796 | INFO | Epoch 1629 Train Time 102.29836916923523s

2025-10-17 23:33:05,681 | INFO | Training epoch 1630, Batch 1000/1000: LR=9.17e-06, Loss=3.71e-03 BER=1.54e-03 FER=1.30e-02
2025-10-17 23:33:05,742 | INFO | Epoch 1630 Train Time 99.94527459144592s

2025-10-17 23:34:44,707 | INFO | Training epoch 1631, Batch 1000/1000: LR=9.13e-06, Loss=3.57e-03 BER=1.47e-03 FER=1.26e-02
2025-10-17 23:34:44,767 | INFO | Epoch 1631 Train Time 99.02307844161987s

2025-10-17 23:36:26,108 | INFO | Training epoch 1632, Batch 1000/1000: LR=9.08e-06, Loss=3.60e-03 BER=1.46e-03 FER=1.25e-02
2025-10-17 23:36:26,203 | INFO | Epoch 1632 Train Time 101.43476414680481s

2025-10-17 23:38:07,387 | INFO | Training epoch 1633, Batch 1000/1000: LR=9.04e-06, Loss=3.71e-03 BER=1.51e-03 FER=1.29e-02
2025-10-17 23:38:07,459 | INFO | Epoch 1633 Train Time 101.254465341568s

2025-10-17 23:39:48,316 | INFO | Training epoch 1634, Batch 1000/1000: LR=9.00e-06, Loss=4.00e-03 BER=1.63e-03 FER=1.39e-02
2025-10-17 23:39:48,379 | INFO | Epoch 1634 Train Time 100.91683721542358s

2025-10-17 23:41:33,032 | INFO | Training epoch 1635, Batch 1000/1000: LR=8.96e-06, Loss=3.74e-03 BER=1.54e-03 FER=1.31e-02
2025-10-17 23:41:33,089 | INFO | Epoch 1635 Train Time 104.70759010314941s

2025-10-17 23:43:21,523 | INFO | Training epoch 1636, Batch 1000/1000: LR=8.92e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.28e-02
2025-10-17 23:43:21,587 | INFO | Epoch 1636 Train Time 108.49657964706421s

2025-10-17 23:45:03,264 | INFO | Training epoch 1637, Batch 1000/1000: LR=8.87e-06, Loss=3.67e-03 BER=1.53e-03 FER=1.32e-02
2025-10-17 23:45:03,317 | INFO | Epoch 1637 Train Time 101.72752237319946s

2025-10-17 23:46:50,089 | INFO | Training epoch 1638, Batch 1000/1000: LR=8.83e-06, Loss=3.55e-03 BER=1.47e-03 FER=1.26e-02
2025-10-17 23:46:50,145 | INFO | Epoch 1638 Train Time 106.82653975486755s

2025-10-17 23:48:32,865 | INFO | Training epoch 1639, Batch 1000/1000: LR=8.79e-06, Loss=3.74e-03 BER=1.56e-03 FER=1.32e-02
2025-10-17 23:48:32,914 | INFO | Epoch 1639 Train Time 102.7672049999237s

2025-10-17 23:50:15,097 | INFO | Training epoch 1640, Batch 1000/1000: LR=8.75e-06, Loss=3.83e-03 BER=1.54e-03 FER=1.33e-02
2025-10-17 23:50:15,150 | INFO | Epoch 1640 Train Time 102.23235940933228s

2025-10-17 23:51:53,503 | INFO | Training epoch 1641, Batch 1000/1000: LR=8.71e-06, Loss=3.71e-03 BER=1.55e-03 FER=1.32e-02
2025-10-17 23:51:53,567 | INFO | Epoch 1641 Train Time 98.41611456871033s

2025-10-17 23:53:30,810 | INFO | Training epoch 1642, Batch 1000/1000: LR=8.66e-06, Loss=3.71e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 23:53:30,860 | INFO | Epoch 1642 Train Time 97.28970265388489s

2025-10-17 23:55:11,357 | INFO | Training epoch 1643, Batch 1000/1000: LR=8.62e-06, Loss=3.74e-03 BER=1.51e-03 FER=1.30e-02
2025-10-17 23:55:11,401 | INFO | Epoch 1643 Train Time 100.53904175758362s

2025-10-17 23:56:52,795 | INFO | Training epoch 1644, Batch 1000/1000: LR=8.58e-06, Loss=3.74e-03 BER=1.52e-03 FER=1.30e-02
2025-10-17 23:56:52,841 | INFO | Epoch 1644 Train Time 101.43873190879822s

2025-10-17 23:58:36,558 | INFO | Training epoch 1645, Batch 1000/1000: LR=8.54e-06, Loss=3.73e-03 BER=1.55e-03 FER=1.33e-02
2025-10-17 23:58:36,625 | INFO | Epoch 1645 Train Time 103.78219294548035s

2025-10-18 00:00:21,765 | INFO | Training epoch 1646, Batch 1000/1000: LR=8.50e-06, Loss=3.63e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 00:00:21,821 | INFO | Epoch 1646 Train Time 105.19434595108032s

2025-10-18 00:01:52,943 | INFO | Training epoch 1647, Batch 1000/1000: LR=8.46e-06, Loss=3.55e-03 BER=1.48e-03 FER=1.27e-02
2025-10-18 00:01:52,988 | INFO | Epoch 1647 Train Time 91.16549491882324s

2025-10-18 00:03:36,168 | INFO | Training epoch 1648, Batch 1000/1000: LR=8.42e-06, Loss=3.73e-03 BER=1.55e-03 FER=1.32e-02
2025-10-18 00:03:36,225 | INFO | Epoch 1648 Train Time 103.23542666435242s

2025-10-18 00:05:06,764 | INFO | Training epoch 1649, Batch 1000/1000: LR=8.38e-06, Loss=3.63e-03 BER=1.51e-03 FER=1.28e-02
2025-10-18 00:05:06,834 | INFO | Epoch 1649 Train Time 90.6084246635437s

2025-10-18 00:06:47,160 | INFO | Training epoch 1650, Batch 1000/1000: LR=8.33e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.29e-02
2025-10-18 00:06:47,228 | INFO | Epoch 1650 Train Time 100.39210867881775s

2025-10-18 00:08:32,182 | INFO | Training epoch 1651, Batch 1000/1000: LR=8.29e-06, Loss=3.63e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 00:08:32,234 | INFO | Epoch 1651 Train Time 105.00392270088196s

2025-10-18 00:10:08,825 | INFO | Training epoch 1652, Batch 1000/1000: LR=8.25e-06, Loss=3.78e-03 BER=1.59e-03 FER=1.36e-02
2025-10-18 00:10:08,895 | INFO | Epoch 1652 Train Time 96.66072225570679s

2025-10-18 00:11:38,180 | INFO | Training epoch 1653, Batch 1000/1000: LR=8.21e-06, Loss=3.61e-03 BER=1.48e-03 FER=1.29e-02
2025-10-18 00:11:38,236 | INFO | Epoch 1653 Train Time 89.33924508094788s

2025-10-18 00:13:16,080 | INFO | Training epoch 1654, Batch 1000/1000: LR=8.17e-06, Loss=3.72e-03 BER=1.53e-03 FER=1.31e-02
2025-10-18 00:13:16,136 | INFO | Epoch 1654 Train Time 97.89903855323792s

2025-10-18 00:14:49,908 | INFO | Training epoch 1655, Batch 1000/1000: LR=8.13e-06, Loss=3.69e-03 BER=1.52e-03 FER=1.31e-02
2025-10-18 00:14:49,979 | INFO | Epoch 1655 Train Time 93.84196376800537s

2025-10-18 00:16:29,393 | INFO | Training epoch 1656, Batch 1000/1000: LR=8.09e-06, Loss=3.74e-03 BER=1.52e-03 FER=1.32e-02
2025-10-18 00:16:29,453 | INFO | Epoch 1656 Train Time 99.47284531593323s

2025-10-18 00:18:06,201 | INFO | Training epoch 1657, Batch 1000/1000: LR=8.05e-06, Loss=3.72e-03 BER=1.55e-03 FER=1.32e-02
2025-10-18 00:18:06,274 | INFO | Epoch 1657 Train Time 96.8191750049591s

2025-10-18 00:19:53,185 | INFO | Training epoch 1658, Batch 1000/1000: LR=8.01e-06, Loss=3.92e-03 BER=1.61e-03 FER=1.36e-02
2025-10-18 00:19:53,247 | INFO | Epoch 1658 Train Time 106.96989154815674s

2025-10-18 00:21:37,960 | INFO | Training epoch 1659, Batch 1000/1000: LR=7.97e-06, Loss=3.67e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 00:21:38,025 | INFO | Epoch 1659 Train Time 104.77761626243591s

2025-10-18 00:23:14,553 | INFO | Training epoch 1660, Batch 1000/1000: LR=7.93e-06, Loss=3.61e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 00:23:14,610 | INFO | Epoch 1660 Train Time 96.58319854736328s

2025-10-18 00:24:47,466 | INFO | Training epoch 1661, Batch 1000/1000: LR=7.89e-06, Loss=3.72e-03 BER=1.55e-03 FER=1.32e-02
2025-10-18 00:24:47,546 | INFO | Epoch 1661 Train Time 92.93478798866272s

2025-10-18 00:26:23,104 | INFO | Training epoch 1662, Batch 1000/1000: LR=7.85e-06, Loss=3.66e-03 BER=1.53e-03 FER=1.30e-02
2025-10-18 00:26:23,164 | INFO | Epoch 1662 Train Time 95.61675786972046s

2025-10-18 00:27:57,216 | INFO | Training epoch 1663, Batch 1000/1000: LR=7.81e-06, Loss=3.69e-03 BER=1.53e-03 FER=1.32e-02
2025-10-18 00:27:57,278 | INFO | Epoch 1663 Train Time 94.11203193664551s

2025-10-18 00:29:34,439 | INFO | Training epoch 1664, Batch 1000/1000: LR=7.78e-06, Loss=3.65e-03 BER=1.52e-03 FER=1.28e-02
2025-10-18 00:29:34,491 | INFO | Epoch 1664 Train Time 97.21251010894775s

2025-10-18 00:31:18,609 | INFO | Training epoch 1665, Batch 1000/1000: LR=7.74e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 00:31:18,671 | INFO | Epoch 1665 Train Time 104.17724180221558s

2025-10-18 00:32:55,319 | INFO | Training epoch 1666, Batch 1000/1000: LR=7.70e-06, Loss=3.58e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 00:32:55,391 | INFO | Epoch 1666 Train Time 96.71824669837952s

2025-10-18 00:34:31,109 | INFO | Training epoch 1667, Batch 1000/1000: LR=7.66e-06, Loss=3.65e-03 BER=1.53e-03 FER=1.29e-02
2025-10-18 00:34:31,171 | INFO | Epoch 1667 Train Time 95.7784628868103s

2025-10-18 00:36:05,775 | INFO | Training epoch 1668, Batch 1000/1000: LR=7.62e-06, Loss=3.71e-03 BER=1.54e-03 FER=1.32e-02
2025-10-18 00:36:05,835 | INFO | Epoch 1668 Train Time 94.66132688522339s

2025-10-18 00:37:49,070 | INFO | Training epoch 1669, Batch 1000/1000: LR=7.58e-06, Loss=3.55e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 00:37:49,131 | INFO | Epoch 1669 Train Time 103.2945487499237s

2025-10-18 00:39:32,322 | INFO | Training epoch 1670, Batch 1000/1000: LR=7.54e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 00:39:32,398 | INFO | Epoch 1670 Train Time 103.26402640342712s

2025-10-18 00:41:11,472 | INFO | Training epoch 1671, Batch 1000/1000: LR=7.50e-06, Loss=3.86e-03 BER=1.56e-03 FER=1.33e-02
2025-10-18 00:41:11,515 | INFO | Epoch 1671 Train Time 99.1154260635376s

2025-10-18 00:42:48,934 | INFO | Training epoch 1672, Batch 1000/1000: LR=7.46e-06, Loss=3.75e-03 BER=1.55e-03 FER=1.33e-02
2025-10-18 00:42:48,996 | INFO | Epoch 1672 Train Time 97.48053550720215s

2025-10-18 00:44:33,791 | INFO | Training epoch 1673, Batch 1000/1000: LR=7.43e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 00:44:33,861 | INFO | Epoch 1673 Train Time 104.86302900314331s

2025-10-18 00:46:05,935 | INFO | Training epoch 1674, Batch 1000/1000: LR=7.39e-06, Loss=3.68e-03 BER=1.50e-03 FER=1.29e-02
2025-10-18 00:46:05,996 | INFO | Epoch 1674 Train Time 92.13330340385437s

2025-10-18 00:47:41,569 | INFO | Training epoch 1675, Batch 1000/1000: LR=7.35e-06, Loss=3.56e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 00:47:41,617 | INFO | Epoch 1675 Train Time 95.61979103088379s

2025-10-18 00:49:18,548 | INFO | Training epoch 1676, Batch 1000/1000: LR=7.31e-06, Loss=3.76e-03 BER=1.54e-03 FER=1.32e-02
2025-10-18 00:49:18,609 | INFO | Epoch 1676 Train Time 96.99167227745056s

2025-10-18 00:50:50,438 | INFO | Training epoch 1677, Batch 1000/1000: LR=7.27e-06, Loss=3.57e-03 BER=1.45e-03 FER=1.27e-02
2025-10-18 00:50:50,530 | INFO | Epoch 1677 Train Time 91.91905093193054s

2025-10-18 00:52:32,288 | INFO | Training epoch 1678, Batch 1000/1000: LR=7.24e-06, Loss=3.78e-03 BER=1.54e-03 FER=1.31e-02
2025-10-18 00:52:32,348 | INFO | Epoch 1678 Train Time 101.8162956237793s

2025-10-18 00:54:08,681 | INFO | Training epoch 1679, Batch 1000/1000: LR=7.20e-06, Loss=3.66e-03 BER=1.50e-03 FER=1.28e-02
2025-10-18 00:54:08,753 | INFO | Epoch 1679 Train Time 96.40426778793335s

2025-10-18 00:55:41,789 | INFO | Training epoch 1680, Batch 1000/1000: LR=7.16e-06, Loss=3.50e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 00:55:41,862 | INFO | Epoch 1680 Train Time 93.10624027252197s

2025-10-18 00:57:23,606 | INFO | Training epoch 1681, Batch 1000/1000: LR=7.12e-06, Loss=3.56e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 00:57:23,677 | INFO | Epoch 1681 Train Time 101.81400322914124s

2025-10-18 00:59:00,897 | INFO | Training epoch 1682, Batch 1000/1000: LR=7.09e-06, Loss=3.63e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 00:59:00,946 | INFO | Epoch 1682 Train Time 97.26616334915161s

2025-10-18 01:00:42,095 | INFO | Training epoch 1683, Batch 1000/1000: LR=7.05e-06, Loss=3.73e-03 BER=1.56e-03 FER=1.31e-02
2025-10-18 01:00:42,167 | INFO | Epoch 1683 Train Time 101.2201738357544s

2025-10-18 01:02:20,987 | INFO | Training epoch 1684, Batch 1000/1000: LR=7.01e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 01:02:21,044 | INFO | Epoch 1684 Train Time 98.87628531455994s

2025-10-18 01:04:04,342 | INFO | Training epoch 1685, Batch 1000/1000: LR=6.97e-06, Loss=3.58e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 01:04:04,396 | INFO | Epoch 1685 Train Time 103.35000705718994s

2025-10-18 01:05:40,181 | INFO | Training epoch 1686, Batch 1000/1000: LR=6.94e-06, Loss=3.76e-03 BER=1.52e-03 FER=1.33e-02
2025-10-18 01:05:40,250 | INFO | Epoch 1686 Train Time 95.85261988639832s

2025-10-18 01:07:18,428 | INFO | Training epoch 1687, Batch 1000/1000: LR=6.90e-06, Loss=3.71e-03 BER=1.56e-03 FER=1.34e-02
2025-10-18 01:07:18,490 | INFO | Epoch 1687 Train Time 98.23783612251282s

2025-10-18 01:08:51,706 | INFO | Training epoch 1688, Batch 1000/1000: LR=6.86e-06, Loss=3.70e-03 BER=1.54e-03 FER=1.33e-02
2025-10-18 01:08:51,755 | INFO | Epoch 1688 Train Time 93.26324844360352s

2025-10-18 01:10:35,214 | INFO | Training epoch 1689, Batch 1000/1000: LR=6.83e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.29e-02
2025-10-18 01:10:35,286 | INFO | Epoch 1689 Train Time 103.52927780151367s

2025-10-18 01:12:18,050 | INFO | Training epoch 1690, Batch 1000/1000: LR=6.79e-06, Loss=3.58e-03 BER=1.48e-03 FER=1.28e-02
2025-10-18 01:12:18,117 | INFO | Epoch 1690 Train Time 102.82970905303955s

2025-10-18 01:14:00,161 | INFO | Training epoch 1691, Batch 1000/1000: LR=6.75e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.29e-02
2025-10-18 01:14:00,208 | INFO | Epoch 1691 Train Time 102.09007740020752s

2025-10-18 01:15:42,990 | INFO | Training epoch 1692, Batch 1000/1000: LR=6.72e-06, Loss=3.59e-03 BER=1.48e-03 FER=1.26e-02
2025-10-18 01:15:43,049 | INFO | Epoch 1692 Train Time 102.83550381660461s

2025-10-18 01:17:21,943 | INFO | Training epoch 1693, Batch 1000/1000: LR=6.68e-06, Loss=3.38e-03 BER=1.39e-03 FER=1.20e-02
2025-10-18 01:17:22,006 | INFO | Epoch 1693 Train Time 98.95655822753906s

2025-10-18 01:17:22,007 | INFO | [P2] saving best_model (QAT) with loss 0.003378 at epoch 1693
2025-10-18 01:18:59,111 | INFO | Training epoch 1694, Batch 1000/1000: LR=6.64e-06, Loss=3.83e-03 BER=1.55e-03 FER=1.32e-02
2025-10-18 01:18:59,177 | INFO | Epoch 1694 Train Time 97.06605553627014s

2025-10-18 01:20:37,395 | INFO | Training epoch 1695, Batch 1000/1000: LR=6.61e-06, Loss=3.46e-03 BER=1.44e-03 FER=1.25e-02
2025-10-18 01:20:37,486 | INFO | Epoch 1695 Train Time 98.30742764472961s

2025-10-18 01:22:15,378 | INFO | Training epoch 1696, Batch 1000/1000: LR=6.57e-06, Loss=3.57e-03 BER=1.48e-03 FER=1.27e-02
2025-10-18 01:22:15,444 | INFO | Epoch 1696 Train Time 97.95578932762146s

2025-10-18 01:23:48,087 | INFO | Training epoch 1697, Batch 1000/1000: LR=6.54e-06, Loss=3.81e-03 BER=1.55e-03 FER=1.32e-02
2025-10-18 01:23:48,171 | INFO | Epoch 1697 Train Time 92.72553825378418s

2025-10-18 01:25:25,276 | INFO | Training epoch 1698, Batch 1000/1000: LR=6.50e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 01:25:25,328 | INFO | Epoch 1698 Train Time 97.15579891204834s

2025-10-18 01:27:06,570 | INFO | Training epoch 1699, Batch 1000/1000: LR=6.47e-06, Loss=3.66e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 01:27:06,628 | INFO | Epoch 1699 Train Time 101.29888820648193s

2025-10-18 01:28:50,491 | INFO | Training epoch 1700, Batch 1000/1000: LR=6.43e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.25e-02
2025-10-18 01:28:50,555 | INFO | Epoch 1700 Train Time 103.92637252807617s

2025-10-18 01:30:30,417 | INFO | Training epoch 1701, Batch 1000/1000: LR=6.40e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 01:30:30,470 | INFO | Epoch 1701 Train Time 99.91359114646912s

2025-10-18 01:32:04,171 | INFO | Training epoch 1702, Batch 1000/1000: LR=6.36e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.28e-02
2025-10-18 01:32:04,221 | INFO | Epoch 1702 Train Time 93.75032138824463s

2025-10-18 01:33:38,366 | INFO | Training epoch 1703, Batch 1000/1000: LR=6.32e-06, Loss=3.72e-03 BER=1.53e-03 FER=1.31e-02
2025-10-18 01:33:38,431 | INFO | Epoch 1703 Train Time 94.208261013031s

2025-10-18 01:35:16,476 | INFO | Training epoch 1704, Batch 1000/1000: LR=6.29e-06, Loss=3.65e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 01:35:16,529 | INFO | Epoch 1704 Train Time 98.09649395942688s

2025-10-18 01:36:50,154 | INFO | Training epoch 1705, Batch 1000/1000: LR=6.25e-06, Loss=3.79e-03 BER=1.53e-03 FER=1.31e-02
2025-10-18 01:36:50,212 | INFO | Epoch 1705 Train Time 93.68177199363708s

2025-10-18 01:38:27,698 | INFO | Training epoch 1706, Batch 1000/1000: LR=6.22e-06, Loss=3.48e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 01:38:27,762 | INFO | Epoch 1706 Train Time 97.54843187332153s

2025-10-18 01:40:00,685 | INFO | Training epoch 1707, Batch 1000/1000: LR=6.19e-06, Loss=3.70e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 01:40:00,763 | INFO | Epoch 1707 Train Time 92.99799227714539s

2025-10-18 01:41:46,417 | INFO | Training epoch 1708, Batch 1000/1000: LR=6.15e-06, Loss=3.71e-03 BER=1.50e-03 FER=1.28e-02
2025-10-18 01:41:46,484 | INFO | Epoch 1708 Train Time 105.7190260887146s

2025-10-18 01:43:23,468 | INFO | Training epoch 1709, Batch 1000/1000: LR=6.12e-06, Loss=3.55e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 01:43:23,543 | INFO | Epoch 1709 Train Time 97.05723977088928s

2025-10-18 01:44:58,817 | INFO | Training epoch 1710, Batch 1000/1000: LR=6.08e-06, Loss=3.73e-03 BER=1.55e-03 FER=1.32e-02
2025-10-18 01:44:58,877 | INFO | Epoch 1710 Train Time 95.3323404788971s

2025-10-18 01:46:35,175 | INFO | Training epoch 1711, Batch 1000/1000: LR=6.05e-06, Loss=3.53e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 01:46:35,245 | INFO | Epoch 1711 Train Time 96.36745476722717s

2025-10-18 01:48:17,300 | INFO | Training epoch 1712, Batch 1000/1000: LR=6.01e-06, Loss=3.73e-03 BER=1.52e-03 FER=1.31e-02
2025-10-18 01:48:17,355 | INFO | Epoch 1712 Train Time 102.1080801486969s

2025-10-18 01:50:03,615 | INFO | Training epoch 1713, Batch 1000/1000: LR=5.98e-06, Loss=3.76e-03 BER=1.54e-03 FER=1.31e-02
2025-10-18 01:50:03,687 | INFO | Epoch 1713 Train Time 106.3315942287445s

2025-10-18 01:51:36,436 | INFO | Training epoch 1714, Batch 1000/1000: LR=5.95e-06, Loss=3.73e-03 BER=1.52e-03 FER=1.31e-02
2025-10-18 01:51:36,502 | INFO | Epoch 1714 Train Time 92.81374049186707s

2025-10-18 01:53:18,146 | INFO | Training epoch 1715, Batch 1000/1000: LR=5.91e-06, Loss=3.45e-03 BER=1.41e-03 FER=1.22e-02
2025-10-18 01:53:18,206 | INFO | Epoch 1715 Train Time 101.7020206451416s

2025-10-18 01:54:51,835 | INFO | Training epoch 1716, Batch 1000/1000: LR=5.88e-06, Loss=3.75e-03 BER=1.54e-03 FER=1.31e-02
2025-10-18 01:54:51,895 | INFO | Epoch 1716 Train Time 93.68632078170776s

2025-10-18 01:56:26,794 | INFO | Training epoch 1717, Batch 1000/1000: LR=5.84e-06, Loss=3.71e-03 BER=1.55e-03 FER=1.34e-02
2025-10-18 01:56:26,846 | INFO | Epoch 1717 Train Time 94.94961452484131s

2025-10-18 01:58:04,544 | INFO | Training epoch 1718, Batch 1000/1000: LR=5.81e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 01:58:04,610 | INFO | Epoch 1718 Train Time 97.7618510723114s

2025-10-18 01:59:43,292 | INFO | Training epoch 1719, Batch 1000/1000: LR=5.78e-06, Loss=3.74e-03 BER=1.50e-03 FER=1.30e-02
2025-10-18 01:59:43,350 | INFO | Epoch 1719 Train Time 98.73887062072754s

2025-10-18 02:01:16,547 | INFO | Training epoch 1720, Batch 1000/1000: LR=5.74e-06, Loss=3.50e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 02:01:16,591 | INFO | Epoch 1720 Train Time 93.23927283287048s

2025-10-18 02:02:54,769 | INFO | Training epoch 1721, Batch 1000/1000: LR=5.71e-06, Loss=3.69e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 02:02:54,808 | INFO | Epoch 1721 Train Time 98.21693921089172s

2025-10-18 02:04:33,062 | INFO | Training epoch 1722, Batch 1000/1000: LR=5.68e-06, Loss=3.67e-03 BER=1.50e-03 FER=1.29e-02
2025-10-18 02:04:33,120 | INFO | Epoch 1722 Train Time 98.31088376045227s

2025-10-18 02:06:01,105 | INFO | Training epoch 1723, Batch 1000/1000: LR=5.65e-06, Loss=3.51e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 02:06:01,157 | INFO | Epoch 1723 Train Time 88.0358350276947s

2025-10-18 02:07:39,547 | INFO | Training epoch 1724, Batch 1000/1000: LR=5.61e-06, Loss=3.49e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 02:07:39,598 | INFO | Epoch 1724 Train Time 98.44040131568909s

2025-10-18 02:09:16,936 | INFO | Training epoch 1725, Batch 1000/1000: LR=5.58e-06, Loss=3.48e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 02:09:16,987 | INFO | Epoch 1725 Train Time 97.38752126693726s

2025-10-18 02:10:56,392 | INFO | Training epoch 1726, Batch 1000/1000: LR=5.55e-06, Loss=3.74e-03 BER=1.54e-03 FER=1.31e-02
2025-10-18 02:10:56,475 | INFO | Epoch 1726 Train Time 99.48608112335205s

2025-10-18 02:12:32,657 | INFO | Training epoch 1727, Batch 1000/1000: LR=5.51e-06, Loss=3.69e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 02:12:32,697 | INFO | Epoch 1727 Train Time 96.21945357322693s

2025-10-18 02:14:06,657 | INFO | Training epoch 1728, Batch 1000/1000: LR=5.48e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 02:14:06,714 | INFO | Epoch 1728 Train Time 94.01627111434937s

2025-10-18 02:15:45,453 | INFO | Training epoch 1729, Batch 1000/1000: LR=5.45e-06, Loss=3.46e-03 BER=1.42e-03 FER=1.20e-02
2025-10-18 02:15:45,504 | INFO | Epoch 1729 Train Time 98.78887033462524s

2025-10-18 02:17:18,757 | INFO | Training epoch 1730, Batch 1000/1000: LR=5.42e-06, Loss=3.60e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 02:17:18,804 | INFO | Epoch 1730 Train Time 93.29869437217712s

2025-10-18 02:18:59,057 | INFO | Training epoch 1731, Batch 1000/1000: LR=5.39e-06, Loss=3.79e-03 BER=1.52e-03 FER=1.31e-02
2025-10-18 02:18:59,110 | INFO | Epoch 1731 Train Time 100.30483555793762s

2025-10-18 02:20:34,012 | INFO | Training epoch 1732, Batch 1000/1000: LR=5.35e-06, Loss=3.59e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 02:20:34,063 | INFO | Epoch 1732 Train Time 94.95116257667542s

2025-10-18 02:22:07,328 | INFO | Training epoch 1733, Batch 1000/1000: LR=5.32e-06, Loss=3.47e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 02:22:07,373 | INFO | Epoch 1733 Train Time 93.30802273750305s

2025-10-18 02:23:42,782 | INFO | Training epoch 1734, Batch 1000/1000: LR=5.29e-06, Loss=3.51e-03 BER=1.41e-03 FER=1.22e-02
2025-10-18 02:23:42,834 | INFO | Epoch 1734 Train Time 95.46085405349731s

2025-10-18 02:25:23,221 | INFO | Training epoch 1735, Batch 1000/1000: LR=5.26e-06, Loss=3.77e-03 BER=1.57e-03 FER=1.34e-02
2025-10-18 02:25:23,283 | INFO | Epoch 1735 Train Time 100.44652247428894s

2025-10-18 02:27:06,310 | INFO | Training epoch 1736, Batch 1000/1000: LR=5.23e-06, Loss=3.38e-03 BER=1.39e-03 FER=1.22e-02
2025-10-18 02:27:06,401 | INFO | Epoch 1736 Train Time 103.1143946647644s

2025-10-18 02:28:37,544 | INFO | Training epoch 1737, Batch 1000/1000: LR=5.20e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 02:28:37,610 | INFO | Epoch 1737 Train Time 91.20772504806519s

2025-10-18 02:30:17,355 | INFO | Training epoch 1738, Batch 1000/1000: LR=5.16e-06, Loss=3.69e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 02:30:17,404 | INFO | Epoch 1738 Train Time 99.79210805892944s

2025-10-18 02:31:48,046 | INFO | Training epoch 1739, Batch 1000/1000: LR=5.13e-06, Loss=3.82e-03 BER=1.57e-03 FER=1.35e-02
2025-10-18 02:31:48,091 | INFO | Epoch 1739 Train Time 90.68511533737183s

2025-10-18 02:33:27,487 | INFO | Training epoch 1740, Batch 1000/1000: LR=5.10e-06, Loss=3.72e-03 BER=1.54e-03 FER=1.32e-02
2025-10-18 02:33:27,553 | INFO | Epoch 1740 Train Time 99.46045279502869s

2025-10-18 02:35:06,198 | INFO | Training epoch 1741, Batch 1000/1000: LR=5.07e-06, Loss=3.74e-03 BER=1.52e-03 FER=1.26e-02
2025-10-18 02:35:06,261 | INFO | Epoch 1741 Train Time 98.70666170120239s

2025-10-18 02:36:52,615 | INFO | Training epoch 1742, Batch 1000/1000: LR=5.04e-06, Loss=3.68e-03 BER=1.52e-03 FER=1.26e-02
2025-10-18 02:36:52,674 | INFO | Epoch 1742 Train Time 106.41176342964172s

2025-10-18 02:38:24,799 | INFO | Training epoch 1743, Batch 1000/1000: LR=5.01e-06, Loss=3.50e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 02:38:24,849 | INFO | Epoch 1743 Train Time 92.17363405227661s

2025-10-18 02:39:54,245 | INFO | Training epoch 1744, Batch 1000/1000: LR=4.98e-06, Loss=3.49e-03 BER=1.42e-03 FER=1.22e-02
2025-10-18 02:39:54,282 | INFO | Epoch 1744 Train Time 89.43160891532898s

2025-10-18 02:41:29,905 | INFO | Training epoch 1745, Batch 1000/1000: LR=4.95e-06, Loss=3.54e-03 BER=1.44e-03 FER=1.26e-02
2025-10-18 02:41:29,986 | INFO | Epoch 1745 Train Time 95.70308327674866s

2025-10-18 02:43:06,645 | INFO | Training epoch 1746, Batch 1000/1000: LR=4.92e-06, Loss=3.74e-03 BER=1.54e-03 FER=1.32e-02
2025-10-18 02:43:06,687 | INFO | Epoch 1746 Train Time 96.69829034805298s

2025-10-18 02:44:47,205 | INFO | Training epoch 1747, Batch 1000/1000: LR=4.89e-06, Loss=3.79e-03 BER=1.57e-03 FER=1.35e-02
2025-10-18 02:44:47,249 | INFO | Epoch 1747 Train Time 100.56092166900635s

2025-10-18 02:46:22,304 | INFO | Training epoch 1748, Batch 1000/1000: LR=4.86e-06, Loss=3.57e-03 BER=1.47e-03 FER=1.25e-02
2025-10-18 02:46:22,357 | INFO | Epoch 1748 Train Time 95.10775947570801s

2025-10-18 02:47:52,829 | INFO | Training epoch 1749, Batch 1000/1000: LR=4.83e-06, Loss=3.56e-03 BER=1.46e-03 FER=1.29e-02
2025-10-18 02:47:52,882 | INFO | Epoch 1749 Train Time 90.52308773994446s

2025-10-18 02:49:26,107 | INFO | Training epoch 1750, Batch 1000/1000: LR=4.80e-06, Loss=3.62e-03 BER=1.48e-03 FER=1.25e-02
2025-10-18 02:49:26,171 | INFO | Epoch 1750 Train Time 93.28684091567993s

2025-10-18 02:51:09,210 | INFO | Training epoch 1751, Batch 1000/1000: LR=4.77e-06, Loss=3.56e-03 BER=1.47e-03 FER=1.26e-02
2025-10-18 02:51:09,263 | INFO | Epoch 1751 Train Time 103.09053945541382s

2025-10-18 02:52:42,145 | INFO | Training epoch 1752, Batch 1000/1000: LR=4.74e-06, Loss=3.66e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 02:52:42,187 | INFO | Epoch 1752 Train Time 92.92215991020203s

2025-10-18 02:54:19,655 | INFO | Training epoch 1753, Batch 1000/1000: LR=4.71e-06, Loss=3.72e-03 BER=1.48e-03 FER=1.27e-02
2025-10-18 02:54:19,710 | INFO | Epoch 1753 Train Time 97.5216224193573s

2025-10-18 02:55:54,723 | INFO | Training epoch 1754, Batch 1000/1000: LR=4.68e-06, Loss=3.74e-03 BER=1.53e-03 FER=1.30e-02
2025-10-18 02:55:54,821 | INFO | Epoch 1754 Train Time 95.10905122756958s

2025-10-18 02:57:31,753 | INFO | Training epoch 1755, Batch 1000/1000: LR=4.65e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 02:57:31,804 | INFO | Epoch 1755 Train Time 96.98174905776978s

2025-10-18 02:59:08,002 | INFO | Training epoch 1756, Batch 1000/1000: LR=4.62e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 02:59:08,050 | INFO | Epoch 1756 Train Time 96.24462628364563s

2025-10-18 03:00:31,441 | INFO | Training epoch 1757, Batch 1000/1000: LR=4.59e-06, Loss=3.60e-03 BER=1.48e-03 FER=1.25e-02
2025-10-18 03:00:31,502 | INFO | Epoch 1757 Train Time 83.45031404495239s

2025-10-18 03:02:07,978 | INFO | Training epoch 1758, Batch 1000/1000: LR=4.56e-06, Loss=3.62e-03 BER=1.48e-03 FER=1.29e-02
2025-10-18 03:02:08,032 | INFO | Epoch 1758 Train Time 96.52930927276611s

2025-10-18 03:03:44,080 | INFO | Training epoch 1759, Batch 1000/1000: LR=4.53e-06, Loss=3.42e-03 BER=1.44e-03 FER=1.23e-02
2025-10-18 03:03:44,139 | INFO | Epoch 1759 Train Time 96.10490822792053s

2025-10-18 03:05:09,478 | INFO | Training epoch 1760, Batch 1000/1000: LR=4.50e-06, Loss=3.70e-03 BER=1.52e-03 FER=1.30e-02
2025-10-18 03:05:09,517 | INFO | Epoch 1760 Train Time 85.37628722190857s

2025-10-18 03:06:48,495 | INFO | Training epoch 1761, Batch 1000/1000: LR=4.48e-06, Loss=3.79e-03 BER=1.59e-03 FER=1.34e-02
2025-10-18 03:06:48,538 | INFO | Epoch 1761 Train Time 99.0198655128479s

2025-10-18 03:08:23,058 | INFO | Training epoch 1762, Batch 1000/1000: LR=4.45e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 03:08:23,102 | INFO | Epoch 1762 Train Time 94.56311011314392s

2025-10-18 03:10:00,723 | INFO | Training epoch 1763, Batch 1000/1000: LR=4.42e-06, Loss=3.66e-03 BER=1.50e-03 FER=1.30e-02
2025-10-18 03:10:00,768 | INFO | Epoch 1763 Train Time 97.66497611999512s

2025-10-18 03:11:34,451 | INFO | Training epoch 1764, Batch 1000/1000: LR=4.39e-06, Loss=3.68e-03 BER=1.53e-03 FER=1.31e-02
2025-10-18 03:11:34,495 | INFO | Epoch 1764 Train Time 93.72546172142029s

2025-10-18 03:13:15,371 | INFO | Training epoch 1765, Batch 1000/1000: LR=4.36e-06, Loss=3.65e-03 BER=1.49e-03 FER=1.30e-02
2025-10-18 03:13:15,437 | INFO | Epoch 1765 Train Time 100.94044780731201s

2025-10-18 03:14:46,852 | INFO | Training epoch 1766, Batch 1000/1000: LR=4.33e-06, Loss=3.77e-03 BER=1.55e-03 FER=1.31e-02
2025-10-18 03:14:46,891 | INFO | Epoch 1766 Train Time 91.45192289352417s

2025-10-18 03:16:20,345 | INFO | Training epoch 1767, Batch 1000/1000: LR=4.31e-06, Loss=3.46e-03 BER=1.43e-03 FER=1.24e-02
2025-10-18 03:16:20,391 | INFO | Epoch 1767 Train Time 93.49869990348816s

2025-10-18 03:17:51,175 | INFO | Training epoch 1768, Batch 1000/1000: LR=4.28e-06, Loss=3.58e-03 BER=1.50e-03 FER=1.29e-02
2025-10-18 03:17:51,226 | INFO | Epoch 1768 Train Time 90.83360648155212s

2025-10-18 03:19:25,181 | INFO | Training epoch 1769, Batch 1000/1000: LR=4.25e-06, Loss=3.62e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 03:19:25,226 | INFO | Epoch 1769 Train Time 93.99928879737854s

2025-10-18 03:21:06,738 | INFO | Training epoch 1770, Batch 1000/1000: LR=4.22e-06, Loss=3.68e-03 BER=1.53e-03 FER=1.30e-02
2025-10-18 03:21:06,784 | INFO | Epoch 1770 Train Time 101.55572128295898s

2025-10-18 03:22:52,166 | INFO | Training epoch 1771, Batch 1000/1000: LR=4.20e-06, Loss=3.69e-03 BER=1.51e-03 FER=1.31e-02
2025-10-18 03:22:52,219 | INFO | Epoch 1771 Train Time 105.4330575466156s

2025-10-18 03:24:27,057 | INFO | Training epoch 1772, Batch 1000/1000: LR=4.17e-06, Loss=3.49e-03 BER=1.44e-03 FER=1.23e-02
2025-10-18 03:24:27,101 | INFO | Epoch 1772 Train Time 94.88130760192871s

2025-10-18 03:26:04,896 | INFO | Training epoch 1773, Batch 1000/1000: LR=4.14e-06, Loss=3.53e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 03:26:04,941 | INFO | Epoch 1773 Train Time 97.83881568908691s

2025-10-18 03:27:47,234 | INFO | Training epoch 1774, Batch 1000/1000: LR=4.11e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.26e-02
2025-10-18 03:27:47,283 | INFO | Epoch 1774 Train Time 102.34057807922363s

2025-10-18 03:29:28,577 | INFO | Training epoch 1775, Batch 1000/1000: LR=4.09e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.28e-02
2025-10-18 03:29:28,641 | INFO | Epoch 1775 Train Time 101.35545349121094s

2025-10-18 03:30:56,237 | INFO | Training epoch 1776, Batch 1000/1000: LR=4.06e-06, Loss=3.68e-03 BER=1.53e-03 FER=1.30e-02
2025-10-18 03:30:56,295 | INFO | Epoch 1776 Train Time 87.65224623680115s

2025-10-18 03:32:36,420 | INFO | Training epoch 1777, Batch 1000/1000: LR=4.03e-06, Loss=3.43e-03 BER=1.42e-03 FER=1.22e-02
2025-10-18 03:32:36,484 | INFO | Epoch 1777 Train Time 100.18704199790955s

2025-10-18 03:34:10,868 | INFO | Training epoch 1778, Batch 1000/1000: LR=4.01e-06, Loss=3.78e-03 BER=1.54e-03 FER=1.33e-02
2025-10-18 03:34:10,925 | INFO | Epoch 1778 Train Time 94.44026827812195s

2025-10-18 03:35:44,095 | INFO | Training epoch 1779, Batch 1000/1000: LR=3.98e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.26e-02
2025-10-18 03:35:44,142 | INFO | Epoch 1779 Train Time 93.21496868133545s

2025-10-18 03:37:19,434 | INFO | Training epoch 1780, Batch 1000/1000: LR=3.95e-06, Loss=3.69e-03 BER=1.52e-03 FER=1.29e-02
2025-10-18 03:37:19,491 | INFO | Epoch 1780 Train Time 95.34720945358276s

2025-10-18 03:38:53,120 | INFO | Training epoch 1781, Batch 1000/1000: LR=3.93e-06, Loss=3.53e-03 BER=1.41e-03 FER=1.20e-02
2025-10-18 03:38:53,167 | INFO | Epoch 1781 Train Time 93.67408561706543s

2025-10-18 03:40:24,310 | INFO | Training epoch 1782, Batch 1000/1000: LR=3.90e-06, Loss=3.69e-03 BER=1.51e-03 FER=1.29e-02
2025-10-18 03:40:24,371 | INFO | Epoch 1782 Train Time 91.20316815376282s

2025-10-18 03:42:05,611 | INFO | Training epoch 1783, Batch 1000/1000: LR=3.87e-06, Loss=3.52e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 03:42:05,667 | INFO | Epoch 1783 Train Time 101.29414510726929s

2025-10-18 03:43:42,835 | INFO | Training epoch 1784, Batch 1000/1000: LR=3.85e-06, Loss=3.47e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 03:43:42,886 | INFO | Epoch 1784 Train Time 97.216481924057s

2025-10-18 03:45:09,779 | INFO | Training epoch 1785, Batch 1000/1000: LR=3.82e-06, Loss=3.45e-03 BER=1.41e-03 FER=1.21e-02
2025-10-18 03:45:09,829 | INFO | Epoch 1785 Train Time 86.94071340560913s

2025-10-18 03:46:45,343 | INFO | Training epoch 1786, Batch 1000/1000: LR=3.80e-06, Loss=3.47e-03 BER=1.45e-03 FER=1.22e-02
2025-10-18 03:46:45,386 | INFO | Epoch 1786 Train Time 95.55609703063965s

2025-10-18 03:48:22,944 | INFO | Training epoch 1787, Batch 1000/1000: LR=3.77e-06, Loss=3.51e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 03:48:22,986 | INFO | Epoch 1787 Train Time 97.59847259521484s

2025-10-18 03:49:56,984 | INFO | Training epoch 1788, Batch 1000/1000: LR=3.74e-06, Loss=3.61e-03 BER=1.48e-03 FER=1.25e-02
2025-10-18 03:49:57,033 | INFO | Epoch 1788 Train Time 94.04545545578003s

2025-10-18 03:51:28,881 | INFO | Training epoch 1789, Batch 1000/1000: LR=3.72e-06, Loss=3.68e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 03:51:28,925 | INFO | Epoch 1789 Train Time 91.89059948921204s

2025-10-18 03:53:01,737 | INFO | Training epoch 1790, Batch 1000/1000: LR=3.69e-06, Loss=3.57e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 03:53:01,800 | INFO | Epoch 1790 Train Time 92.87400937080383s

2025-10-18 03:54:35,478 | INFO | Training epoch 1791, Batch 1000/1000: LR=3.67e-06, Loss=3.66e-03 BER=1.51e-03 FER=1.28e-02
2025-10-18 03:54:35,541 | INFO | Epoch 1791 Train Time 93.73903489112854s

2025-10-18 03:56:06,734 | INFO | Training epoch 1792, Batch 1000/1000: LR=3.64e-06, Loss=3.43e-03 BER=1.41e-03 FER=1.24e-02
2025-10-18 03:56:06,780 | INFO | Epoch 1792 Train Time 91.23624086380005s

2025-10-18 03:57:47,652 | INFO | Training epoch 1793, Batch 1000/1000: LR=3.62e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 03:57:47,721 | INFO | Epoch 1793 Train Time 100.93975925445557s

2025-10-18 03:59:19,168 | INFO | Training epoch 1794, Batch 1000/1000: LR=3.59e-06, Loss=3.74e-03 BER=1.53e-03 FER=1.30e-02
2025-10-18 03:59:19,211 | INFO | Epoch 1794 Train Time 91.48821020126343s

2025-10-18 04:00:52,916 | INFO | Training epoch 1795, Batch 1000/1000: LR=3.57e-06, Loss=3.38e-03 BER=1.41e-03 FER=1.19e-02
2025-10-18 04:00:52,976 | INFO | Epoch 1795 Train Time 93.7641851902008s

2025-10-18 04:02:26,874 | INFO | Training epoch 1796, Batch 1000/1000: LR=3.54e-06, Loss=3.52e-03 BER=1.43e-03 FER=1.23e-02
2025-10-18 04:02:26,932 | INFO | Epoch 1796 Train Time 93.95356106758118s

2025-10-18 04:03:55,255 | INFO | Training epoch 1797, Batch 1000/1000: LR=3.52e-06, Loss=3.54e-03 BER=1.47e-03 FER=1.26e-02
2025-10-18 04:03:55,314 | INFO | Epoch 1797 Train Time 88.38049173355103s

2025-10-18 04:05:24,613 | INFO | Training epoch 1798, Batch 1000/1000: LR=3.50e-06, Loss=3.57e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 04:05:24,675 | INFO | Epoch 1798 Train Time 89.35930705070496s

2025-10-18 04:06:58,741 | INFO | Training epoch 1799, Batch 1000/1000: LR=3.47e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 04:06:58,804 | INFO | Epoch 1799 Train Time 94.12853622436523s

2025-10-18 04:08:35,146 | INFO | Training epoch 1800, Batch 1000/1000: LR=3.45e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 04:08:35,197 | INFO | Epoch 1800 Train Time 96.39084315299988s

2025-10-18 04:10:11,550 | INFO | Training epoch 1801, Batch 1000/1000: LR=3.42e-06, Loss=3.57e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 04:10:11,603 | INFO | Epoch 1801 Train Time 96.40483617782593s

2025-10-18 04:11:42,164 | INFO | Training epoch 1802, Batch 1000/1000: LR=3.40e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 04:11:42,210 | INFO | Epoch 1802 Train Time 90.60623669624329s

2025-10-18 04:13:18,745 | INFO | Training epoch 1803, Batch 1000/1000: LR=3.37e-06, Loss=3.47e-03 BER=1.44e-03 FER=1.23e-02
2025-10-18 04:13:18,795 | INFO | Epoch 1803 Train Time 96.58297944068909s

2025-10-18 04:14:55,339 | INFO | Training epoch 1804, Batch 1000/1000: LR=3.35e-06, Loss=3.77e-03 BER=1.57e-03 FER=1.32e-02
2025-10-18 04:14:55,392 | INFO | Epoch 1804 Train Time 96.59602737426758s

2025-10-18 04:16:33,643 | INFO | Training epoch 1805, Batch 1000/1000: LR=3.33e-06, Loss=3.70e-03 BER=1.49e-03 FER=1.29e-02
2025-10-18 04:16:33,704 | INFO | Epoch 1805 Train Time 98.31033372879028s

2025-10-18 04:18:04,703 | INFO | Training epoch 1806, Batch 1000/1000: LR=3.30e-06, Loss=3.48e-03 BER=1.41e-03 FER=1.22e-02
2025-10-18 04:18:04,765 | INFO | Epoch 1806 Train Time 91.05973649024963s

2025-10-18 04:19:44,582 | INFO | Training epoch 1807, Batch 1000/1000: LR=3.28e-06, Loss=3.51e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 04:19:44,644 | INFO | Epoch 1807 Train Time 99.87730145454407s

2025-10-18 04:21:22,144 | INFO | Training epoch 1808, Batch 1000/1000: LR=3.26e-06, Loss=3.62e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 04:21:22,190 | INFO | Epoch 1808 Train Time 97.54367899894714s

2025-10-18 04:22:57,706 | INFO | Training epoch 1809, Batch 1000/1000: LR=3.23e-06, Loss=3.67e-03 BER=1.53e-03 FER=1.28e-02
2025-10-18 04:22:57,757 | INFO | Epoch 1809 Train Time 95.56588053703308s

2025-10-18 04:24:31,382 | INFO | Training epoch 1810, Batch 1000/1000: LR=3.21e-06, Loss=3.55e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 04:24:31,437 | INFO | Epoch 1810 Train Time 93.67773222923279s

2025-10-18 04:26:05,564 | INFO | Training epoch 1811, Batch 1000/1000: LR=3.19e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 04:26:05,615 | INFO | Epoch 1811 Train Time 94.17714715003967s

2025-10-18 04:27:41,833 | INFO | Training epoch 1812, Batch 1000/1000: LR=3.17e-06, Loss=3.42e-03 BER=1.42e-03 FER=1.23e-02
2025-10-18 04:27:41,886 | INFO | Epoch 1812 Train Time 96.2694501876831s

2025-10-18 04:29:24,008 | INFO | Training epoch 1813, Batch 1000/1000: LR=3.14e-06, Loss=3.56e-03 BER=1.44e-03 FER=1.26e-02
2025-10-18 04:29:24,064 | INFO | Epoch 1813 Train Time 102.17660999298096s

2025-10-18 04:30:57,843 | INFO | Training epoch 1814, Batch 1000/1000: LR=3.12e-06, Loss=3.44e-03 BER=1.43e-03 FER=1.23e-02
2025-10-18 04:30:57,890 | INFO | Epoch 1814 Train Time 93.8246340751648s

2025-10-18 04:32:32,827 | INFO | Training epoch 1815, Batch 1000/1000: LR=3.10e-06, Loss=3.81e-03 BER=1.57e-03 FER=1.34e-02
2025-10-18 04:32:32,872 | INFO | Epoch 1815 Train Time 94.97988605499268s

2025-10-18 04:34:05,553 | INFO | Training epoch 1816, Batch 1000/1000: LR=3.08e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.26e-02
2025-10-18 04:34:05,615 | INFO | Epoch 1816 Train Time 92.74193143844604s

2025-10-18 04:35:36,134 | INFO | Training epoch 1817, Batch 1000/1000: LR=3.05e-06, Loss=3.64e-03 BER=1.51e-03 FER=1.27e-02
2025-10-18 04:35:36,191 | INFO | Epoch 1817 Train Time 90.57484412193298s

2025-10-18 04:37:01,565 | INFO | Training epoch 1818, Batch 1000/1000: LR=3.03e-06, Loss=3.37e-03 BER=1.39e-03 FER=1.21e-02
2025-10-18 04:37:01,606 | INFO | Epoch 1818 Train Time 85.41426658630371s

2025-10-18 04:37:01,607 | INFO | [P2] saving best_model (QAT) with loss 0.003369 at epoch 1818
2025-10-18 04:38:33,636 | INFO | Training epoch 1819, Batch 1000/1000: LR=3.01e-06, Loss=3.56e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 04:38:33,701 | INFO | Epoch 1819 Train Time 92.01510953903198s

2025-10-18 04:40:10,170 | INFO | Training epoch 1820, Batch 1000/1000: LR=2.99e-06, Loss=3.63e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 04:40:10,223 | INFO | Epoch 1820 Train Time 96.52052927017212s

2025-10-18 04:41:43,545 | INFO | Training epoch 1821, Batch 1000/1000: LR=2.97e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.29e-02
2025-10-18 04:41:43,602 | INFO | Epoch 1821 Train Time 93.37804174423218s

2025-10-18 04:43:12,978 | INFO | Training epoch 1822, Batch 1000/1000: LR=2.94e-06, Loss=3.61e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 04:43:13,025 | INFO | Epoch 1822 Train Time 89.42120361328125s

2025-10-18 04:44:52,641 | INFO | Training epoch 1823, Batch 1000/1000: LR=2.92e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 04:44:52,686 | INFO | Epoch 1823 Train Time 99.65992093086243s

2025-10-18 04:46:34,580 | INFO | Training epoch 1824, Batch 1000/1000: LR=2.90e-06, Loss=3.39e-03 BER=1.42e-03 FER=1.22e-02
2025-10-18 04:46:34,640 | INFO | Epoch 1824 Train Time 101.9527850151062s

2025-10-18 04:48:11,230 | INFO | Training epoch 1825, Batch 1000/1000: LR=2.88e-06, Loss=3.51e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 04:48:11,281 | INFO | Epoch 1825 Train Time 96.63963198661804s

2025-10-18 04:49:50,531 | INFO | Training epoch 1826, Batch 1000/1000: LR=2.86e-06, Loss=3.51e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 04:49:50,578 | INFO | Epoch 1826 Train Time 99.2954638004303s

2025-10-18 04:51:27,703 | INFO | Training epoch 1827, Batch 1000/1000: LR=2.84e-06, Loss=3.66e-03 BER=1.50e-03 FER=1.30e-02
2025-10-18 04:51:27,782 | INFO | Epoch 1827 Train Time 97.20334053039551s

2025-10-18 04:53:00,670 | INFO | Training epoch 1828, Batch 1000/1000: LR=2.82e-06, Loss=3.47e-03 BER=1.41e-03 FER=1.21e-02
2025-10-18 04:53:00,719 | INFO | Epoch 1828 Train Time 92.93537068367004s

2025-10-18 04:54:34,828 | INFO | Training epoch 1829, Batch 1000/1000: LR=2.80e-06, Loss=3.65e-03 BER=1.51e-03 FER=1.26e-02
2025-10-18 04:54:34,892 | INFO | Epoch 1829 Train Time 94.17227339744568s

2025-10-18 04:56:04,392 | INFO | Training epoch 1830, Batch 1000/1000: LR=2.77e-06, Loss=3.53e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 04:56:04,441 | INFO | Epoch 1830 Train Time 89.54696917533875s

2025-10-18 04:57:39,459 | INFO | Training epoch 1831, Batch 1000/1000: LR=2.75e-06, Loss=3.58e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 04:57:39,523 | INFO | Epoch 1831 Train Time 95.07985162734985s

2025-10-18 04:59:10,749 | INFO | Training epoch 1832, Batch 1000/1000: LR=2.73e-06, Loss=3.63e-03 BER=1.49e-03 FER=1.29e-02
2025-10-18 04:59:10,795 | INFO | Epoch 1832 Train Time 91.27161026000977s

2025-10-18 05:00:51,410 | INFO | Training epoch 1833, Batch 1000/1000: LR=2.71e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 05:00:51,469 | INFO | Epoch 1833 Train Time 100.67237830162048s

2025-10-18 05:02:23,366 | INFO | Training epoch 1834, Batch 1000/1000: LR=2.69e-06, Loss=3.62e-03 BER=1.51e-03 FER=1.28e-02
2025-10-18 05:02:23,432 | INFO | Epoch 1834 Train Time 91.96023511886597s

2025-10-18 05:03:49,769 | INFO | Training epoch 1835, Batch 1000/1000: LR=2.67e-06, Loss=3.62e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 05:03:49,814 | INFO | Epoch 1835 Train Time 86.3805902004242s

2025-10-18 05:05:19,910 | INFO | Training epoch 1836, Batch 1000/1000: LR=2.65e-06, Loss=3.71e-03 BER=1.53e-03 FER=1.31e-02
2025-10-18 05:05:19,973 | INFO | Epoch 1836 Train Time 90.1579475402832s

2025-10-18 05:06:56,156 | INFO | Training epoch 1837, Batch 1000/1000: LR=2.63e-06, Loss=3.70e-03 BER=1.52e-03 FER=1.28e-02
2025-10-18 05:06:56,211 | INFO | Epoch 1837 Train Time 96.2370138168335s

2025-10-18 05:08:33,939 | INFO | Training epoch 1838, Batch 1000/1000: LR=2.61e-06, Loss=3.46e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 05:08:33,984 | INFO | Epoch 1838 Train Time 97.77131080627441s

2025-10-18 05:10:20,954 | INFO | Training epoch 1839, Batch 1000/1000: LR=2.59e-06, Loss=3.41e-03 BER=1.40e-03 FER=1.19e-02
2025-10-18 05:10:21,022 | INFO | Epoch 1839 Train Time 107.03589487075806s

2025-10-18 05:11:56,011 | INFO | Training epoch 1840, Batch 1000/1000: LR=2.57e-06, Loss=3.65e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 05:11:56,060 | INFO | Epoch 1840 Train Time 95.03619003295898s

2025-10-18 05:13:28,133 | INFO | Training epoch 1841, Batch 1000/1000: LR=2.56e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.30e-02
2025-10-18 05:13:28,181 | INFO | Epoch 1841 Train Time 92.12012481689453s

2025-10-18 05:15:04,507 | INFO | Training epoch 1842, Batch 1000/1000: LR=2.54e-06, Loss=3.66e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 05:15:04,577 | INFO | Epoch 1842 Train Time 96.39483094215393s

2025-10-18 05:16:44,442 | INFO | Training epoch 1843, Batch 1000/1000: LR=2.52e-06, Loss=3.61e-03 BER=1.51e-03 FER=1.28e-02
2025-10-18 05:16:44,488 | INFO | Epoch 1843 Train Time 99.9090051651001s

2025-10-18 05:18:21,410 | INFO | Training epoch 1844, Batch 1000/1000: LR=2.50e-06, Loss=3.33e-03 BER=1.38e-03 FER=1.18e-02
2025-10-18 05:18:21,455 | INFO | Epoch 1844 Train Time 96.96612429618835s

2025-10-18 05:18:21,456 | INFO | [P2] saving best_model (QAT) with loss 0.003326 at epoch 1844
2025-10-18 05:19:50,583 | INFO | Training epoch 1845, Batch 1000/1000: LR=2.48e-06, Loss=3.51e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 05:19:50,627 | INFO | Epoch 1845 Train Time 89.08944463729858s

2025-10-18 05:21:22,643 | INFO | Training epoch 1846, Batch 1000/1000: LR=2.46e-06, Loss=3.37e-03 BER=1.38e-03 FER=1.20e-02
2025-10-18 05:21:22,689 | INFO | Epoch 1846 Train Time 92.06141877174377s

2025-10-18 05:22:56,853 | INFO | Training epoch 1847, Batch 1000/1000: LR=2.44e-06, Loss=3.54e-03 BER=1.45e-03 FER=1.23e-02
2025-10-18 05:22:56,924 | INFO | Epoch 1847 Train Time 94.23365211486816s

2025-10-18 05:24:32,639 | INFO | Training epoch 1848, Batch 1000/1000: LR=2.42e-06, Loss=3.52e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 05:24:32,687 | INFO | Epoch 1848 Train Time 95.76189541816711s

2025-10-18 05:26:08,427 | INFO | Training epoch 1849, Batch 1000/1000: LR=2.40e-06, Loss=3.42e-03 BER=1.42e-03 FER=1.22e-02
2025-10-18 05:26:08,484 | INFO | Epoch 1849 Train Time 95.79499959945679s

2025-10-18 05:27:40,452 | INFO | Training epoch 1850, Batch 1000/1000: LR=2.39e-06, Loss=3.54e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 05:27:40,523 | INFO | Epoch 1850 Train Time 92.03841233253479s

2025-10-18 05:29:14,885 | INFO | Training epoch 1851, Batch 1000/1000: LR=2.37e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.25e-02
2025-10-18 05:29:14,944 | INFO | Epoch 1851 Train Time 94.41924500465393s

2025-10-18 05:30:51,936 | INFO | Training epoch 1852, Batch 1000/1000: LR=2.35e-06, Loss=3.50e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 05:30:51,981 | INFO | Epoch 1852 Train Time 97.0347912311554s

2025-10-18 05:32:28,431 | INFO | Training epoch 1853, Batch 1000/1000: LR=2.33e-06, Loss=3.56e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 05:32:28,510 | INFO | Epoch 1853 Train Time 96.52812123298645s

2025-10-18 05:34:05,535 | INFO | Training epoch 1854, Batch 1000/1000: LR=2.31e-06, Loss=3.60e-03 BER=1.48e-03 FER=1.27e-02
2025-10-18 05:34:05,601 | INFO | Epoch 1854 Train Time 97.08994150161743s

2025-10-18 05:35:43,236 | INFO | Training epoch 1855, Batch 1000/1000: LR=2.30e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 05:35:43,276 | INFO | Epoch 1855 Train Time 97.67238640785217s

2025-10-18 05:37:19,438 | INFO | Training epoch 1856, Batch 1000/1000: LR=2.28e-06, Loss=3.47e-03 BER=1.41e-03 FER=1.21e-02
2025-10-18 05:37:19,497 | INFO | Epoch 1856 Train Time 96.21985840797424s

2025-10-18 05:38:55,256 | INFO | Training epoch 1857, Batch 1000/1000: LR=2.26e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 05:38:55,325 | INFO | Epoch 1857 Train Time 95.82636737823486s

2025-10-18 05:40:39,129 | INFO | Training epoch 1858, Batch 1000/1000: LR=2.24e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.28e-02
2025-10-18 05:40:39,186 | INFO | Epoch 1858 Train Time 103.85918188095093s

2025-10-18 05:42:16,547 | INFO | Training epoch 1859, Batch 1000/1000: LR=2.23e-06, Loss=3.59e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 05:42:16,610 | INFO | Epoch 1859 Train Time 97.42323017120361s

2025-10-18 05:43:48,555 | INFO | Training epoch 1860, Batch 1000/1000: LR=2.21e-06, Loss=3.63e-03 BER=1.50e-03 FER=1.25e-02
2025-10-18 05:43:48,597 | INFO | Epoch 1860 Train Time 91.98563885688782s

2025-10-18 05:45:24,336 | INFO | Training epoch 1861, Batch 1000/1000: LR=2.19e-06, Loss=3.66e-03 BER=1.53e-03 FER=1.27e-02
2025-10-18 05:45:24,401 | INFO | Epoch 1861 Train Time 95.80299258232117s

2025-10-18 05:47:08,157 | INFO | Training epoch 1862, Batch 1000/1000: LR=2.18e-06, Loss=3.48e-03 BER=1.44e-03 FER=1.23e-02
2025-10-18 05:47:08,218 | INFO | Epoch 1862 Train Time 103.81550431251526s

2025-10-18 05:48:42,140 | INFO | Training epoch 1863, Batch 1000/1000: LR=2.16e-06, Loss=3.44e-03 BER=1.41e-03 FER=1.22e-02
2025-10-18 05:48:42,183 | INFO | Epoch 1863 Train Time 93.96483373641968s

2025-10-18 05:50:16,351 | INFO | Training epoch 1864, Batch 1000/1000: LR=2.14e-06, Loss=3.61e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 05:50:16,406 | INFO | Epoch 1864 Train Time 94.22036147117615s

2025-10-18 05:51:48,909 | INFO | Training epoch 1865, Batch 1000/1000: LR=2.13e-06, Loss=3.57e-03 BER=1.47e-03 FER=1.26e-02
2025-10-18 05:51:48,976 | INFO | Epoch 1865 Train Time 92.56832027435303s

2025-10-18 05:53:28,415 | INFO | Training epoch 1866, Batch 1000/1000: LR=2.11e-06, Loss=3.57e-03 BER=1.48e-03 FER=1.28e-02
2025-10-18 05:53:28,504 | INFO | Epoch 1866 Train Time 99.52551245689392s

2025-10-18 05:55:01,078 | INFO | Training epoch 1867, Batch 1000/1000: LR=2.09e-06, Loss=3.52e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 05:55:01,163 | INFO | Epoch 1867 Train Time 92.65458106994629s

2025-10-18 05:56:40,329 | INFO | Training epoch 1868, Batch 1000/1000: LR=2.08e-06, Loss=3.71e-03 BER=1.53e-03 FER=1.29e-02
2025-10-18 05:56:40,395 | INFO | Epoch 1868 Train Time 99.2304413318634s

2025-10-18 05:58:27,332 | INFO | Training epoch 1869, Batch 1000/1000: LR=2.06e-06, Loss=3.52e-03 BER=1.46e-03 FER=1.26e-02
2025-10-18 05:58:27,389 | INFO | Epoch 1869 Train Time 106.9921019077301s

2025-10-18 06:00:01,116 | INFO | Training epoch 1870, Batch 1000/1000: LR=2.04e-06, Loss=3.33e-03 BER=1.34e-03 FER=1.14e-02
2025-10-18 06:00:01,179 | INFO | Epoch 1870 Train Time 93.78856086730957s

2025-10-18 06:01:33,037 | INFO | Training epoch 1871, Batch 1000/1000: LR=2.03e-06, Loss=3.71e-03 BER=1.51e-03 FER=1.28e-02
2025-10-18 06:01:33,100 | INFO | Epoch 1871 Train Time 91.92009115219116s

2025-10-18 06:03:10,548 | INFO | Training epoch 1872, Batch 1000/1000: LR=2.01e-06, Loss=3.66e-03 BER=1.51e-03 FER=1.27e-02
2025-10-18 06:03:10,609 | INFO | Epoch 1872 Train Time 97.50810313224792s

2025-10-18 06:04:46,619 | INFO | Training epoch 1873, Batch 1000/1000: LR=2.00e-06, Loss=3.57e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 06:04:46,679 | INFO | Epoch 1873 Train Time 96.06814384460449s

2025-10-18 06:06:25,631 | INFO | Training epoch 1874, Batch 1000/1000: LR=1.98e-06, Loss=3.55e-03 BER=1.43e-03 FER=1.23e-02
2025-10-18 06:06:25,681 | INFO | Epoch 1874 Train Time 99.0008192062378s

2025-10-18 06:08:01,215 | INFO | Training epoch 1875, Batch 1000/1000: LR=1.97e-06, Loss=3.62e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 06:08:01,280 | INFO | Epoch 1875 Train Time 95.59631109237671s

2025-10-18 06:09:38,944 | INFO | Training epoch 1876, Batch 1000/1000: LR=1.95e-06, Loss=3.48e-03 BER=1.44e-03 FER=1.23e-02
2025-10-18 06:09:39,020 | INFO | Epoch 1876 Train Time 97.73866486549377s

2025-10-18 06:11:16,822 | INFO | Training epoch 1877, Batch 1000/1000: LR=1.94e-06, Loss=3.56e-03 BER=1.45e-03 FER=1.26e-02
2025-10-18 06:11:16,882 | INFO | Epoch 1877 Train Time 97.86067628860474s

2025-10-18 06:13:06,008 | INFO | Training epoch 1878, Batch 1000/1000: LR=1.92e-06, Loss=3.56e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 06:13:06,073 | INFO | Epoch 1878 Train Time 109.18922519683838s

2025-10-18 06:14:49,855 | INFO | Training epoch 1879, Batch 1000/1000: LR=1.91e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 06:14:49,905 | INFO | Epoch 1879 Train Time 103.83080649375916s

2025-10-18 06:16:29,740 | INFO | Training epoch 1880, Batch 1000/1000: LR=1.89e-06, Loss=3.60e-03 BER=1.50e-03 FER=1.26e-02
2025-10-18 06:16:29,793 | INFO | Epoch 1880 Train Time 99.88626146316528s

2025-10-18 06:18:11,518 | INFO | Training epoch 1881, Batch 1000/1000: LR=1.88e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 06:18:11,618 | INFO | Epoch 1881 Train Time 101.82292556762695s

2025-10-18 06:19:45,828 | INFO | Training epoch 1882, Batch 1000/1000: LR=1.86e-06, Loss=3.60e-03 BER=1.46e-03 FER=1.28e-02
2025-10-18 06:19:45,886 | INFO | Epoch 1882 Train Time 94.26628303527832s

2025-10-18 06:21:24,997 | INFO | Training epoch 1883, Batch 1000/1000: LR=1.85e-06, Loss=3.63e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 06:21:25,093 | INFO | Epoch 1883 Train Time 99.20532965660095s

2025-10-18 06:23:12,037 | INFO | Training epoch 1884, Batch 1000/1000: LR=1.83e-06, Loss=3.43e-03 BER=1.40e-03 FER=1.21e-02
2025-10-18 06:23:12,135 | INFO | Epoch 1884 Train Time 107.04068231582642s

2025-10-18 06:24:54,545 | INFO | Training epoch 1885, Batch 1000/1000: LR=1.82e-06, Loss=3.54e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 06:24:54,600 | INFO | Epoch 1885 Train Time 102.46193504333496s

2025-10-18 06:26:32,334 | INFO | Training epoch 1886, Batch 1000/1000: LR=1.81e-06, Loss=3.59e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 06:26:32,402 | INFO | Epoch 1886 Train Time 97.80112910270691s

2025-10-18 06:28:15,858 | INFO | Training epoch 1887, Batch 1000/1000: LR=1.79e-06, Loss=3.55e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 06:28:15,917 | INFO | Epoch 1887 Train Time 103.51332426071167s

2025-10-18 06:30:06,539 | INFO | Training epoch 1888, Batch 1000/1000: LR=1.78e-06, Loss=3.50e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 06:30:06,598 | INFO | Epoch 1888 Train Time 110.67816805839539s

2025-10-18 06:31:47,052 | INFO | Training epoch 1889, Batch 1000/1000: LR=1.76e-06, Loss=3.61e-03 BER=1.50e-03 FER=1.26e-02
2025-10-18 06:31:47,141 | INFO | Epoch 1889 Train Time 100.54158020019531s

2025-10-18 06:33:29,316 | INFO | Training epoch 1890, Batch 1000/1000: LR=1.75e-06, Loss=3.47e-03 BER=1.41e-03 FER=1.23e-02
2025-10-18 06:33:29,393 | INFO | Epoch 1890 Train Time 102.24999117851257s

2025-10-18 06:35:09,053 | INFO | Training epoch 1891, Batch 1000/1000: LR=1.74e-06, Loss=3.71e-03 BER=1.53e-03 FER=1.30e-02
2025-10-18 06:35:09,115 | INFO | Epoch 1891 Train Time 99.72166466712952s

2025-10-18 06:36:54,500 | INFO | Training epoch 1892, Batch 1000/1000: LR=1.72e-06, Loss=3.64e-03 BER=1.48e-03 FER=1.25e-02
2025-10-18 06:36:54,562 | INFO | Epoch 1892 Train Time 105.44514393806458s

2025-10-18 06:38:20,445 | INFO | Training epoch 1893, Batch 1000/1000: LR=1.71e-06, Loss=3.52e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 06:38:20,510 | INFO | Epoch 1893 Train Time 85.94557547569275s

2025-10-18 06:39:56,727 | INFO | Training epoch 1894, Batch 1000/1000: LR=1.70e-06, Loss=3.56e-03 BER=1.47e-03 FER=1.26e-02
2025-10-18 06:39:56,787 | INFO | Epoch 1894 Train Time 96.2756552696228s

2025-10-18 06:41:37,537 | INFO | Training epoch 1895, Batch 1000/1000: LR=1.68e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.26e-02
2025-10-18 06:41:37,605 | INFO | Epoch 1895 Train Time 100.8164575099945s

2025-10-18 06:43:12,057 | INFO | Training epoch 1896, Batch 1000/1000: LR=1.67e-06, Loss=3.49e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 06:43:12,120 | INFO | Epoch 1896 Train Time 94.51392555236816s

2025-10-18 06:44:49,832 | INFO | Training epoch 1897, Batch 1000/1000: LR=1.66e-06, Loss=3.34e-03 BER=1.39e-03 FER=1.19e-02
2025-10-18 06:44:49,902 | INFO | Epoch 1897 Train Time 97.77891230583191s

2025-10-18 06:46:23,847 | INFO | Training epoch 1898, Batch 1000/1000: LR=1.65e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 06:46:23,912 | INFO | Epoch 1898 Train Time 94.00913643836975s

2025-10-18 06:48:00,141 | INFO | Training epoch 1899, Batch 1000/1000: LR=1.63e-06, Loss=3.42e-03 BER=1.42e-03 FER=1.20e-02
2025-10-18 06:48:00,201 | INFO | Epoch 1899 Train Time 96.2874002456665s

2025-10-18 06:49:35,748 | INFO | Training epoch 1900, Batch 1000/1000: LR=1.62e-06, Loss=3.44e-03 BER=1.40e-03 FER=1.19e-02
2025-10-18 06:49:35,795 | INFO | Epoch 1900 Train Time 95.59148216247559s

2025-10-18 06:51:06,736 | INFO | Training epoch 1901, Batch 1000/1000: LR=1.61e-06, Loss=3.62e-03 BER=1.48e-03 FER=1.26e-02
2025-10-18 06:51:06,803 | INFO | Epoch 1901 Train Time 91.00581955909729s

2025-10-18 06:52:40,234 | INFO | Training epoch 1902, Batch 1000/1000: LR=1.60e-06, Loss=3.52e-03 BER=1.47e-03 FER=1.25e-02
2025-10-18 06:52:40,287 | INFO | Epoch 1902 Train Time 93.47987627983093s

2025-10-18 06:54:28,778 | INFO | Training epoch 1903, Batch 1000/1000: LR=1.59e-06, Loss=3.71e-03 BER=1.51e-03 FER=1.30e-02
2025-10-18 06:54:28,829 | INFO | Epoch 1903 Train Time 108.54126691818237s

2025-10-18 06:56:05,747 | INFO | Training epoch 1904, Batch 1000/1000: LR=1.57e-06, Loss=3.52e-03 BER=1.47e-03 FER=1.23e-02
2025-10-18 06:56:05,794 | INFO | Epoch 1904 Train Time 96.96333336830139s

2025-10-18 06:57:48,033 | INFO | Training epoch 1905, Batch 1000/1000: LR=1.56e-06, Loss=3.67e-03 BER=1.52e-03 FER=1.30e-02
2025-10-18 06:57:48,085 | INFO | Epoch 1905 Train Time 102.28979921340942s

2025-10-18 06:59:32,665 | INFO | Training epoch 1906, Batch 1000/1000: LR=1.55e-06, Loss=3.60e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 06:59:32,732 | INFO | Epoch 1906 Train Time 104.64645528793335s

2025-10-18 07:01:15,110 | INFO | Training epoch 1907, Batch 1000/1000: LR=1.54e-06, Loss=3.61e-03 BER=1.48e-03 FER=1.25e-02
2025-10-18 07:01:15,161 | INFO | Epoch 1907 Train Time 102.4267406463623s

2025-10-18 07:02:48,622 | INFO | Training epoch 1908, Batch 1000/1000: LR=1.53e-06, Loss=3.50e-03 BER=1.43e-03 FER=1.26e-02
2025-10-18 07:02:48,676 | INFO | Epoch 1908 Train Time 93.51368069648743s

2025-10-18 07:04:21,524 | INFO | Training epoch 1909, Batch 1000/1000: LR=1.52e-06, Loss=3.44e-03 BER=1.41e-03 FER=1.20e-02
2025-10-18 07:04:21,620 | INFO | Epoch 1909 Train Time 92.94376730918884s

2025-10-18 07:06:00,132 | INFO | Training epoch 1910, Batch 1000/1000: LR=1.50e-06, Loss=3.50e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 07:06:00,189 | INFO | Epoch 1910 Train Time 98.56595015525818s

2025-10-18 07:07:31,903 | INFO | Training epoch 1911, Batch 1000/1000: LR=1.49e-06, Loss=3.50e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 07:07:31,975 | INFO | Epoch 1911 Train Time 91.78456950187683s

2025-10-18 07:09:13,829 | INFO | Training epoch 1912, Batch 1000/1000: LR=1.48e-06, Loss=3.61e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 07:09:13,911 | INFO | Epoch 1912 Train Time 101.93456745147705s

2025-10-18 07:10:52,499 | INFO | Training epoch 1913, Batch 1000/1000: LR=1.47e-06, Loss=3.59e-03 BER=1.50e-03 FER=1.29e-02
2025-10-18 07:10:52,561 | INFO | Epoch 1913 Train Time 98.64813470840454s

2025-10-18 07:12:35,047 | INFO | Training epoch 1914, Batch 1000/1000: LR=1.46e-06, Loss=3.72e-03 BER=1.52e-03 FER=1.29e-02
2025-10-18 07:12:35,116 | INFO | Epoch 1914 Train Time 102.55392098426819s

2025-10-18 07:14:05,731 | INFO | Training epoch 1915, Batch 1000/1000: LR=1.45e-06, Loss=3.54e-03 BER=1.48e-03 FER=1.24e-02
2025-10-18 07:14:05,812 | INFO | Epoch 1915 Train Time 90.69412302970886s

2025-10-18 07:15:41,078 | INFO | Training epoch 1916, Batch 1000/1000: LR=1.44e-06, Loss=3.47e-03 BER=1.41e-03 FER=1.20e-02
2025-10-18 07:15:41,126 | INFO | Epoch 1916 Train Time 95.31256937980652s

2025-10-18 07:17:12,375 | INFO | Training epoch 1917, Batch 1000/1000: LR=1.43e-06, Loss=3.61e-03 BER=1.48e-03 FER=1.28e-02
2025-10-18 07:17:12,416 | INFO | Epoch 1917 Train Time 91.28944396972656s

2025-10-18 07:18:44,033 | INFO | Training epoch 1918, Batch 1000/1000: LR=1.42e-06, Loss=3.60e-03 BER=1.48e-03 FER=1.26e-02
2025-10-18 07:18:44,087 | INFO | Epoch 1918 Train Time 91.66936182975769s

2025-10-18 07:20:13,554 | INFO | Training epoch 1919, Batch 1000/1000: LR=1.41e-06, Loss=3.57e-03 BER=1.46e-03 FER=1.21e-02
2025-10-18 07:20:13,633 | INFO | Epoch 1919 Train Time 89.54485702514648s

2025-10-18 07:21:54,293 | INFO | Training epoch 1920, Batch 1000/1000: LR=1.40e-06, Loss=3.39e-03 BER=1.42e-03 FER=1.22e-02
2025-10-18 07:21:54,369 | INFO | Epoch 1920 Train Time 100.7339735031128s

2025-10-18 07:23:32,358 | INFO | Training epoch 1921, Batch 1000/1000: LR=1.39e-06, Loss=3.57e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 07:23:32,410 | INFO | Epoch 1921 Train Time 98.03569030761719s

2025-10-18 07:25:13,587 | INFO | Training epoch 1922, Batch 1000/1000: LR=1.38e-06, Loss=3.59e-03 BER=1.47e-03 FER=1.25e-02
2025-10-18 07:25:13,631 | INFO | Epoch 1922 Train Time 101.21977233886719s

2025-10-18 07:26:50,913 | INFO | Training epoch 1923, Batch 1000/1000: LR=1.37e-06, Loss=3.56e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 07:26:50,975 | INFO | Epoch 1923 Train Time 97.34295463562012s

2025-10-18 07:28:27,838 | INFO | Training epoch 1924, Batch 1000/1000: LR=1.36e-06, Loss=3.80e-03 BER=1.58e-03 FER=1.33e-02
2025-10-18 07:28:27,923 | INFO | Epoch 1924 Train Time 96.94627547264099s

2025-10-18 07:30:10,504 | INFO | Training epoch 1925, Batch 1000/1000: LR=1.35e-06, Loss=3.59e-03 BER=1.47e-03 FER=1.25e-02
2025-10-18 07:30:10,576 | INFO | Epoch 1925 Train Time 102.6523027420044s

2025-10-18 07:31:51,252 | INFO | Training epoch 1926, Batch 1000/1000: LR=1.34e-06, Loss=3.44e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 07:31:51,302 | INFO | Epoch 1926 Train Time 100.72404789924622s

2025-10-18 07:33:25,216 | INFO | Training epoch 1927, Batch 1000/1000: LR=1.33e-06, Loss=3.49e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 07:33:25,274 | INFO | Epoch 1927 Train Time 93.9704258441925s

2025-10-18 07:35:02,844 | INFO | Training epoch 1928, Batch 1000/1000: LR=1.33e-06, Loss=3.45e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 07:35:02,891 | INFO | Epoch 1928 Train Time 97.61582374572754s

2025-10-18 07:36:36,445 | INFO | Training epoch 1929, Batch 1000/1000: LR=1.32e-06, Loss=3.65e-03 BER=1.50e-03 FER=1.28e-02
2025-10-18 07:36:36,490 | INFO | Epoch 1929 Train Time 93.59725761413574s

2025-10-18 07:38:18,155 | INFO | Training epoch 1930, Batch 1000/1000: LR=1.31e-06, Loss=3.39e-03 BER=1.40e-03 FER=1.23e-02
2025-10-18 07:38:18,207 | INFO | Epoch 1930 Train Time 101.71637773513794s

2025-10-18 07:39:48,891 | INFO | Training epoch 1931, Batch 1000/1000: LR=1.30e-06, Loss=3.52e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 07:39:48,957 | INFO | Epoch 1931 Train Time 90.74917936325073s

2025-10-18 07:41:32,520 | INFO | Training epoch 1932, Batch 1000/1000: LR=1.29e-06, Loss=3.51e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 07:41:32,579 | INFO | Epoch 1932 Train Time 103.62030291557312s

2025-10-18 07:43:08,034 | INFO | Training epoch 1933, Batch 1000/1000: LR=1.28e-06, Loss=3.58e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 07:43:08,080 | INFO | Epoch 1933 Train Time 95.49897623062134s

2025-10-18 07:44:46,655 | INFO | Training epoch 1934, Batch 1000/1000: LR=1.27e-06, Loss=3.44e-03 BER=1.41e-03 FER=1.24e-02
2025-10-18 07:44:46,720 | INFO | Epoch 1934 Train Time 98.63924217224121s

2025-10-18 07:46:18,661 | INFO | Training epoch 1935, Batch 1000/1000: LR=1.27e-06, Loss=3.64e-03 BER=1.50e-03 FER=1.27e-02
2025-10-18 07:46:18,730 | INFO | Epoch 1935 Train Time 92.00931096076965s

2025-10-18 07:47:59,345 | INFO | Training epoch 1936, Batch 1000/1000: LR=1.26e-06, Loss=3.53e-03 BER=1.47e-03 FER=1.25e-02
2025-10-18 07:47:59,385 | INFO | Epoch 1936 Train Time 100.6529848575592s

2025-10-18 07:49:33,929 | INFO | Training epoch 1937, Batch 1000/1000: LR=1.25e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.24e-02
2025-10-18 07:49:33,992 | INFO | Epoch 1937 Train Time 94.60582733154297s

2025-10-18 07:51:04,623 | INFO | Training epoch 1938, Batch 1000/1000: LR=1.24e-06, Loss=3.49e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 07:51:04,681 | INFO | Epoch 1938 Train Time 90.68776655197144s

2025-10-18 07:52:35,734 | INFO | Training epoch 1939, Batch 1000/1000: LR=1.23e-06, Loss=3.50e-03 BER=1.43e-03 FER=1.23e-02
2025-10-18 07:52:35,790 | INFO | Epoch 1939 Train Time 91.10640406608582s

2025-10-18 07:54:11,224 | INFO | Training epoch 1940, Batch 1000/1000: LR=1.23e-06, Loss=3.54e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 07:54:11,292 | INFO | Epoch 1940 Train Time 95.49994611740112s

2025-10-18 07:55:56,926 | INFO | Training epoch 1941, Batch 1000/1000: LR=1.22e-06, Loss=3.65e-03 BER=1.52e-03 FER=1.27e-02
2025-10-18 07:55:56,986 | INFO | Epoch 1941 Train Time 105.6922390460968s

2025-10-18 07:57:41,961 | INFO | Training epoch 1942, Batch 1000/1000: LR=1.21e-06, Loss=3.50e-03 BER=1.45e-03 FER=1.23e-02
2025-10-18 07:57:42,035 | INFO | Epoch 1942 Train Time 105.04806447029114s

2025-10-18 07:59:20,912 | INFO | Training epoch 1943, Batch 1000/1000: LR=1.21e-06, Loss=3.74e-03 BER=1.55e-03 FER=1.31e-02
2025-10-18 07:59:20,983 | INFO | Epoch 1943 Train Time 98.94626569747925s

2025-10-18 08:01:04,531 | INFO | Training epoch 1944, Batch 1000/1000: LR=1.20e-06, Loss=3.70e-03 BER=1.52e-03 FER=1.30e-02
2025-10-18 08:01:04,613 | INFO | Epoch 1944 Train Time 103.62708330154419s

2025-10-18 08:02:34,754 | INFO | Training epoch 1945, Batch 1000/1000: LR=1.19e-06, Loss=3.42e-03 BER=1.40e-03 FER=1.17e-02
2025-10-18 08:02:34,797 | INFO | Epoch 1945 Train Time 90.18257236480713s

2025-10-18 08:04:10,914 | INFO | Training epoch 1946, Batch 1000/1000: LR=1.18e-06, Loss=3.55e-03 BER=1.48e-03 FER=1.26e-02
2025-10-18 08:04:10,976 | INFO | Epoch 1946 Train Time 96.17868781089783s

2025-10-18 08:05:52,741 | INFO | Training epoch 1947, Batch 1000/1000: LR=1.18e-06, Loss=3.69e-03 BER=1.53e-03 FER=1.32e-02
2025-10-18 08:05:52,810 | INFO | Epoch 1947 Train Time 101.83275771141052s

2025-10-18 08:07:38,468 | INFO | Training epoch 1948, Batch 1000/1000: LR=1.17e-06, Loss=3.68e-03 BER=1.52e-03 FER=1.30e-02
2025-10-18 08:07:38,548 | INFO | Epoch 1948 Train Time 105.7312753200531s

2025-10-18 08:09:26,766 | INFO | Training epoch 1949, Batch 1000/1000: LR=1.17e-06, Loss=3.60e-03 BER=1.50e-03 FER=1.28e-02
2025-10-18 08:09:26,832 | INFO | Epoch 1949 Train Time 108.28102922439575s

2025-10-18 08:11:02,636 | INFO | Training epoch 1950, Batch 1000/1000: LR=1.16e-06, Loss=3.62e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 08:11:02,686 | INFO | Epoch 1950 Train Time 95.85143089294434s

2025-10-18 08:12:48,537 | INFO | Training epoch 1951, Batch 1000/1000: LR=1.15e-06, Loss=3.50e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 08:12:48,593 | INFO | Epoch 1951 Train Time 105.90639972686768s

2025-10-18 08:14:20,130 | INFO | Training epoch 1952, Batch 1000/1000: LR=1.15e-06, Loss=3.48e-03 BER=1.44e-03 FER=1.19e-02
2025-10-18 08:14:20,194 | INFO | Epoch 1952 Train Time 91.59989500045776s

2025-10-18 08:15:57,250 | INFO | Training epoch 1953, Batch 1000/1000: LR=1.14e-06, Loss=3.53e-03 BER=1.42e-03 FER=1.23e-02
2025-10-18 08:15:57,313 | INFO | Epoch 1953 Train Time 97.11679911613464s

2025-10-18 08:17:34,889 | INFO | Training epoch 1954, Batch 1000/1000: LR=1.13e-06, Loss=3.43e-03 BER=1.39e-03 FER=1.19e-02
2025-10-18 08:17:34,949 | INFO | Epoch 1954 Train Time 97.63474082946777s

2025-10-18 08:19:22,565 | INFO | Training epoch 1955, Batch 1000/1000: LR=1.13e-06, Loss=3.49e-03 BER=1.45e-03 FER=1.22e-02
2025-10-18 08:19:22,631 | INFO | Epoch 1955 Train Time 107.68036937713623s

2025-10-18 08:20:59,562 | INFO | Training epoch 1956, Batch 1000/1000: LR=1.12e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.29e-02
2025-10-18 08:20:59,627 | INFO | Epoch 1956 Train Time 96.99501347541809s

2025-10-18 08:22:35,860 | INFO | Training epoch 1957, Batch 1000/1000: LR=1.12e-06, Loss=3.53e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 08:22:35,919 | INFO | Epoch 1957 Train Time 96.28923606872559s

2025-10-18 08:24:11,828 | INFO | Training epoch 1958, Batch 1000/1000: LR=1.11e-06, Loss=3.57e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 08:24:11,894 | INFO | Epoch 1958 Train Time 95.97380089759827s

2025-10-18 08:25:48,453 | INFO | Training epoch 1959, Batch 1000/1000: LR=1.11e-06, Loss=3.50e-03 BER=1.45e-03 FER=1.24e-02
2025-10-18 08:25:48,508 | INFO | Epoch 1959 Train Time 96.61120843887329s

2025-10-18 08:27:31,082 | INFO | Training epoch 1960, Batch 1000/1000: LR=1.10e-06, Loss=3.51e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 08:27:31,147 | INFO | Epoch 1960 Train Time 102.63679647445679s

2025-10-18 08:29:10,364 | INFO | Training epoch 1961, Batch 1000/1000: LR=1.10e-06, Loss=3.42e-03 BER=1.42e-03 FER=1.21e-02
2025-10-18 08:29:10,426 | INFO | Epoch 1961 Train Time 99.2773609161377s

2025-10-18 08:30:53,140 | INFO | Training epoch 1962, Batch 1000/1000: LR=1.09e-06, Loss=3.58e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 08:30:53,193 | INFO | Epoch 1962 Train Time 102.76629376411438s

2025-10-18 08:32:32,096 | INFO | Training epoch 1963, Batch 1000/1000: LR=1.09e-06, Loss=3.53e-03 BER=1.43e-03 FER=1.21e-02
2025-10-18 08:32:32,174 | INFO | Epoch 1963 Train Time 98.97828197479248s

2025-10-18 08:34:14,041 | INFO | Training epoch 1964, Batch 1000/1000: LR=1.08e-06, Loss=3.45e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 08:34:14,112 | INFO | Epoch 1964 Train Time 101.93670320510864s

2025-10-18 08:35:49,771 | INFO | Training epoch 1965, Batch 1000/1000: LR=1.08e-06, Loss=3.57e-03 BER=1.44e-03 FER=1.23e-02
2025-10-18 08:35:49,830 | INFO | Epoch 1965 Train Time 95.71618795394897s

2025-10-18 08:37:24,741 | INFO | Training epoch 1966, Batch 1000/1000: LR=1.07e-06, Loss=3.53e-03 BER=1.47e-03 FER=1.23e-02
2025-10-18 08:37:24,794 | INFO | Epoch 1966 Train Time 94.96298170089722s

2025-10-18 08:39:01,827 | INFO | Training epoch 1967, Batch 1000/1000: LR=1.07e-06, Loss=3.52e-03 BER=1.46e-03 FER=1.21e-02
2025-10-18 08:39:01,895 | INFO | Epoch 1967 Train Time 97.09799408912659s

2025-10-18 08:40:36,307 | INFO | Training epoch 1968, Batch 1000/1000: LR=1.07e-06, Loss=3.58e-03 BER=1.49e-03 FER=1.28e-02
2025-10-18 08:40:36,384 | INFO | Epoch 1968 Train Time 94.48837089538574s

2025-10-18 08:42:16,052 | INFO | Training epoch 1969, Batch 1000/1000: LR=1.06e-06, Loss=3.46e-03 BER=1.42e-03 FER=1.23e-02
2025-10-18 08:42:16,138 | INFO | Epoch 1969 Train Time 99.75060296058655s

2025-10-18 08:43:58,705 | INFO | Training epoch 1970, Batch 1000/1000: LR=1.06e-06, Loss=3.33e-03 BER=1.37e-03 FER=1.18e-02
2025-10-18 08:43:58,763 | INFO | Epoch 1970 Train Time 102.62252473831177s

2025-10-18 08:45:35,588 | INFO | Training epoch 1971, Batch 1000/1000: LR=1.05e-06, Loss=3.35e-03 BER=1.37e-03 FER=1.19e-02
2025-10-18 08:45:35,661 | INFO | Epoch 1971 Train Time 96.89684271812439s

2025-10-18 08:47:07,036 | INFO | Training epoch 1972, Batch 1000/1000: LR=1.05e-06, Loss=3.43e-03 BER=1.43e-03 FER=1.21e-02
2025-10-18 08:47:07,103 | INFO | Epoch 1972 Train Time 91.43789291381836s

2025-10-18 08:48:44,038 | INFO | Training epoch 1973, Batch 1000/1000: LR=1.05e-06, Loss=3.53e-03 BER=1.44e-03 FER=1.24e-02
2025-10-18 08:48:44,109 | INFO | Epoch 1973 Train Time 97.00444173812866s

2025-10-18 08:50:18,699 | INFO | Training epoch 1974, Batch 1000/1000: LR=1.04e-06, Loss=3.53e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 08:50:18,784 | INFO | Epoch 1974 Train Time 94.67272114753723s

2025-10-18 08:51:54,951 | INFO | Training epoch 1975, Batch 1000/1000: LR=1.04e-06, Loss=3.58e-03 BER=1.48e-03 FER=1.26e-02
2025-10-18 08:51:55,004 | INFO | Epoch 1975 Train Time 96.21843934059143s

2025-10-18 08:53:30,189 | INFO | Training epoch 1976, Batch 1000/1000: LR=1.04e-06, Loss=3.58e-03 BER=1.46e-03 FER=1.27e-02
2025-10-18 08:53:30,252 | INFO | Epoch 1976 Train Time 95.24639177322388s

2025-10-18 08:55:17,356 | INFO | Training epoch 1977, Batch 1000/1000: LR=1.04e-06, Loss=3.51e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 08:55:17,407 | INFO | Epoch 1977 Train Time 107.1527259349823s

2025-10-18 08:56:59,985 | INFO | Training epoch 1978, Batch 1000/1000: LR=1.03e-06, Loss=3.44e-03 BER=1.41e-03 FER=1.20e-02
2025-10-18 08:57:00,028 | INFO | Epoch 1978 Train Time 102.62027168273926s

2025-10-18 08:58:45,621 | INFO | Training epoch 1979, Batch 1000/1000: LR=1.03e-06, Loss=3.40e-03 BER=1.38e-03 FER=1.18e-02
2025-10-18 08:58:45,690 | INFO | Epoch 1979 Train Time 105.66069626808167s

2025-10-18 09:00:28,720 | INFO | Training epoch 1980, Batch 1000/1000: LR=1.03e-06, Loss=3.60e-03 BER=1.47e-03 FER=1.27e-02
2025-10-18 09:00:28,782 | INFO | Epoch 1980 Train Time 103.08935809135437s

2025-10-18 09:02:18,918 | INFO | Training epoch 1981, Batch 1000/1000: LR=1.02e-06, Loss=3.53e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 09:02:19,012 | INFO | Epoch 1981 Train Time 110.22562718391418s

2025-10-18 09:04:00,790 | INFO | Training epoch 1982, Batch 1000/1000: LR=1.02e-06, Loss=3.62e-03 BER=1.52e-03 FER=1.29e-02
2025-10-18 09:04:00,839 | INFO | Epoch 1982 Train Time 101.82431626319885s

2025-10-18 09:05:47,728 | INFO | Training epoch 1983, Batch 1000/1000: LR=1.02e-06, Loss=3.50e-03 BER=1.46e-03 FER=1.23e-02
2025-10-18 09:05:47,781 | INFO | Epoch 1983 Train Time 106.94036078453064s

2025-10-18 09:07:24,443 | INFO | Training epoch 1984, Batch 1000/1000: LR=1.02e-06, Loss=3.66e-03 BER=1.49e-03 FER=1.27e-02
2025-10-18 09:07:24,503 | INFO | Epoch 1984 Train Time 96.72081637382507s

2025-10-18 09:09:03,831 | INFO | Training epoch 1985, Batch 1000/1000: LR=1.02e-06, Loss=3.56e-03 BER=1.46e-03 FER=1.24e-02
2025-10-18 09:09:03,893 | INFO | Epoch 1985 Train Time 99.38849902153015s

2025-10-18 09:10:41,297 | INFO | Training epoch 1986, Batch 1000/1000: LR=1.01e-06, Loss=3.52e-03 BER=1.45e-03 FER=1.25e-02
2025-10-18 09:10:41,348 | INFO | Epoch 1986 Train Time 97.45382523536682s

2025-10-18 09:12:23,445 | INFO | Training epoch 1987, Batch 1000/1000: LR=1.01e-06, Loss=3.49e-03 BER=1.43e-03 FER=1.22e-02
2025-10-18 09:12:23,484 | INFO | Epoch 1987 Train Time 102.13492107391357s

2025-10-18 09:14:02,834 | INFO | Training epoch 1988, Batch 1000/1000: LR=1.01e-06, Loss=3.47e-03 BER=1.40e-03 FER=1.22e-02
2025-10-18 09:14:02,894 | INFO | Epoch 1988 Train Time 99.40858793258667s

2025-10-18 09:15:40,561 | INFO | Training epoch 1989, Batch 1000/1000: LR=1.01e-06, Loss=3.63e-03 BER=1.48e-03 FER=1.27e-02
2025-10-18 09:15:40,631 | INFO | Epoch 1989 Train Time 97.73460507392883s

2025-10-18 09:17:16,965 | INFO | Training epoch 1990, Batch 1000/1000: LR=1.01e-06, Loss=3.42e-03 BER=1.41e-03 FER=1.20e-02
2025-10-18 09:17:17,008 | INFO | Epoch 1990 Train Time 96.37530469894409s

2025-10-18 09:18:50,726 | INFO | Training epoch 1991, Batch 1000/1000: LR=1.01e-06, Loss=3.52e-03 BER=1.44e-03 FER=1.22e-02
2025-10-18 09:18:50,780 | INFO | Epoch 1991 Train Time 93.77076554298401s

2025-10-18 09:20:20,850 | INFO | Training epoch 1992, Batch 1000/1000: LR=1.00e-06, Loss=3.46e-03 BER=1.43e-03 FER=1.23e-02
2025-10-18 09:20:20,928 | INFO | Epoch 1992 Train Time 90.1470594406128s

2025-10-18 09:21:49,773 | INFO | Training epoch 1993, Batch 1000/1000: LR=1.00e-06, Loss=3.51e-03 BER=1.47e-03 FER=1.24e-02
2025-10-18 09:21:49,841 | INFO | Epoch 1993 Train Time 88.91095781326294s

2025-10-18 09:23:22,937 | INFO | Training epoch 1994, Batch 1000/1000: LR=1.00e-06, Loss=3.36e-03 BER=1.38e-03 FER=1.18e-02
2025-10-18 09:23:23,001 | INFO | Epoch 1994 Train Time 93.15929388999939s

2025-10-18 09:24:59,251 | INFO | Training epoch 1995, Batch 1000/1000: LR=1.00e-06, Loss=3.42e-03 BER=1.41e-03 FER=1.21e-02
2025-10-18 09:24:59,294 | INFO | Epoch 1995 Train Time 96.2909688949585s

2025-10-18 09:26:28,601 | INFO | Training epoch 1996, Batch 1000/1000: LR=1.00e-06, Loss=3.41e-03 BER=1.40e-03 FER=1.21e-02
2025-10-18 09:26:28,661 | INFO | Epoch 1996 Train Time 89.3653154373169s

2025-10-18 09:28:03,115 | INFO | Training epoch 1997, Batch 1000/1000: LR=1.00e-06, Loss=3.64e-03 BER=1.49e-03 FER=1.26e-02
2025-10-18 09:28:03,183 | INFO | Epoch 1997 Train Time 94.52104544639587s

2025-10-18 09:29:40,138 | INFO | Training epoch 1998, Batch 1000/1000: LR=1.00e-06, Loss=3.59e-03 BER=1.46e-03 FER=1.25e-02
2025-10-18 09:29:40,213 | INFO | Epoch 1998 Train Time 97.02775144577026s

2025-10-18 09:31:19,694 | INFO | Training epoch 1999, Batch 1000/1000: LR=1.00e-06, Loss=3.39e-03 BER=1.39e-03 FER=1.19e-02
2025-10-18 09:31:19,766 | INFO | Epoch 1999 Train Time 99.55169296264648s

2025-10-18 09:32:58,639 | INFO | Training epoch 2000, Batch 1000/1000: LR=1.00e-06, Loss=3.59e-03 BER=1.48e-03 FER=1.24e-02
2025-10-18 09:32:58,694 | INFO | Epoch 2000 Train Time 98.92505979537964s

2025-10-18 09:32:58,725 | INFO | Checkpoint saved: runs/20251015_153247/stage2_qat__LDPC_n49_k24__Ndec10_d128_h8.pth
2025-10-18 09:32:58,747 | INFO | Checkpoint saved: runs/20251015_153247/stage2_qat__LDPC_n49_k24__Ndec10_d128_h8__e2000_loss0.003586.pth
2025-10-18 09:32:59,039 | INFO | Loaded checkpoint: runs/20251015_153247/stage2_qat__LDPC_n49_k24__Ndec10_d128_h8.pth (strict=False)
2025-10-18 09:32:59,069 | INFO | Checkpoint saved: runs/20251015_153247/stage2_infer_frozen__LDPC_n49_k24__Ndec10_d128_h8__e1844_loss0.003326.pth
2025-10-18 09:33:06,495 | INFO | FER count threshold reached for EbN0:4
2025-10-18 09:33:06,576 | INFO | Test EbN0=4, BER=1.24e-03
2025-10-18 09:33:19,679 | INFO | FER count threshold reached for EbN0:5
2025-10-18 09:33:19,762 | INFO | Test EbN0=5, BER=6.32e-05
2025-10-18 09:42:31,987 | INFO | FER count threshold reached for EbN0:6
2025-10-18 09:42:32,090 | INFO | Test EbN0=6, BER=1.38e-06
2025-10-18 09:42:32,090 | INFO | 
Test Loss 4: 3.0250e-03 5: 1.6091e-04 6: 4.5223e-06
2025-10-18 09:42:32,090 | INFO | Test FER 4: 1.0782e-02 5: 6.1073e-04 6: 1.6101e-05
2025-10-18 09:42:32,090 | INFO | Test BER 4: 1.2391e-03 5: 6.3183e-05 6: 1.3762e-06
2025-10-18 09:42:32,090 | INFO | Test -ln(BER) 4: 6.6934e+00 5: 9.6695e+00 6: 1.3496e+01
2025-10-18 09:42:32,090 | INFO | # of testing samples: [100352.0, 165376.0, 6273024.0]
 Test Time 573.0212354660034 s

2025-10-18 09:42:32,102 | INFO | Done.
